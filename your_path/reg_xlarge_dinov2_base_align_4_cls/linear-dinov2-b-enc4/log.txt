[[34m2025-10-03 22:37:54[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 22:39:31[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 22:39:32[0m] using MLP layer as FFN
[[34m2025-10-03 22:41:10[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 22:41:11[0m] using MLP layer as FFN
[[34m2025-10-03 22:42:02[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 22:42:03[0m] using MLP layer as FFN
[[34m2025-10-03 22:42:07[0m] SiT Parameters: 140,185,216
[[34m2025-10-03 22:43:44[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 22:43:44[0m] using MLP layer as FFN
[[34m2025-10-03 22:43:49[0m] SiT Parameters: 140,185,216
[[34m2025-10-03 22:44:08[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-03 23:00:45[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 23:00:46[0m] using MLP layer as FFN
[[34m2025-10-03 23:00:50[0m] SiT Parameters: 140,221,984
[[34m2025-10-03 23:01:09[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-03 23:01:31[0m] Generating EMA samples done.
[[34m2025-10-03 23:01:31[0m] Step: 1, Training Logs: loss_final: 1.902024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.100000, grad_norm: 2.244669
[[34m2025-10-03 23:01:32[0m] Step: 2, Training Logs: loss_final: 1.856680, loss_mean: 1.682286, proj_loss: -0.027754, loss_mean_cls: 0.117975, deep_loss: 0.084173, grad_norm: 2.136117
[[34m2025-10-03 23:01:33[0m] Step: 3, Training Logs: loss_final: 1.840682, loss_mean: 1.709359, proj_loss: -0.049180, loss_mean_cls: 0.118187, deep_loss: 0.062316, grad_norm: 2.315531
[[34m2025-10-03 23:01:34[0m] Step: 4, Training Logs: loss_final: 1.783689, loss_mean: 1.679668, proj_loss: -0.064759, loss_mean_cls: 0.117801, deep_loss: 0.050980, grad_norm: 1.962716
[[34m2025-10-03 23:01:34[0m] Step: 5, Training Logs: loss_final: 1.763214, loss_mean: 1.685981, proj_loss: -0.075713, loss_mean_cls: 0.118604, deep_loss: 0.034343, grad_norm: 2.220485
[[34m2025-10-03 23:01:35[0m] Step: 6, Training Logs: loss_final: 1.722666, loss_mean: 1.667782, proj_loss: -0.081707, loss_mean_cls: 0.117982, deep_loss: 0.018610, grad_norm: 2.067180
[[34m2025-10-03 23:01:35[0m] Step: 7, Training Logs: loss_final: 1.690045, loss_mean: 1.659082, proj_loss: -0.088526, loss_mean_cls: 0.117600, deep_loss: 0.001889, grad_norm: 2.063352
[[34m2025-10-03 23:01:36[0m] Step: 8, Training Logs: loss_final: 1.653644, loss_mean: 1.657113, proj_loss: -0.092953, loss_mean_cls: 0.117818, deep_loss: -0.028334, grad_norm: 2.234945
[[34m2025-10-03 23:01:36[0m] Step: 9, Training Logs: loss_final: 1.627937, loss_mean: 1.643938, proj_loss: -0.097129, loss_mean_cls: 0.118039, deep_loss: -0.036911, grad_norm: 2.129354
[[34m2025-10-03 23:01:36[0m] Step: 10, Training Logs: loss_final: 1.569122, loss_mean: 1.643963, proj_loss: -0.097844, loss_mean_cls: 0.117759, deep_loss: -0.094756, grad_norm: 2.621488
[[34m2025-10-03 23:01:37[0m] Step: 11, Training Logs: loss_final: 1.551509, loss_mean: 1.615961, proj_loss: -0.102589, loss_mean_cls: 0.117684, deep_loss: -0.079547, grad_norm: 2.401214
[[34m2025-10-03 23:01:37[0m] Step: 12, Training Logs: loss_final: 1.489146, loss_mean: 1.607632, proj_loss: -0.104662, loss_mean_cls: 0.117591, deep_loss: -0.131415, grad_norm: 2.854919
[[34m2025-10-03 23:01:38[0m] Step: 13, Training Logs: loss_final: 1.425974, loss_mean: 1.576676, proj_loss: -0.108930, loss_mean_cls: 0.117403, deep_loss: -0.159176, grad_norm: 2.884147
[[34m2025-10-03 23:01:38[0m] Step: 14, Training Logs: loss_final: 1.339334, loss_mean: 1.582880, proj_loss: -0.112828, loss_mean_cls: 0.117240, deep_loss: -0.247959, grad_norm: 3.489061
[[34m2025-10-03 23:01:38[0m] Step: 15, Training Logs: loss_final: 1.273827, loss_mean: 1.582203, proj_loss: -0.113961, loss_mean_cls: 0.117403, deep_loss: -0.311817, grad_norm: 4.093540
[[34m2025-10-03 23:01:39[0m] Step: 16, Training Logs: loss_final: 1.215870, loss_mean: 1.599933, proj_loss: -0.115602, loss_mean_cls: 0.116836, deep_loss: -0.385296, grad_norm: 4.942963
[[34m2025-10-03 23:01:40[0m] Step: 17, Training Logs: loss_final: 1.229781, loss_mean: 1.621525, proj_loss: -0.119935, loss_mean_cls: 0.117474, deep_loss: -0.389283, grad_norm: 4.927518
[[34m2025-10-03 23:01:40[0m] Step: 18, Training Logs: loss_final: 1.143746, loss_mean: 1.591568, proj_loss: -0.120350, loss_mean_cls: 0.116792, deep_loss: -0.444264, grad_norm: 5.237954
[[34m2025-10-03 23:01:41[0m] Step: 19, Training Logs: loss_final: 1.073285, loss_mean: 1.605798, proj_loss: -0.124584, loss_mean_cls: 0.117283, deep_loss: -0.525212, grad_norm: 5.269878
[[34m2025-10-03 23:01:42[0m] Step: 20, Training Logs: loss_final: 0.957967, loss_mean: 1.592136, proj_loss: -0.124034, loss_mean_cls: 0.117202, deep_loss: -0.627336, grad_norm: 5.507720
[[34m2025-10-03 23:01:43[0m] Step: 21, Training Logs: loss_final: 0.909962, loss_mean: 1.560460, proj_loss: -0.124168, loss_mean_cls: 0.116730, deep_loss: -0.643060, grad_norm: 5.024128
[[34m2025-10-03 23:01:44[0m] Step: 22, Training Logs: loss_final: 0.850568, loss_mean: 1.557044, proj_loss: -0.129652, loss_mean_cls: 0.116603, deep_loss: -0.693427, grad_norm: 5.826669
[[34m2025-10-03 23:01:45[0m] Step: 23, Training Logs: loss_final: 0.772621, loss_mean: 1.557169, proj_loss: -0.129002, loss_mean_cls: 0.117429, deep_loss: -0.772975, grad_norm: 6.096902
[[34m2025-10-03 23:01:45[0m] Step: 24, Training Logs: loss_final: 0.740794, loss_mean: 1.520295, proj_loss: -0.131591, loss_mean_cls: 0.116628, deep_loss: -0.764538, grad_norm: 5.220123
[[34m2025-10-03 23:01:46[0m] Step: 25, Training Logs: loss_final: 0.457764, loss_mean: 1.535045, proj_loss: -0.133341, loss_mean_cls: 0.116045, deep_loss: -1.059985, grad_norm: 6.427812
[[34m2025-10-03 23:01:47[0m] Step: 26, Training Logs: loss_final: 0.371241, loss_mean: 1.497625, proj_loss: -0.132093, loss_mean_cls: 0.116780, deep_loss: -1.111071, grad_norm: 6.559532
[[34m2025-10-03 23:01:48[0m] Step: 27, Training Logs: loss_final: 0.103613, loss_mean: 1.513916, proj_loss: -0.133318, loss_mean_cls: 0.116238, deep_loss: -1.393223, grad_norm: 8.493073
[[34m2025-10-03 23:01:49[0m] Step: 28, Training Logs: loss_final: 0.189452, loss_mean: 1.486314, proj_loss: -0.134926, loss_mean_cls: 0.116649, deep_loss: -1.278585, grad_norm: 6.925495
[[34m2025-10-03 23:01:50[0m] Step: 29, Training Logs: loss_final: -0.175449, loss_mean: 1.469849, proj_loss: -0.133835, loss_mean_cls: 0.117134, deep_loss: -1.628597, grad_norm: 8.112772
[[34m2025-10-03 23:01:50[0m] Step: 30, Training Logs: loss_final: -0.339026, loss_mean: 1.469424, proj_loss: -0.135098, loss_mean_cls: 0.116197, deep_loss: -1.789549, grad_norm: 8.661064
[[34m2025-10-03 23:01:51[0m] Step: 31, Training Logs: loss_final: -0.443910, loss_mean: 1.461831, proj_loss: -0.136612, loss_mean_cls: 0.115881, deep_loss: -1.885010, grad_norm: 8.821856
[[34m2025-10-03 23:01:52[0m] Step: 32, Training Logs: loss_final: -0.662084, loss_mean: 1.465278, proj_loss: -0.135893, loss_mean_cls: 0.115446, deep_loss: -2.106915, grad_norm: 9.229658
[[34m2025-10-03 23:01:53[0m] Step: 33, Training Logs: loss_final: -0.978833, loss_mean: 1.456379, proj_loss: -0.136244, loss_mean_cls: 0.115949, deep_loss: -2.414917, grad_norm: 10.358461
[[34m2025-10-03 23:01:54[0m] Step: 34, Training Logs: loss_final: -0.828078, loss_mean: 1.461073, proj_loss: -0.132752, loss_mean_cls: 0.115824, deep_loss: -2.272222, grad_norm: 12.474362
[[34m2025-10-03 23:01:55[0m] Step: 35, Training Logs: loss_final: -1.235458, loss_mean: 1.458996, proj_loss: -0.135662, loss_mean_cls: 0.116162, deep_loss: -2.674954, grad_norm: 13.137460
[[34m2025-10-03 23:01:55[0m] Step: 36, Training Logs: loss_final: -1.404910, loss_mean: 1.455003, proj_loss: -0.133539, loss_mean_cls: 0.116050, deep_loss: -2.842424, grad_norm: 11.276482
[[34m2025-10-03 23:01:56[0m] Step: 37, Training Logs: loss_final: -1.956345, loss_mean: 1.424919, proj_loss: -0.133612, loss_mean_cls: 0.115669, deep_loss: -3.363321, grad_norm: 16.588934
[[34m2025-10-03 23:01:57[0m] Step: 38, Training Logs: loss_final: -2.520384, loss_mean: 1.476199, proj_loss: -0.131730, loss_mean_cls: 0.116642, deep_loss: -3.981495, grad_norm: 14.823775
[[34m2025-10-03 23:01:58[0m] Step: 39, Training Logs: loss_final: -2.843419, loss_mean: 1.425584, proj_loss: -0.131695, loss_mean_cls: 0.115384, deep_loss: -4.252691, grad_norm: 14.964985
[[34m2025-10-03 23:01:59[0m] Step: 40, Training Logs: loss_final: -3.404949, loss_mean: 1.413534, proj_loss: -0.131897, loss_mean_cls: 0.116740, deep_loss: -4.803326, grad_norm: 18.919209
[[34m2025-10-03 23:02:00[0m] Step: 41, Training Logs: loss_final: -3.942715, loss_mean: 1.424108, proj_loss: -0.131709, loss_mean_cls: 0.116371, deep_loss: -5.351485, grad_norm: 21.656691
[[34m2025-10-03 23:02:00[0m] Step: 42, Training Logs: loss_final: -4.682734, loss_mean: 1.411959, proj_loss: -0.131159, loss_mean_cls: 0.115911, deep_loss: -6.079445, grad_norm: 24.074656
[[34m2025-10-03 23:02:01[0m] Step: 43, Training Logs: loss_final: -5.061109, loss_mean: 1.412103, proj_loss: -0.131759, loss_mean_cls: 0.116398, deep_loss: -6.457850, grad_norm: 23.334387
[[34m2025-10-03 23:02:01[0m] Step: 44, Training Logs: loss_final: -5.728480, loss_mean: 1.422871, proj_loss: -0.128511, loss_mean_cls: 0.116176, deep_loss: -7.139016, grad_norm: 42.083889
[[34m2025-10-03 23:02:02[0m] Step: 45, Training Logs: loss_final: -5.823272, loss_mean: 1.454096, proj_loss: -0.128528, loss_mean_cls: 0.116891, deep_loss: -7.265731, grad_norm: 35.264397
[[34m2025-10-03 23:02:02[0m] Step: 46, Training Logs: loss_final: -7.500225, loss_mean: 1.459615, proj_loss: -0.129032, loss_mean_cls: 0.116489, deep_loss: -8.947297, grad_norm: 27.920452
[[34m2025-10-03 23:02:03[0m] Step: 47, Training Logs: loss_final: -7.835441, loss_mean: 1.427608, proj_loss: -0.124749, loss_mean_cls: 0.116121, deep_loss: -9.254421, grad_norm: 37.047337
[[34m2025-10-03 23:02:03[0m] Step: 48, Training Logs: loss_final: -8.384978, loss_mean: 1.443326, proj_loss: -0.125377, loss_mean_cls: 0.116306, deep_loss: -9.819234, grad_norm: 55.603733
[[34m2025-10-03 23:02:03[0m] Step: 49, Training Logs: loss_final: -8.333402, loss_mean: 1.526670, proj_loss: -0.123791, loss_mean_cls: 0.117167, deep_loss: -9.853449, grad_norm: 54.087547
[[34m2025-10-03 23:02:04[0m] Step: 50, Training Logs: loss_final: -9.577744, loss_mean: 1.448665, proj_loss: -0.122698, loss_mean_cls: 0.117076, deep_loss: -11.020786, grad_norm: 37.205338
[[34m2025-10-03 23:02:04[0m] Step: 51, Training Logs: loss_final: -12.023198, loss_mean: 1.435998, proj_loss: -0.120261, loss_mean_cls: 0.116777, deep_loss: -13.455712, grad_norm: 51.071648
[[34m2025-10-03 23:02:05[0m] Step: 52, Training Logs: loss_final: -8.394291, loss_mean: 1.449264, proj_loss: -0.118160, loss_mean_cls: 0.116420, deep_loss: -9.841815, grad_norm: inf
[[34m2025-10-03 23:02:05[0m] Step: 53, Training Logs: loss_final: -9.829987, loss_mean: 1.468933, proj_loss: -0.115709, loss_mean_cls: 0.116451, deep_loss: -11.299662, grad_norm: 101.540405
[[34m2025-10-03 23:02:05[0m] Step: 54, Training Logs: loss_final: -13.832787, loss_mean: 1.504797, proj_loss: -0.117043, loss_mean_cls: 0.116945, deep_loss: -15.337486, grad_norm: 74.159843
[[34m2025-10-03 23:02:06[0m] Step: 55, Training Logs: loss_final: -8.968054, loss_mean: 1.576253, proj_loss: -0.113173, loss_mean_cls: 0.117256, deep_loss: -10.548389, grad_norm: 102.003204
[[34m2025-10-03 23:02:06[0m] Step: 56, Training Logs: loss_final: -11.787377, loss_mean: 1.530782, proj_loss: -0.109983, loss_mean_cls: 0.116848, deep_loss: -13.325025, grad_norm: 91.000229
[[34m2025-10-03 23:02:07[0m] Step: 57, Training Logs: loss_final: -14.218157, loss_mean: 1.528405, proj_loss: -0.107057, loss_mean_cls: 0.117283, deep_loss: -15.756788, grad_norm: inf
[[34m2025-10-03 23:02:08[0m] Step: 58, Training Logs: loss_final: -12.980133, loss_mean: 1.513638, proj_loss: -0.106327, loss_mean_cls: 0.116987, deep_loss: -14.504431, grad_norm: 136.480011
[[34m2025-10-03 23:02:09[0m] Step: 59, Training Logs: loss_final: -13.199811, loss_mean: 1.494940, proj_loss: -0.103768, loss_mean_cls: 0.117154, deep_loss: -14.708138, grad_norm: 107.466263
[[34m2025-10-03 23:02:10[0m] Step: 60, Training Logs: loss_final: -15.123642, loss_mean: 1.529994, proj_loss: -0.104951, loss_mean_cls: 0.116283, deep_loss: -16.664968, grad_norm: 107.000191
[[34m2025-10-03 23:02:10[0m] Step: 61, Training Logs: loss_final: -18.104395, loss_mean: 1.476430, proj_loss: -0.107433, loss_mean_cls: 0.117463, deep_loss: -19.590855, grad_norm: 99.014305
[[34m2025-10-03 23:02:11[0m] Step: 62, Training Logs: loss_final: -19.170958, loss_mean: 1.525627, proj_loss: -0.106232, loss_mean_cls: 0.116634, deep_loss: -20.706987, grad_norm: 137.645020
[[34m2025-10-03 23:02:12[0m] Step: 63, Training Logs: loss_final: -20.735252, loss_mean: 1.539729, proj_loss: -0.106258, loss_mean_cls: 0.117994, deep_loss: -22.286716, grad_norm: 83.361443
[[34m2025-10-03 23:02:13[0m] Step: 64, Training Logs: loss_final: -21.151440, loss_mean: 1.503431, proj_loss: -0.102073, loss_mean_cls: 0.117485, deep_loss: -22.670282, grad_norm: 149.230270
[[34m2025-10-03 23:02:14[0m] Step: 65, Training Logs: loss_final: -24.749664, loss_mean: 1.543199, proj_loss: -0.101078, loss_mean_cls: 0.116353, deep_loss: -26.308140, grad_norm: 78.052864
[[34m2025-10-03 23:02:15[0m] Step: 66, Training Logs: loss_final: -20.970932, loss_mean: 1.575056, proj_loss: -0.095910, loss_mean_cls: 0.117213, deep_loss: -22.567289, grad_norm: inf
[[34m2025-10-03 23:02:15[0m] Step: 67, Training Logs: loss_final: -18.509781, loss_mean: 1.564458, proj_loss: -0.097436, loss_mean_cls: 0.117486, deep_loss: -20.094292, grad_norm: 259.330597
[[34m2025-10-03 23:02:16[0m] Step: 68, Training Logs: loss_final: -24.606884, loss_mean: 1.551721, proj_loss: -0.099404, loss_mean_cls: 0.116499, deep_loss: -26.175699, grad_norm: 264.959595
[[34m2025-10-03 23:02:17[0m] Step: 69, Training Logs: loss_final: -22.277512, loss_mean: 1.629023, proj_loss: -0.099493, loss_mean_cls: 0.117066, deep_loss: -23.924109, grad_norm: 225.403488
[[34m2025-10-03 23:02:18[0m] Step: 70, Training Logs: loss_final: -28.977833, loss_mean: 1.596906, proj_loss: -0.099369, loss_mean_cls: 0.116758, deep_loss: -30.592129, grad_norm: 185.295853
[[34m2025-10-03 23:02:19[0m] Step: 71, Training Logs: loss_final: -29.485954, loss_mean: 1.643960, proj_loss: -0.090911, loss_mean_cls: 0.117270, deep_loss: -31.156275, grad_norm: 254.045731
[[34m2025-10-03 23:02:20[0m] Step: 72, Training Logs: loss_final: -26.626261, loss_mean: 1.602232, proj_loss: -0.088874, loss_mean_cls: 0.116975, deep_loss: -28.256594, grad_norm: 252.484863
[[34m2025-10-03 23:02:20[0m] Step: 73, Training Logs: loss_final: -32.306709, loss_mean: 1.579233, proj_loss: -0.091325, loss_mean_cls: 0.117175, deep_loss: -33.911793, grad_norm: 139.965393
[[34m2025-10-03 23:02:21[0m] Step: 74, Training Logs: loss_final: -32.312252, loss_mean: 1.571196, proj_loss: -0.092556, loss_mean_cls: 0.116898, deep_loss: -33.907791, grad_norm: inf
[[34m2025-10-03 23:02:22[0m] Step: 75, Training Logs: loss_final: -31.389591, loss_mean: 1.561408, proj_loss: -0.091586, loss_mean_cls: 0.117735, deep_loss: -32.977150, grad_norm: 300.159698
[[34m2025-10-03 23:02:23[0m] Step: 76, Training Logs: loss_final: -37.379032, loss_mean: 1.635862, proj_loss: -0.089359, loss_mean_cls: 0.117370, deep_loss: -39.042908, grad_norm: 155.063797
[[34m2025-10-03 23:02:24[0m] Step: 77, Training Logs: loss_final: -32.726700, loss_mean: 1.531972, proj_loss: -0.087549, loss_mean_cls: 0.117305, deep_loss: -34.288429, grad_norm: 485.009094
[[34m2025-10-03 23:02:25[0m] Step: 78, Training Logs: loss_final: -38.100403, loss_mean: 1.550760, proj_loss: -0.090568, loss_mean_cls: 0.116988, deep_loss: -39.677582, grad_norm: 371.308533
[[34m2025-10-03 23:02:25[0m] Step: 79, Training Logs: loss_final: -39.555485, loss_mean: 1.595820, proj_loss: -0.086303, loss_mean_cls: 0.117327, deep_loss: -41.182327, grad_norm: 249.078247
[[34m2025-10-03 23:02:26[0m] Step: 80, Training Logs: loss_final: -44.650818, loss_mean: 1.580307, proj_loss: -0.085451, loss_mean_cls: 0.117552, deep_loss: -46.263229, grad_norm: 314.440979
[[34m2025-10-03 23:02:27[0m] Step: 81, Training Logs: loss_final: -40.522308, loss_mean: 1.564214, proj_loss: -0.078681, loss_mean_cls: 0.117183, deep_loss: -42.125023, grad_norm: 340.049835
[[34m2025-10-03 23:02:28[0m] Step: 82, Training Logs: loss_final: -44.946709, loss_mean: 1.554830, proj_loss: -0.080700, loss_mean_cls: 0.117676, deep_loss: -46.538513, grad_norm: 319.443542
[[34m2025-10-03 23:02:29[0m] Step: 83, Training Logs: loss_final: -48.617683, loss_mean: 1.559988, proj_loss: -0.086362, loss_mean_cls: 0.117367, deep_loss: -50.208675, grad_norm: 290.866058
[[34m2025-10-03 23:02:29[0m] Step: 84, Training Logs: loss_final: -47.075386, loss_mean: 1.550771, proj_loss: -0.086647, loss_mean_cls: 0.117177, deep_loss: -48.656689, grad_norm: inf
[[34m2025-10-03 23:02:29[0m] Step: 85, Training Logs: loss_final: -45.676689, loss_mean: 1.539610, proj_loss: -0.086376, loss_mean_cls: 0.116891, deep_loss: -47.246819, grad_norm: 445.104126
[[34m2025-10-03 23:02:30[0m] Step: 86, Training Logs: loss_final: -50.299828, loss_mean: 1.565466, proj_loss: -0.087323, loss_mean_cls: 0.118013, deep_loss: -51.895981, grad_norm: 257.086090
[[34m2025-10-03 23:02:30[0m] Step: 87, Training Logs: loss_final: -62.495090, loss_mean: 1.564172, proj_loss: -0.088835, loss_mean_cls: 0.117681, deep_loss: -64.088104, grad_norm: 410.823608
[[34m2025-10-03 23:02:30[0m] Step: 88, Training Logs: loss_final: -58.226662, loss_mean: 1.538456, proj_loss: -0.089217, loss_mean_cls: 0.117075, deep_loss: -59.792976, grad_norm: 482.490784
[[34m2025-10-03 23:02:31[0m] Step: 89, Training Logs: loss_final: -63.944939, loss_mean: 1.535378, proj_loss: -0.087410, loss_mean_cls: 0.117297, deep_loss: -65.510208, grad_norm: 172.814911
[[34m2025-10-03 23:02:32[0m] Step: 90, Training Logs: loss_final: -61.067154, loss_mean: 1.533634, proj_loss: -0.087602, loss_mean_cls: 0.117411, deep_loss: -62.630600, grad_norm: 686.947754
[[34m2025-10-03 23:02:33[0m] Step: 91, Training Logs: loss_final: -60.686508, loss_mean: 1.548731, proj_loss: -0.090861, loss_mean_cls: 0.117964, deep_loss: -62.262344, grad_norm: 314.793152
[[34m2025-10-03 23:02:34[0m] Step: 92, Training Logs: loss_final: -59.452843, loss_mean: 1.579259, proj_loss: -0.093058, loss_mean_cls: 0.117706, deep_loss: -61.056751, grad_norm: 399.667938
[[34m2025-10-03 23:02:34[0m] Step: 93, Training Logs: loss_final: -69.403854, loss_mean: 1.589525, proj_loss: -0.091718, loss_mean_cls: 0.116262, deep_loss: -71.017929, grad_norm: 441.424896
[[34m2025-10-03 23:02:35[0m] Step: 94, Training Logs: loss_final: -67.494202, loss_mean: 1.558669, proj_loss: -0.091845, loss_mean_cls: 0.117651, deep_loss: -69.078674, grad_norm: 705.781982
[[34m2025-10-03 23:02:36[0m] Step: 95, Training Logs: loss_final: -80.691200, loss_mean: 1.546861, proj_loss: -0.091508, loss_mean_cls: 0.117388, deep_loss: -82.263947, grad_norm: 322.656952
[[34m2025-10-03 23:02:37[0m] Step: 96, Training Logs: loss_final: -63.655151, loss_mean: 1.598493, proj_loss: -0.091059, loss_mean_cls: 0.117487, deep_loss: -65.280075, grad_norm: 712.827454
[[34m2025-10-03 23:02:38[0m] Step: 97, Training Logs: loss_final: -77.243210, loss_mean: 1.582553, proj_loss: -0.087123, loss_mean_cls: 0.117742, deep_loss: -78.856384, grad_norm: 639.875916
[[34m2025-10-03 23:02:39[0m] Step: 98, Training Logs: loss_final: -66.045883, loss_mean: 1.574432, proj_loss: -0.079473, loss_mean_cls: 0.117843, deep_loss: -67.658684, grad_norm: inf
[[34m2025-10-03 23:02:39[0m] Step: 99, Training Logs: loss_final: -72.592850, loss_mean: 1.571308, proj_loss: -0.080811, loss_mean_cls: 0.117746, deep_loss: -74.201096, grad_norm: 667.280212
[[34m2025-10-03 23:02:40[0m] Step: 100, Training Logs: loss_final: -75.337433, loss_mean: 1.580769, proj_loss: -0.083591, loss_mean_cls: 0.117453, deep_loss: -76.952072, grad_norm: 704.941895
[[34m2025-10-03 23:02:41[0m] Step: 101, Training Logs: loss_final: -84.363495, loss_mean: 1.592576, proj_loss: -0.088756, loss_mean_cls: 0.116899, deep_loss: -85.984215, grad_norm: 790.012634
[[34m2025-10-03 23:02:42[0m] Step: 102, Training Logs: loss_final: -89.899673, loss_mean: 1.578046, proj_loss: -0.091515, loss_mean_cls: 0.116964, deep_loss: -91.503166, grad_norm: 494.134644
[[34m2025-10-03 23:02:43[0m] Step: 103, Training Logs: loss_final: -88.227180, loss_mean: 1.580608, proj_loss: -0.092692, loss_mean_cls: 0.117871, deep_loss: -89.832962, grad_norm: 961.826172
[[34m2025-10-03 23:04:04[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 23:04:05[0m] using MLP layer as FFN
[[34m2025-10-03 23:04:09[0m] SiT Parameters: 140,221,984
[[34m2025-10-03 23:04:28[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-03 23:04:48[0m] Generating EMA samples done.
[[34m2025-10-03 23:04:48[0m] Step: 1, Training Logs: loss_final: 1.902024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.100000, grad_norm: 1.201294
[[34m2025-10-03 23:04:50[0m] Step: 2, Training Logs: loss_final: 1.872003, loss_mean: 1.682286, proj_loss: -0.027763, loss_mean_cls: 0.117974, deep_loss: 0.099505, grad_norm: 0.926803
[[34m2025-10-03 23:04:51[0m] Step: 3, Training Logs: loss_final: 1.877029, loss_mean: 1.709284, proj_loss: -0.049267, loss_mean_cls: 0.118187, deep_loss: 0.098825, grad_norm: 0.840508
[[34m2025-10-03 23:04:52[0m] Step: 4, Training Logs: loss_final: 1.830649, loss_mean: 1.679409, proj_loss: -0.065037, loss_mean_cls: 0.117800, deep_loss: 0.098478, grad_norm: 0.680785
[[34m2025-10-03 23:04:53[0m] Step: 5, Training Logs: loss_final: 1.825804, loss_mean: 1.685238, proj_loss: -0.076018, loss_mean_cls: 0.118603, deep_loss: 0.097981, grad_norm: 0.803953
[[34m2025-10-03 23:04:54[0m] Step: 6, Training Logs: loss_final: 1.799753, loss_mean: 1.666323, proj_loss: -0.082076, loss_mean_cls: 0.117980, deep_loss: 0.097526, grad_norm: 0.770381
[[34m2025-10-03 23:04:56[0m] Step: 7, Training Logs: loss_final: 1.782327, loss_mean: 1.656586, proj_loss: -0.088942, loss_mean_cls: 0.117597, deep_loss: 0.097086, grad_norm: 0.845348
[[34m2025-10-03 23:04:57[0m] Step: 8, Training Logs: loss_final: 1.773892, loss_mean: 1.653283, proj_loss: -0.093514, loss_mean_cls: 0.117813, deep_loss: 0.096310, grad_norm: 0.865210
[[34m2025-10-03 23:04:58[0m] Step: 9, Training Logs: loss_final: 1.754387, loss_mean: 1.638149, proj_loss: -0.097993, loss_mean_cls: 0.118029, deep_loss: 0.096203, grad_norm: 0.840690
[[34m2025-10-03 23:04:59[0m] Step: 10, Training Logs: loss_final: 1.752085, loss_mean: 1.638584, proj_loss: -0.099191, loss_mean_cls: 0.117742, deep_loss: 0.094950, grad_norm: 1.076593
[[34m2025-10-03 23:05:00[0m] Step: 11, Training Logs: loss_final: 1.720874, loss_mean: 1.612691, proj_loss: -0.104842, loss_mean_cls: 0.117659, deep_loss: 0.095366, grad_norm: 1.007834
[[34m2025-10-03 23:05:01[0m] Step: 12, Training Logs: loss_final: 1.709930, loss_mean: 1.605673, proj_loss: -0.107589, loss_mean_cls: 0.117555, deep_loss: 0.094291, grad_norm: 1.120335
[[34m2025-10-03 23:05:03[0m] Step: 13, Training Logs: loss_final: 1.670613, loss_mean: 1.572010, proj_loss: -0.112542, loss_mean_cls: 0.117361, deep_loss: 0.093784, grad_norm: 0.922903
[[34m2025-10-03 23:05:03[0m] Step: 14, Training Logs: loss_final: 1.681389, loss_mean: 1.589428, proj_loss: -0.117143, loss_mean_cls: 0.117198, deep_loss: 0.091905, grad_norm: 1.001851
[[34m2025-10-03 23:05:04[0m] Step: 15, Training Logs: loss_final: 1.653536, loss_mean: 1.563381, proj_loss: -0.118377, loss_mean_cls: 0.117349, deep_loss: 0.091182, grad_norm: 0.847277
[[34m2025-10-03 23:05:05[0m] Step: 16, Training Logs: loss_final: 1.655532, loss_mean: 1.569300, proj_loss: -0.120528, loss_mean_cls: 0.116779, deep_loss: 0.089980, grad_norm: 0.963789
[[34m2025-10-03 23:05:06[0m] Step: 17, Training Logs: loss_final: 1.620840, loss_mean: 1.538550, proj_loss: -0.125311, loss_mean_cls: 0.117408, deep_loss: 0.090193, grad_norm: 0.791377
[[34m2025-10-03 23:05:07[0m] Step: 18, Training Logs: loss_final: 1.612538, loss_mean: 1.532674, proj_loss: -0.126523, loss_mean_cls: 0.116738, deep_loss: 0.089649, grad_norm: 0.807401
[[34m2025-10-03 23:05:08[0m] Step: 19, Training Logs: loss_final: 1.612209, loss_mean: 1.537963, proj_loss: -0.131211, loss_mean_cls: 0.117219, deep_loss: 0.088238, grad_norm: 0.784704
[[34m2025-10-03 23:05:09[0m] Step: 20, Training Logs: loss_final: 1.603040, loss_mean: 1.530737, proj_loss: -0.131496, loss_mean_cls: 0.117110, deep_loss: 0.086690, grad_norm: 0.797378
[[34m2025-10-03 23:05:10[0m] Step: 21, Training Logs: loss_final: 1.583843, loss_mean: 1.512265, proj_loss: -0.131868, loss_mean_cls: 0.116646, deep_loss: 0.086800, grad_norm: 0.856522
[[34m2025-10-03 23:05:12[0m] Step: 22, Training Logs: loss_final: 1.558110, loss_mean: 1.493526, proj_loss: -0.137549, loss_mean_cls: 0.116489, deep_loss: 0.085644, grad_norm: 0.741418
[[34m2025-10-03 23:05:13[0m] Step: 23, Training Logs: loss_final: 1.546496, loss_mean: 1.481584, proj_loss: -0.136668, loss_mean_cls: 0.117315, deep_loss: 0.084264, grad_norm: 0.839201
[[34m2025-10-03 23:05:14[0m] Step: 24, Training Logs: loss_final: 1.527127, loss_mean: 1.464984, proj_loss: -0.139157, loss_mean_cls: 0.116490, deep_loss: 0.084810, grad_norm: 0.802357
[[34m2025-10-03 23:05:15[0m] Step: 25, Training Logs: loss_final: 1.519397, loss_mean: 1.465259, proj_loss: -0.141367, loss_mean_cls: 0.115848, deep_loss: 0.079656, grad_norm: 0.693324
[[34m2025-10-03 23:05:16[0m] Step: 26, Training Logs: loss_final: 1.480685, loss_mean: 1.425989, proj_loss: -0.140093, loss_mean_cls: 0.116582, deep_loss: 0.078207, grad_norm: 0.846208
[[34m2025-10-03 23:05:18[0m] Step: 27, Training Logs: loss_final: 1.484847, loss_mean: 1.436943, proj_loss: -0.141612, loss_mean_cls: 0.115959, deep_loss: 0.073556, grad_norm: 0.607149
[[34m2025-10-03 23:05:19[0m] Step: 28, Training Logs: loss_final: 1.470988, loss_mean: 1.422449, proj_loss: -0.142864, loss_mean_cls: 0.116363, deep_loss: 0.075039, grad_norm: 0.684397
[[34m2025-10-03 23:05:20[0m] Step: 29, Training Logs: loss_final: 1.450530, loss_mean: 1.404647, proj_loss: -0.141957, loss_mean_cls: 0.116772, deep_loss: 0.071067, grad_norm: 0.634868
[[34m2025-10-03 23:05:21[0m] Step: 30, Training Logs: loss_final: 1.458673, loss_mean: 1.419035, proj_loss: -0.143657, loss_mean_cls: 0.115824, deep_loss: 0.067471, grad_norm: 1.009782
[[34m2025-10-03 23:05:22[0m] Step: 31, Training Logs: loss_final: 1.449826, loss_mean: 1.412435, proj_loss: -0.145611, loss_mean_cls: 0.115405, deep_loss: 0.067597, grad_norm: 0.896182
[[34m2025-10-03 23:05:23[0m] Step: 32, Training Logs: loss_final: 1.434315, loss_mean: 1.402035, proj_loss: -0.145542, loss_mean_cls: 0.114887, deep_loss: 0.062936, grad_norm: 0.631332
[[34m2025-10-03 23:05:25[0m] Step: 33, Training Logs: loss_final: 1.428452, loss_mean: 1.400650, proj_loss: -0.146595, loss_mean_cls: 0.115324, deep_loss: 0.059072, grad_norm: 0.939080
[[34m2025-10-03 23:05:26[0m] Step: 34, Training Logs: loss_final: 1.422936, loss_mean: 1.393247, proj_loss: -0.143966, loss_mean_cls: 0.115102, deep_loss: 0.058553, grad_norm: 1.240394
[[34m2025-10-03 23:05:27[0m] Step: 35, Training Logs: loss_final: 1.398411, loss_mean: 1.375475, proj_loss: -0.147098, loss_mean_cls: 0.115447, deep_loss: 0.054588, grad_norm: 0.724550
[[34m2025-10-03 23:05:28[0m] Step: 36, Training Logs: loss_final: 1.407176, loss_mean: 1.381509, proj_loss: -0.145590, loss_mean_cls: 0.115222, deep_loss: 0.056035, grad_norm: 1.040645
[[34m2025-10-03 23:05:29[0m] Step: 37, Training Logs: loss_final: 1.371001, loss_mean: 1.355849, proj_loss: -0.146587, loss_mean_cls: 0.114654, deep_loss: 0.047085, grad_norm: 0.919454
[[34m2025-10-03 23:05:31[0m] Step: 38, Training Logs: loss_final: 1.385879, loss_mean: 1.373019, proj_loss: -0.145885, loss_mean_cls: 0.115545, deep_loss: 0.043199, grad_norm: 0.906609
[[34m2025-10-03 23:05:32[0m] Step: 39, Training Logs: loss_final: 1.370002, loss_mean: 1.362871, proj_loss: -0.146712, loss_mean_cls: 0.114192, deep_loss: 0.039650, grad_norm: 0.714776
[[34m2025-10-03 23:05:33[0m] Step: 40, Training Logs: loss_final: 1.340740, loss_mean: 1.342579, proj_loss: -0.148973, loss_mean_cls: 0.115496, deep_loss: 0.031638, grad_norm: 0.808289
[[34m2025-10-03 23:05:34[0m] Step: 41, Training Logs: loss_final: 1.317815, loss_mean: 1.326562, proj_loss: -0.148219, loss_mean_cls: 0.114949, deep_loss: 0.024523, grad_norm: 0.723238
[[34m2025-10-03 23:05:35[0m] Step: 42, Training Logs: loss_final: 1.306310, loss_mean: 1.322500, proj_loss: -0.147561, loss_mean_cls: 0.114272, deep_loss: 0.017099, grad_norm: 0.744165
[[34m2025-10-03 23:05:37[0m] Step: 43, Training Logs: loss_final: 1.296169, loss_mean: 1.317929, proj_loss: -0.148304, loss_mean_cls: 0.114454, deep_loss: 0.012090, grad_norm: 1.032775
[[34m2025-10-03 23:05:38[0m] Step: 44, Training Logs: loss_final: 1.354898, loss_mean: 1.383155, proj_loss: -0.147305, loss_mean_cls: 0.114107, deep_loss: 0.004941, grad_norm: 3.292501
[[34m2025-10-03 23:05:39[0m] Step: 45, Training Logs: loss_final: 1.310376, loss_mean: 1.340837, proj_loss: -0.147035, loss_mean_cls: 0.114585, deep_loss: 0.001988, grad_norm: 2.493108
[[34m2025-10-03 23:05:40[0m] Step: 46, Training Logs: loss_final: 1.336595, loss_mean: 1.385852, proj_loss: -0.148774, loss_mean_cls: 0.113800, deep_loss: -0.014282, grad_norm: 3.304622
[[34m2025-10-03 23:05:40[0m] Step: 47, Training Logs: loss_final: 1.291237, loss_mean: 1.340934, proj_loss: -0.147723, loss_mean_cls: 0.113512, deep_loss: -0.015486, grad_norm: 1.813417
[[34m2025-10-03 23:05:41[0m] Step: 48, Training Logs: loss_final: 1.293252, loss_mean: 1.351055, proj_loss: -0.148353, loss_mean_cls: 0.113538, deep_loss: -0.022988, grad_norm: 2.027486
[[34m2025-10-03 23:05:42[0m] Step: 49, Training Logs: loss_final: 1.250508, loss_mean: 1.314723, proj_loss: -0.148412, loss_mean_cls: 0.114515, deep_loss: -0.030318, grad_norm: 2.163635
[[34m2025-10-03 23:05:43[0m] Step: 50, Training Logs: loss_final: 1.257155, loss_mean: 1.318173, proj_loss: -0.148348, loss_mean_cls: 0.114212, deep_loss: -0.026882, grad_norm: 1.710867
[[34m2025-10-03 23:05:44[0m] Step: 51, Training Logs: loss_final: 1.222571, loss_mean: 1.304483, proj_loss: -0.147855, loss_mean_cls: 0.114167, deep_loss: -0.048224, grad_norm: 1.623305
[[34m2025-10-03 23:05:46[0m] Step: 52, Training Logs: loss_final: 1.192844, loss_mean: 1.270149, proj_loss: -0.150086, loss_mean_cls: 0.113547, deep_loss: -0.040766, grad_norm: 1.592470
[[34m2025-10-03 23:05:47[0m] Step: 53, Training Logs: loss_final: 1.187141, loss_mean: 1.284548, proj_loss: -0.148843, loss_mean_cls: 0.113455, deep_loss: -0.062018, grad_norm: 1.858515
[[34m2025-10-03 23:05:48[0m] Step: 54, Training Logs: loss_final: 1.196565, loss_mean: 1.318691, proj_loss: -0.148529, loss_mean_cls: 0.113786, deep_loss: -0.087383, grad_norm: 2.234822
[[34m2025-10-03 23:05:49[0m] Step: 55, Training Logs: loss_final: 1.156742, loss_mean: 1.265414, proj_loss: -0.150504, loss_mean_cls: 0.114070, deep_loss: -0.072238, grad_norm: 1.710080
[[34m2025-10-03 23:05:50[0m] Step: 56, Training Logs: loss_final: 1.192491, loss_mean: 1.310779, proj_loss: -0.150168, loss_mean_cls: 0.113395, deep_loss: -0.081515, grad_norm: 3.271585
[[34m2025-10-03 23:05:52[0m] Step: 57, Training Logs: loss_final: 1.117325, loss_mean: 1.280777, proj_loss: -0.151112, loss_mean_cls: 0.113808, deep_loss: -0.126148, grad_norm: 1.887957
[[34m2025-10-03 23:05:53[0m] Step: 58, Training Logs: loss_final: 1.129091, loss_mean: 1.283029, proj_loss: -0.149972, loss_mean_cls: 0.113836, deep_loss: -0.117802, grad_norm: 3.104430
[[34m2025-10-03 23:05:54[0m] Step: 59, Training Logs: loss_final: 1.065582, loss_mean: 1.223342, proj_loss: -0.148960, loss_mean_cls: 0.114010, deep_loss: -0.122810, grad_norm: 2.037977
[[34m2025-10-03 23:05:55[0m] Step: 60, Training Logs: loss_final: 1.082296, loss_mean: 1.260882, proj_loss: -0.150198, loss_mean_cls: 0.113150, deep_loss: -0.141538, grad_norm: 2.477546
[[34m2025-10-03 23:05:56[0m] Step: 61, Training Logs: loss_final: 0.989685, loss_mean: 1.205510, proj_loss: -0.151818, loss_mean_cls: 0.114308, deep_loss: -0.178316, grad_norm: 2.224184
[[34m2025-10-03 23:05:58[0m] Step: 62, Training Logs: loss_final: 1.071630, loss_mean: 1.283555, proj_loss: -0.150121, loss_mean_cls: 0.113743, deep_loss: -0.175546, grad_norm: 4.477008
[[34m2025-10-03 23:05:59[0m] Step: 63, Training Logs: loss_final: 0.961323, loss_mean: 1.194875, proj_loss: -0.149631, loss_mean_cls: 0.115228, deep_loss: -0.199149, grad_norm: 3.271157
[[34m2025-10-03 23:06:00[0m] Step: 64, Training Logs: loss_final: 1.006175, loss_mean: 1.245714, proj_loss: -0.148990, loss_mean_cls: 0.114216, deep_loss: -0.204765, grad_norm: 3.917584
[[34m2025-10-03 23:06:01[0m] Step: 65, Training Logs: loss_final: 1.029811, loss_mean: 1.312533, proj_loss: -0.150993, loss_mean_cls: 0.112707, deep_loss: -0.244436, grad_norm: 4.137317
[[34m2025-10-03 23:06:02[0m] Step: 66, Training Logs: loss_final: 0.922008, loss_mean: 1.239045, proj_loss: -0.150149, loss_mean_cls: 0.113690, deep_loss: -0.280579, grad_norm: 3.112206
[[34m2025-10-03 23:06:03[0m] Step: 67, Training Logs: loss_final: 0.917635, loss_mean: 1.239527, proj_loss: -0.151237, loss_mean_cls: 0.113822, deep_loss: -0.284477, grad_norm: 3.519349
[[34m2025-10-03 23:06:05[0m] Step: 68, Training Logs: loss_final: 0.878624, loss_mean: 1.241147, proj_loss: -0.150096, loss_mean_cls: 0.113344, deep_loss: -0.325771, grad_norm: 3.693986
[[34m2025-10-03 23:06:06[0m] Step: 69, Training Logs: loss_final: 0.862911, loss_mean: 1.240267, proj_loss: -0.148401, loss_mean_cls: 0.114355, deep_loss: -0.343309, grad_norm: 3.787580
[[34m2025-10-03 23:06:07[0m] Step: 70, Training Logs: loss_final: 0.760810, loss_mean: 1.213897, proj_loss: -0.151515, loss_mean_cls: 0.113868, deep_loss: -0.415440, grad_norm: 2.889687
[[34m2025-10-03 23:06:08[0m] Step: 71, Training Logs: loss_final: 0.827964, loss_mean: 1.294064, proj_loss: -0.150753, loss_mean_cls: 0.113886, deep_loss: -0.429232, grad_norm: 4.400712
[[34m2025-10-03 23:06:09[0m] Step: 72, Training Logs: loss_final: 0.758901, loss_mean: 1.247203, proj_loss: -0.150346, loss_mean_cls: 0.113535, deep_loss: -0.451491, grad_norm: 4.418978
[[34m2025-10-03 23:06:11[0m] Step: 73, Training Logs: loss_final: 0.686680, loss_mean: 1.239139, proj_loss: -0.148946, loss_mean_cls: 0.114436, deep_loss: -0.517950, grad_norm: 4.585405
[[34m2025-10-03 23:06:12[0m] Step: 74, Training Logs: loss_final: 0.672887, loss_mean: 1.233650, proj_loss: -0.149346, loss_mean_cls: 0.114535, deep_loss: -0.525953, grad_norm: 4.978184
[[34m2025-10-03 23:06:13[0m] Step: 75, Training Logs: loss_final: 0.639302, loss_mean: 1.237877, proj_loss: -0.148006, loss_mean_cls: 0.115329, deep_loss: -0.565898, grad_norm: 6.743974
[[34m2025-10-03 23:06:14[0m] Step: 76, Training Logs: loss_final: 0.533099, loss_mean: 1.231902, proj_loss: -0.147714, loss_mean_cls: 0.115307, deep_loss: -0.666397, grad_norm: 6.535664
[[34m2025-10-03 23:06:15[0m] Step: 77, Training Logs: loss_final: 0.459256, loss_mean: 1.191731, proj_loss: -0.146775, loss_mean_cls: 0.115142, deep_loss: -0.700842, grad_norm: 4.404571
[[34m2025-10-03 23:06:16[0m] Step: 78, Training Logs: loss_final: 0.367810, loss_mean: 1.202308, proj_loss: -0.148359, loss_mean_cls: 0.114663, deep_loss: -0.800802, grad_norm: 4.163177
[[34m2025-10-03 23:06:16[0m] Step: 79, Training Logs: loss_final: 0.416441, loss_mean: 1.243629, proj_loss: -0.146808, loss_mean_cls: 0.115140, deep_loss: -0.795521, grad_norm: 6.752977
[[34m2025-10-03 23:06:18[0m] Step: 80, Training Logs: loss_final: 0.337550, loss_mean: 1.261715, proj_loss: -0.146907, loss_mean_cls: 0.115413, deep_loss: -0.892671, grad_norm: inf
[[34m2025-10-03 23:06:19[0m] Step: 81, Training Logs: loss_final: 0.380264, loss_mean: 1.253814, proj_loss: -0.147201, loss_mean_cls: 0.114901, deep_loss: -0.841249, grad_norm: 7.273211
[[34m2025-10-03 23:06:20[0m] Step: 82, Training Logs: loss_final: 0.223008, loss_mean: 1.251468, proj_loss: -0.147249, loss_mean_cls: 0.115399, deep_loss: -0.996610, grad_norm: 5.605011
[[34m2025-10-03 23:06:21[0m] Step: 83, Training Logs: loss_final: 0.224333, loss_mean: 1.255211, proj_loss: -0.149789, loss_mean_cls: 0.115189, deep_loss: -0.996278, grad_norm: 8.110008
[[34m2025-10-03 23:06:22[0m] Step: 84, Training Logs: loss_final: 0.153652, loss_mean: 1.259145, proj_loss: -0.144544, loss_mean_cls: 0.115236, deep_loss: -1.076186, grad_norm: 11.515309
[[34m2025-10-03 23:06:23[0m] Step: 85, Training Logs: loss_final: -0.019097, loss_mean: 1.206092, proj_loss: -0.144961, loss_mean_cls: 0.115217, deep_loss: -1.195446, grad_norm: 6.085372
[[34m2025-10-03 23:06:25[0m] Step: 86, Training Logs: loss_final: -0.121641, loss_mean: 1.177777, proj_loss: -0.145362, loss_mean_cls: 0.116053, deep_loss: -1.270109, grad_norm: 7.196548
[[34m2025-10-03 23:06:26[0m] Step: 87, Training Logs: loss_final: -0.145689, loss_mean: 1.283643, proj_loss: -0.145815, loss_mean_cls: 0.115542, deep_loss: -1.399058, grad_norm: 11.939133
[[34m2025-10-03 23:06:27[0m] Step: 88, Training Logs: loss_final: -0.313085, loss_mean: 1.257423, proj_loss: -0.145228, loss_mean_cls: 0.115557, deep_loss: -1.540837, grad_norm: 9.779236
[[34m2025-10-03 23:06:28[0m] Step: 89, Training Logs: loss_final: 0.266000, loss_mean: 1.312029, proj_loss: -0.141933, loss_mean_cls: 0.115812, deep_loss: -1.019907, grad_norm: inf
[[34m2025-10-03 23:06:29[0m] Step: 90, Training Logs: loss_final: 0.227611, loss_mean: 1.351190, proj_loss: -0.143756, loss_mean_cls: 0.115338, deep_loss: -1.095161, grad_norm: 21.595541
[[34m2025-10-03 23:06:31[0m] Step: 91, Training Logs: loss_final: -0.322495, loss_mean: 1.344686, proj_loss: -0.144259, loss_mean_cls: 0.116194, deep_loss: -1.639116, grad_norm: 14.417957
[[34m2025-10-03 23:06:32[0m] Step: 92, Training Logs: loss_final: 0.306094, loss_mean: 1.418845, proj_loss: -0.142137, loss_mean_cls: 0.116728, deep_loss: -1.087343, grad_norm: 24.869564
[[34m2025-10-03 23:06:33[0m] Step: 93, Training Logs: loss_final: -0.365245, loss_mean: 1.360397, proj_loss: -0.141479, loss_mean_cls: 0.115140, deep_loss: -1.699303, grad_norm: 17.277479
[[34m2025-10-03 23:06:34[0m] Step: 94, Training Logs: loss_final: 0.221064, loss_mean: 1.453695, proj_loss: -0.139140, loss_mean_cls: 0.116685, deep_loss: -1.210176, grad_norm: 25.841045
[[34m2025-10-03 23:06:35[0m] Step: 95, Training Logs: loss_final: -0.174683, loss_mean: 1.387233, proj_loss: -0.136249, loss_mean_cls: 0.115905, deep_loss: -1.541572, grad_norm: 20.367411
[[34m2025-10-03 23:18:43[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 23:18:43[0m] using MLP layer as FFN
[[34m2025-10-03 23:18:48[0m] SiT Parameters: 140,221,984
[[34m2025-10-03 23:19:07[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-03 23:19:27[0m] Generating EMA samples done.
[[34m2025-10-03 23:19:27[0m] Step: 1, Training Logs: loss_final: 1.902024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:29[0m] Step: 2, Training Logs: loss_final: 1.907075, loss_mean: 1.690185, proj_loss: -0.001133, loss_mean_cls: 0.118023, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:30[0m] Step: 3, Training Logs: loss_final: 1.943526, loss_mean: 1.725911, proj_loss: -0.000692, loss_mean_cls: 0.118307, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:31[0m] Step: 4, Training Logs: loss_final: 1.913802, loss_mean: 1.696726, proj_loss: -0.000918, loss_mean_cls: 0.117994, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:32[0m] Step: 5, Training Logs: loss_final: 1.926213, loss_mean: 1.708497, proj_loss: -0.001132, loss_mean_cls: 0.118849, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:33[0m] Step: 6, Training Logs: loss_final: 1.911109, loss_mean: 1.693091, proj_loss: -0.000262, loss_mean_cls: 0.118281, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:34[0m] Step: 7, Training Logs: loss_final: 1.905225, loss_mean: 1.687699, proj_loss: -0.000456, loss_mean_cls: 0.117981, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:36[0m] Step: 8, Training Logs: loss_final: 1.915341, loss_mean: 1.697358, proj_loss: -0.000295, loss_mean_cls: 0.118278, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:37[0m] Step: 9, Training Logs: loss_final: 1.906894, loss_mean: 1.689859, proj_loss: -0.001537, loss_mean_cls: 0.118572, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:38[0m] Step: 10, Training Logs: loss_final: 1.930090, loss_mean: 1.711735, proj_loss: 0.000060, loss_mean_cls: 0.118295, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:39[0m] Step: 11, Training Logs: loss_final: 1.908421, loss_mean: 1.690543, proj_loss: -0.000472, loss_mean_cls: 0.118350, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:40[0m] Step: 12, Training Logs: loss_final: 1.926679, loss_mean: 1.708093, proj_loss: 0.000305, loss_mean_cls: 0.118281, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:41[0m] Step: 13, Training Logs: loss_final: 1.894913, loss_mean: 1.677251, proj_loss: -0.000555, loss_mean_cls: 0.118217, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:42[0m] Step: 14, Training Logs: loss_final: 1.910075, loss_mean: 1.692572, proj_loss: -0.000743, loss_mean_cls: 0.118246, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:44[0m] Step: 15, Training Logs: loss_final: 1.908943, loss_mean: 1.690525, proj_loss: 0.000005, loss_mean_cls: 0.118413, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:45[0m] Step: 16, Training Logs: loss_final: 1.932615, loss_mean: 1.715268, proj_loss: -0.000550, loss_mean_cls: 0.117897, deep_loss: 0.100000, grad_norm: nan
[[34m2025-10-03 23:19:46[0m] Step: 17, Training Logs: loss_final: 1.899142, loss_mean: 1.680075, proj_loss: 0.000387, loss_mean_cls: 0.118681, deep_loss: 0.100000, grad_norm: inf
[[34m2025-10-03 23:19:47[0m] Step: 18, Training Logs: loss_final: 1.896044, loss_mean: 1.678042, proj_loss: -0.000140, loss_mean_cls: 0.118143, deep_loss: 0.100000, grad_norm: inf
[[34m2025-10-03 23:19:48[0m] Step: 19, Training Logs: loss_final: 1.914490, loss_mean: 1.697539, proj_loss: -0.001746, loss_mean_cls: 0.118697, deep_loss: 0.100000, grad_norm: inf
[[34m2025-10-03 23:19:49[0m] Step: 20, Training Logs: loss_final: 1.929560, loss_mean: 1.711682, proj_loss: -0.000745, loss_mean_cls: 0.118622, deep_loss: 0.100000, grad_norm: 9431432.000000
[[34m2025-10-03 23:19:51[0m] Step: 21, Training Logs: loss_final: 1.899181, loss_mean: 1.690341, proj_loss: -0.000138, loss_mean_cls: 0.118155, deep_loss: 0.090823, grad_norm: 1.330141
[[34m2025-10-03 23:19:52[0m] Step: 22, Training Logs: loss_final: 1.883099, loss_mean: 1.683681, proj_loss: -0.008920, loss_mean_cls: 0.118272, deep_loss: 0.090067, grad_norm: 1.056343
[[34m2025-10-03 23:19:53[0m] Step: 23, Training Logs: loss_final: 1.883696, loss_mean: 1.691116, proj_loss: -0.015808, loss_mean_cls: 0.119007, deep_loss: 0.089382, grad_norm: 0.978164
[[34m2025-10-03 23:19:54[0m] Step: 24, Training Logs: loss_final: 1.836119, loss_mean: 1.656046, proj_loss: -0.025638, loss_mean_cls: 0.118366, deep_loss: 0.087344, grad_norm: 0.925606
[[34m2025-10-03 23:19:55[0m] Step: 25, Training Logs: loss_final: 1.849776, loss_mean: 1.682803, proj_loss: -0.032778, loss_mean_cls: 0.117807, deep_loss: 0.081944, grad_norm: 1.199117
[[34m2025-10-03 23:19:56[0m] Step: 26, Training Logs: loss_final: 1.830599, loss_mean: 1.672309, proj_loss: -0.038303, loss_mean_cls: 0.118371, deep_loss: 0.078222, grad_norm: 1.607979
[[34m2025-10-03 23:19:57[0m] Step: 27, Training Logs: loss_final: 1.831065, loss_mean: 1.685124, proj_loss: -0.044413, loss_mean_cls: 0.118006, deep_loss: 0.072348, grad_norm: 1.594335
[[34m2025-10-03 23:19:58[0m] Step: 28, Training Logs: loss_final: 1.818523, loss_mean: 1.674301, proj_loss: -0.049771, loss_mean_cls: 0.118265, deep_loss: 0.075727, grad_norm: 3.007090
[[34m2025-10-03 23:19:59[0m] Step: 29, Training Logs: loss_final: 1.803824, loss_mean: 1.659579, proj_loss: -0.052656, loss_mean_cls: 0.118891, deep_loss: 0.078010, grad_norm: 2.909926
[[34m2025-10-03 23:20:00[0m] Step: 30, Training Logs: loss_final: 1.811135, loss_mean: 1.673098, proj_loss: -0.055503, loss_mean_cls: 0.117864, deep_loss: 0.075676, grad_norm: 2.776258
[[34m2025-10-03 23:20:00[0m] Step: 31, Training Logs: loss_final: 1.769805, loss_mean: 1.638031, proj_loss: -0.060398, loss_mean_cls: 0.117657, deep_loss: 0.074515, grad_norm: 2.545592
[[34m2025-10-03 23:20:01[0m] Step: 32, Training Logs: loss_final: 1.772346, loss_mean: 1.645479, proj_loss: -0.060625, loss_mean_cls: 0.117259, deep_loss: 0.070233, grad_norm: 1.898338
[[34m2025-10-03 23:20:03[0m] Step: 33, Training Logs: loss_final: 1.782344, loss_mean: 1.659762, proj_loss: -0.063786, loss_mean_cls: 0.117645, deep_loss: 0.068722, grad_norm: 1.621598
[[34m2025-10-03 23:20:04[0m] Step: 34, Training Logs: loss_final: 1.751178, loss_mean: 1.627709, proj_loss: -0.063425, loss_mean_cls: 0.117320, deep_loss: 0.069574, grad_norm: 1.577727
[[34m2025-10-03 23:20:05[0m] Step: 35, Training Logs: loss_final: 1.779682, loss_mean: 1.659419, proj_loss: -0.065727, loss_mean_cls: 0.117760, deep_loss: 0.068230, grad_norm: 1.620125
[[34m2025-10-03 23:20:06[0m] Step: 36, Training Logs: loss_final: 1.737975, loss_mean: 1.616188, proj_loss: -0.066503, loss_mean_cls: 0.117616, deep_loss: 0.070673, grad_norm: 1.697962
[[34m2025-10-03 23:20:07[0m] Step: 37, Training Logs: loss_final: 1.744311, loss_mean: 1.626692, proj_loss: -0.067176, loss_mean_cls: 0.117192, deep_loss: 0.067603, grad_norm: 1.679321
[[34m2025-10-03 23:20:08[0m] Step: 38, Training Logs: loss_final: 1.762091, loss_mean: 1.647054, proj_loss: -0.069631, loss_mean_cls: 0.118116, deep_loss: 0.066552, grad_norm: 1.154469
[[34m2025-10-03 23:20:10[0m] Step: 39, Training Logs: loss_final: 1.735730, loss_mean: 1.621320, proj_loss: -0.069256, loss_mean_cls: 0.116886, deep_loss: 0.066780, grad_norm: 1.072945
[[34m2025-10-03 23:20:11[0m] Step: 40, Training Logs: loss_final: 1.742079, loss_mean: 1.629857, proj_loss: -0.071581, loss_mean_cls: 0.117992, deep_loss: 0.065812, grad_norm: 1.420451
[[34m2025-10-03 23:20:12[0m] Step: 41, Training Logs: loss_final: 1.728780, loss_mean: 1.617483, proj_loss: -0.071347, loss_mean_cls: 0.117561, deep_loss: 0.065083, grad_norm: 1.555789
[[34m2025-10-03 23:20:13[0m] Step: 42, Training Logs: loss_final: 1.733653, loss_mean: 1.622769, proj_loss: -0.070599, loss_mean_cls: 0.116903, deep_loss: 0.064580, grad_norm: 1.196345
[[34m2025-10-03 23:20:14[0m] Step: 43, Training Logs: loss_final: 1.724159, loss_mean: 1.613425, proj_loss: -0.070978, loss_mean_cls: 0.117205, deep_loss: 0.064507, grad_norm: 1.092972
[[34m2025-10-03 23:20:16[0m] Step: 44, Training Logs: loss_final: 1.716143, loss_mean: 1.605909, proj_loss: -0.071315, loss_mean_cls: 0.116863, deep_loss: 0.064686, grad_norm: 1.182002
[[34m2025-10-03 23:20:17[0m] Step: 45, Training Logs: loss_final: 1.700934, loss_mean: 1.589545, proj_loss: -0.071224, loss_mean_cls: 0.117322, deep_loss: 0.065291, grad_norm: 1.303712
[[34m2025-10-03 23:20:18[0m] Step: 46, Training Logs: loss_final: 1.714311, loss_mean: 1.604871, proj_loss: -0.072868, loss_mean_cls: 0.116672, deep_loss: 0.065636, grad_norm: 1.097362
[[34m2025-10-03 23:20:19[0m] Step: 47, Training Logs: loss_final: 1.698416, loss_mean: 1.589013, proj_loss: -0.072519, loss_mean_cls: 0.116434, deep_loss: 0.065488, grad_norm: 0.990632
[[34m2025-10-03 23:20:20[0m] Step: 48, Training Logs: loss_final: 1.708788, loss_mean: 1.601784, proj_loss: -0.073694, loss_mean_cls: 0.116506, deep_loss: 0.064192, grad_norm: 1.084317
[[34m2025-10-03 23:20:21[0m] Step: 49, Training Logs: loss_final: 1.702227, loss_mean: 1.595073, proj_loss: -0.073568, loss_mean_cls: 0.117185, deep_loss: 0.063536, grad_norm: 1.146986
[[34m2025-10-03 23:20:23[0m] Step: 50, Training Logs: loss_final: 1.690100, loss_mean: 1.582529, proj_loss: -0.074031, loss_mean_cls: 0.116878, deep_loss: 0.064725, grad_norm: 1.182403
[[34m2025-10-03 23:20:24[0m] Step: 51, Training Logs: loss_final: 1.706744, loss_mean: 1.601034, proj_loss: -0.073846, loss_mean_cls: 0.116621, deep_loss: 0.062935, grad_norm: 1.259385
[[34m2025-10-03 23:20:25[0m] Step: 52, Training Logs: loss_final: 1.655615, loss_mean: 1.550145, proj_loss: -0.074509, loss_mean_cls: 0.116246, deep_loss: 0.063734, grad_norm: 0.796511
[[34m2025-10-03 23:20:26[0m] Step: 53, Training Logs: loss_final: 1.675539, loss_mean: 1.568945, proj_loss: -0.072872, loss_mean_cls: 0.116044, deep_loss: 0.063422, grad_norm: 1.075845
[[34m2025-10-03 23:20:27[0m] Step: 54, Training Logs: loss_final: 1.700123, loss_mean: 1.594606, proj_loss: -0.074186, loss_mean_cls: 0.116650, deep_loss: 0.063053, grad_norm: 1.018387
[[34m2025-10-03 23:20:29[0m] Step: 55, Training Logs: loss_final: 1.658770, loss_mean: 1.553430, proj_loss: -0.074844, loss_mean_cls: 0.116818, deep_loss: 0.063367, grad_norm: 1.109345
[[34m2025-10-03 23:20:30[0m] Step: 56, Training Logs: loss_final: 1.668267, loss_mean: 1.563094, proj_loss: -0.075813, loss_mean_cls: 0.116202, deep_loss: 0.064785, grad_norm: 0.863162
[[34m2025-10-03 23:20:31[0m] Step: 57, Training Logs: loss_final: 1.681503, loss_mean: 1.576798, proj_loss: -0.074698, loss_mean_cls: 0.116606, deep_loss: 0.062797, grad_norm: 1.057700
[[34m2025-10-03 23:20:32[0m] Step: 58, Training Logs: loss_final: 1.660730, loss_mean: 1.556136, proj_loss: -0.075035, loss_mean_cls: 0.116354, deep_loss: 0.063276, grad_norm: 1.018427
[[34m2025-10-03 23:20:33[0m] Step: 59, Training Logs: loss_final: 1.647845, loss_mean: 1.543258, proj_loss: -0.073689, loss_mean_cls: 0.116426, deep_loss: 0.061851, grad_norm: 0.784045
[[34m2025-10-03 23:20:34[0m] Step: 60, Training Logs: loss_final: 1.668859, loss_mean: 1.565618, proj_loss: -0.074505, loss_mean_cls: 0.115511, deep_loss: 0.062236, grad_norm: 0.814479
[[34m2025-10-03 23:20:35[0m] Step: 61, Training Logs: loss_final: 1.654315, loss_mean: 1.553599, proj_loss: -0.075798, loss_mean_cls: 0.116709, deep_loss: 0.059805, grad_norm: 0.853377
[[34m2025-10-03 23:20:35[0m] Step: 62, Training Logs: loss_final: 1.645990, loss_mean: 1.542621, proj_loss: -0.074875, loss_mean_cls: 0.115626, deep_loss: 0.062618, grad_norm: 0.918328
[[34m2025-10-03 23:20:37[0m] Step: 63, Training Logs: loss_final: 1.650868, loss_mean: 1.549157, proj_loss: -0.075339, loss_mean_cls: 0.117100, deep_loss: 0.059951, grad_norm: 0.797973
[[34m2025-10-03 23:20:38[0m] Step: 64, Training Logs: loss_final: 1.639481, loss_mean: 1.535939, proj_loss: -0.074793, loss_mean_cls: 0.116186, deep_loss: 0.062149, grad_norm: 0.911285
[[34m2025-10-03 23:20:39[0m] Step: 65, Training Logs: loss_final: 1.646341, loss_mean: 1.542573, proj_loss: -0.075934, loss_mean_cls: 0.114872, deep_loss: 0.064830, grad_norm: 0.749030
[[34m2025-10-03 23:20:40[0m] Step: 66, Training Logs: loss_final: 1.640822, loss_mean: 1.537794, proj_loss: -0.074352, loss_mean_cls: 0.115542, deep_loss: 0.061838, grad_norm: 0.870440
[[34m2025-10-03 23:20:41[0m] Step: 67, Training Logs: loss_final: 1.623683, loss_mean: 1.521634, proj_loss: -0.075943, loss_mean_cls: 0.115822, deep_loss: 0.062171, grad_norm: 0.802655
[[34m2025-10-03 23:20:43[0m] Step: 68, Training Logs: loss_final: 1.630090, loss_mean: 1.525504, proj_loss: -0.073896, loss_mean_cls: 0.114768, deep_loss: 0.063713, grad_norm: 0.776246
[[34m2025-10-03 23:20:44[0m] Step: 69, Training Logs: loss_final: 1.630512, loss_mean: 1.524908, proj_loss: -0.073306, loss_mean_cls: 0.115478, deep_loss: 0.063432, grad_norm: 0.791897
[[34m2025-10-03 23:20:45[0m] Step: 70, Training Logs: loss_final: 1.616361, loss_mean: 1.516179, proj_loss: -0.075520, loss_mean_cls: 0.114867, deep_loss: 0.060835, grad_norm: 0.750096
[[34m2025-10-03 23:20:46[0m] Step: 71, Training Logs: loss_final: 1.661766, loss_mean: 1.557480, proj_loss: -0.074651, loss_mean_cls: 0.114894, deep_loss: 0.064043, grad_norm: 0.918805
[[34m2025-10-03 23:20:47[0m] Step: 72, Training Logs: loss_final: 1.615294, loss_mean: 1.512155, proj_loss: -0.073732, loss_mean_cls: 0.114615, deep_loss: 0.062256, grad_norm: 1.025161
[[34m2025-10-03 23:20:48[0m] Step: 73, Training Logs: loss_final: 1.633118, loss_mean: 1.531428, proj_loss: -0.075082, loss_mean_cls: 0.115178, deep_loss: 0.061594, grad_norm: 1.010879
[[34m2025-10-03 23:20:50[0m] Step: 74, Training Logs: loss_final: 1.617035, loss_mean: 1.513903, proj_loss: -0.075094, loss_mean_cls: 0.114857, deep_loss: 0.063370, grad_norm: 0.897455
[[34m2025-10-03 23:20:51[0m] Step: 75, Training Logs: loss_final: 1.610474, loss_mean: 1.505848, proj_loss: -0.073280, loss_mean_cls: 0.115659, deep_loss: 0.062247, grad_norm: 0.782278
[[34m2025-10-03 23:20:52[0m] Step: 76, Training Logs: loss_final: 1.625771, loss_mean: 1.524221, proj_loss: -0.073059, loss_mean_cls: 0.115215, deep_loss: 0.059394, grad_norm: 0.740512
[[34m2025-10-03 23:20:53[0m] Step: 77, Training Logs: loss_final: 1.616585, loss_mean: 1.514830, proj_loss: -0.073631, loss_mean_cls: 0.115078, deep_loss: 0.060307, grad_norm: 0.985916
[[34m2025-10-03 23:20:54[0m] Step: 78, Training Logs: loss_final: 1.609912, loss_mean: 1.507383, proj_loss: -0.074670, loss_mean_cls: 0.114578, deep_loss: 0.062621, grad_norm: 0.978068
[[34m2025-10-03 23:20:55[0m] Step: 79, Training Logs: loss_final: 1.605688, loss_mean: 1.500794, proj_loss: -0.073109, loss_mean_cls: 0.114919, deep_loss: 0.063085, grad_norm: 0.998759
[[34m2025-10-03 23:20:57[0m] Step: 80, Training Logs: loss_final: 1.608002, loss_mean: 1.505553, proj_loss: -0.074113, loss_mean_cls: 0.114634, deep_loss: 0.061927, grad_norm: 0.920342
[[34m2025-10-03 23:20:58[0m] Step: 81, Training Logs: loss_final: 1.601809, loss_mean: 1.498944, proj_loss: -0.074311, loss_mean_cls: 0.114146, deep_loss: 0.063029, grad_norm: 1.228466
[[34m2025-10-03 23:20:59[0m] Step: 82, Training Logs: loss_final: 1.594330, loss_mean: 1.488438, proj_loss: -0.072816, loss_mean_cls: 0.114571, deep_loss: 0.064138, grad_norm: 1.117621
[[34m2025-10-03 23:21:00[0m] Step: 83, Training Logs: loss_final: 1.601402, loss_mean: 1.498230, proj_loss: -0.075344, loss_mean_cls: 0.114178, deep_loss: 0.064338, grad_norm: 0.973945
[[34m2025-10-03 23:21:01[0m] Step: 84, Training Logs: loss_final: 1.595539, loss_mean: 1.492390, proj_loss: -0.072204, loss_mean_cls: 0.114216, deep_loss: 0.061137, grad_norm: 1.465758
[[34m2025-10-03 23:21:03[0m] Step: 85, Training Logs: loss_final: 1.571509, loss_mean: 1.469508, proj_loss: -0.073896, loss_mean_cls: 0.114037, deep_loss: 0.061860, grad_norm: 1.263716
[[34m2025-10-03 23:21:04[0m] Step: 86, Training Logs: loss_final: 1.572882, loss_mean: 1.469862, proj_loss: -0.075439, loss_mean_cls: 0.114610, deep_loss: 0.063848, grad_norm: 1.498631
[[34m2025-10-03 23:21:05[0m] Step: 87, Training Logs: loss_final: 1.602030, loss_mean: 1.498076, proj_loss: -0.073785, loss_mean_cls: 0.114046, deep_loss: 0.063694, grad_norm: 1.974572
[[34m2025-10-03 23:21:06[0m] Step: 88, Training Logs: loss_final: 1.593231, loss_mean: 1.490632, proj_loss: -0.073272, loss_mean_cls: 0.113666, deep_loss: 0.062207, grad_norm: 1.510055
[[34m2025-10-03 23:21:07[0m] Step: 89, Training Logs: loss_final: 1.560277, loss_mean: 1.459600, proj_loss: -0.074431, loss_mean_cls: 0.114072, deep_loss: 0.061037, grad_norm: 1.236701
[[34m2025-10-03 23:21:08[0m] Step: 90, Training Logs: loss_final: 1.557421, loss_mean: 1.454607, proj_loss: -0.073962, loss_mean_cls: 0.113241, deep_loss: 0.063535, grad_norm: 1.430158
[[34m2025-10-03 23:21:09[0m] Step: 91, Training Logs: loss_final: 1.562672, loss_mean: 1.459279, proj_loss: -0.074788, loss_mean_cls: 0.113840, deep_loss: 0.064341, grad_norm: 1.419526
[[34m2025-10-03 23:21:10[0m] Step: 92, Training Logs: loss_final: 1.549607, loss_mean: 1.447646, proj_loss: -0.075182, loss_mean_cls: 0.113906, deep_loss: 0.063237, grad_norm: 1.244233
[[34m2025-10-03 23:21:11[0m] Step: 93, Training Logs: loss_final: 1.577959, loss_mean: 1.474091, proj_loss: -0.072491, loss_mean_cls: 0.112350, deep_loss: 0.064010, grad_norm: 1.969341
[[34m2025-10-03 23:25:08[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 23:25:08[0m] using MLP layer as FFN
[[34m2025-10-03 23:25:13[0m] SiT Parameters: 140,221,984
[[34m2025-10-03 23:25:33[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-03 23:25:54[0m] Generating EMA samples done.
[[34m2025-10-03 23:25:54[0m] Step: 1, Training Logs: loss_final: 1.852024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:25:55[0m] Step: 2, Training Logs: loss_final: 1.857074, loss_mean: 1.690185, proj_loss: -0.001133, loss_mean_cls: 0.118023, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:25:56[0m] Step: 3, Training Logs: loss_final: 1.893526, loss_mean: 1.725911, proj_loss: -0.000692, loss_mean_cls: 0.118307, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:25:57[0m] Step: 4, Training Logs: loss_final: 1.863802, loss_mean: 1.696726, proj_loss: -0.000918, loss_mean_cls: 0.117994, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:25:58[0m] Step: 5, Training Logs: loss_final: 1.876213, loss_mean: 1.708497, proj_loss: -0.001132, loss_mean_cls: 0.118849, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:25:59[0m] Step: 6, Training Logs: loss_final: 1.861109, loss_mean: 1.693091, proj_loss: -0.000262, loss_mean_cls: 0.118281, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:01[0m] Step: 7, Training Logs: loss_final: 1.855225, loss_mean: 1.687699, proj_loss: -0.000456, loss_mean_cls: 0.117981, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:02[0m] Step: 8, Training Logs: loss_final: 1.865341, loss_mean: 1.697358, proj_loss: -0.000295, loss_mean_cls: 0.118278, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:03[0m] Step: 9, Training Logs: loss_final: 1.856894, loss_mean: 1.689859, proj_loss: -0.001537, loss_mean_cls: 0.118572, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:04[0m] Step: 10, Training Logs: loss_final: 1.880090, loss_mean: 1.711735, proj_loss: 0.000060, loss_mean_cls: 0.118295, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:05[0m] Step: 11, Training Logs: loss_final: 1.858421, loss_mean: 1.690543, proj_loss: -0.000472, loss_mean_cls: 0.118350, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:06[0m] Step: 12, Training Logs: loss_final: 1.876679, loss_mean: 1.708093, proj_loss: 0.000305, loss_mean_cls: 0.118281, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:07[0m] Step: 13, Training Logs: loss_final: 1.844913, loss_mean: 1.677251, proj_loss: -0.000555, loss_mean_cls: 0.118217, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:09[0m] Step: 14, Training Logs: loss_final: 1.860075, loss_mean: 1.692572, proj_loss: -0.000743, loss_mean_cls: 0.118246, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:10[0m] Step: 15, Training Logs: loss_final: 1.858943, loss_mean: 1.690525, proj_loss: 0.000005, loss_mean_cls: 0.118413, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:26:11[0m] Step: 16, Training Logs: loss_final: 1.882615, loss_mean: 1.715268, proj_loss: -0.000550, loss_mean_cls: 0.117897, deep_loss: 0.050000, grad_norm: inf
[[34m2025-10-03 23:26:12[0m] Step: 17, Training Logs: loss_final: 1.849142, loss_mean: 1.680075, proj_loss: 0.000387, loss_mean_cls: 0.118681, deep_loss: 0.050000, grad_norm: inf
[[34m2025-10-03 23:26:13[0m] Step: 18, Training Logs: loss_final: 1.846045, loss_mean: 1.678042, proj_loss: -0.000140, loss_mean_cls: 0.118143, deep_loss: 0.050000, grad_norm: 3722890.000000
[[34m2025-10-03 23:26:15[0m] Step: 19, Training Logs: loss_final: 1.857450, loss_mean: 1.695266, proj_loss: -0.001839, loss_mean_cls: 0.118697, deep_loss: 0.045326, grad_norm: 1.190755
[[34m2025-10-03 23:26:16[0m] Step: 20, Training Logs: loss_final: 1.844619, loss_mean: 1.702542, proj_loss: -0.020595, loss_mean_cls: 0.118579, deep_loss: 0.044093, grad_norm: 1.050217
[[34m2025-10-03 23:26:17[0m] Step: 21, Training Logs: loss_final: 1.800544, loss_mean: 1.677884, proj_loss: -0.039088, loss_mean_cls: 0.118060, deep_loss: 0.043688, grad_norm: 0.775306
[[34m2025-10-03 23:26:18[0m] Step: 22, Training Logs: loss_final: 1.779429, loss_mean: 1.675088, proj_loss: -0.057046, loss_mean_cls: 0.118154, deep_loss: 0.043234, grad_norm: 0.852297
[[34m2025-10-03 23:26:19[0m] Step: 23, Training Logs: loss_final: 1.779888, loss_mean: 1.684882, proj_loss: -0.066553, loss_mean_cls: 0.118888, deep_loss: 0.042671, grad_norm: 0.965044
[[34m2025-10-03 23:26:20[0m] Step: 24, Training Logs: loss_final: 1.732833, loss_mean: 1.650599, proj_loss: -0.077307, loss_mean_cls: 0.118220, deep_loss: 0.041321, grad_norm: 0.960293
[[34m2025-10-03 23:26:21[0m] Step: 25, Training Logs: loss_final: 1.747092, loss_mean: 1.675151, proj_loss: -0.083633, loss_mean_cls: 0.117665, deep_loss: 0.037908, grad_norm: 1.071154
[[34m2025-10-03 23:26:22[0m] Step: 26, Training Logs: loss_final: 1.732247, loss_mean: 1.664828, proj_loss: -0.086854, loss_mean_cls: 0.118246, deep_loss: 0.036027, grad_norm: 1.194038
[[34m2025-10-03 23:26:23[0m] Step: 27, Training Logs: loss_final: 1.740818, loss_mean: 1.677012, proj_loss: -0.090684, loss_mean_cls: 0.117870, deep_loss: 0.036620, grad_norm: 1.458346
[[34m2025-10-03 23:26:24[0m] Step: 28, Training Logs: loss_final: 1.726117, loss_mean: 1.666409, proj_loss: -0.095203, loss_mean_cls: 0.118141, deep_loss: 0.036770, grad_norm: 1.446737
[[34m2025-10-03 23:26:25[0m] Step: 29, Training Logs: loss_final: 1.709269, loss_mean: 1.651642, proj_loss: -0.096746, loss_mean_cls: 0.118754, deep_loss: 0.035619, grad_norm: 1.320680
[[34m2025-10-03 23:26:26[0m] Step: 30, Training Logs: loss_final: 1.716177, loss_mean: 1.663823, proj_loss: -0.099535, loss_mean_cls: 0.117709, deep_loss: 0.034179, grad_norm: 1.129575
[[34m2025-10-03 23:26:27[0m] Step: 31, Training Logs: loss_final: 1.678647, loss_mean: 1.629526, proj_loss: -0.103071, loss_mean_cls: 0.117498, deep_loss: 0.034694, grad_norm: 0.943847
[[34m2025-10-03 23:26:28[0m] Step: 32, Training Logs: loss_final: 1.682899, loss_mean: 1.635468, proj_loss: -0.103929, loss_mean_cls: 0.117073, deep_loss: 0.034288, grad_norm: 1.005134
[[34m2025-10-03 23:26:29[0m] Step: 33, Training Logs: loss_final: 1.691422, loss_mean: 1.647628, proj_loss: -0.107760, loss_mean_cls: 0.117478, deep_loss: 0.034075, grad_norm: 1.045364
[[34m2025-10-03 23:26:30[0m] Step: 34, Training Logs: loss_final: 1.660637, loss_mean: 1.617001, proj_loss: -0.107513, loss_mean_cls: 0.117152, deep_loss: 0.033998, grad_norm: 0.942616
[[34m2025-10-03 23:26:32[0m] Step: 35, Training Logs: loss_final: 1.684045, loss_mean: 1.645791, proj_loss: -0.112052, loss_mean_cls: 0.117595, deep_loss: 0.032713, grad_norm: 0.913674
[[34m2025-10-03 23:26:33[0m] Step: 36, Training Logs: loss_final: 1.642717, loss_mean: 1.604280, proj_loss: -0.112842, loss_mean_cls: 0.117412, deep_loss: 0.033867, grad_norm: 0.967890
[[34m2025-10-03 23:26:34[0m] Step: 37, Training Logs: loss_final: 1.644093, loss_mean: 1.610562, proj_loss: -0.116153, loss_mean_cls: 0.116967, deep_loss: 0.032716, grad_norm: 1.056014
[[34m2025-10-03 23:26:35[0m] Step: 38, Training Logs: loss_final: 1.658207, loss_mean: 1.626789, proj_loss: -0.119257, loss_mean_cls: 0.117879, deep_loss: 0.032796, grad_norm: 0.941455
[[34m2025-10-03 23:26:36[0m] Step: 39, Training Logs: loss_final: 1.624300, loss_mean: 1.596063, proj_loss: -0.121545, loss_mean_cls: 0.116626, deep_loss: 0.033156, grad_norm: 0.905618
[[34m2025-10-03 23:26:38[0m] Step: 40, Training Logs: loss_final: 1.621074, loss_mean: 1.597490, proj_loss: -0.126396, loss_mean_cls: 0.117717, deep_loss: 0.032264, grad_norm: 0.846665
[[34m2025-10-03 23:26:39[0m] Step: 41, Training Logs: loss_final: 1.600162, loss_mean: 1.578737, proj_loss: -0.127753, loss_mean_cls: 0.117250, deep_loss: 0.031929, grad_norm: 0.821030
[[34m2025-10-03 23:26:40[0m] Step: 42, Training Logs: loss_final: 1.595330, loss_mean: 1.575721, proj_loss: -0.128980, loss_mean_cls: 0.116559, deep_loss: 0.032031, grad_norm: 0.798184
[[34m2025-10-03 23:26:41[0m] Step: 43, Training Logs: loss_final: 1.575297, loss_mean: 1.557883, proj_loss: -0.131369, loss_mean_cls: 0.116795, deep_loss: 0.031988, grad_norm: 0.750102
[[34m2025-10-03 23:26:42[0m] Step: 44, Training Logs: loss_final: 1.556523, loss_mean: 1.540485, proj_loss: -0.132291, loss_mean_cls: 0.116392, deep_loss: 0.031938, grad_norm: 0.677275
[[34m2025-10-03 23:26:43[0m] Step: 45, Training Logs: loss_final: 1.542782, loss_mean: 1.526733, proj_loss: -0.133216, loss_mean_cls: 0.116787, deep_loss: 0.032479, grad_norm: 0.920515
[[34m2025-10-03 23:26:45[0m] Step: 46, Training Logs: loss_final: 1.553559, loss_mean: 1.540362, proj_loss: -0.136130, loss_mean_cls: 0.116073, deep_loss: 0.033254, grad_norm: 1.154952
[[34m2025-10-03 23:26:46[0m] Step: 47, Training Logs: loss_final: 1.524724, loss_mean: 1.512530, proj_loss: -0.136114, loss_mean_cls: 0.115780, deep_loss: 0.032527, grad_norm: 0.574934
[[34m2025-10-03 23:26:47[0m] Step: 48, Training Logs: loss_final: 1.535114, loss_mean: 1.525505, proj_loss: -0.138066, loss_mean_cls: 0.115778, deep_loss: 0.031898, grad_norm: 0.724633
[[34m2025-10-03 23:26:48[0m] Step: 49, Training Logs: loss_final: 1.537916, loss_mean: 1.528954, proj_loss: -0.138894, loss_mean_cls: 0.116406, deep_loss: 0.031451, grad_norm: 1.132510
[[34m2025-10-03 23:26:49[0m] Step: 50, Training Logs: loss_final: 1.539016, loss_mean: 1.530019, proj_loss: -0.139757, loss_mean_cls: 0.116018, deep_loss: 0.032736, grad_norm: 1.625239
[[34m2025-10-03 23:26:50[0m] Step: 51, Training Logs: loss_final: 1.527546, loss_mean: 1.520394, proj_loss: -0.139677, loss_mean_cls: 0.115794, deep_loss: 0.031035, grad_norm: 0.747811
[[34m2025-10-03 23:26:52[0m] Step: 52, Training Logs: loss_final: 1.519969, loss_mean: 1.515359, proj_loss: -0.142827, loss_mean_cls: 0.115316, deep_loss: 0.032121, grad_norm: 2.238130
[[34m2025-10-03 23:26:53[0m] Step: 53, Training Logs: loss_final: 1.527988, loss_mean: 1.522946, proj_loss: -0.141904, loss_mean_cls: 0.115068, deep_loss: 0.031879, grad_norm: 2.138056
[[34m2025-10-03 23:26:54[0m] Step: 54, Training Logs: loss_final: 1.516918, loss_mean: 1.511791, proj_loss: -0.141808, loss_mean_cls: 0.115613, deep_loss: 0.031322, grad_norm: 0.726289
[[34m2025-10-03 23:26:55[0m] Step: 55, Training Logs: loss_final: 1.494459, loss_mean: 1.491334, proj_loss: -0.144274, loss_mean_cls: 0.115734, deep_loss: 0.031664, grad_norm: 1.594608
[[34m2025-10-03 23:26:56[0m] Step: 56, Training Logs: loss_final: 1.481288, loss_mean: 1.478377, proj_loss: -0.144289, loss_mean_cls: 0.115039, deep_loss: 0.032160, grad_norm: 0.670814
[[34m2025-10-03 23:26:57[0m] Step: 57, Training Logs: loss_final: 1.512715, loss_mean: 1.510837, proj_loss: -0.145601, loss_mean_cls: 0.115395, deep_loss: 0.032084, grad_norm: 2.114520
[[34m2025-10-03 23:26:58[0m] Step: 58, Training Logs: loss_final: 1.448089, loss_mean: 1.446415, proj_loss: -0.144876, loss_mean_cls: 0.115128, deep_loss: 0.031422, grad_norm: 0.723061
[[34m2025-10-03 23:26:59[0m] Step: 59, Training Logs: loss_final: 1.428188, loss_mean: 1.426179, proj_loss: -0.144035, loss_mean_cls: 0.115210, deep_loss: 0.030834, grad_norm: 1.141568
[[34m2025-10-03 23:27:00[0m] Step: 60, Training Logs: loss_final: 1.476831, loss_mean: 1.476496, proj_loss: -0.145646, loss_mean_cls: 0.114250, deep_loss: 0.031731, grad_norm: 2.082066
[[34m2025-10-03 23:27:00[0m] Step: 61, Training Logs: loss_final: 1.393705, loss_mean: 1.395781, proj_loss: -0.147238, loss_mean_cls: 0.115313, deep_loss: 0.029849, grad_norm: 1.035394
[[34m2025-10-03 23:27:01[0m] Step: 62, Training Logs: loss_final: 1.400578, loss_mean: 1.401154, proj_loss: -0.146095, loss_mean_cls: 0.114267, deep_loss: 0.031252, grad_norm: 0.954926
[[34m2025-10-03 23:27:03[0m] Step: 63, Training Logs: loss_final: 1.388429, loss_mean: 1.388480, proj_loss: -0.145884, loss_mean_cls: 0.115715, deep_loss: 0.030118, grad_norm: 1.281565
[[34m2025-10-03 23:27:04[0m] Step: 64, Training Logs: loss_final: 1.382161, loss_mean: 1.382112, proj_loss: -0.145757, loss_mean_cls: 0.114735, deep_loss: 0.031071, grad_norm: 1.001003
[[34m2025-10-03 23:27:05[0m] Step: 65, Training Logs: loss_final: 1.458098, loss_mean: 1.459059, proj_loss: -0.147491, loss_mean_cls: 0.113281, deep_loss: 0.033249, grad_norm: 2.132532
[[34m2025-10-03 23:27:06[0m] Step: 66, Training Logs: loss_final: 1.386988, loss_mean: 1.388404, proj_loss: -0.146428, loss_mean_cls: 0.114051, deep_loss: 0.030961, grad_norm: 1.060534
[[34m2025-10-03 23:27:07[0m] Step: 67, Training Logs: loss_final: 1.397485, loss_mean: 1.399305, proj_loss: -0.147793, loss_mean_cls: 0.114210, deep_loss: 0.031764, grad_norm: 2.177401
[[34m2025-10-03 23:27:09[0m] Step: 68, Training Logs: loss_final: 1.400687, loss_mean: 1.402426, proj_loss: -0.147175, loss_mean_cls: 0.113290, deep_loss: 0.032146, grad_norm: 1.870404
[[34m2025-10-03 23:27:10[0m] Step: 69, Training Logs: loss_final: 1.380020, loss_mean: 1.380151, proj_loss: -0.145695, loss_mean_cls: 0.114052, deep_loss: 0.031512, grad_norm: 1.201892
[[34m2025-10-03 23:27:11[0m] Step: 70, Training Logs: loss_final: 1.365879, loss_mean: 1.370807, proj_loss: -0.148653, loss_mean_cls: 0.113368, deep_loss: 0.030356, grad_norm: 1.435735
[[34m2025-10-03 23:27:12[0m] Step: 71, Training Logs: loss_final: 1.434862, loss_mean: 1.437857, proj_loss: -0.148359, loss_mean_cls: 0.113269, deep_loss: 0.032095, grad_norm: 1.761425
[[34m2025-10-03 23:27:13[0m] Step: 72, Training Logs: loss_final: 1.370749, loss_mean: 1.374900, proj_loss: -0.148432, loss_mean_cls: 0.113149, deep_loss: 0.031131, grad_norm: 1.393311
[[34m2025-10-03 23:27:14[0m] Step: 73, Training Logs: loss_final: 1.372904, loss_mean: 1.376174, proj_loss: -0.147473, loss_mean_cls: 0.113760, deep_loss: 0.030442, grad_norm: 0.760629
[[34m2025-10-03 23:27:16[0m] Step: 74, Training Logs: loss_final: 1.384867, loss_mean: 1.388312, proj_loss: -0.148247, loss_mean_cls: 0.113291, deep_loss: 0.031510, grad_norm: 1.227866
[[34m2025-10-03 23:27:17[0m] Step: 75, Training Logs: loss_final: 1.367743, loss_mean: 1.369634, proj_loss: -0.147108, loss_mean_cls: 0.114157, deep_loss: 0.031061, grad_norm: 1.356719
[[34m2025-10-03 23:27:18[0m] Step: 76, Training Logs: loss_final: 1.360975, loss_mean: 1.364668, proj_loss: -0.146848, loss_mean_cls: 0.113721, deep_loss: 0.029435, grad_norm: 1.091693
[[34m2025-10-03 23:27:19[0m] Step: 77, Training Logs: loss_final: 1.355385, loss_mean: 1.358471, proj_loss: -0.146537, loss_mean_cls: 0.113439, deep_loss: 0.030012, grad_norm: 1.606235
[[34m2025-10-03 23:27:20[0m] Step: 78, Training Logs: loss_final: 1.365222, loss_mean: 1.369626, proj_loss: -0.148251, loss_mean_cls: 0.113004, deep_loss: 0.030844, grad_norm: 1.505935
[[34m2025-10-03 23:27:21[0m] Step: 79, Training Logs: loss_final: 1.376589, loss_mean: 1.379272, proj_loss: -0.147050, loss_mean_cls: 0.113281, deep_loss: 0.031085, grad_norm: 2.025146
[[34m2025-10-03 23:27:23[0m] Step: 80, Training Logs: loss_final: 1.376840, loss_mean: 1.380692, proj_loss: -0.147420, loss_mean_cls: 0.112921, deep_loss: 0.030646, grad_norm: 2.270174
[[34m2025-10-03 23:27:24[0m] Step: 81, Training Logs: loss_final: 1.409921, loss_mean: 1.412698, proj_loss: -0.147436, loss_mean_cls: 0.112306, deep_loss: 0.032353, grad_norm: 4.125180
[[34m2025-10-03 23:27:25[0m] Step: 82, Training Logs: loss_final: 1.422737, loss_mean: 1.425087, proj_loss: -0.147333, loss_mean_cls: 0.112843, deep_loss: 0.032139, grad_norm: 4.488738
[[34m2025-10-03 23:27:26[0m] Step: 83, Training Logs: loss_final: 1.456409, loss_mean: 1.460819, proj_loss: -0.150044, loss_mean_cls: 0.112475, deep_loss: 0.033158, grad_norm: 5.539126
[[34m2025-10-03 23:27:27[0m] Step: 84, Training Logs: loss_final: 1.377733, loss_mean: 1.379817, proj_loss: -0.145715, loss_mean_cls: 0.112586, deep_loss: 0.031045, grad_norm: 3.640758
[[34m2025-10-03 23:27:28[0m] Step: 85, Training Logs: loss_final: 1.393870, loss_mean: 1.396511, proj_loss: -0.146169, loss_mean_cls: 0.112365, deep_loss: 0.031163, grad_norm: 5.068495
[[34m2025-10-03 23:27:30[0m] Step: 86, Training Logs: loss_final: 1.403761, loss_mean: 1.406604, proj_loss: -0.147029, loss_mean_cls: 0.113158, deep_loss: 0.031029, grad_norm: 5.224255
[[34m2025-10-03 23:27:31[0m] Step: 87, Training Logs: loss_final: 1.413681, loss_mean: 1.417472, proj_loss: -0.147366, loss_mean_cls: 0.112464, deep_loss: 0.031111, grad_norm: 3.899454
[[34m2025-10-03 23:27:32[0m] Step: 88, Training Logs: loss_final: 1.436558, loss_mean: 1.437461, proj_loss: -0.146537, loss_mean_cls: 0.112322, deep_loss: 0.033312, grad_norm: 5.843190
[[34m2025-10-03 23:27:33[0m] Step: 89, Training Logs: loss_final: 1.366597, loss_mean: 1.369687, proj_loss: -0.147070, loss_mean_cls: 0.112711, deep_loss: 0.031269, grad_norm: 4.328040
[[34m2025-10-03 23:27:34[0m] Step: 90, Training Logs: loss_final: 1.383086, loss_mean: 1.388854, proj_loss: -0.148631, loss_mean_cls: 0.111718, deep_loss: 0.031144, grad_norm: 4.337485
[[34m2025-10-03 23:27:35[0m] Step: 91, Training Logs: loss_final: 1.373240, loss_mean: 1.377749, proj_loss: -0.148036, loss_mean_cls: 0.112222, deep_loss: 0.031305, grad_norm: 3.572266
[[34m2025-10-03 23:27:35[0m] Step: 92, Training Logs: loss_final: 1.368219, loss_mean: 1.373355, proj_loss: -0.149216, loss_mean_cls: 0.112549, deep_loss: 0.031532, grad_norm: 4.128796
[[34m2025-10-03 23:27:36[0m] Step: 93, Training Logs: loss_final: 1.396704, loss_mean: 1.401251, proj_loss: -0.146881, loss_mean_cls: 0.111034, deep_loss: 0.031300, grad_norm: 4.254699
[[34m2025-10-03 23:27:37[0m] Step: 94, Training Logs: loss_final: 1.377566, loss_mean: 1.382129, proj_loss: -0.148522, loss_mean_cls: 0.112532, deep_loss: 0.031427, grad_norm: 4.286557
[[34m2025-10-03 23:27:38[0m] Step: 95, Training Logs: loss_final: 1.377148, loss_mean: 1.381670, proj_loss: -0.146720, loss_mean_cls: 0.111505, deep_loss: 0.030692, grad_norm: 4.186183
[[34m2025-10-03 23:27:39[0m] Step: 96, Training Logs: loss_final: 1.398079, loss_mean: 1.401095, proj_loss: -0.146536, loss_mean_cls: 0.111871, deep_loss: 0.031649, grad_norm: 4.013966
[[34m2025-10-03 23:27:40[0m] Step: 97, Training Logs: loss_final: 1.383999, loss_mean: 1.388237, proj_loss: -0.147856, loss_mean_cls: 0.111881, deep_loss: 0.031737, grad_norm: 2.514960
[[34m2025-10-03 23:27:41[0m] Step: 98, Training Logs: loss_final: 1.365075, loss_mean: 1.367155, proj_loss: -0.146178, loss_mean_cls: 0.112190, deep_loss: 0.031908, grad_norm: 3.305807
[[34m2025-10-03 23:27:42[0m] Step: 99, Training Logs: loss_final: 1.385798, loss_mean: 1.389102, proj_loss: -0.146884, loss_mean_cls: 0.111630, deep_loss: 0.031951, grad_norm: 3.520928
[[34m2025-10-03 23:27:44[0m] Step: 100, Training Logs: loss_final: 1.374315, loss_mean: 1.377956, proj_loss: -0.146638, loss_mean_cls: 0.112278, deep_loss: 0.030718, grad_norm: 2.615709
[[34m2025-10-03 23:27:45[0m] Step: 101, Training Logs: loss_final: 1.369577, loss_mean: 1.372960, proj_loss: -0.145414, loss_mean_cls: 0.112113, deep_loss: 0.029918, grad_norm: 3.708468
[[34m2025-10-03 23:27:46[0m] Step: 102, Training Logs: loss_final: 1.360885, loss_mean: 1.366501, proj_loss: -0.148089, loss_mean_cls: 0.111916, deep_loss: 0.030556, grad_norm: 3.726048
[[34m2025-10-03 23:27:47[0m] Step: 103, Training Logs: loss_final: 1.369436, loss_mean: 1.374147, proj_loss: -0.148711, loss_mean_cls: 0.112178, deep_loss: 0.031821, grad_norm: 3.273155
[[34m2025-10-03 23:27:48[0m] Step: 104, Training Logs: loss_final: 1.371068, loss_mean: 1.375049, proj_loss: -0.146559, loss_mean_cls: 0.111396, deep_loss: 0.031182, grad_norm: 4.075589
[[34m2025-10-03 23:27:49[0m] Step: 105, Training Logs: loss_final: 1.388678, loss_mean: 1.390479, proj_loss: -0.145680, loss_mean_cls: 0.111729, deep_loss: 0.032149, grad_norm: 5.957345
[[34m2025-10-03 23:27:51[0m] Step: 106, Training Logs: loss_final: 1.452505, loss_mean: 1.454541, proj_loss: -0.146827, loss_mean_cls: 0.112081, deep_loss: 0.032710, grad_norm: 9.355639
[[34m2025-10-03 23:27:52[0m] Step: 107, Training Logs: loss_final: 1.369159, loss_mean: 1.371773, proj_loss: -0.145746, loss_mean_cls: 0.111753, deep_loss: 0.031379, grad_norm: 4.898695
[[34m2025-10-03 23:27:53[0m] Step: 108, Training Logs: loss_final: 1.388630, loss_mean: 1.390730, proj_loss: -0.145891, loss_mean_cls: 0.112462, deep_loss: 0.031329, grad_norm: 6.167366
[[34m2025-10-03 23:27:54[0m] Step: 109, Training Logs: loss_final: 1.428264, loss_mean: 1.428890, proj_loss: -0.145065, loss_mean_cls: 0.111639, deep_loss: 0.032801, grad_norm: 5.776086
[[34m2025-10-03 23:27:55[0m] Step: 110, Training Logs: loss_final: 1.400452, loss_mean: 1.402396, proj_loss: -0.144890, loss_mean_cls: 0.111834, deep_loss: 0.031112, grad_norm: 4.741098
[[34m2025-10-03 23:27:56[0m] Step: 111, Training Logs: loss_final: 1.397459, loss_mean: 1.398003, proj_loss: -0.144515, loss_mean_cls: 0.112707, deep_loss: 0.031265, grad_norm: 4.084492
[[34m2025-10-03 23:27:58[0m] Step: 112, Training Logs: loss_final: 1.428136, loss_mean: 1.427764, proj_loss: -0.143889, loss_mean_cls: 0.112098, deep_loss: 0.032162, grad_norm: 5.197307
[[34m2025-10-03 23:27:59[0m] Step: 113, Training Logs: loss_final: 1.388882, loss_mean: 1.389858, proj_loss: -0.144247, loss_mean_cls: 0.112047, deep_loss: 0.031224, grad_norm: 5.040125
[[34m2025-10-03 23:28:00[0m] Step: 114, Training Logs: loss_final: 1.381123, loss_mean: 1.382056, proj_loss: -0.144733, loss_mean_cls: 0.112059, deep_loss: 0.031741, grad_norm: 4.201500
[[34m2025-10-03 23:28:01[0m] Step: 115, Training Logs: loss_final: 1.376341, loss_mean: 1.376525, proj_loss: -0.143163, loss_mean_cls: 0.112082, deep_loss: 0.030897, grad_norm: 4.250019
[[34m2025-10-03 23:28:02[0m] Step: 116, Training Logs: loss_final: 1.406655, loss_mean: 1.404388, proj_loss: -0.142440, loss_mean_cls: 0.112616, deep_loss: 0.032091, grad_norm: 4.541863
[[34m2025-10-03 23:28:03[0m] Step: 117, Training Logs: loss_final: 1.385649, loss_mean: 1.387825, proj_loss: -0.146096, loss_mean_cls: 0.111659, deep_loss: 0.032260, grad_norm: 4.312057
[[34m2025-10-03 23:28:05[0m] Step: 118, Training Logs: loss_final: 1.393821, loss_mean: 1.390884, proj_loss: -0.141203, loss_mean_cls: 0.112582, deep_loss: 0.031558, grad_norm: 7.052070
[[34m2025-10-03 23:28:06[0m] Step: 119, Training Logs: loss_final: 1.386185, loss_mean: 1.384283, proj_loss: -0.142686, loss_mean_cls: 0.112747, deep_loss: 0.031840, grad_norm: 5.612587
[[34m2025-10-03 23:28:07[0m] Step: 120, Training Logs: loss_final: 1.398700, loss_mean: 1.395452, proj_loss: -0.140958, loss_mean_cls: 0.112007, deep_loss: 0.032199, grad_norm: 5.643440
[[34m2025-10-03 23:28:08[0m] Step: 121, Training Logs: loss_final: 1.449171, loss_mean: 1.446627, proj_loss: -0.142292, loss_mean_cls: 0.111656, deep_loss: 0.033180, grad_norm: 9.384198
[[34m2025-10-03 23:28:09[0m] Step: 122, Training Logs: loss_final: 1.451224, loss_mean: 1.445203, proj_loss: -0.140288, loss_mean_cls: 0.113352, deep_loss: 0.032956, grad_norm: 13.141263
[[34m2025-10-03 23:28:10[0m] Step: 123, Training Logs: loss_final: 1.406015, loss_mean: 1.403588, proj_loss: -0.142149, loss_mean_cls: 0.111833, deep_loss: 0.032743, grad_norm: 7.014182
[[34m2025-10-03 23:28:11[0m] Step: 124, Training Logs: loss_final: 1.469845, loss_mean: 1.463537, proj_loss: -0.139446, loss_mean_cls: 0.112322, deep_loss: 0.033432, grad_norm: 10.974236
[[34m2025-10-03 23:28:12[0m] Step: 125, Training Logs: loss_final: 1.444208, loss_mean: 1.437309, proj_loss: -0.138883, loss_mean_cls: 0.112694, deep_loss: 0.033089, grad_norm: 8.941001
[[34m2025-10-03 23:28:13[0m] Step: 126, Training Logs: loss_final: 1.424301, loss_mean: 1.416788, proj_loss: -0.137156, loss_mean_cls: 0.112553, deep_loss: 0.032116, grad_norm: 7.984377
[[34m2025-10-03 23:28:13[0m] Step: 127, Training Logs: loss_final: 1.447374, loss_mean: 1.440475, proj_loss: -0.137621, loss_mean_cls: 0.111536, deep_loss: 0.032983, grad_norm: 8.718038
[[34m2025-10-03 23:28:14[0m] Step: 128, Training Logs: loss_final: 1.427860, loss_mean: 1.420069, proj_loss: -0.137004, loss_mean_cls: 0.111306, deep_loss: 0.033489, grad_norm: 8.525554
[[34m2025-10-03 23:28:15[0m] Step: 129, Training Logs: loss_final: 1.430703, loss_mean: 1.422395, proj_loss: -0.137087, loss_mean_cls: 0.111949, deep_loss: 0.033446, grad_norm: 9.061835
[[34m2025-10-03 23:28:17[0m] Step: 130, Training Logs: loss_final: 1.434410, loss_mean: 1.427559, proj_loss: -0.137928, loss_mean_cls: 0.111002, deep_loss: 0.033777, grad_norm: 7.248904
[[34m2025-10-03 23:28:18[0m] Step: 131, Training Logs: loss_final: 1.424609, loss_mean: 1.414152, proj_loss: -0.134676, loss_mean_cls: 0.112204, deep_loss: 0.032930, grad_norm: 7.630579
[[34m2025-10-03 23:28:19[0m] Step: 132, Training Logs: loss_final: 1.380185, loss_mean: 1.372767, proj_loss: -0.136360, loss_mean_cls: 0.112922, deep_loss: 0.030856, grad_norm: 4.940884
[[34m2025-10-03 23:28:20[0m] Step: 133, Training Logs: loss_final: 1.408566, loss_mean: 1.398436, proj_loss: -0.134551, loss_mean_cls: 0.112135, deep_loss: 0.032545, grad_norm: 6.695939
[[34m2025-10-03 23:28:21[0m] Step: 134, Training Logs: loss_final: 1.415871, loss_mean: 1.408394, proj_loss: -0.135964, loss_mean_cls: 0.111398, deep_loss: 0.032042, grad_norm: 6.086179
[[34m2025-10-03 23:28:22[0m] Step: 135, Training Logs: loss_final: 1.449954, loss_mean: 1.440836, proj_loss: -0.136050, loss_mean_cls: 0.111943, deep_loss: 0.033225, grad_norm: 10.202505
[[34m2025-10-03 23:28:24[0m] Step: 136, Training Logs: loss_final: 1.423965, loss_mean: 1.412838, proj_loss: -0.134279, loss_mean_cls: 0.112524, deep_loss: 0.032883, grad_norm: 9.012616
[[34m2025-10-03 23:28:25[0m] Step: 137, Training Logs: loss_final: 1.391455, loss_mean: 1.380324, proj_loss: -0.133282, loss_mean_cls: 0.112866, deep_loss: 0.031548, grad_norm: 6.664268
[[34m2025-10-03 23:28:26[0m] Step: 138, Training Logs: loss_final: 1.423571, loss_mean: 1.413944, proj_loss: -0.135181, loss_mean_cls: 0.112473, deep_loss: 0.032335, grad_norm: 10.574692
[[34m2025-10-03 23:28:27[0m] Step: 139, Training Logs: loss_final: 1.416655, loss_mean: 1.406088, proj_loss: -0.134208, loss_mean_cls: 0.112420, deep_loss: 0.032355, grad_norm: 9.769444
[[34m2025-10-03 23:28:29[0m] Step: 140, Training Logs: loss_final: 1.423257, loss_mean: 1.412309, proj_loss: -0.133872, loss_mean_cls: 0.111963, deep_loss: 0.032857, grad_norm: 6.917357
[[34m2025-10-03 23:28:30[0m] Step: 141, Training Logs: loss_final: 1.432746, loss_mean: 1.420118, proj_loss: -0.133843, loss_mean_cls: 0.112963, deep_loss: 0.033508, grad_norm: 9.874367
[[34m2025-10-03 23:28:31[0m] Step: 142, Training Logs: loss_final: 1.404531, loss_mean: 1.395299, proj_loss: -0.134755, loss_mean_cls: 0.112686, deep_loss: 0.031301, grad_norm: 6.796175
[[34m2025-10-03 23:28:32[0m] Step: 143, Training Logs: loss_final: 1.399338, loss_mean: 1.392185, proj_loss: -0.136319, loss_mean_cls: 0.112079, deep_loss: 0.031393, grad_norm: 9.319567
[[34m2025-10-03 23:28:33[0m] Step: 144, Training Logs: loss_final: 1.370990, loss_mean: 1.361726, proj_loss: -0.135116, loss_mean_cls: 0.112369, deep_loss: 0.032011, grad_norm: 6.774255
[[34m2025-10-03 23:28:34[0m] Step: 145, Training Logs: loss_final: 1.417509, loss_mean: 1.405888, proj_loss: -0.132972, loss_mean_cls: 0.112836, deep_loss: 0.031756, grad_norm: 8.070102
[[34m2025-10-03 23:28:36[0m] Step: 146, Training Logs: loss_final: 1.423231, loss_mean: 1.412597, proj_loss: -0.133784, loss_mean_cls: 0.111545, deep_loss: 0.032872, grad_norm: 6.808743
[[34m2025-10-03 23:28:37[0m] Step: 147, Training Logs: loss_final: 1.419488, loss_mean: 1.407515, proj_loss: -0.131538, loss_mean_cls: 0.112353, deep_loss: 0.031158, grad_norm: 7.658472
[[34m2025-10-03 23:28:38[0m] Step: 148, Training Logs: loss_final: 1.407819, loss_mean: 1.396954, proj_loss: -0.133068, loss_mean_cls: 0.111922, deep_loss: 0.032011, grad_norm: 6.720611
[[34m2025-10-03 23:28:39[0m] Step: 149, Training Logs: loss_final: 1.371615, loss_mean: 1.359420, proj_loss: -0.131599, loss_mean_cls: 0.112295, deep_loss: 0.031500, grad_norm: 5.084142
[[34m2025-10-03 23:28:40[0m] Step: 150, Training Logs: loss_final: 1.384568, loss_mean: 1.372156, proj_loss: -0.131717, loss_mean_cls: 0.112788, deep_loss: 0.031341, grad_norm: 6.831867
[[34m2025-10-03 23:28:41[0m] Step: 151, Training Logs: loss_final: 1.386013, loss_mean: 1.374261, proj_loss: -0.131634, loss_mean_cls: 0.111867, deep_loss: 0.031519, grad_norm: 4.438432
[[34m2025-10-03 23:28:43[0m] Step: 152, Training Logs: loss_final: 1.379476, loss_mean: 1.366643, proj_loss: -0.130722, loss_mean_cls: 0.112015, deep_loss: 0.031540, grad_norm: 6.684758
[[34m2025-10-03 23:28:44[0m] Step: 153, Training Logs: loss_final: 1.392942, loss_mean: 1.379927, proj_loss: -0.131956, loss_mean_cls: 0.112452, deep_loss: 0.032519, grad_norm: 7.253740
[[34m2025-10-03 23:28:45[0m] Step: 154, Training Logs: loss_final: 1.400274, loss_mean: 1.385999, proj_loss: -0.130424, loss_mean_cls: 0.112136, deep_loss: 0.032562, grad_norm: 6.790618
[[34m2025-10-03 23:28:46[0m] Step: 155, Training Logs: loss_final: 1.401245, loss_mean: 1.387417, proj_loss: -0.128395, loss_mean_cls: 0.111519, deep_loss: 0.030703, grad_norm: 7.417440
[[34m2025-10-03 23:28:47[0m] Step: 156, Training Logs: loss_final: 1.348887, loss_mean: 1.332829, proj_loss: -0.127816, loss_mean_cls: 0.113660, deep_loss: 0.030214, grad_norm: 5.778979
[[34m2025-10-03 23:28:48[0m] Step: 157, Training Logs: loss_final: 1.400875, loss_mean: 1.385020, proj_loss: -0.127330, loss_mean_cls: 0.111918, deep_loss: 0.031267, grad_norm: 8.791683
[[34m2025-10-03 23:28:49[0m] Step: 158, Training Logs: loss_final: 1.399638, loss_mean: 1.378208, proj_loss: -0.122745, loss_mean_cls: 0.112939, deep_loss: 0.031237, grad_norm: 5.999091
[[34m2025-10-03 23:28:49[0m] Step: 159, Training Logs: loss_final: 1.404898, loss_mean: 1.385775, proj_loss: -0.124722, loss_mean_cls: 0.112384, deep_loss: 0.031461, grad_norm: 9.471449
[[34m2025-10-03 23:28:50[0m] Step: 160, Training Logs: loss_final: 1.398175, loss_mean: 1.378119, proj_loss: -0.123715, loss_mean_cls: 0.111835, deep_loss: 0.031936, grad_norm: 6.452485
[[34m2025-10-03 23:28:51[0m] Step: 161, Training Logs: loss_final: 1.393238, loss_mean: 1.369791, proj_loss: -0.120895, loss_mean_cls: 0.112530, deep_loss: 0.031812, grad_norm: 7.523977
[[34m2025-10-03 23:28:52[0m] Step: 162, Training Logs: loss_final: 1.399625, loss_mean: 1.375181, proj_loss: -0.119834, loss_mean_cls: 0.112951, deep_loss: 0.031326, grad_norm: 6.986073
[[34m2025-10-03 23:28:53[0m] Step: 163, Training Logs: loss_final: 1.378859, loss_mean: 1.355648, proj_loss: -0.119847, loss_mean_cls: 0.112163, deep_loss: 0.030895, grad_norm: 5.206658
[[34m2025-10-03 23:28:55[0m] Step: 164, Training Logs: loss_final: 1.383909, loss_mean: 1.356377, proj_loss: -0.116781, loss_mean_cls: 0.113115, deep_loss: 0.031198, grad_norm: 8.281888
[[34m2025-10-03 23:28:56[0m] Step: 165, Training Logs: loss_final: 1.428638, loss_mean: 1.398522, proj_loss: -0.115272, loss_mean_cls: 0.112837, deep_loss: 0.032550, grad_norm: 7.588266
[[34m2025-10-03 23:28:57[0m] Step: 166, Training Logs: loss_final: 1.379403, loss_mean: 1.350715, proj_loss: -0.115866, loss_mean_cls: 0.113182, deep_loss: 0.031372, grad_norm: 6.401978
[[34m2025-10-03 23:28:58[0m] Step: 167, Training Logs: loss_final: 1.417615, loss_mean: 1.384079, proj_loss: -0.111410, loss_mean_cls: 0.112928, deep_loss: 0.032018, grad_norm: 10.268182
[[34m2025-10-03 23:28:59[0m] Step: 168, Training Logs: loss_final: 1.375115, loss_mean: 1.340219, proj_loss: -0.109764, loss_mean_cls: 0.113311, deep_loss: 0.031348, grad_norm: 6.956321
[[34m2025-10-03 23:29:00[0m] Step: 169, Training Logs: loss_final: 1.404182, loss_mean: 1.369519, proj_loss: -0.109682, loss_mean_cls: 0.112763, deep_loss: 0.031582, grad_norm: 8.954318
[[34m2025-10-03 23:29:02[0m] Step: 170, Training Logs: loss_final: 1.405192, loss_mean: 1.370042, proj_loss: -0.109189, loss_mean_cls: 0.112478, deep_loss: 0.031861, grad_norm: 8.383881
[[34m2025-10-03 23:29:03[0m] Step: 171, Training Logs: loss_final: 1.423968, loss_mean: 1.385879, proj_loss: -0.107193, loss_mean_cls: 0.112465, deep_loss: 0.032817, grad_norm: 8.983376
[[34m2025-10-03 23:29:04[0m] Step: 172, Training Logs: loss_final: 1.389766, loss_mean: 1.348792, proj_loss: -0.103739, loss_mean_cls: 0.113364, deep_loss: 0.031349, grad_norm: 8.448739
[[34m2025-10-03 23:29:05[0m] Step: 173, Training Logs: loss_final: 1.403031, loss_mean: 1.365209, proj_loss: -0.106527, loss_mean_cls: 0.112885, deep_loss: 0.031465, grad_norm: 5.855443
[[34m2025-10-03 23:29:06[0m] Step: 174, Training Logs: loss_final: 1.402457, loss_mean: 1.360868, proj_loss: -0.102728, loss_mean_cls: 0.113030, deep_loss: 0.031286, grad_norm: 8.648826
[[34m2025-10-03 23:29:07[0m] Step: 175, Training Logs: loss_final: 1.417150, loss_mean: 1.375636, proj_loss: -0.103205, loss_mean_cls: 0.112253, deep_loss: 0.032467, grad_norm: 10.242246
[[34m2025-10-03 23:29:09[0m] Step: 176, Training Logs: loss_final: 1.396255, loss_mean: 1.353704, proj_loss: -0.100426, loss_mean_cls: 0.111898, deep_loss: 0.031079, grad_norm: 7.625493
[[34m2025-10-03 23:29:10[0m] Step: 177, Training Logs: loss_final: 1.395324, loss_mean: 1.352527, proj_loss: -0.100765, loss_mean_cls: 0.111823, deep_loss: 0.031739, grad_norm: 4.355269
[[34m2025-10-03 23:29:11[0m] Step: 178, Training Logs: loss_final: 1.403165, loss_mean: 1.355837, proj_loss: -0.097419, loss_mean_cls: 0.112871, deep_loss: 0.031876, grad_norm: 6.895165
[[34m2025-10-03 23:29:12[0m] Step: 179, Training Logs: loss_final: 1.391170, loss_mean: 1.341726, proj_loss: -0.094157, loss_mean_cls: 0.112569, deep_loss: 0.031032, grad_norm: 7.750444
[[34m2025-10-03 23:29:13[0m] Step: 180, Training Logs: loss_final: 1.398921, loss_mean: 1.348161, proj_loss: -0.093213, loss_mean_cls: 0.112516, deep_loss: 0.031457, grad_norm: 6.311216
[[34m2025-10-03 23:29:14[0m] Step: 181, Training Logs: loss_final: 1.382400, loss_mean: 1.329812, proj_loss: -0.092263, loss_mean_cls: 0.113438, deep_loss: 0.031414, grad_norm: 6.435836
[[34m2025-10-03 23:29:16[0m] Step: 182, Training Logs: loss_final: 1.362164, loss_mean: 1.312003, proj_loss: -0.093475, loss_mean_cls: 0.112565, deep_loss: 0.031070, grad_norm: 7.543533
[[34m2025-10-03 23:29:17[0m] Step: 183, Training Logs: loss_final: 1.400843, loss_mean: 1.347540, proj_loss: -0.089944, loss_mean_cls: 0.112221, deep_loss: 0.031026, grad_norm: 5.997911
[[34m2025-10-03 23:29:18[0m] Step: 184, Training Logs: loss_final: 1.413031, loss_mean: 1.354998, proj_loss: -0.086784, loss_mean_cls: 0.113160, deep_loss: 0.031656, grad_norm: 9.856444
[[34m2025-10-03 23:29:19[0m] Step: 185, Training Logs: loss_final: 1.393960, loss_mean: 1.336489, proj_loss: -0.086147, loss_mean_cls: 0.112424, deep_loss: 0.031193, grad_norm: 8.967134
[[34m2025-10-03 23:29:20[0m] Step: 186, Training Logs: loss_final: 1.443590, loss_mean: 1.384870, proj_loss: -0.085698, loss_mean_cls: 0.111669, deep_loss: 0.032748, grad_norm: 7.262801
[[34m2025-10-03 23:29:21[0m] Step: 187, Training Logs: loss_final: 1.402472, loss_mean: 1.341366, proj_loss: -0.082756, loss_mean_cls: 0.112557, deep_loss: 0.031305, grad_norm: 8.321965
[[34m2025-10-03 23:29:23[0m] Step: 188, Training Logs: loss_final: 1.411583, loss_mean: 1.348082, proj_loss: -0.080918, loss_mean_cls: 0.112746, deep_loss: 0.031673, grad_norm: 11.007234
[[34m2025-10-03 23:29:24[0m] Step: 189, Training Logs: loss_final: 1.434234, loss_mean: 1.371919, proj_loss: -0.081905, loss_mean_cls: 0.112925, deep_loss: 0.031295, grad_norm: 11.933158
[[34m2025-10-03 23:29:24[0m] Step: 190, Training Logs: loss_final: 1.471140, loss_mean: 1.410117, proj_loss: -0.082934, loss_mean_cls: 0.111642, deep_loss: 0.032315, grad_norm: 12.619130
[[34m2025-10-03 23:29:25[0m] Step: 191, Training Logs: loss_final: 1.442700, loss_mean: 1.377816, proj_loss: -0.079084, loss_mean_cls: 0.112422, deep_loss: 0.031546, grad_norm: 10.830460
[[34m2025-10-03 23:29:26[0m] Step: 192, Training Logs: loss_final: 1.444223, loss_mean: 1.381376, proj_loss: -0.081665, loss_mean_cls: 0.112851, deep_loss: 0.031661, grad_norm: 9.036086
[[34m2025-10-03 23:29:27[0m] Step: 193, Training Logs: loss_final: 1.429248, loss_mean: 1.366231, proj_loss: -0.080795, loss_mean_cls: 0.111878, deep_loss: 0.031934, grad_norm: 8.611487
[[34m2025-10-03 23:29:28[0m] Step: 194, Training Logs: loss_final: 1.446906, loss_mean: 1.379024, proj_loss: -0.077870, loss_mean_cls: 0.113173, deep_loss: 0.032579, grad_norm: 8.611091
[[34m2025-10-03 23:29:30[0m] Step: 195, Training Logs: loss_final: 1.439918, loss_mean: 1.374618, proj_loss: -0.079449, loss_mean_cls: 0.111941, deep_loss: 0.032809, grad_norm: 9.845054
[[34m2025-10-03 23:29:31[0m] Step: 196, Training Logs: loss_final: 1.443901, loss_mean: 1.375011, proj_loss: -0.076441, loss_mean_cls: 0.112756, deep_loss: 0.032575, grad_norm: 11.171990
[[34m2025-10-03 23:29:32[0m] Step: 197, Training Logs: loss_final: 1.451708, loss_mean: 1.383616, proj_loss: -0.077484, loss_mean_cls: 0.111663, deep_loss: 0.033913, grad_norm: 9.233278
[[34m2025-10-03 23:29:33[0m] Step: 198, Training Logs: loss_final: 1.466645, loss_mean: 1.393279, proj_loss: -0.073469, loss_mean_cls: 0.112603, deep_loss: 0.034233, grad_norm: 11.352879
[[34m2025-10-03 23:29:34[0m] Step: 199, Training Logs: loss_final: 1.433395, loss_mean: 1.362514, proj_loss: -0.072681, loss_mean_cls: 0.111451, deep_loss: 0.032111, grad_norm: 9.892406
[[34m2025-10-03 23:29:35[0m] Step: 200, Training Logs: loss_final: 1.500439, loss_mean: 1.426507, proj_loss: -0.071807, loss_mean_cls: 0.112147, deep_loss: 0.033592, grad_norm: 14.541034
[[34m2025-10-03 23:29:37[0m] Step: 201, Training Logs: loss_final: 1.468252, loss_mean: 1.395594, proj_loss: -0.073615, loss_mean_cls: 0.112533, deep_loss: 0.033740, grad_norm: 9.715845
[[34m2025-10-03 23:29:38[0m] Step: 202, Training Logs: loss_final: 1.444530, loss_mean: 1.371321, proj_loss: -0.072400, loss_mean_cls: 0.113007, deep_loss: 0.032602, grad_norm: 8.106879
[[34m2025-10-03 23:29:39[0m] Step: 203, Training Logs: loss_final: 1.482746, loss_mean: 1.408440, proj_loss: -0.072587, loss_mean_cls: 0.113296, deep_loss: 0.033597, grad_norm: 11.468236
[[34m2025-10-03 23:29:40[0m] Step: 204, Training Logs: loss_final: 1.488223, loss_mean: 1.414891, proj_loss: -0.072679, loss_mean_cls: 0.112777, deep_loss: 0.033235, grad_norm: 13.384549
[[34m2025-10-03 23:29:41[0m] Step: 205, Training Logs: loss_final: 1.497773, loss_mean: 1.423674, proj_loss: -0.072122, loss_mean_cls: 0.112857, deep_loss: 0.033365, grad_norm: 12.933145
[[34m2025-10-03 23:29:42[0m] Step: 206, Training Logs: loss_final: 1.482721, loss_mean: 1.410698, proj_loss: -0.072495, loss_mean_cls: 0.111660, deep_loss: 0.032859, grad_norm: 8.755456
[[34m2025-10-03 23:29:44[0m] Step: 207, Training Logs: loss_final: 1.454744, loss_mean: 1.378407, proj_loss: -0.068944, loss_mean_cls: 0.112918, deep_loss: 0.032364, grad_norm: 7.739180
[[34m2025-10-03 23:29:45[0m] Step: 208, Training Logs: loss_final: 1.470688, loss_mean: 1.394011, proj_loss: -0.069394, loss_mean_cls: 0.112552, deep_loss: 0.033519, grad_norm: 9.465329
[[34m2025-10-03 23:29:46[0m] Step: 209, Training Logs: loss_final: 1.452545, loss_mean: 1.376480, proj_loss: -0.069913, loss_mean_cls: 0.111977, deep_loss: 0.034002, grad_norm: 9.668038
[[34m2025-10-03 23:29:47[0m] Step: 210, Training Logs: loss_final: 1.459300, loss_mean: 1.383540, proj_loss: -0.069496, loss_mean_cls: 0.111930, deep_loss: 0.033327, grad_norm: 6.874676
[[34m2025-10-03 23:29:48[0m] Step: 211, Training Logs: loss_final: 1.496507, loss_mean: 1.421349, proj_loss: -0.069865, loss_mean_cls: 0.111661, deep_loss: 0.033362, grad_norm: 11.583383
[[34m2025-10-03 23:29:49[0m] Step: 212, Training Logs: loss_final: 1.476429, loss_mean: 1.402185, proj_loss: -0.070033, loss_mean_cls: 0.111576, deep_loss: 0.032701, grad_norm: 8.801100
[[34m2025-10-03 23:29:51[0m] Step: 213, Training Logs: loss_final: 1.523403, loss_mean: 1.440961, proj_loss: -0.063902, loss_mean_cls: 0.111766, deep_loss: 0.034578, grad_norm: 14.696023
[[34m2025-10-03 23:29:52[0m] Step: 214, Training Logs: loss_final: 1.468176, loss_mean: 1.388440, proj_loss: -0.065164, loss_mean_cls: 0.112085, deep_loss: 0.032815, grad_norm: 10.195918
[[34m2025-10-03 23:29:53[0m] Step: 215, Training Logs: loss_final: 1.460700, loss_mean: 1.384936, proj_loss: -0.068884, loss_mean_cls: 0.111944, deep_loss: 0.032705, grad_norm: 10.595022
[[34m2025-10-03 23:29:54[0m] Step: 216, Training Logs: loss_final: 1.439855, loss_mean: 1.361138, proj_loss: -0.066223, loss_mean_cls: 0.112416, deep_loss: 0.032524, grad_norm: 9.546990
[[34m2025-10-03 23:29:55[0m] Step: 217, Training Logs: loss_final: 1.415211, loss_mean: 1.334132, proj_loss: -0.062916, loss_mean_cls: 0.112315, deep_loss: 0.031679, grad_norm: 10.298566
[[34m2025-10-03 23:29:56[0m] Step: 218, Training Logs: loss_final: 1.456714, loss_mean: 1.375347, proj_loss: -0.064493, loss_mean_cls: 0.112759, deep_loss: 0.033102, grad_norm: 12.340514
[[34m2025-10-03 23:29:58[0m] Step: 219, Training Logs: loss_final: 1.461719, loss_mean: 1.378056, proj_loss: -0.061757, loss_mean_cls: 0.112526, deep_loss: 0.032895, grad_norm: 11.272434
[[34m2025-10-03 23:29:59[0m] Step: 220, Training Logs: loss_final: 1.522101, loss_mean: 1.431254, proj_loss: -0.057078, loss_mean_cls: 0.113080, deep_loss: 0.034844, grad_norm: 20.129248
[[34m2025-10-03 23:29:59[0m] Step: 221, Training Logs: loss_final: 1.504684, loss_mean: 1.420480, proj_loss: -0.061823, loss_mean_cls: 0.112509, deep_loss: 0.033519, grad_norm: 18.768538
[[34m2025-10-03 23:30:00[0m] Step: 222, Training Logs: loss_final: 1.521256, loss_mean: 1.440631, proj_loss: -0.065822, loss_mean_cls: 0.111751, deep_loss: 0.034696, grad_norm: 16.351147
[[34m2025-10-03 23:30:01[0m] Step: 223, Training Logs: loss_final: 1.520712, loss_mean: 1.440292, proj_loss: -0.066120, loss_mean_cls: 0.111851, deep_loss: 0.034689, grad_norm: 14.643217
[[34m2025-10-03 23:30:02[0m] Step: 224, Training Logs: loss_final: 1.463663, loss_mean: 1.381855, proj_loss: -0.064458, loss_mean_cls: 0.113307, deep_loss: 0.032959, grad_norm: 11.955624
[[34m2025-10-03 23:30:03[0m] Step: 225, Training Logs: loss_final: 1.467516, loss_mean: 1.385227, proj_loss: -0.062877, loss_mean_cls: 0.113033, deep_loss: 0.032132, grad_norm: 10.825857
[[34m2025-10-03 23:30:04[0m] Step: 226, Training Logs: loss_final: 1.501134, loss_mean: 1.417265, proj_loss: -0.061095, loss_mean_cls: 0.112315, deep_loss: 0.032648, grad_norm: 12.031693
[[34m2025-10-03 23:30:06[0m] Step: 227, Training Logs: loss_final: 1.467949, loss_mean: 1.382184, proj_loss: -0.059523, loss_mean_cls: 0.112515, deep_loss: 0.032773, grad_norm: 13.886312
[[34m2025-10-03 23:30:07[0m] Step: 228, Training Logs: loss_final: 1.471458, loss_mean: 1.384978, proj_loss: -0.058088, loss_mean_cls: 0.112433, deep_loss: 0.032136, grad_norm: 11.937671
[[34m2025-10-03 23:30:08[0m] Step: 229, Training Logs: loss_final: 1.439925, loss_mean: 1.356014, proj_loss: -0.061097, loss_mean_cls: 0.112968, deep_loss: 0.032041, grad_norm: 11.276637
[[34m2025-10-03 23:30:09[0m] Step: 230, Training Logs: loss_final: 1.449107, loss_mean: 1.363540, proj_loss: -0.059158, loss_mean_cls: 0.111914, deep_loss: 0.032811, grad_norm: 9.581201
[[34m2025-10-03 23:30:10[0m] Step: 231, Training Logs: loss_final: 1.483539, loss_mean: 1.397747, proj_loss: -0.058755, loss_mean_cls: 0.111369, deep_loss: 0.033178, grad_norm: 10.589252
[[34m2025-10-03 23:30:11[0m] Step: 232, Training Logs: loss_final: 1.449200, loss_mean: 1.362059, proj_loss: -0.057046, loss_mean_cls: 0.112812, deep_loss: 0.031375, grad_norm: 5.679170
[[34m2025-10-03 23:30:13[0m] Step: 233, Training Logs: loss_final: 1.537347, loss_mean: 1.454308, proj_loss: -0.062575, loss_mean_cls: 0.112069, deep_loss: 0.033545, grad_norm: 18.645899
[[34m2025-10-03 23:30:14[0m] Step: 234, Training Logs: loss_final: 1.484689, loss_mean: 1.398094, proj_loss: -0.059144, loss_mean_cls: 0.111611, deep_loss: 0.034128, grad_norm: 14.846298
[[34m2025-10-03 23:30:15[0m] Step: 235, Training Logs: loss_final: 1.426984, loss_mean: 1.337040, proj_loss: -0.055244, loss_mean_cls: 0.112979, deep_loss: 0.032209, grad_norm: 10.023929
[[34m2025-10-03 23:30:16[0m] Step: 236, Training Logs: loss_final: 1.459354, loss_mean: 1.372871, proj_loss: -0.058607, loss_mean_cls: 0.112673, deep_loss: 0.032416, grad_norm: 14.439604
[[34m2025-10-03 23:30:17[0m] Step: 237, Training Logs: loss_final: 1.453424, loss_mean: 1.364250, proj_loss: -0.054532, loss_mean_cls: 0.112260, deep_loss: 0.031447, grad_norm: 13.405661
[[34m2025-10-03 23:30:18[0m] Step: 238, Training Logs: loss_final: 1.455533, loss_mean: 1.366196, proj_loss: -0.054648, loss_mean_cls: 0.111797, deep_loss: 0.032187, grad_norm: 12.375463
[[34m2025-10-03 23:30:20[0m] Step: 239, Training Logs: loss_final: 1.467116, loss_mean: 1.377358, proj_loss: -0.055015, loss_mean_cls: 0.112731, deep_loss: 0.032042, grad_norm: 8.831139
[[34m2025-10-03 23:30:21[0m] Step: 240, Training Logs: loss_final: 1.415484, loss_mean: 1.328746, proj_loss: -0.056906, loss_mean_cls: 0.112814, deep_loss: 0.030830, grad_norm: 7.267108
[[34m2025-10-03 23:30:22[0m] Step: 241, Training Logs: loss_final: 1.441092, loss_mean: 1.351455, proj_loss: -0.053592, loss_mean_cls: 0.111972, deep_loss: 0.031257, grad_norm: 10.264643
[[34m2025-10-03 23:30:23[0m] Step: 242, Training Logs: loss_final: 1.428613, loss_mean: 1.339693, proj_loss: -0.054656, loss_mean_cls: 0.112233, deep_loss: 0.031342, grad_norm: 9.471435
[[34m2025-10-03 23:30:24[0m] Step: 243, Training Logs: loss_final: 1.408039, loss_mean: 1.316950, proj_loss: -0.052251, loss_mean_cls: 0.112379, deep_loss: 0.030962, grad_norm: 7.147062
[[34m2025-10-03 23:30:25[0m] Step: 244, Training Logs: loss_final: 1.508846, loss_mean: 1.409900, proj_loss: -0.047595, loss_mean_cls: 0.112628, deep_loss: 0.033913, grad_norm: 17.083809
[[34m2025-10-03 23:30:27[0m] Step: 245, Training Logs: loss_final: 1.461079, loss_mean: 1.365739, proj_loss: -0.049659, loss_mean_cls: 0.112630, deep_loss: 0.032369, grad_norm: 14.112686
[[34m2025-10-03 23:30:28[0m] Step: 246, Training Logs: loss_final: 1.467833, loss_mean: 1.375445, proj_loss: -0.052816, loss_mean_cls: 0.112579, deep_loss: 0.032626, grad_norm: 13.490535
[[34m2025-10-03 23:30:29[0m] Step: 247, Training Logs: loss_final: 1.455218, loss_mean: 1.360755, proj_loss: -0.049685, loss_mean_cls: 0.112647, deep_loss: 0.031500, grad_norm: 10.386071
[[34m2025-10-03 23:30:30[0m] Step: 248, Training Logs: loss_final: 1.480990, loss_mean: 1.380153, proj_loss: -0.044360, loss_mean_cls: 0.112961, deep_loss: 0.032236, grad_norm: 16.277384
[[34m2025-10-03 23:30:31[0m] Step: 249, Training Logs: loss_final: 1.456288, loss_mean: 1.357517, proj_loss: -0.045667, loss_mean_cls: 0.112664, deep_loss: 0.031774, grad_norm: 13.412731
[[34m2025-10-03 23:30:32[0m] Step: 250, Training Logs: loss_final: 1.501701, loss_mean: 1.405885, proj_loss: -0.049720, loss_mean_cls: 0.112285, deep_loss: 0.033251, grad_norm: 14.693225
[[34m2025-10-03 23:30:34[0m] Step: 251, Training Logs: loss_final: 1.483535, loss_mean: 1.388271, proj_loss: -0.050908, loss_mean_cls: 0.112684, deep_loss: 0.033488, grad_norm: 17.733927
[[34m2025-10-03 23:30:34[0m] Step: 252, Training Logs: loss_final: 1.492802, loss_mean: 1.397452, proj_loss: -0.050939, loss_mean_cls: 0.113200, deep_loss: 0.033088, grad_norm: 14.278706
[[34m2025-10-03 23:30:35[0m] Step: 253, Training Logs: loss_final: 1.471654, loss_mean: 1.376560, proj_loss: -0.049628, loss_mean_cls: 0.112175, deep_loss: 0.032547, grad_norm: 13.183216
[[34m2025-10-03 23:30:36[0m] Step: 254, Training Logs: loss_final: 1.499816, loss_mean: 1.402485, proj_loss: -0.046734, loss_mean_cls: 0.111808, deep_loss: 0.032257, grad_norm: 13.169455
[[34m2025-10-03 23:30:37[0m] Step: 255, Training Logs: loss_final: 1.433853, loss_mean: 1.338047, proj_loss: -0.047672, loss_mean_cls: 0.112914, deep_loss: 0.030563, grad_norm: 8.473025
[[34m2025-10-03 23:30:38[0m] Step: 256, Training Logs: loss_final: 1.472191, loss_mean: 1.383074, proj_loss: -0.054486, loss_mean_cls: 0.111876, deep_loss: 0.031727, grad_norm: 11.601804
[[34m2025-10-03 23:30:39[0m] Step: 257, Training Logs: loss_final: 1.522653, loss_mean: 1.430803, proj_loss: -0.053906, loss_mean_cls: 0.112726, deep_loss: 0.033028, grad_norm: 19.612886
[[34m2025-10-03 23:30:41[0m] Step: 258, Training Logs: loss_final: 1.529552, loss_mean: 1.441303, proj_loss: -0.056984, loss_mean_cls: 0.112343, deep_loss: 0.032889, grad_norm: 17.439146
[[34m2025-10-03 23:30:42[0m] Step: 259, Training Logs: loss_final: 1.458791, loss_mean: 1.368877, proj_loss: -0.053684, loss_mean_cls: 0.112748, deep_loss: 0.030851, grad_norm: 10.269534
[[34m2025-10-03 23:30:43[0m] Step: 260, Training Logs: loss_final: 1.431213, loss_mean: 1.337948, proj_loss: -0.051338, loss_mean_cls: 0.113163, deep_loss: 0.031439, grad_norm: 5.819732
[[34m2025-10-03 23:30:44[0m] Step: 261, Training Logs: loss_final: 1.441547, loss_mean: 1.345690, proj_loss: -0.048626, loss_mean_cls: 0.113106, deep_loss: 0.031378, grad_norm: 11.367455
[[34m2025-10-03 23:30:45[0m] Step: 262, Training Logs: loss_final: 1.460011, loss_mean: 1.366927, proj_loss: -0.050172, loss_mean_cls: 0.111941, deep_loss: 0.031315, grad_norm: 11.801347
[[34m2025-10-03 23:30:46[0m] Step: 263, Training Logs: loss_final: 1.460586, loss_mean: 1.370163, proj_loss: -0.053067, loss_mean_cls: 0.111451, deep_loss: 0.032039, grad_norm: 7.397910
[[34m2025-10-03 23:30:48[0m] Step: 264, Training Logs: loss_final: 1.416038, loss_mean: 1.322811, proj_loss: -0.050370, loss_mean_cls: 0.111936, deep_loss: 0.031662, grad_norm: 6.835646
[[34m2025-10-03 23:30:49[0m] Step: 265, Training Logs: loss_final: 1.438426, loss_mean: 1.343638, proj_loss: -0.048391, loss_mean_cls: 0.112380, deep_loss: 0.030800, grad_norm: 6.702770
[[34m2025-10-03 23:30:50[0m] Step: 266, Training Logs: loss_final: 1.408563, loss_mean: 1.313537, proj_loss: -0.047922, loss_mean_cls: 0.112879, deep_loss: 0.030068, grad_norm: 7.742836
[[34m2025-10-03 23:30:51[0m] Step: 267, Training Logs: loss_final: 1.399810, loss_mean: 1.305955, proj_loss: -0.049043, loss_mean_cls: 0.112219, deep_loss: 0.030678, grad_norm: 4.679369
[[34m2025-10-03 23:30:52[0m] Step: 268, Training Logs: loss_final: 1.402230, loss_mean: 1.306075, proj_loss: -0.047206, loss_mean_cls: 0.113422, deep_loss: 0.029938, grad_norm: 8.335984
[[34m2025-10-03 23:30:53[0m] Step: 269, Training Logs: loss_final: 1.454472, loss_mean: 1.356442, proj_loss: -0.046014, loss_mean_cls: 0.112985, deep_loss: 0.031060, grad_norm: 9.005584
[[34m2025-10-03 23:30:55[0m] Step: 270, Training Logs: loss_final: 1.403386, loss_mean: 1.307275, proj_loss: -0.046748, loss_mean_cls: 0.113370, deep_loss: 0.029489, grad_norm: 6.344136
[[34m2025-10-03 23:30:56[0m] Step: 271, Training Logs: loss_final: 1.481913, loss_mean: 1.383887, proj_loss: -0.046264, loss_mean_cls: 0.112579, deep_loss: 0.031711, grad_norm: 15.069472
[[34m2025-10-03 23:30:57[0m] Step: 272, Training Logs: loss_final: 1.420496, loss_mean: 1.321422, proj_loss: -0.044239, loss_mean_cls: 0.112558, deep_loss: 0.030756, grad_norm: 8.732047
[[34m2025-10-03 23:30:58[0m] Step: 273, Training Logs: loss_final: 1.459719, loss_mean: 1.357554, proj_loss: -0.041916, loss_mean_cls: 0.113002, deep_loss: 0.031079, grad_norm: 13.642933
[[34m2025-10-03 23:30:59[0m] Step: 274, Training Logs: loss_final: 1.461413, loss_mean: 1.359020, proj_loss: -0.041724, loss_mean_cls: 0.112253, deep_loss: 0.031865, grad_norm: 9.596684
[[34m2025-10-03 23:31:00[0m] Step: 275, Training Logs: loss_final: 1.457471, loss_mean: 1.358283, proj_loss: -0.043188, loss_mean_cls: 0.112076, deep_loss: 0.030299, grad_norm: 11.562511
[[34m2025-10-03 23:31:02[0m] Step: 276, Training Logs: loss_final: 1.468273, loss_mean: 1.366156, proj_loss: -0.041798, loss_mean_cls: 0.112177, deep_loss: 0.031738, grad_norm: 10.658051
[[34m2025-10-03 23:31:03[0m] Step: 277, Training Logs: loss_final: 1.498697, loss_mean: 1.396373, proj_loss: -0.042676, loss_mean_cls: 0.112087, deep_loss: 0.032912, grad_norm: 12.790602
[[34m2025-10-03 23:31:04[0m] Step: 278, Training Logs: loss_final: 1.469942, loss_mean: 1.366557, proj_loss: -0.043224, loss_mean_cls: 0.113471, deep_loss: 0.033138, grad_norm: 10.245526
[[34m2025-10-03 23:31:05[0m] Step: 279, Training Logs: loss_final: 1.474896, loss_mean: 1.377216, proj_loss: -0.046479, loss_mean_cls: 0.112682, deep_loss: 0.031477, grad_norm: 8.130998
[[34m2025-10-03 23:31:06[0m] Step: 280, Training Logs: loss_final: 1.481602, loss_mean: 1.382840, proj_loss: -0.044900, loss_mean_cls: 0.112327, deep_loss: 0.031335, grad_norm: 14.048981
[[34m2025-10-03 23:31:07[0m] Step: 281, Training Logs: loss_final: 1.509152, loss_mean: 1.406548, proj_loss: -0.041680, loss_mean_cls: 0.112549, deep_loss: 0.031734, grad_norm: 13.649562
[[34m2025-10-03 23:31:08[0m] Step: 282, Training Logs: loss_final: 1.433485, loss_mean: 1.330360, proj_loss: -0.040615, loss_mean_cls: 0.112706, deep_loss: 0.031033, grad_norm: 7.592409
[[34m2025-10-03 23:31:09[0m] Step: 283, Training Logs: loss_final: 1.438917, loss_mean: 1.335018, proj_loss: -0.040242, loss_mean_cls: 0.113386, deep_loss: 0.030754, grad_norm: 9.449179
[[34m2025-10-03 23:31:10[0m] Step: 284, Training Logs: loss_final: 1.431772, loss_mean: 1.328650, proj_loss: -0.039312, loss_mean_cls: 0.112838, deep_loss: 0.029595, grad_norm: 5.490056
[[34m2025-10-03 23:31:11[0m] Step: 285, Training Logs: loss_final: 1.450380, loss_mean: 1.345998, proj_loss: -0.039723, loss_mean_cls: 0.112965, deep_loss: 0.031139, grad_norm: 8.419930
[[34m2025-10-03 23:31:12[0m] Step: 286, Training Logs: loss_final: 1.432677, loss_mean: 1.328508, proj_loss: -0.039883, loss_mean_cls: 0.112860, deep_loss: 0.031192, grad_norm: 10.001864
[[34m2025-10-03 23:31:13[0m] Step: 287, Training Logs: loss_final: 1.447507, loss_mean: 1.342283, proj_loss: -0.039454, loss_mean_cls: 0.112565, deep_loss: 0.032113, grad_norm: 8.669090
[[34m2025-10-03 23:31:14[0m] Step: 288, Training Logs: loss_final: 1.440772, loss_mean: 1.335393, proj_loss: -0.039440, loss_mean_cls: 0.113253, deep_loss: 0.031565, grad_norm: 6.909741
[[34m2025-10-03 23:31:16[0m] Step: 289, Training Logs: loss_final: 1.449145, loss_mean: 1.345648, proj_loss: -0.040158, loss_mean_cls: 0.112429, deep_loss: 0.031226, grad_norm: 6.408525
[[34m2025-10-03 23:31:17[0m] Step: 290, Training Logs: loss_final: 1.431966, loss_mean: 1.327605, proj_loss: -0.039752, loss_mean_cls: 0.113550, deep_loss: 0.030563, grad_norm: 9.318441
[[34m2025-10-03 23:31:18[0m] Step: 291, Training Logs: loss_final: 1.434104, loss_mean: 1.330311, proj_loss: -0.040080, loss_mean_cls: 0.112456, deep_loss: 0.031417, grad_norm: 9.958828
[[34m2025-10-03 23:31:19[0m] Step: 292, Training Logs: loss_final: 1.429529, loss_mean: 1.322695, proj_loss: -0.037614, loss_mean_cls: 0.113249, deep_loss: 0.031199, grad_norm: 10.065664
[[34m2025-10-03 23:31:20[0m] Step: 293, Training Logs: loss_final: 1.422319, loss_mean: 1.316112, proj_loss: -0.038398, loss_mean_cls: 0.113425, deep_loss: 0.031181, grad_norm: 6.532040
[[34m2025-10-03 23:31:21[0m] Step: 294, Training Logs: loss_final: 1.437780, loss_mean: 1.330335, proj_loss: -0.037856, loss_mean_cls: 0.113719, deep_loss: 0.031583, grad_norm: 7.536091
[[34m2025-10-03 23:31:23[0m] Step: 295, Training Logs: loss_final: 1.428764, loss_mean: 1.319922, proj_loss: -0.035070, loss_mean_cls: 0.113158, deep_loss: 0.030754, grad_norm: 9.079846
[[34m2025-10-03 23:31:24[0m] Step: 296, Training Logs: loss_final: 1.430983, loss_mean: 1.322862, proj_loss: -0.036624, loss_mean_cls: 0.113683, deep_loss: 0.031061, grad_norm: 10.290722
[[34m2025-10-03 23:31:25[0m] Step: 297, Training Logs: loss_final: 1.418492, loss_mean: 1.309023, proj_loss: -0.034560, loss_mean_cls: 0.113274, deep_loss: 0.030755, grad_norm: 8.827503
[[34m2025-10-03 23:31:26[0m] Step: 298, Training Logs: loss_final: 1.421591, loss_mean: 1.313662, proj_loss: -0.036564, loss_mean_cls: 0.112746, deep_loss: 0.031747, grad_norm: 4.556936
[[34m2025-10-03 23:31:27[0m] Step: 299, Training Logs: loss_final: 1.453000, loss_mean: 1.347072, proj_loss: -0.037714, loss_mean_cls: 0.111992, deep_loss: 0.031650, grad_norm: 9.840333
[[34m2025-10-03 23:31:28[0m] Step: 300, Training Logs: loss_final: 1.458294, loss_mean: 1.349782, proj_loss: -0.036294, loss_mean_cls: 0.112821, deep_loss: 0.031985, grad_norm: 8.859885
[[34m2025-10-03 23:31:30[0m] Step: 301, Training Logs: loss_final: 1.420214, loss_mean: 1.313300, proj_loss: -0.036893, loss_mean_cls: 0.112246, deep_loss: 0.031561, grad_norm: 7.368115
[[34m2025-10-03 23:31:31[0m] Step: 302, Training Logs: loss_final: 1.423450, loss_mean: 1.318205, proj_loss: -0.037688, loss_mean_cls: 0.112552, deep_loss: 0.030380, grad_norm: 6.389820
[[34m2025-10-03 23:31:32[0m] Step: 303, Training Logs: loss_final: 1.429960, loss_mean: 1.325820, proj_loss: -0.038924, loss_mean_cls: 0.112523, deep_loss: 0.030540, grad_norm: 13.650489
[[34m2025-10-03 23:31:33[0m] Step: 304, Training Logs: loss_final: 1.398741, loss_mean: 1.290680, proj_loss: -0.035399, loss_mean_cls: 0.113282, deep_loss: 0.030178, grad_norm: 10.729561
[[34m2025-10-03 23:31:34[0m] Step: 305, Training Logs: loss_final: 1.379993, loss_mean: 1.271127, proj_loss: -0.035141, loss_mean_cls: 0.114124, deep_loss: 0.029883, grad_norm: 13.399163
[[34m2025-10-03 23:31:35[0m] Step: 306, Training Logs: loss_final: 1.381565, loss_mean: 1.271841, proj_loss: -0.033379, loss_mean_cls: 0.113663, deep_loss: 0.029439, grad_norm: 10.486160
[[34m2025-10-03 23:31:37[0m] Step: 307, Training Logs: loss_final: 1.395161, loss_mean: 1.291297, proj_loss: -0.038074, loss_mean_cls: 0.111722, deep_loss: 0.030216, grad_norm: 4.829980
[[34m2025-10-03 23:31:38[0m] Step: 308, Training Logs: loss_final: 1.422660, loss_mean: 1.314152, proj_loss: -0.035099, loss_mean_cls: 0.113104, deep_loss: 0.030503, grad_norm: 7.406155
[[34m2025-10-03 23:31:39[0m] Step: 309, Training Logs: loss_final: 1.431609, loss_mean: 1.325953, proj_loss: -0.036761, loss_mean_cls: 0.111772, deep_loss: 0.030645, grad_norm: 8.802998
[[34m2025-10-03 23:31:40[0m] Step: 310, Training Logs: loss_final: 1.413424, loss_mean: 1.305936, proj_loss: -0.035748, loss_mean_cls: 0.112353, deep_loss: 0.030883, grad_norm: 6.591395
[[34m2025-10-03 23:31:41[0m] Step: 311, Training Logs: loss_final: 1.422830, loss_mean: 1.316602, proj_loss: -0.037855, loss_mean_cls: 0.112991, deep_loss: 0.031092, grad_norm: 8.539668
[[34m2025-10-03 23:31:43[0m] Step: 312, Training Logs: loss_final: 1.422435, loss_mean: 1.314697, proj_loss: -0.036587, loss_mean_cls: 0.113008, deep_loss: 0.031318, grad_norm: 9.571646
[[34m2025-10-03 23:31:43[0m] Step: 313, Training Logs: loss_final: 1.478131, loss_mean: 1.365564, proj_loss: -0.032424, loss_mean_cls: 0.112833, deep_loss: 0.032157, grad_norm: 21.408480
[[34m2025-10-03 23:31:44[0m] Step: 314, Training Logs: loss_final: 1.499131, loss_mean: 1.387352, proj_loss: -0.033167, loss_mean_cls: 0.112191, deep_loss: 0.032755, grad_norm: 24.661804
[[34m2025-10-03 23:31:45[0m] Step: 315, Training Logs: loss_final: 1.436046, loss_mean: 1.325944, proj_loss: -0.034053, loss_mean_cls: 0.112842, deep_loss: 0.031312, grad_norm: 13.937848
[[34m2025-10-03 23:31:46[0m] Step: 316, Training Logs: loss_final: 1.417562, loss_mean: 1.309291, proj_loss: -0.036475, loss_mean_cls: 0.113035, deep_loss: 0.031712, grad_norm: 13.936067
[[34m2025-10-03 23:31:47[0m] Step: 317, Training Logs: loss_final: 1.433763, loss_mean: 1.323004, proj_loss: -0.034204, loss_mean_cls: 0.112928, deep_loss: 0.032035, grad_norm: 16.254974
[[34m2025-10-03 23:31:48[0m] Step: 318, Training Logs: loss_final: 1.429355, loss_mean: 1.321484, proj_loss: -0.035479, loss_mean_cls: 0.112846, deep_loss: 0.030504, grad_norm: 15.704567
[[34m2025-10-03 23:31:50[0m] Step: 319, Training Logs: loss_final: 1.426261, loss_mean: 1.315668, proj_loss: -0.033300, loss_mean_cls: 0.113154, deep_loss: 0.030739, grad_norm: 17.457340
[[34m2025-10-03 23:31:51[0m] Step: 320, Training Logs: loss_final: 1.422285, loss_mean: 1.312272, proj_loss: -0.032450, loss_mean_cls: 0.112392, deep_loss: 0.030071, grad_norm: 10.794660
[[34m2025-10-03 23:31:52[0m] Step: 321, Training Logs: loss_final: 1.429096, loss_mean: 1.322685, proj_loss: -0.036200, loss_mean_cls: 0.112137, deep_loss: 0.030474, grad_norm: 12.144013
[[34m2025-10-03 23:31:53[0m] Step: 322, Training Logs: loss_final: 1.417516, loss_mean: 1.311085, proj_loss: -0.035903, loss_mean_cls: 0.111984, deep_loss: 0.030350, grad_norm: 14.373538
[[34m2025-10-03 23:31:54[0m] Step: 323, Training Logs: loss_final: 1.408829, loss_mean: 1.302629, proj_loss: -0.036231, loss_mean_cls: 0.111768, deep_loss: 0.030663, grad_norm: 8.176553
[[34m2025-10-03 23:31:55[0m] Step: 324, Training Logs: loss_final: 1.422817, loss_mean: 1.314768, proj_loss: -0.034105, loss_mean_cls: 0.112141, deep_loss: 0.030012, grad_norm: 10.763929
[[34m2025-10-03 23:31:57[0m] Step: 325, Training Logs: loss_final: 1.412948, loss_mean: 1.305270, proj_loss: -0.035137, loss_mean_cls: 0.113198, deep_loss: 0.029616, grad_norm: 13.636868
[[34m2025-10-03 23:31:58[0m] Step: 326, Training Logs: loss_final: 1.425327, loss_mean: 1.320510, proj_loss: -0.036817, loss_mean_cls: 0.111663, deep_loss: 0.029972, grad_norm: 11.126214
[[34m2025-10-03 23:31:59[0m] Step: 327, Training Logs: loss_final: 1.415921, loss_mean: 1.308926, proj_loss: -0.034976, loss_mean_cls: 0.111488, deep_loss: 0.030484, grad_norm: 13.269293
[[34m2025-10-03 23:32:00[0m] Step: 328, Training Logs: loss_final: 1.385096, loss_mean: 1.278063, proj_loss: -0.035835, loss_mean_cls: 0.112826, deep_loss: 0.030041, grad_norm: 12.879433
[[34m2025-10-03 23:32:01[0m] Step: 329, Training Logs: loss_final: 1.423908, loss_mean: 1.316765, proj_loss: -0.034624, loss_mean_cls: 0.111976, deep_loss: 0.029790, grad_norm: 14.377299
[[34m2025-10-03 23:32:02[0m] Step: 330, Training Logs: loss_final: 1.436307, loss_mean: 1.328489, proj_loss: -0.034564, loss_mean_cls: 0.112398, deep_loss: 0.029984, grad_norm: 19.867439
[[34m2025-10-03 23:32:04[0m] Step: 331, Training Logs: loss_final: 1.430773, loss_mean: 1.324015, proj_loss: -0.035985, loss_mean_cls: 0.111834, deep_loss: 0.030910, grad_norm: 9.356997
[[34m2025-10-03 23:32:05[0m] Step: 332, Training Logs: loss_final: 1.419464, loss_mean: 1.313388, proj_loss: -0.036622, loss_mean_cls: 0.112315, deep_loss: 0.030384, grad_norm: 12.023056
[[34m2025-10-03 23:32:06[0m] Step: 333, Training Logs: loss_final: 1.455975, loss_mean: 1.350107, proj_loss: -0.037599, loss_mean_cls: 0.112481, deep_loss: 0.030986, grad_norm: 20.148893
[[34m2025-10-03 23:32:07[0m] Step: 334, Training Logs: loss_final: 1.433707, loss_mean: 1.324463, proj_loss: -0.033552, loss_mean_cls: 0.112254, deep_loss: 0.030542, grad_norm: 14.866748
[[34m2025-10-03 23:32:08[0m] Step: 335, Training Logs: loss_final: 1.397200, loss_mean: 1.289531, proj_loss: -0.034141, loss_mean_cls: 0.112358, deep_loss: 0.029453, grad_norm: 9.286371
[[34m2025-10-03 23:32:09[0m] Step: 336, Training Logs: loss_final: 1.431127, loss_mean: 1.324771, proj_loss: -0.036311, loss_mean_cls: 0.112296, deep_loss: 0.030371, grad_norm: 11.157574
[[34m2025-10-03 23:32:11[0m] Step: 337, Training Logs: loss_final: 1.407218, loss_mean: 1.299520, proj_loss: -0.033466, loss_mean_cls: 0.112090, deep_loss: 0.029075, grad_norm: 10.670541
[[34m2025-10-03 23:32:12[0m] Step: 338, Training Logs: loss_final: 1.390473, loss_mean: 1.283811, proj_loss: -0.034329, loss_mean_cls: 0.111840, deep_loss: 0.029152, grad_norm: 11.058802
[[34m2025-10-03 23:32:13[0m] Step: 339, Training Logs: loss_final: 1.406201, loss_mean: 1.299013, proj_loss: -0.035409, loss_mean_cls: 0.112109, deep_loss: 0.030488, grad_norm: 10.062929
[[34m2025-10-03 23:32:14[0m] Step: 340, Training Logs: loss_final: 1.411872, loss_mean: 1.303631, proj_loss: -0.033619, loss_mean_cls: 0.112113, deep_loss: 0.029748, grad_norm: 5.042818
[[34m2025-10-03 23:32:15[0m] Step: 341, Training Logs: loss_final: 1.407900, loss_mean: 1.301787, proj_loss: -0.036222, loss_mean_cls: 0.112573, deep_loss: 0.029762, grad_norm: 10.134043
[[34m2025-10-03 23:32:16[0m] Step: 342, Training Logs: loss_final: 1.426876, loss_mean: 1.321966, proj_loss: -0.037173, loss_mean_cls: 0.111423, deep_loss: 0.030659, grad_norm: 11.268364
[[34m2025-10-03 23:32:18[0m] Step: 343, Training Logs: loss_final: 1.389224, loss_mean: 1.281002, proj_loss: -0.034639, loss_mean_cls: 0.112610, deep_loss: 0.030251, grad_norm: 8.164823
[[34m2025-10-03 23:32:18[0m] Step: 344, Training Logs: loss_final: 1.389379, loss_mean: 1.282151, proj_loss: -0.034785, loss_mean_cls: 0.112795, deep_loss: 0.029218, grad_norm: 6.945299
[[34m2025-10-03 23:32:19[0m] Step: 345, Training Logs: loss_final: 1.397357, loss_mean: 1.289065, proj_loss: -0.034892, loss_mean_cls: 0.112442, deep_loss: 0.030742, grad_norm: 10.714701
[[34m2025-10-03 23:32:20[0m] Step: 346, Training Logs: loss_final: 1.398727, loss_mean: 1.288569, proj_loss: -0.033111, loss_mean_cls: 0.112975, deep_loss: 0.030293, grad_norm: 11.742113
[[34m2025-10-03 23:32:21[0m] Step: 347, Training Logs: loss_final: 1.402261, loss_mean: 1.294014, proj_loss: -0.034743, loss_mean_cls: 0.112090, deep_loss: 0.030900, grad_norm: 10.485180
[[34m2025-10-03 23:32:22[0m] Step: 348, Training Logs: loss_final: 1.380955, loss_mean: 1.270142, proj_loss: -0.032217, loss_mean_cls: 0.112943, deep_loss: 0.030088, grad_norm: 11.580276
[[34m2025-10-03 23:32:23[0m] Step: 349, Training Logs: loss_final: 1.410460, loss_mean: 1.300978, proj_loss: -0.033621, loss_mean_cls: 0.112609, deep_loss: 0.030495, grad_norm: 9.061523
[[34m2025-10-03 23:32:25[0m] Step: 350, Training Logs: loss_final: 1.439336, loss_mean: 1.328685, proj_loss: -0.033170, loss_mean_cls: 0.112625, deep_loss: 0.031196, grad_norm: 17.011898
[[34m2025-10-03 23:32:26[0m] Step: 351, Training Logs: loss_final: 1.415174, loss_mean: 1.304721, proj_loss: -0.032272, loss_mean_cls: 0.112035, deep_loss: 0.030690, grad_norm: 16.121300
[[34m2025-10-03 23:32:27[0m] Step: 352, Training Logs: loss_final: 1.429782, loss_mean: 1.317793, proj_loss: -0.031971, loss_mean_cls: 0.112798, deep_loss: 0.031162, grad_norm: 14.828243
[[34m2025-10-03 23:32:28[0m] Step: 353, Training Logs: loss_final: 1.424722, loss_mean: 1.314456, proj_loss: -0.032606, loss_mean_cls: 0.112103, deep_loss: 0.030769, grad_norm: 13.556525
[[34m2025-10-03 23:32:29[0m] Step: 354, Training Logs: loss_final: 1.419137, loss_mean: 1.308436, proj_loss: -0.031984, loss_mean_cls: 0.112146, deep_loss: 0.030539, grad_norm: 11.564560
[[34m2025-10-03 23:32:30[0m] Step: 355, Training Logs: loss_final: 1.428065, loss_mean: 1.317319, proj_loss: -0.032247, loss_mean_cls: 0.112262, deep_loss: 0.030731, grad_norm: 11.775197
[[34m2025-10-03 23:32:32[0m] Step: 356, Training Logs: loss_final: 1.387069, loss_mean: 1.276989, proj_loss: -0.032883, loss_mean_cls: 0.112602, deep_loss: 0.030362, grad_norm: 11.994526
[[34m2025-10-03 23:32:33[0m] Step: 357, Training Logs: loss_final: 1.424082, loss_mean: 1.315992, proj_loss: -0.034625, loss_mean_cls: 0.111475, deep_loss: 0.031241, grad_norm: 12.095018
[[34m2025-10-03 23:32:34[0m] Step: 358, Training Logs: loss_final: 1.397452, loss_mean: 1.288786, proj_loss: -0.033199, loss_mean_cls: 0.111956, deep_loss: 0.029909, grad_norm: 11.757353
[[34m2025-10-03 23:32:35[0m] Step: 359, Training Logs: loss_final: 1.410635, loss_mean: 1.298042, proj_loss: -0.030768, loss_mean_cls: 0.112125, deep_loss: 0.031237, grad_norm: 12.890035
[[34m2025-10-03 23:32:36[0m] Step: 360, Training Logs: loss_final: 1.361540, loss_mean: 1.248919, proj_loss: -0.030864, loss_mean_cls: 0.112975, deep_loss: 0.030510, grad_norm: 15.451303
[[34m2025-10-03 23:32:37[0m] Step: 361, Training Logs: loss_final: 1.404304, loss_mean: 1.292679, proj_loss: -0.032052, loss_mean_cls: 0.112902, deep_loss: 0.030776, grad_norm: 10.708093
[[34m2025-10-03 23:32:39[0m] Step: 362, Training Logs: loss_final: 1.386198, loss_mean: 1.276388, proj_loss: -0.032118, loss_mean_cls: 0.112683, deep_loss: 0.029245, grad_norm: 12.262922
[[34m2025-10-03 23:32:40[0m] Step: 363, Training Logs: loss_final: 1.399810, loss_mean: 1.289718, proj_loss: -0.032257, loss_mean_cls: 0.112268, deep_loss: 0.030082, grad_norm: 13.296365
[[34m2025-10-03 23:32:41[0m] Step: 364, Training Logs: loss_final: 1.403567, loss_mean: 1.294234, proj_loss: -0.032827, loss_mean_cls: 0.111913, deep_loss: 0.030247, grad_norm: 11.402728
[[34m2025-10-03 23:32:42[0m] Step: 365, Training Logs: loss_final: 1.400232, loss_mean: 1.291092, proj_loss: -0.032533, loss_mean_cls: 0.112294, deep_loss: 0.029378, grad_norm: 10.707154
[[34m2025-10-03 23:32:43[0m] Step: 366, Training Logs: loss_final: 1.390866, loss_mean: 1.281952, proj_loss: -0.032365, loss_mean_cls: 0.111345, deep_loss: 0.029934, grad_norm: 12.573063
[[34m2025-10-03 23:32:45[0m] Step: 367, Training Logs: loss_final: 1.403027, loss_mean: 1.291323, proj_loss: -0.030024, loss_mean_cls: 0.111429, deep_loss: 0.030299, grad_norm: 12.104059
[[34m2025-10-03 23:32:46[0m] Step: 368, Training Logs: loss_final: 1.410411, loss_mean: 1.297876, proj_loss: -0.030039, loss_mean_cls: 0.112347, deep_loss: 0.030228, grad_norm: 8.541345
[[34m2025-10-03 23:32:47[0m] Step: 369, Training Logs: loss_final: 1.424510, loss_mean: 1.316191, proj_loss: -0.033117, loss_mean_cls: 0.111240, deep_loss: 0.030196, grad_norm: 8.431630
[[34m2025-10-03 23:32:48[0m] Step: 370, Training Logs: loss_final: 1.390057, loss_mean: 1.278399, proj_loss: -0.029900, loss_mean_cls: 0.112024, deep_loss: 0.029534, grad_norm: 9.106672
[[34m2025-10-03 23:32:49[0m] Step: 371, Training Logs: loss_final: 1.392177, loss_mean: 1.282585, proj_loss: -0.032145, loss_mean_cls: 0.111915, deep_loss: 0.029823, grad_norm: 11.931505
[[34m2025-10-03 23:32:50[0m] Step: 372, Training Logs: loss_final: 1.411539, loss_mean: 1.301102, proj_loss: -0.031568, loss_mean_cls: 0.111981, deep_loss: 0.030023, grad_norm: 17.948277
[[34m2025-10-03 23:32:52[0m] Step: 373, Training Logs: loss_final: 1.383298, loss_mean: 1.270744, proj_loss: -0.029669, loss_mean_cls: 0.112399, deep_loss: 0.029824, grad_norm: 14.053867
[[34m2025-10-03 23:32:53[0m] Step: 374, Training Logs: loss_final: 1.397990, loss_mean: 1.286643, proj_loss: -0.030746, loss_mean_cls: 0.112247, deep_loss: 0.029846, grad_norm: 11.593571
[[34m2025-10-03 23:32:53[0m] Step: 375, Training Logs: loss_final: 1.421827, loss_mean: 1.309811, proj_loss: -0.029798, loss_mean_cls: 0.111352, deep_loss: 0.030462, grad_norm: 11.357714
[[34m2025-10-03 23:32:54[0m] Step: 376, Training Logs: loss_final: 1.436217, loss_mean: 1.326130, proj_loss: -0.031467, loss_mean_cls: 0.110865, deep_loss: 0.030689, grad_norm: 12.444155
[[34m2025-10-03 23:32:55[0m] Step: 377, Training Logs: loss_final: 1.394143, loss_mean: 1.283783, proj_loss: -0.030914, loss_mean_cls: 0.111280, deep_loss: 0.029993, grad_norm: 11.217196
[[34m2025-10-03 23:32:56[0m] Step: 378, Training Logs: loss_final: 1.393926, loss_mean: 1.283283, proj_loss: -0.031374, loss_mean_cls: 0.112381, deep_loss: 0.029636, grad_norm: 8.214606
[[34m2025-10-03 23:32:57[0m] Step: 379, Training Logs: loss_final: 1.390649, loss_mean: 1.278451, proj_loss: -0.029244, loss_mean_cls: 0.111496, deep_loss: 0.029946, grad_norm: 8.863662
[[34m2025-10-03 23:32:58[0m] Step: 380, Training Logs: loss_final: 1.412358, loss_mean: 1.302312, proj_loss: -0.030641, loss_mean_cls: 0.110331, deep_loss: 0.030356, grad_norm: 10.440287
[[34m2025-10-03 23:32:59[0m] Step: 381, Training Logs: loss_final: 1.400668, loss_mean: 1.291039, proj_loss: -0.031851, loss_mean_cls: 0.112071, deep_loss: 0.029411, grad_norm: 10.194635
[[34m2025-10-03 23:33:00[0m] Step: 382, Training Logs: loss_final: 1.404730, loss_mean: 1.293817, proj_loss: -0.031240, loss_mean_cls: 0.111726, deep_loss: 0.030427, grad_norm: 9.974325
[[34m2025-10-03 23:33:01[0m] Step: 383, Training Logs: loss_final: 1.434206, loss_mean: 1.325295, proj_loss: -0.032936, loss_mean_cls: 0.111046, deep_loss: 0.030800, grad_norm: 10.415950
[[34m2025-10-03 23:33:02[0m] Step: 384, Training Logs: loss_final: 1.397866, loss_mean: 1.286127, proj_loss: -0.030473, loss_mean_cls: 0.112277, deep_loss: 0.029935, grad_norm: 12.826870
[[34m2025-10-03 23:33:03[0m] Step: 385, Training Logs: loss_final: 1.413809, loss_mean: 1.301610, proj_loss: -0.030394, loss_mean_cls: 0.112027, deep_loss: 0.030565, grad_norm: 10.366329
[[34m2025-10-03 23:33:05[0m] Step: 386, Training Logs: loss_final: 1.403738, loss_mean: 1.290143, proj_loss: -0.028820, loss_mean_cls: 0.112448, deep_loss: 0.029967, grad_norm: 16.156729
[[34m2025-10-03 23:33:06[0m] Step: 387, Training Logs: loss_final: 1.417414, loss_mean: 1.304143, proj_loss: -0.029602, loss_mean_cls: 0.111347, deep_loss: 0.031526, grad_norm: 16.524153
[[34m2025-10-03 23:33:07[0m] Step: 388, Training Logs: loss_final: 1.433093, loss_mean: 1.319243, proj_loss: -0.028970, loss_mean_cls: 0.111509, deep_loss: 0.031310, grad_norm: 15.667892
[[34m2025-10-03 23:33:08[0m] Step: 389, Training Logs: loss_final: 1.400543, loss_mean: 1.287639, proj_loss: -0.028203, loss_mean_cls: 0.111579, deep_loss: 0.029528, grad_norm: 11.190818
[[34m2025-10-03 23:33:09[0m] Step: 390, Training Logs: loss_final: 1.404526, loss_mean: 1.294945, proj_loss: -0.031382, loss_mean_cls: 0.110719, deep_loss: 0.030243, grad_norm: 11.990581
[[34m2025-10-03 23:33:10[0m] Step: 391, Training Logs: loss_final: 1.446862, loss_mean: 1.334178, proj_loss: -0.029885, loss_mean_cls: 0.111069, deep_loss: 0.031500, grad_norm: 14.631669
[[34m2025-10-03 23:33:12[0m] Step: 392, Training Logs: loss_final: 1.395856, loss_mean: 1.283473, proj_loss: -0.030425, loss_mean_cls: 0.112315, deep_loss: 0.030492, grad_norm: 18.071701
[[34m2025-10-03 23:33:13[0m] Step: 393, Training Logs: loss_final: 1.420529, loss_mean: 1.307100, proj_loss: -0.029477, loss_mean_cls: 0.112583, deep_loss: 0.030323, grad_norm: 17.175694
[[34m2025-10-03 23:33:14[0m] Step: 394, Training Logs: loss_final: 1.400084, loss_mean: 1.285355, proj_loss: -0.027354, loss_mean_cls: 0.111368, deep_loss: 0.030716, grad_norm: 11.824428
[[34m2025-10-03 23:33:15[0m] Step: 395, Training Logs: loss_final: 1.397038, loss_mean: 1.282893, proj_loss: -0.027999, loss_mean_cls: 0.112201, deep_loss: 0.029943, grad_norm: 14.093217
[[34m2025-10-03 23:33:16[0m] Step: 396, Training Logs: loss_final: 1.410688, loss_mean: 1.299877, proj_loss: -0.030490, loss_mean_cls: 0.111246, deep_loss: 0.030056, grad_norm: 16.918434
[[34m2025-10-03 23:33:17[0m] Step: 397, Training Logs: loss_final: 1.397506, loss_mean: 1.285779, proj_loss: -0.030763, loss_mean_cls: 0.112278, deep_loss: 0.030213, grad_norm: 15.858769
[[34m2025-10-03 23:33:19[0m] Step: 398, Training Logs: loss_final: 1.394040, loss_mean: 1.282642, proj_loss: -0.029743, loss_mean_cls: 0.111230, deep_loss: 0.029911, grad_norm: 11.316050
[[34m2025-10-03 23:33:20[0m] Step: 399, Training Logs: loss_final: 1.429148, loss_mean: 1.317101, proj_loss: -0.028813, loss_mean_cls: 0.110418, deep_loss: 0.030442, grad_norm: 17.090252
[[34m2025-10-03 23:33:21[0m] Step: 400, Training Logs: loss_final: 1.369401, loss_mean: 1.257933, proj_loss: -0.029760, loss_mean_cls: 0.112141, deep_loss: 0.029088, grad_norm: 18.118586
[[34m2025-10-03 23:33:22[0m] Step: 401, Training Logs: loss_final: 1.398417, loss_mean: 1.284156, proj_loss: -0.026828, loss_mean_cls: 0.111591, deep_loss: 0.029498, grad_norm: 19.314499
[[34m2025-10-03 23:33:23[0m] Step: 402, Training Logs: loss_final: 1.394385, loss_mean: 1.283472, proj_loss: -0.031551, loss_mean_cls: 0.111914, deep_loss: 0.030550, grad_norm: 14.142442
[[34m2025-10-03 23:33:24[0m] Step: 403, Training Logs: loss_final: 1.376632, loss_mean: 1.265612, proj_loss: -0.031140, loss_mean_cls: 0.112608, deep_loss: 0.029552, grad_norm: 11.800897
[[34m2025-10-03 23:33:26[0m] Step: 404, Training Logs: loss_final: 1.409387, loss_mean: 1.297977, proj_loss: -0.029557, loss_mean_cls: 0.110842, deep_loss: 0.030125, grad_norm: 19.572960
[[34m2025-10-03 23:33:27[0m] Step: 405, Training Logs: loss_final: 1.437821, loss_mean: 1.326332, proj_loss: -0.029924, loss_mean_cls: 0.109703, deep_loss: 0.031710, grad_norm: 17.589561
[[34m2025-10-03 23:33:28[0m] Step: 406, Training Logs: loss_final: 1.444489, loss_mean: 1.332275, proj_loss: -0.030763, loss_mean_cls: 0.110711, deep_loss: 0.032266, grad_norm: 18.285152
[[34m2025-10-03 23:33:29[0m] Step: 407, Training Logs: loss_final: 1.430256, loss_mean: 1.316680, proj_loss: -0.028294, loss_mean_cls: 0.111130, deep_loss: 0.030740, grad_norm: 13.249880
[[34m2025-10-03 23:33:30[0m] Step: 408, Training Logs: loss_final: 1.423359, loss_mean: 1.311648, proj_loss: -0.029971, loss_mean_cls: 0.110587, deep_loss: 0.031095, grad_norm: 12.095799
[[34m2025-10-03 23:33:31[0m] Step: 409, Training Logs: loss_final: 1.432436, loss_mean: 1.319189, proj_loss: -0.029290, loss_mean_cls: 0.111083, deep_loss: 0.031454, grad_norm: 17.307354
[[34m2025-10-03 23:33:32[0m] Step: 410, Training Logs: loss_final: 1.449722, loss_mean: 1.336829, proj_loss: -0.031086, loss_mean_cls: 0.111818, deep_loss: 0.032160, grad_norm: 18.902094
[[34m2025-10-03 23:33:32[0m] Step: 411, Training Logs: loss_final: 1.445878, loss_mean: 1.332377, proj_loss: -0.029146, loss_mean_cls: 0.111225, deep_loss: 0.031422, grad_norm: 12.293173
[[34m2025-10-03 23:33:33[0m] Step: 412, Training Logs: loss_final: 1.421134, loss_mean: 1.309894, proj_loss: -0.030779, loss_mean_cls: 0.111076, deep_loss: 0.030943, grad_norm: 10.226928
[[34m2025-10-03 23:33:35[0m] Step: 413, Training Logs: loss_final: 1.446757, loss_mean: 1.334340, proj_loss: -0.029105, loss_mean_cls: 0.110275, deep_loss: 0.031247, grad_norm: 10.298424
[[34m2025-10-03 23:33:36[0m] Step: 414, Training Logs: loss_final: 1.423391, loss_mean: 1.312610, proj_loss: -0.030324, loss_mean_cls: 0.110636, deep_loss: 0.030469, grad_norm: 13.261240
[[34m2025-10-03 23:33:37[0m] Step: 415, Training Logs: loss_final: 1.420046, loss_mean: 1.306457, proj_loss: -0.029769, loss_mean_cls: 0.111778, deep_loss: 0.031580, grad_norm: 15.419262
[[34m2025-10-03 23:33:38[0m] Step: 416, Training Logs: loss_final: 1.424672, loss_mean: 1.311665, proj_loss: -0.029234, loss_mean_cls: 0.111898, deep_loss: 0.030342, grad_norm: 12.780993
[[34m2025-10-03 23:33:39[0m] Step: 417, Training Logs: loss_final: 1.441918, loss_mean: 1.330001, proj_loss: -0.030599, loss_mean_cls: 0.111409, deep_loss: 0.031108, grad_norm: 12.800388
[[34m2025-10-03 23:33:40[0m] Step: 418, Training Logs: loss_final: 1.417948, loss_mean: 1.304353, proj_loss: -0.029343, loss_mean_cls: 0.112003, deep_loss: 0.030935, grad_norm: 12.467518
[[34m2025-10-03 23:33:42[0m] Step: 419, Training Logs: loss_final: 1.445900, loss_mean: 1.331818, proj_loss: -0.029904, loss_mean_cls: 0.111946, deep_loss: 0.032040, grad_norm: 26.895947
[[34m2025-10-03 23:33:43[0m] Step: 420, Training Logs: loss_final: 1.436502, loss_mean: 1.323054, proj_loss: -0.029874, loss_mean_cls: 0.111699, deep_loss: 0.031623, grad_norm: 25.043060
[[34m2025-10-03 23:33:44[0m] Step: 421, Training Logs: loss_final: 1.446894, loss_mean: 1.335990, proj_loss: -0.031355, loss_mean_cls: 0.109891, deep_loss: 0.032368, grad_norm: 21.875441
[[34m2025-10-03 23:33:45[0m] Step: 422, Training Logs: loss_final: 1.399598, loss_mean: 1.288906, proj_loss: -0.031355, loss_mean_cls: 0.111061, deep_loss: 0.030986, grad_norm: 14.176481
[[34m2025-10-03 23:33:46[0m] Step: 423, Training Logs: loss_final: 1.430039, loss_mean: 1.318172, proj_loss: -0.029598, loss_mean_cls: 0.110811, deep_loss: 0.030654, grad_norm: 20.300650
[[34m2025-10-03 23:33:47[0m] Step: 424, Training Logs: loss_final: 1.438071, loss_mean: 1.325759, proj_loss: -0.030363, loss_mean_cls: 0.111038, deep_loss: 0.031638, grad_norm: 20.998993
[[34m2025-10-03 23:33:49[0m] Step: 425, Training Logs: loss_final: 1.419141, loss_mean: 1.305957, proj_loss: -0.029849, loss_mean_cls: 0.111168, deep_loss: 0.031865, grad_norm: 20.538984
[[34m2025-10-03 23:33:50[0m] Step: 426, Training Logs: loss_final: 1.414903, loss_mean: 1.302230, proj_loss: -0.029449, loss_mean_cls: 0.110982, deep_loss: 0.031140, grad_norm: 16.652018
[[34m2025-10-03 23:33:51[0m] Step: 427, Training Logs: loss_final: 1.406126, loss_mean: 1.293583, proj_loss: -0.028893, loss_mean_cls: 0.111121, deep_loss: 0.030315, grad_norm: 15.446398
[[34m2025-10-03 23:33:52[0m] Step: 428, Training Logs: loss_final: 1.449010, loss_mean: 1.338036, proj_loss: -0.030256, loss_mean_cls: 0.109165, deep_loss: 0.032066, grad_norm: 19.146862
[[34m2025-10-03 23:33:53[0m] Step: 429, Training Logs: loss_final: 1.453235, loss_mean: 1.339251, proj_loss: -0.029438, loss_mean_cls: 0.110605, deep_loss: 0.032818, grad_norm: 18.679329
[[34m2025-10-03 23:33:54[0m] Step: 430, Training Logs: loss_final: 1.435187, loss_mean: 1.324919, proj_loss: -0.032161, loss_mean_cls: 0.110240, deep_loss: 0.032189, grad_norm: 16.990019
[[34m2025-10-03 23:33:56[0m] Step: 431, Training Logs: loss_final: 1.402776, loss_mean: 1.292333, proj_loss: -0.031365, loss_mean_cls: 0.111366, deep_loss: 0.030441, grad_norm: 11.858090
[[34m2025-10-03 23:33:57[0m] Step: 432, Training Logs: loss_final: 1.430509, loss_mean: 1.320052, proj_loss: -0.030756, loss_mean_cls: 0.109787, deep_loss: 0.031427, grad_norm: 18.749956
[[34m2025-10-03 23:33:58[0m] Step: 433, Training Logs: loss_final: 1.458089, loss_mean: 1.348771, proj_loss: -0.032403, loss_mean_cls: 0.108801, deep_loss: 0.032919, grad_norm: 15.303452
[[34m2025-10-03 23:33:59[0m] Step: 434, Training Logs: loss_final: 1.400197, loss_mean: 1.289752, proj_loss: -0.030879, loss_mean_cls: 0.109874, deep_loss: 0.031450, grad_norm: 16.854042
[[34m2025-10-03 23:34:00[0m] Step: 435, Training Logs: loss_final: 1.386548, loss_mean: 1.277738, proj_loss: -0.031830, loss_mean_cls: 0.110905, deep_loss: 0.029735, grad_norm: 13.573838
[[34m2025-10-03 23:34:01[0m] Step: 436, Training Logs: loss_final: 1.432090, loss_mean: 1.320641, proj_loss: -0.029835, loss_mean_cls: 0.110583, deep_loss: 0.030700, grad_norm: 18.352812
[[34m2025-10-03 23:34:03[0m] Step: 437, Training Logs: loss_final: 1.418391, loss_mean: 1.310283, proj_loss: -0.034522, loss_mean_cls: 0.110072, deep_loss: 0.032558, grad_norm: 17.327820
[[34m2025-10-03 23:34:04[0m] Step: 438, Training Logs: loss_final: 1.410468, loss_mean: 1.299792, proj_loss: -0.031601, loss_mean_cls: 0.110687, deep_loss: 0.031590, grad_norm: 17.220505
[[34m2025-10-03 23:34:05[0m] Step: 439, Training Logs: loss_final: 1.403624, loss_mean: 1.294818, proj_loss: -0.030913, loss_mean_cls: 0.108906, deep_loss: 0.030813, grad_norm: 16.337831
[[34m2025-10-03 23:34:06[0m] Step: 440, Training Logs: loss_final: 1.411222, loss_mean: 1.299311, proj_loss: -0.029333, loss_mean_cls: 0.110618, deep_loss: 0.030625, grad_norm: 17.050112
[[34m2025-10-03 23:34:06[0m] Step: 441, Training Logs: loss_final: 1.409154, loss_mean: 1.296498, proj_loss: -0.029285, loss_mean_cls: 0.111099, deep_loss: 0.030842, grad_norm: 20.021078
[[34m2025-10-03 23:34:07[0m] Step: 442, Training Logs: loss_final: 1.430589, loss_mean: 1.317151, proj_loss: -0.028053, loss_mean_cls: 0.109963, deep_loss: 0.031529, grad_norm: 16.972754
[[34m2025-10-03 23:34:08[0m] Step: 443, Training Logs: loss_final: 1.398560, loss_mean: 1.286958, proj_loss: -0.029695, loss_mean_cls: 0.110612, deep_loss: 0.030684, grad_norm: 11.746779
[[34m2025-10-03 23:34:10[0m] Step: 444, Training Logs: loss_final: 1.443809, loss_mean: 1.334971, proj_loss: -0.032396, loss_mean_cls: 0.109518, deep_loss: 0.031716, grad_norm: 16.497980
[[34m2025-10-03 23:34:11[0m] Step: 445, Training Logs: loss_final: 1.432732, loss_mean: 1.322972, proj_loss: -0.032711, loss_mean_cls: 0.110281, deep_loss: 0.032190, grad_norm: 19.957071
[[34m2025-10-03 23:34:12[0m] Step: 446, Training Logs: loss_final: 1.423952, loss_mean: 1.311984, proj_loss: -0.030160, loss_mean_cls: 0.110123, deep_loss: 0.032005, grad_norm: 14.445829
[[34m2025-10-03 23:34:13[0m] Step: 447, Training Logs: loss_final: 1.421155, loss_mean: 1.306995, proj_loss: -0.028599, loss_mean_cls: 0.111154, deep_loss: 0.031606, grad_norm: 14.167510
[[34m2025-10-03 23:34:14[0m] Step: 448, Training Logs: loss_final: 1.454649, loss_mean: 1.343485, proj_loss: -0.030597, loss_mean_cls: 0.110146, deep_loss: 0.031616, grad_norm: 30.717869
[[34m2025-10-03 23:34:15[0m] Step: 449, Training Logs: loss_final: 1.418181, loss_mean: 1.306163, proj_loss: -0.030480, loss_mean_cls: 0.111445, deep_loss: 0.031053, grad_norm: 33.932892
[[34m2025-10-03 23:34:17[0m] Step: 450, Training Logs: loss_final: 1.434554, loss_mean: 1.323906, proj_loss: -0.031832, loss_mean_cls: 0.110921, deep_loss: 0.031559, grad_norm: 23.698519
[[34m2025-10-03 23:34:18[0m] Step: 451, Training Logs: loss_final: 1.413160, loss_mean: 1.304296, proj_loss: -0.032351, loss_mean_cls: 0.110508, deep_loss: 0.030706, grad_norm: 16.562014
[[34m2025-10-03 23:34:19[0m] Step: 452, Training Logs: loss_final: 1.405319, loss_mean: 1.295191, proj_loss: -0.030288, loss_mean_cls: 0.110490, deep_loss: 0.029926, grad_norm: 21.559870
[[34m2025-10-03 23:34:20[0m] Step: 453, Training Logs: loss_final: 1.445448, loss_mean: 1.335456, proj_loss: -0.031241, loss_mean_cls: 0.109793, deep_loss: 0.031441, grad_norm: 22.134592
[[34m2025-10-03 23:34:21[0m] Step: 454, Training Logs: loss_final: 1.431282, loss_mean: 1.319452, proj_loss: -0.030841, loss_mean_cls: 0.111460, deep_loss: 0.031210, grad_norm: 25.808746
[[34m2025-10-03 23:34:22[0m] Step: 455, Training Logs: loss_final: 1.404049, loss_mean: 1.294129, proj_loss: -0.031681, loss_mean_cls: 0.111170, deep_loss: 0.030431, grad_norm: 21.065594
[[34m2025-10-03 23:34:24[0m] Step: 456, Training Logs: loss_final: 1.361908, loss_mean: 1.250293, proj_loss: -0.028950, loss_mean_cls: 0.112112, deep_loss: 0.028453, grad_norm: 12.063342
[[34m2025-10-03 23:34:25[0m] Step: 457, Training Logs: loss_final: 1.400980, loss_mean: 1.290064, proj_loss: -0.031304, loss_mean_cls: 0.111174, deep_loss: 0.031046, grad_norm: 19.963060
[[34m2025-10-03 23:34:26[0m] Step: 458, Training Logs: loss_final: 1.412641, loss_mean: 1.299569, proj_loss: -0.029137, loss_mean_cls: 0.110406, deep_loss: 0.031803, grad_norm: 27.040745
[[34m2025-10-03 23:34:27[0m] Step: 459, Training Logs: loss_final: 1.419430, loss_mean: 1.307999, proj_loss: -0.030654, loss_mean_cls: 0.109976, deep_loss: 0.032109, grad_norm: 19.287918
[[34m2025-10-03 23:34:28[0m] Step: 460, Training Logs: loss_final: 1.428383, loss_mean: 1.317976, proj_loss: -0.030437, loss_mean_cls: 0.110410, deep_loss: 0.030435, grad_norm: 15.375274
[[34m2025-10-03 23:34:29[0m] Step: 461, Training Logs: loss_final: 1.385802, loss_mean: 1.273918, proj_loss: -0.028637, loss_mean_cls: 0.110531, deep_loss: 0.029990, grad_norm: 23.508619
[[34m2025-10-03 23:34:31[0m] Step: 462, Training Logs: loss_final: 1.428214, loss_mean: 1.316871, proj_loss: -0.030358, loss_mean_cls: 0.109252, deep_loss: 0.032450, grad_norm: 25.103886
[[34m2025-10-03 23:34:32[0m] Step: 463, Training Logs: loss_final: 1.405628, loss_mean: 1.293461, proj_loss: -0.032016, loss_mean_cls: 0.110275, deep_loss: 0.033907, grad_norm: 21.741356
[[34m2025-10-03 23:34:33[0m] Step: 464, Training Logs: loss_final: 1.442494, loss_mean: 1.328893, proj_loss: -0.030204, loss_mean_cls: 0.111301, deep_loss: 0.032503, grad_norm: 22.083128
[[34m2025-10-03 23:34:34[0m] Step: 465, Training Logs: loss_final: 1.420070, loss_mean: 1.307725, proj_loss: -0.029815, loss_mean_cls: 0.111099, deep_loss: 0.031061, grad_norm: 26.675472
[[34m2025-10-03 23:34:35[0m] Step: 466, Training Logs: loss_final: 1.465364, loss_mean: 1.352957, proj_loss: -0.029044, loss_mean_cls: 0.109493, deep_loss: 0.031958, grad_norm: 23.848726
[[34m2025-10-03 23:34:36[0m] Step: 467, Training Logs: loss_final: 1.485759, loss_mean: 1.373919, proj_loss: -0.029345, loss_mean_cls: 0.110300, deep_loss: 0.030885, grad_norm: 42.186543
[[34m2025-10-03 23:34:38[0m] Step: 468, Training Logs: loss_final: 1.499248, loss_mean: 1.387709, proj_loss: -0.031968, loss_mean_cls: 0.111064, deep_loss: 0.032442, grad_norm: 46.259819
[[34m2025-10-03 23:34:39[0m] Step: 469, Training Logs: loss_final: 1.448468, loss_mean: 1.335901, proj_loss: -0.031359, loss_mean_cls: 0.111228, deep_loss: 0.032699, grad_norm: 34.161880
[[34m2025-10-03 23:34:40[0m] Step: 470, Training Logs: loss_final: 1.451499, loss_mean: 1.339094, proj_loss: -0.030844, loss_mean_cls: 0.109373, deep_loss: 0.033875, grad_norm: 24.063795
[[34m2025-10-03 23:34:41[0m] Step: 471, Training Logs: loss_final: 1.449903, loss_mean: 1.336889, proj_loss: -0.031171, loss_mean_cls: 0.110965, deep_loss: 0.033219, grad_norm: 33.499237
[[34m2025-10-03 23:34:41[0m] Step: 472, Training Logs: loss_final: 1.489727, loss_mean: 1.378068, proj_loss: -0.031125, loss_mean_cls: 0.110054, deep_loss: 0.032731, grad_norm: 35.000378
[[34m2025-10-03 23:34:42[0m] Step: 473, Training Logs: loss_final: 1.452658, loss_mean: 1.341473, proj_loss: -0.030929, loss_mean_cls: 0.110254, deep_loss: 0.031860, grad_norm: 32.143032
[[34m2025-10-03 23:34:44[0m] Step: 474, Training Logs: loss_final: 1.419763, loss_mean: 1.304694, proj_loss: -0.027816, loss_mean_cls: 0.110235, deep_loss: 0.032649, grad_norm: 23.799318
[[34m2025-10-03 23:34:45[0m] Step: 475, Training Logs: loss_final: 1.440421, loss_mean: 1.327859, proj_loss: -0.029807, loss_mean_cls: 0.110197, deep_loss: 0.032171, grad_norm: 23.596498
[[34m2025-10-03 23:34:46[0m] Step: 476, Training Logs: loss_final: 1.412385, loss_mean: 1.299590, proj_loss: -0.028866, loss_mean_cls: 0.110473, deep_loss: 0.031189, grad_norm: 26.815128
[[34m2025-10-03 23:34:47[0m] Step: 477, Training Logs: loss_final: 1.408063, loss_mean: 1.297983, proj_loss: -0.031537, loss_mean_cls: 0.110771, deep_loss: 0.030845, grad_norm: 20.785757
[[34m2025-10-03 23:34:48[0m] Step: 478, Training Logs: loss_final: 1.411816, loss_mean: 1.300820, proj_loss: -0.030535, loss_mean_cls: 0.109800, deep_loss: 0.031731, grad_norm: 13.352361
[[34m2025-10-03 23:34:49[0m] Step: 479, Training Logs: loss_final: 1.410099, loss_mean: 1.300605, proj_loss: -0.031321, loss_mean_cls: 0.109221, deep_loss: 0.031594, grad_norm: 12.835228
[[34m2025-10-03 23:34:51[0m] Step: 480, Training Logs: loss_final: 1.401610, loss_mean: 1.293073, proj_loss: -0.031976, loss_mean_cls: 0.109819, deep_loss: 0.030693, grad_norm: 14.114694
[[34m2025-10-03 23:34:52[0m] Step: 481, Training Logs: loss_final: 1.411025, loss_mean: 1.304117, proj_loss: -0.032190, loss_mean_cls: 0.108799, deep_loss: 0.030298, grad_norm: 16.660231
[[34m2025-10-03 23:34:53[0m] Step: 482, Training Logs: loss_final: 1.367616, loss_mean: 1.257475, proj_loss: -0.030258, loss_mean_cls: 0.110502, deep_loss: 0.029897, grad_norm: 12.064272
[[34m2025-10-03 23:34:54[0m] Step: 483, Training Logs: loss_final: 1.432741, loss_mean: 1.323486, proj_loss: -0.032016, loss_mean_cls: 0.109267, deep_loss: 0.032004, grad_norm: 29.163042
[[34m2025-10-03 23:34:55[0m] Step: 484, Training Logs: loss_final: 1.489491, loss_mean: 1.376081, proj_loss: -0.029940, loss_mean_cls: 0.110046, deep_loss: 0.033303, grad_norm: 36.079082
[[34m2025-10-03 23:34:56[0m] Step: 485, Training Logs: loss_final: 1.503903, loss_mean: 1.391694, proj_loss: -0.031418, loss_mean_cls: 0.110067, deep_loss: 0.033560, grad_norm: 25.531902
[[34m2025-10-03 23:34:58[0m] Step: 486, Training Logs: loss_final: 1.526494, loss_mean: 1.413338, proj_loss: -0.032234, loss_mean_cls: 0.112209, deep_loss: 0.033181, grad_norm: 24.500961
[[34m2025-10-03 23:34:59[0m] Step: 487, Training Logs: loss_final: 1.579521, loss_mean: 1.463678, proj_loss: -0.029792, loss_mean_cls: 0.111496, deep_loss: 0.034138, grad_norm: 35.941444
[[34m2025-10-03 23:35:00[0m] Step: 488, Training Logs: loss_final: 1.604614, loss_mean: 1.487722, proj_loss: -0.031267, loss_mean_cls: 0.112646, deep_loss: 0.035514, grad_norm: 40.241081
[[34m2025-10-03 23:35:01[0m] Step: 489, Training Logs: loss_final: 1.603093, loss_mean: 1.486086, proj_loss: -0.030592, loss_mean_cls: 0.111477, deep_loss: 0.036123, grad_norm: 35.148895
[[34m2025-10-03 23:35:02[0m] Step: 490, Training Logs: loss_final: 1.572030, loss_mean: 1.457434, proj_loss: -0.031355, loss_mean_cls: 0.111485, deep_loss: 0.034466, grad_norm: 27.342079
[[34m2025-10-03 23:35:03[0m] Step: 491, Training Logs: loss_final: 1.587110, loss_mean: 1.469507, proj_loss: -0.032012, loss_mean_cls: 0.112975, deep_loss: 0.036640, grad_norm: 24.025480
[[34m2025-10-03 23:35:05[0m] Step: 492, Training Logs: loss_final: 1.578335, loss_mean: 1.462321, proj_loss: -0.032021, loss_mean_cls: 0.110342, deep_loss: 0.037693, grad_norm: 23.869633
[[34m2025-10-03 23:35:06[0m] Step: 493, Training Logs: loss_final: 1.584387, loss_mean: 1.467677, proj_loss: -0.030524, loss_mean_cls: 0.110935, deep_loss: 0.036299, grad_norm: 27.985668
[[34m2025-10-03 23:35:07[0m] Step: 494, Training Logs: loss_final: 1.606344, loss_mean: 1.489409, proj_loss: -0.030365, loss_mean_cls: 0.110065, deep_loss: 0.037235, grad_norm: 24.528269
[[34m2025-10-03 23:35:08[0m] Step: 495, Training Logs: loss_final: 1.578161, loss_mean: 1.463232, proj_loss: -0.031866, loss_mean_cls: 0.111447, deep_loss: 0.035349, grad_norm: 22.820232
[[34m2025-10-03 23:35:09[0m] Step: 496, Training Logs: loss_final: 1.579144, loss_mean: 1.463422, proj_loss: -0.032363, loss_mean_cls: 0.112102, deep_loss: 0.035983, grad_norm: 22.870823
[[34m2025-10-03 23:35:10[0m] Step: 497, Training Logs: loss_final: 1.593432, loss_mean: 1.479209, proj_loss: -0.031950, loss_mean_cls: 0.110556, deep_loss: 0.035618, grad_norm: 21.537525
[[34m2025-10-03 23:35:12[0m] Step: 498, Training Logs: loss_final: 1.566465, loss_mean: 1.452608, proj_loss: -0.031064, loss_mean_cls: 0.110183, deep_loss: 0.034738, grad_norm: 19.724344
[[34m2025-10-03 23:35:13[0m] Step: 499, Training Logs: loss_final: 1.567721, loss_mean: 1.454917, proj_loss: -0.032529, loss_mean_cls: 0.109623, deep_loss: 0.035710, grad_norm: 16.445749
[[34m2025-10-03 23:35:14[0m] Step: 500, Training Logs: loss_final: 1.562333, loss_mean: 1.443784, proj_loss: -0.032576, loss_mean_cls: 0.110179, deep_loss: 0.040947, grad_norm: 15.761608
[[34m2025-10-03 23:35:15[0m] Step: 501, Training Logs: loss_final: 1.561943, loss_mean: 1.440700, proj_loss: -0.031721, loss_mean_cls: 0.110627, deep_loss: 0.042337, grad_norm: 17.545635
[[34m2025-10-03 23:35:16[0m] Step: 502, Training Logs: loss_final: 1.558058, loss_mean: 1.437038, proj_loss: -0.031634, loss_mean_cls: 0.109872, deep_loss: 0.042782, grad_norm: 14.754024
[[34m2025-10-03 23:35:16[0m] Step: 503, Training Logs: loss_final: 1.559827, loss_mean: 1.440545, proj_loss: -0.031698, loss_mean_cls: 0.109132, deep_loss: 0.041849, grad_norm: 12.488215
[[34m2025-10-03 23:35:17[0m] Step: 504, Training Logs: loss_final: 1.553722, loss_mean: 1.434425, proj_loss: -0.032073, loss_mean_cls: 0.109905, deep_loss: 0.041466, grad_norm: 13.130731
[[34m2025-10-03 23:35:19[0m] Step: 505, Training Logs: loss_final: 1.544712, loss_mean: 1.426903, proj_loss: -0.032291, loss_mean_cls: 0.108976, deep_loss: 0.041125, grad_norm: 13.257581
[[34m2025-10-03 23:35:20[0m] Step: 506, Training Logs: loss_final: 1.547926, loss_mean: 1.431218, proj_loss: -0.031723, loss_mean_cls: 0.109113, deep_loss: 0.039318, grad_norm: 12.313971
[[34m2025-10-03 23:35:21[0m] Step: 507, Training Logs: loss_final: 1.546627, loss_mean: 1.428352, proj_loss: -0.031436, loss_mean_cls: 0.110668, deep_loss: 0.039043, grad_norm: 14.312528
[[34m2025-10-03 23:35:22[0m] Step: 508, Training Logs: loss_final: 1.531539, loss_mean: 1.412448, proj_loss: -0.030307, loss_mean_cls: 0.109085, deep_loss: 0.040313, grad_norm: 14.274155
[[34m2025-10-03 23:35:23[0m] Step: 509, Training Logs: loss_final: 1.517318, loss_mean: 1.396101, proj_loss: -0.030873, loss_mean_cls: 0.111748, deep_loss: 0.040342, grad_norm: 14.031800
[[34m2025-10-03 23:35:24[0m] Step: 510, Training Logs: loss_final: 1.525232, loss_mean: 1.408251, proj_loss: -0.031703, loss_mean_cls: 0.109831, deep_loss: 0.038854, grad_norm: 14.678030
[[34m2025-10-03 23:35:26[0m] Step: 511, Training Logs: loss_final: 1.514613, loss_mean: 1.396642, proj_loss: -0.030301, loss_mean_cls: 0.110140, deep_loss: 0.038132, grad_norm: 10.934735
[[34m2025-10-03 23:35:27[0m] Step: 512, Training Logs: loss_final: 1.510481, loss_mean: 1.391788, proj_loss: -0.031904, loss_mean_cls: 0.110899, deep_loss: 0.039698, grad_norm: 14.155401
[[34m2025-10-03 23:35:28[0m] Step: 513, Training Logs: loss_final: 1.517303, loss_mean: 1.397359, proj_loss: -0.031500, loss_mean_cls: 0.111128, deep_loss: 0.040317, grad_norm: 18.213505
[[34m2025-10-03 23:35:29[0m] Step: 514, Training Logs: loss_final: 1.529677, loss_mean: 1.409078, proj_loss: -0.031665, loss_mean_cls: 0.111045, deep_loss: 0.041219, grad_norm: 18.840090
[[34m2025-10-03 23:35:30[0m] Step: 515, Training Logs: loss_final: 1.535845, loss_mean: 1.415019, proj_loss: -0.030230, loss_mean_cls: 0.110356, deep_loss: 0.040700, grad_norm: 12.176020
[[34m2025-10-03 23:35:31[0m] Step: 516, Training Logs: loss_final: 1.537359, loss_mean: 1.414555, proj_loss: -0.029802, loss_mean_cls: 0.110572, deep_loss: 0.042034, grad_norm: 14.477155
[[34m2025-10-03 23:35:33[0m] Step: 517, Training Logs: loss_final: 1.544072, loss_mean: 1.421718, proj_loss: -0.030825, loss_mean_cls: 0.110080, deep_loss: 0.043098, grad_norm: 19.076862
[[34m2025-10-03 23:35:34[0m] Step: 518, Training Logs: loss_final: 1.546335, loss_mean: 1.423612, proj_loss: -0.030751, loss_mean_cls: 0.110087, deep_loss: 0.043386, grad_norm: 15.456931
[[34m2025-10-03 23:35:35[0m] Step: 519, Training Logs: loss_final: 1.537272, loss_mean: 1.415760, proj_loss: -0.031733, loss_mean_cls: 0.109989, deep_loss: 0.043256, grad_norm: 14.407239
[[34m2025-10-03 23:35:36[0m] Step: 520, Training Logs: loss_final: 1.548358, loss_mean: 1.425364, proj_loss: -0.030237, loss_mean_cls: 0.109893, deep_loss: 0.043337, grad_norm: 13.469094
[[34m2025-10-03 23:35:37[0m] Step: 521, Training Logs: loss_final: 1.531986, loss_mean: 1.410156, proj_loss: -0.030774, loss_mean_cls: 0.109703, deep_loss: 0.042900, grad_norm: 11.000780
[[34m2025-10-03 23:35:38[0m] Step: 522, Training Logs: loss_final: 1.523766, loss_mean: 1.401969, proj_loss: -0.031832, loss_mean_cls: 0.110212, deep_loss: 0.043418, grad_norm: 17.223522
[[34m2025-10-03 23:35:40[0m] Step: 523, Training Logs: loss_final: 1.518571, loss_mean: 1.396027, proj_loss: -0.030898, loss_mean_cls: 0.110635, deep_loss: 0.042807, grad_norm: 18.135193
[[34m2025-10-03 23:35:41[0m] Step: 524, Training Logs: loss_final: 1.531443, loss_mean: 1.410234, proj_loss: -0.031597, loss_mean_cls: 0.109995, deep_loss: 0.042811, grad_norm: 12.275040
[[34m2025-10-03 23:35:42[0m] Step: 525, Training Logs: loss_final: 1.549968, loss_mean: 1.428894, proj_loss: -0.030753, loss_mean_cls: 0.109535, deep_loss: 0.042292, grad_norm: 16.735077
[[34m2025-10-03 23:35:43[0m] Step: 526, Training Logs: loss_final: 1.523865, loss_mean: 1.403132, proj_loss: -0.031578, loss_mean_cls: 0.109729, deep_loss: 0.042580, grad_norm: 21.785337
[[34m2025-10-03 23:35:44[0m] Step: 527, Training Logs: loss_final: 1.517372, loss_mean: 1.397733, proj_loss: -0.032690, loss_mean_cls: 0.109698, deep_loss: 0.042631, grad_norm: 18.943386
[[34m2025-10-03 23:35:45[0m] Step: 528, Training Logs: loss_final: 1.539369, loss_mean: 1.419647, proj_loss: -0.031858, loss_mean_cls: 0.109813, deep_loss: 0.041767, grad_norm: 20.048458
[[34m2025-10-03 23:35:47[0m] Step: 529, Training Logs: loss_final: 1.541537, loss_mean: 1.420478, proj_loss: -0.031028, loss_mean_cls: 0.110022, deep_loss: 0.042066, grad_norm: 19.904741
[[34m2025-10-03 23:35:48[0m] Step: 530, Training Logs: loss_final: 1.533398, loss_mean: 1.411717, proj_loss: -0.030493, loss_mean_cls: 0.110843, deep_loss: 0.041331, grad_norm: 26.446838
[[34m2025-10-03 23:35:49[0m] Step: 531, Training Logs: loss_final: 1.534914, loss_mean: 1.413313, proj_loss: -0.030595, loss_mean_cls: 0.109850, deep_loss: 0.042347, grad_norm: 24.844992
[[34m2025-10-03 23:35:50[0m] Step: 532, Training Logs: loss_final: 1.542949, loss_mean: 1.422007, proj_loss: -0.030306, loss_mean_cls: 0.109085, deep_loss: 0.042163, grad_norm: 16.918571
[[34m2025-10-03 23:35:50[0m] Step: 533, Training Logs: loss_final: 1.513476, loss_mean: 1.390293, proj_loss: -0.028588, loss_mean_cls: 0.110329, deep_loss: 0.041441, grad_norm: 21.515427
[[34m2025-10-03 23:35:51[0m] Step: 534, Training Logs: loss_final: 1.525402, loss_mean: 1.403094, proj_loss: -0.030076, loss_mean_cls: 0.111029, deep_loss: 0.041355, grad_norm: 24.499977
[[34m2025-10-03 23:35:53[0m] Step: 535, Training Logs: loss_final: 1.529611, loss_mean: 1.407013, proj_loss: -0.030311, loss_mean_cls: 0.110763, deep_loss: 0.042145, grad_norm: 23.285183
[[34m2025-10-03 23:35:54[0m] Step: 536, Training Logs: loss_final: 1.548569, loss_mean: 1.428198, proj_loss: -0.030869, loss_mean_cls: 0.109369, deep_loss: 0.041871, grad_norm: 21.117626
[[34m2025-10-03 23:35:55[0m] Step: 537, Training Logs: loss_final: 1.543826, loss_mean: 1.423640, proj_loss: -0.030620, loss_mean_cls: 0.109291, deep_loss: 0.041516, grad_norm: 23.911787
[[34m2025-10-03 23:35:56[0m] Step: 538, Training Logs: loss_final: 1.539218, loss_mean: 1.418309, proj_loss: -0.030336, loss_mean_cls: 0.110638, deep_loss: 0.040607, grad_norm: 28.956427
[[34m2025-10-03 23:35:57[0m] Step: 539, Training Logs: loss_final: 1.539045, loss_mean: 1.418402, proj_loss: -0.029165, loss_mean_cls: 0.110019, deep_loss: 0.039790, grad_norm: 25.374998
[[34m2025-10-03 23:35:58[0m] Step: 540, Training Logs: loss_final: 1.542685, loss_mean: 1.423828, proj_loss: -0.031327, loss_mean_cls: 0.109833, deep_loss: 0.040350, grad_norm: 24.844955
[[34m2025-10-03 23:36:00[0m] Step: 541, Training Logs: loss_final: 1.540105, loss_mean: 1.419610, proj_loss: -0.030420, loss_mean_cls: 0.110696, deep_loss: 0.040218, grad_norm: 25.088257
[[34m2025-10-03 23:36:01[0m] Step: 542, Training Logs: loss_final: 1.551368, loss_mean: 1.429724, proj_loss: -0.028558, loss_mean_cls: 0.110178, deep_loss: 0.040023, grad_norm: 31.140396
[[34m2025-10-03 23:36:02[0m] Step: 543, Training Logs: loss_final: 1.547538, loss_mean: 1.428292, proj_loss: -0.031445, loss_mean_cls: 0.109722, deep_loss: 0.040970, grad_norm: 29.899874
[[34m2025-10-03 23:36:03[0m] Step: 544, Training Logs: loss_final: 1.529203, loss_mean: 1.408366, proj_loss: -0.031469, loss_mean_cls: 0.111151, deep_loss: 0.041155, grad_norm: 26.565626
[[34m2025-10-03 23:36:04[0m] Step: 545, Training Logs: loss_final: 1.539593, loss_mean: 1.418104, proj_loss: -0.028622, loss_mean_cls: 0.110090, deep_loss: 0.040021, grad_norm: 26.060385
[[34m2025-10-03 23:36:05[0m] Step: 546, Training Logs: loss_final: 1.522194, loss_mean: 1.403566, proj_loss: -0.031755, loss_mean_cls: 0.109779, deep_loss: 0.040604, grad_norm: 29.173189
[[34m2025-10-03 23:36:07[0m] Step: 547, Training Logs: loss_final: 1.539840, loss_mean: 1.424771, proj_loss: -0.033669, loss_mean_cls: 0.108832, deep_loss: 0.039907, grad_norm: 29.725773
[[34m2025-10-03 23:36:08[0m] Step: 548, Training Logs: loss_final: 1.549992, loss_mean: 1.432146, proj_loss: -0.031678, loss_mean_cls: 0.109708, deep_loss: 0.039816, grad_norm: 27.366146
[[34m2025-10-03 23:36:09[0m] Step: 549, Training Logs: loss_final: 1.534078, loss_mean: 1.415826, proj_loss: -0.031446, loss_mean_cls: 0.109494, deep_loss: 0.040205, grad_norm: 25.005798
[[34m2025-10-03 23:36:10[0m] Step: 550, Training Logs: loss_final: 1.530144, loss_mean: 1.410526, proj_loss: -0.029809, loss_mean_cls: 0.110112, deep_loss: 0.039315, grad_norm: 27.034975
[[34m2025-10-03 23:36:11[0m] Step: 551, Training Logs: loss_final: 1.523413, loss_mean: 1.404984, proj_loss: -0.031046, loss_mean_cls: 0.109356, deep_loss: 0.040120, grad_norm: 26.637451
[[34m2025-10-03 23:36:12[0m] Step: 552, Training Logs: loss_final: 1.539572, loss_mean: 1.421512, proj_loss: -0.031563, loss_mean_cls: 0.109743, deep_loss: 0.039881, grad_norm: 30.048241
[[34m2025-10-03 23:36:14[0m] Step: 553, Training Logs: loss_final: 1.543620, loss_mean: 1.426316, proj_loss: -0.031635, loss_mean_cls: 0.108689, deep_loss: 0.040250, grad_norm: 23.998888
[[34m2025-10-03 23:36:15[0m] Step: 554, Training Logs: loss_final: 1.512684, loss_mean: 1.395189, proj_loss: -0.032003, loss_mean_cls: 0.109948, deep_loss: 0.039550, grad_norm: 22.495483
[[34m2025-10-03 23:36:16[0m] Step: 555, Training Logs: loss_final: 1.525423, loss_mean: 1.409619, proj_loss: -0.032147, loss_mean_cls: 0.108816, deep_loss: 0.039135, grad_norm: 27.383108
[[34m2025-10-03 23:36:17[0m] Step: 556, Training Logs: loss_final: 1.525804, loss_mean: 1.405933, proj_loss: -0.029849, loss_mean_cls: 0.110075, deep_loss: 0.039645, grad_norm: 21.559290
[[34m2025-10-03 23:36:18[0m] Step: 557, Training Logs: loss_final: 1.528688, loss_mean: 1.410554, proj_loss: -0.030128, loss_mean_cls: 0.110037, deep_loss: 0.038225, grad_norm: 19.823420
[[34m2025-10-03 23:36:19[0m] Step: 558, Training Logs: loss_final: 1.524853, loss_mean: 1.406385, proj_loss: -0.030680, loss_mean_cls: 0.109971, deep_loss: 0.039176, grad_norm: 23.595114
[[34m2025-10-03 23:36:21[0m] Step: 559, Training Logs: loss_final: 1.530660, loss_mean: 1.414411, proj_loss: -0.031987, loss_mean_cls: 0.108970, deep_loss: 0.039266, grad_norm: 23.241877
[[34m2025-10-03 23:36:22[0m] Step: 560, Training Logs: loss_final: 1.504212, loss_mean: 1.386288, proj_loss: -0.031751, loss_mean_cls: 0.110657, deep_loss: 0.039018, grad_norm: 24.259106
[[34m2025-10-03 23:36:23[0m] Step: 561, Training Logs: loss_final: 1.500712, loss_mean: 1.381176, proj_loss: -0.030804, loss_mean_cls: 0.110692, deep_loss: 0.039648, grad_norm: 25.161293
[[34m2025-10-03 23:36:24[0m] Step: 562, Training Logs: loss_final: 1.509188, loss_mean: 1.393400, proj_loss: -0.032902, loss_mean_cls: 0.109511, deep_loss: 0.039179, grad_norm: 23.875490
[[34m2025-10-03 23:36:25[0m] Step: 563, Training Logs: loss_final: 1.512452, loss_mean: 1.395287, proj_loss: -0.031551, loss_mean_cls: 0.110342, deep_loss: 0.038373, grad_norm: 23.721609
[[34m2025-10-03 23:36:25[0m] Step: 564, Training Logs: loss_final: 1.522577, loss_mean: 1.404962, proj_loss: -0.031149, loss_mean_cls: 0.109544, deep_loss: 0.039220, grad_norm: 22.588345
[[34m2025-10-03 23:36:26[0m] Step: 565, Training Logs: loss_final: 1.533742, loss_mean: 1.415923, proj_loss: -0.030160, loss_mean_cls: 0.108882, deep_loss: 0.039097, grad_norm: 20.903332
[[34m2025-10-03 23:36:27[0m] Step: 566, Training Logs: loss_final: 1.511473, loss_mean: 1.394419, proj_loss: -0.031083, loss_mean_cls: 0.109140, deep_loss: 0.038997, grad_norm: 26.269846
[[34m2025-10-03 23:36:28[0m] Step: 567, Training Logs: loss_final: 1.510077, loss_mean: 1.391888, proj_loss: -0.029861, loss_mean_cls: 0.108873, deep_loss: 0.039176, grad_norm: 23.886538
[[34m2025-10-03 23:36:29[0m] Step: 568, Training Logs: loss_final: 1.507361, loss_mean: 1.390789, proj_loss: -0.031788, loss_mean_cls: 0.108804, deep_loss: 0.039557, grad_norm: 18.489534
[[34m2025-10-03 23:36:31[0m] Step: 569, Training Logs: loss_final: 1.535880, loss_mean: 1.418883, proj_loss: -0.031015, loss_mean_cls: 0.108958, deep_loss: 0.039055, grad_norm: 24.483467
[[34m2025-10-03 23:36:32[0m] Step: 570, Training Logs: loss_final: 1.505543, loss_mean: 1.386428, proj_loss: -0.030974, loss_mean_cls: 0.110641, deep_loss: 0.039447, grad_norm: 22.674603
[[34m2025-10-03 23:36:33[0m] Step: 571, Training Logs: loss_final: 1.513207, loss_mean: 1.398134, proj_loss: -0.032324, loss_mean_cls: 0.108861, deep_loss: 0.038536, grad_norm: 26.659555
[[34m2025-10-03 23:36:34[0m] Step: 572, Training Logs: loss_final: 1.514055, loss_mean: 1.398397, proj_loss: -0.032704, loss_mean_cls: 0.109301, deep_loss: 0.039060, grad_norm: 27.232084
[[34m2025-10-03 23:36:35[0m] Step: 573, Training Logs: loss_final: 1.516080, loss_mean: 1.397632, proj_loss: -0.030729, loss_mean_cls: 0.110226, deep_loss: 0.038950, grad_norm: 27.048096
[[34m2025-10-03 23:36:37[0m] Step: 574, Training Logs: loss_final: 1.527734, loss_mean: 1.411675, proj_loss: -0.031989, loss_mean_cls: 0.109478, deep_loss: 0.038570, grad_norm: 32.017212
[[34m2025-10-03 23:36:38[0m] Step: 575, Training Logs: loss_final: 1.523347, loss_mean: 1.409052, proj_loss: -0.031964, loss_mean_cls: 0.108676, deep_loss: 0.037583, grad_norm: 31.215153
[[34m2025-10-03 23:36:39[0m] Step: 576, Training Logs: loss_final: 1.536031, loss_mean: 1.421028, proj_loss: -0.031512, loss_mean_cls: 0.107763, deep_loss: 0.038751, grad_norm: 33.084854
[[34m2025-10-03 23:36:40[0m] Step: 577, Training Logs: loss_final: 1.522685, loss_mean: 1.405950, proj_loss: -0.030319, loss_mean_cls: 0.109242, deep_loss: 0.037812, grad_norm: 34.623672
[[34m2025-10-03 23:36:41[0m] Step: 578, Training Logs: loss_final: 1.497222, loss_mean: 1.378858, proj_loss: -0.029939, loss_mean_cls: 0.111510, deep_loss: 0.036793, grad_norm: 27.451960
[[34m2025-10-03 23:36:42[0m] Step: 579, Training Logs: loss_final: 1.517473, loss_mean: 1.402390, proj_loss: -0.030895, loss_mean_cls: 0.108997, deep_loss: 0.036980, grad_norm: 26.353634
[[34m2025-10-03 23:36:44[0m] Step: 580, Training Logs: loss_final: 1.558198, loss_mean: 1.445147, proj_loss: -0.030458, loss_mean_cls: 0.106888, deep_loss: 0.036621, grad_norm: 36.688271
[[34m2025-10-03 23:36:45[0m] Step: 581, Training Logs: loss_final: 1.510942, loss_mean: 1.397225, proj_loss: -0.032272, loss_mean_cls: 0.109140, deep_loss: 0.036849, grad_norm: 32.231476
[[34m2025-10-03 23:36:46[0m] Step: 582, Training Logs: loss_final: 1.502099, loss_mean: 1.388129, proj_loss: -0.032532, loss_mean_cls: 0.109187, deep_loss: 0.037315, grad_norm: 24.325897
[[34m2025-10-03 23:36:47[0m] Step: 583, Training Logs: loss_final: 1.538499, loss_mean: 1.424734, proj_loss: -0.031489, loss_mean_cls: 0.107895, deep_loss: 0.037359, grad_norm: 30.561914
[[34m2025-10-03 23:36:48[0m] Step: 584, Training Logs: loss_final: 1.542633, loss_mean: 1.425200, proj_loss: -0.029868, loss_mean_cls: 0.109452, deep_loss: 0.037848, grad_norm: 32.501835
[[34m2025-10-03 23:36:49[0m] Step: 585, Training Logs: loss_final: 1.526550, loss_mean: 1.410504, proj_loss: -0.029870, loss_mean_cls: 0.109494, deep_loss: 0.036422, grad_norm: 26.078505
[[34m2025-10-03 23:36:50[0m] Step: 586, Training Logs: loss_final: 1.521811, loss_mean: 1.408475, proj_loss: -0.031345, loss_mean_cls: 0.108571, deep_loss: 0.036111, grad_norm: 29.632685
[[34m2025-10-03 23:36:52[0m] Step: 587, Training Logs: loss_final: 1.515360, loss_mean: 1.401259, proj_loss: -0.030875, loss_mean_cls: 0.109177, deep_loss: 0.035799, grad_norm: 27.060066
[[34m2025-10-03 23:36:53[0m] Step: 588, Training Logs: loss_final: 1.511256, loss_mean: 1.394484, proj_loss: -0.029828, loss_mean_cls: 0.109167, deep_loss: 0.037432, grad_norm: 29.470667
[[34m2025-10-03 23:36:54[0m] Step: 589, Training Logs: loss_final: 1.491035, loss_mean: 1.375704, proj_loss: -0.031258, loss_mean_cls: 0.110321, deep_loss: 0.036268, grad_norm: 33.274250
[[34m2025-10-03 23:36:55[0m] Step: 590, Training Logs: loss_final: 1.498302, loss_mean: 1.382179, proj_loss: -0.030207, loss_mean_cls: 0.109774, deep_loss: 0.036555, grad_norm: 27.855305
[[34m2025-10-03 23:36:56[0m] Step: 591, Training Logs: loss_final: 1.522332, loss_mean: 1.407777, proj_loss: -0.030168, loss_mean_cls: 0.108970, deep_loss: 0.035753, grad_norm: 31.293634
[[34m2025-10-03 23:36:57[0m] Step: 592, Training Logs: loss_final: 1.505297, loss_mean: 1.389860, proj_loss: -0.030406, loss_mean_cls: 0.110274, deep_loss: 0.035568, grad_norm: 34.946663
[[34m2025-10-03 23:36:59[0m] Step: 593, Training Logs: loss_final: 1.510969, loss_mean: 1.394588, proj_loss: -0.030929, loss_mean_cls: 0.111114, deep_loss: 0.036197, grad_norm: 25.977104
[[34m2025-10-03 23:37:00[0m] Step: 594, Training Logs: loss_final: 1.517461, loss_mean: 1.404839, proj_loss: -0.029817, loss_mean_cls: 0.108837, deep_loss: 0.033602, grad_norm: 28.448891
[[34m2025-10-03 23:37:00[0m] Step: 595, Training Logs: loss_final: 1.514200, loss_mean: 1.399375, proj_loss: -0.030910, loss_mean_cls: 0.109881, deep_loss: 0.035855, grad_norm: 40.942360
[[34m2025-10-03 23:37:01[0m] Step: 596, Training Logs: loss_final: 1.509127, loss_mean: 1.392907, proj_loss: -0.031098, loss_mean_cls: 0.110620, deep_loss: 0.036698, grad_norm: 31.861868
[[34m2025-10-03 23:37:02[0m] Step: 597, Training Logs: loss_final: 1.503376, loss_mean: 1.388084, proj_loss: -0.030313, loss_mean_cls: 0.109160, deep_loss: 0.036444, grad_norm: 27.981873
[[34m2025-10-03 23:37:03[0m] Step: 598, Training Logs: loss_final: 1.502446, loss_mean: 1.390261, proj_loss: -0.031263, loss_mean_cls: 0.109327, deep_loss: 0.034121, grad_norm: 35.112476
[[34m2025-10-03 23:37:04[0m] Step: 599, Training Logs: loss_final: 1.490156, loss_mean: 1.376940, proj_loss: -0.031482, loss_mean_cls: 0.109317, deep_loss: 0.035381, grad_norm: 30.936762
[[34m2025-10-03 23:37:05[0m] Step: 600, Training Logs: loss_final: 1.504845, loss_mean: 1.391951, proj_loss: -0.030770, loss_mean_cls: 0.108916, deep_loss: 0.034748, grad_norm: 25.158918
[[34m2025-10-03 23:37:06[0m] Step: 601, Training Logs: loss_final: 1.495756, loss_mean: 1.380506, proj_loss: -0.030966, loss_mean_cls: 0.109940, deep_loss: 0.036276, grad_norm: 32.983025
[[34m2025-10-03 23:37:08[0m] Step: 602, Training Logs: loss_final: 1.518332, loss_mean: 1.402500, proj_loss: -0.029356, loss_mean_cls: 0.109500, deep_loss: 0.035688, grad_norm: 34.027706
[[34m2025-10-03 23:37:09[0m] Step: 603, Training Logs: loss_final: 1.508543, loss_mean: 1.394455, proj_loss: -0.030531, loss_mean_cls: 0.109691, deep_loss: 0.034929, grad_norm: 29.299767
[[34m2025-10-03 23:37:10[0m] Step: 604, Training Logs: loss_final: 1.506286, loss_mean: 1.393178, proj_loss: -0.030873, loss_mean_cls: 0.109294, deep_loss: 0.034686, grad_norm: 28.918423
[[34m2025-10-03 23:37:11[0m] Step: 605, Training Logs: loss_final: 1.503210, loss_mean: 1.388064, proj_loss: -0.030333, loss_mean_cls: 0.109881, deep_loss: 0.035599, grad_norm: 34.973751
[[34m2025-10-03 23:37:12[0m] Step: 606, Training Logs: loss_final: 1.481960, loss_mean: 1.368360, proj_loss: -0.031160, loss_mean_cls: 0.110083, deep_loss: 0.034677, grad_norm: 30.612713
[[34m2025-10-03 23:37:13[0m] Step: 607, Training Logs: loss_final: 1.491072, loss_mean: 1.377103, proj_loss: -0.030669, loss_mean_cls: 0.109280, deep_loss: 0.035358, grad_norm: 32.655083
[[34m2025-10-03 23:37:15[0m] Step: 608, Training Logs: loss_final: 1.510495, loss_mean: 1.396452, proj_loss: -0.031643, loss_mean_cls: 0.109741, deep_loss: 0.035946, grad_norm: 35.254372
[[34m2025-10-03 23:37:16[0m] Step: 609, Training Logs: loss_final: 1.510291, loss_mean: 1.395135, proj_loss: -0.030944, loss_mean_cls: 0.110611, deep_loss: 0.035490, grad_norm: 32.381298
[[34m2025-10-03 23:37:17[0m] Step: 610, Training Logs: loss_final: 1.523110, loss_mean: 1.408127, proj_loss: -0.029642, loss_mean_cls: 0.109355, deep_loss: 0.035270, grad_norm: 43.554050
[[34m2025-10-03 23:37:18[0m] Step: 611, Training Logs: loss_final: 1.526118, loss_mean: 1.411744, proj_loss: -0.030297, loss_mean_cls: 0.109051, deep_loss: 0.035621, grad_norm: 41.308094
[[34m2025-10-03 23:37:19[0m] Step: 612, Training Logs: loss_final: 1.526544, loss_mean: 1.412453, proj_loss: -0.029876, loss_mean_cls: 0.108470, deep_loss: 0.035497, grad_norm: 33.219955
[[34m2025-10-03 23:37:20[0m] Step: 613, Training Logs: loss_final: 1.518485, loss_mean: 1.405621, proj_loss: -0.030815, loss_mean_cls: 0.109721, deep_loss: 0.033958, grad_norm: 41.098976
[[34m2025-10-03 23:37:22[0m] Step: 614, Training Logs: loss_final: 1.533149, loss_mean: 1.421262, proj_loss: -0.031173, loss_mean_cls: 0.107914, deep_loss: 0.035146, grad_norm: 42.399509
[[34m2025-10-03 23:37:23[0m] Step: 615, Training Logs: loss_final: 1.499713, loss_mean: 1.384104, proj_loss: -0.028962, loss_mean_cls: 0.109784, deep_loss: 0.034787, grad_norm: 33.671253
[[34m2025-10-03 23:37:24[0m] Step: 616, Training Logs: loss_final: 1.514940, loss_mean: 1.405184, proj_loss: -0.031904, loss_mean_cls: 0.107956, deep_loss: 0.033703, grad_norm: 41.479412
[[34m2025-10-03 23:37:25[0m] Step: 617, Training Logs: loss_final: 1.535798, loss_mean: 1.421905, proj_loss: -0.029620, loss_mean_cls: 0.108175, deep_loss: 0.035338, grad_norm: 49.187817
[[34m2025-10-03 23:37:26[0m] Step: 618, Training Logs: loss_final: 1.456471, loss_mean: 1.341286, proj_loss: -0.030296, loss_mean_cls: 0.111067, deep_loss: 0.034415, grad_norm: 41.944553
[[34m2025-10-03 23:37:27[0m] Step: 619, Training Logs: loss_final: 1.520020, loss_mean: 1.408448, proj_loss: -0.031262, loss_mean_cls: 0.108473, deep_loss: 0.034360, grad_norm: 29.731968
[[34m2025-10-03 23:37:29[0m] Step: 620, Training Logs: loss_final: 1.493883, loss_mean: 1.383222, proj_loss: -0.031392, loss_mean_cls: 0.107868, deep_loss: 0.034185, grad_norm: 41.580177
[[34m2025-10-03 23:37:30[0m] Step: 621, Training Logs: loss_final: 1.511589, loss_mean: 1.398147, proj_loss: -0.030454, loss_mean_cls: 0.109301, deep_loss: 0.034595, grad_norm: 48.165054
[[34m2025-10-03 23:37:31[0m] Step: 622, Training Logs: loss_final: 1.485308, loss_mean: 1.372821, proj_loss: -0.031025, loss_mean_cls: 0.110328, deep_loss: 0.033183, grad_norm: 37.177265
[[34m2025-10-03 23:37:32[0m] Step: 623, Training Logs: loss_final: 1.506063, loss_mean: 1.393993, proj_loss: -0.030071, loss_mean_cls: 0.108217, deep_loss: 0.033924, grad_norm: 34.693249
[[34m2025-10-03 23:37:33[0m] Step: 624, Training Logs: loss_final: 1.497696, loss_mean: 1.385860, proj_loss: -0.031537, loss_mean_cls: 0.108966, deep_loss: 0.034407, grad_norm: 40.011276
[[34m2025-10-03 23:37:35[0m] Step: 625, Training Logs: loss_final: 1.510250, loss_mean: 1.398125, proj_loss: -0.030573, loss_mean_cls: 0.108525, deep_loss: 0.034173, grad_norm: 42.166893
[[34m2025-10-03 23:37:35[0m] Step: 626, Training Logs: loss_final: 1.521900, loss_mean: 1.408718, proj_loss: -0.029409, loss_mean_cls: 0.107909, deep_loss: 0.034682, grad_norm: 36.506077
[[34m2025-10-03 23:37:36[0m] Step: 627, Training Logs: loss_final: 1.502319, loss_mean: 1.391793, proj_loss: -0.031783, loss_mean_cls: 0.108287, deep_loss: 0.034022, grad_norm: 35.441212
[[34m2025-10-03 23:37:37[0m] Step: 628, Training Logs: loss_final: 1.502583, loss_mean: 1.389869, proj_loss: -0.031313, loss_mean_cls: 0.110202, deep_loss: 0.033825, grad_norm: 42.277130
[[34m2025-10-03 23:37:38[0m] Step: 629, Training Logs: loss_final: 1.496452, loss_mean: 1.381528, proj_loss: -0.028671, loss_mean_cls: 0.109184, deep_loss: 0.034412, grad_norm: 40.796555
[[34m2025-10-03 23:37:39[0m] Step: 630, Training Logs: loss_final: 1.506282, loss_mean: 1.393837, proj_loss: -0.030474, loss_mean_cls: 0.108337, deep_loss: 0.034582, grad_norm: 36.684586
[[34m2025-10-03 23:37:40[0m] Step: 631, Training Logs: loss_final: 1.486550, loss_mean: 1.374162, proj_loss: -0.031476, loss_mean_cls: 0.108799, deep_loss: 0.035065, grad_norm: 42.953983
[[34m2025-10-03 23:37:41[0m] Step: 632, Training Logs: loss_final: 1.488670, loss_mean: 1.377026, proj_loss: -0.032082, loss_mean_cls: 0.109206, deep_loss: 0.034520, grad_norm: 45.239880
[[34m2025-10-03 23:37:43[0m] Step: 633, Training Logs: loss_final: 1.527776, loss_mean: 1.415433, proj_loss: -0.030964, loss_mean_cls: 0.108483, deep_loss: 0.034824, grad_norm: 40.845203
[[34m2025-10-03 23:37:44[0m] Step: 634, Training Logs: loss_final: 1.517631, loss_mean: 1.403788, proj_loss: -0.029033, loss_mean_cls: 0.109166, deep_loss: 0.033710, grad_norm: 44.917370
[[34m2025-10-03 23:37:45[0m] Step: 635, Training Logs: loss_final: 1.507722, loss_mean: 1.395897, proj_loss: -0.030733, loss_mean_cls: 0.108621, deep_loss: 0.033937, grad_norm: 41.158188
[[34m2025-10-03 23:37:46[0m] Step: 636, Training Logs: loss_final: 1.530552, loss_mean: 1.414807, proj_loss: -0.029184, loss_mean_cls: 0.108996, deep_loss: 0.035933, grad_norm: 40.910889
[[34m2025-10-03 23:37:47[0m] Step: 637, Training Logs: loss_final: 1.542391, loss_mean: 1.431983, proj_loss: -0.031624, loss_mean_cls: 0.108366, deep_loss: 0.033665, grad_norm: 39.771564
[[34m2025-10-03 23:37:48[0m] Step: 638, Training Logs: loss_final: 1.550308, loss_mean: 1.438831, proj_loss: -0.031021, loss_mean_cls: 0.107458, deep_loss: 0.035041, grad_norm: 40.990711
[[34m2025-10-03 23:37:50[0m] Step: 639, Training Logs: loss_final: 1.515720, loss_mean: 1.402051, proj_loss: -0.029649, loss_mean_cls: 0.108915, deep_loss: 0.034403, grad_norm: 42.353302
[[34m2025-10-03 23:37:51[0m] Step: 640, Training Logs: loss_final: 1.527600, loss_mean: 1.416613, proj_loss: -0.030974, loss_mean_cls: 0.108031, deep_loss: 0.033930, grad_norm: 32.810467
[[34m2025-10-03 23:37:52[0m] Step: 641, Training Logs: loss_final: 1.535588, loss_mean: 1.425549, proj_loss: -0.030737, loss_mean_cls: 0.106684, deep_loss: 0.034092, grad_norm: 34.485710
[[34m2025-10-03 23:37:53[0m] Step: 642, Training Logs: loss_final: 1.514294, loss_mean: 1.401768, proj_loss: -0.028971, loss_mean_cls: 0.108189, deep_loss: 0.033307, grad_norm: 37.536243
[[34m2025-10-03 23:37:54[0m] Step: 643, Training Logs: loss_final: 1.482138, loss_mean: 1.370346, proj_loss: -0.031233, loss_mean_cls: 0.109347, deep_loss: 0.033678, grad_norm: 34.954727
[[34m2025-10-03 23:37:55[0m] Step: 644, Training Logs: loss_final: 1.504623, loss_mean: 1.393931, proj_loss: -0.030647, loss_mean_cls: 0.108878, deep_loss: 0.032462, grad_norm: 35.135792
[[34m2025-10-03 23:37:57[0m] Step: 645, Training Logs: loss_final: 1.507497, loss_mean: 1.395319, proj_loss: -0.029693, loss_mean_cls: 0.107990, deep_loss: 0.033881, grad_norm: 30.371283
[[34m2025-10-03 23:37:58[0m] Step: 646, Training Logs: loss_final: 1.490566, loss_mean: 1.378960, proj_loss: -0.030297, loss_mean_cls: 0.109170, deep_loss: 0.032734, grad_norm: 39.249874
[[34m2025-10-03 23:37:59[0m] Step: 647, Training Logs: loss_final: 1.488676, loss_mean: 1.378777, proj_loss: -0.030600, loss_mean_cls: 0.108086, deep_loss: 0.032413, grad_norm: 34.913292
[[34m2025-10-03 23:38:00[0m] Step: 648, Training Logs: loss_final: 1.505421, loss_mean: 1.391897, proj_loss: -0.030308, loss_mean_cls: 0.110475, deep_loss: 0.033357, grad_norm: 37.444141
[[34m2025-10-03 23:38:01[0m] Step: 649, Training Logs: loss_final: 1.472938, loss_mean: 1.361672, proj_loss: -0.031120, loss_mean_cls: 0.109892, deep_loss: 0.032494, grad_norm: 37.040237
[[34m2025-10-03 23:38:02[0m] Step: 650, Training Logs: loss_final: 1.493220, loss_mean: 1.381010, proj_loss: -0.031197, loss_mean_cls: 0.109437, deep_loss: 0.033969, grad_norm: 28.042894
[[34m2025-10-03 23:38:04[0m] Step: 651, Training Logs: loss_final: 1.492481, loss_mean: 1.381460, proj_loss: -0.031036, loss_mean_cls: 0.108887, deep_loss: 0.033169, grad_norm: 30.687939
[[34m2025-10-03 23:38:05[0m] Step: 652, Training Logs: loss_final: 1.492037, loss_mean: 1.379598, proj_loss: -0.030347, loss_mean_cls: 0.109272, deep_loss: 0.033515, grad_norm: 34.597771
[[34m2025-10-03 23:38:06[0m] Step: 653, Training Logs: loss_final: 1.484934, loss_mean: 1.375107, proj_loss: -0.032460, loss_mean_cls: 0.109271, deep_loss: 0.033017, grad_norm: 30.141882
[[34m2025-10-03 23:38:07[0m] Step: 654, Training Logs: loss_final: 1.508114, loss_mean: 1.396618, proj_loss: -0.031258, loss_mean_cls: 0.108487, deep_loss: 0.034267, grad_norm: 37.113655
[[34m2025-10-03 23:38:08[0m] Step: 655, Training Logs: loss_final: 1.495102, loss_mean: 1.383023, proj_loss: -0.029924, loss_mean_cls: 0.108221, deep_loss: 0.033782, grad_norm: 34.077175
[[34m2025-10-03 23:38:09[0m] Step: 656, Training Logs: loss_final: 1.514135, loss_mean: 1.402447, proj_loss: -0.030447, loss_mean_cls: 0.107809, deep_loss: 0.034326, grad_norm: 33.950890
[[34m2025-10-03 23:38:10[0m] Step: 657, Training Logs: loss_final: 1.500907, loss_mean: 1.388555, proj_loss: -0.030881, loss_mean_cls: 0.109180, deep_loss: 0.034053, grad_norm: 30.005188
[[34m2025-10-03 23:38:11[0m] Step: 658, Training Logs: loss_final: 1.488770, loss_mean: 1.376495, proj_loss: -0.030861, loss_mean_cls: 0.109072, deep_loss: 0.034064, grad_norm: 34.110043
[[34m2025-10-03 23:38:12[0m] Step: 659, Training Logs: loss_final: 1.498121, loss_mean: 1.386544, proj_loss: -0.030962, loss_mean_cls: 0.108787, deep_loss: 0.033752, grad_norm: 37.992702
[[34m2025-10-03 23:38:13[0m] Step: 660, Training Logs: loss_final: 1.513671, loss_mean: 1.398292, proj_loss: -0.028995, loss_mean_cls: 0.109448, deep_loss: 0.034926, grad_norm: 31.348043
[[34m2025-10-03 23:38:14[0m] Step: 661, Training Logs: loss_final: 1.505883, loss_mean: 1.393798, proj_loss: -0.030415, loss_mean_cls: 0.109302, deep_loss: 0.033198, grad_norm: 34.753803
[[34m2025-10-03 23:38:15[0m] Step: 662, Training Logs: loss_final: 1.498703, loss_mean: 1.385264, proj_loss: -0.031099, loss_mean_cls: 0.109943, deep_loss: 0.034595, grad_norm: 33.658318
[[34m2025-10-03 23:38:16[0m] Step: 663, Training Logs: loss_final: 1.511586, loss_mean: 1.399309, proj_loss: -0.030395, loss_mean_cls: 0.108448, deep_loss: 0.034224, grad_norm: 36.022484
[[34m2025-10-03 23:38:17[0m] Step: 664, Training Logs: loss_final: 1.475413, loss_mean: 1.363898, proj_loss: -0.030100, loss_mean_cls: 0.108998, deep_loss: 0.032616, grad_norm: 35.828217
[[34m2025-10-03 23:38:18[0m] Step: 665, Training Logs: loss_final: 1.524853, loss_mean: 1.411115, proj_loss: -0.030333, loss_mean_cls: 0.109718, deep_loss: 0.034353, grad_norm: 41.735363
[[34m2025-10-03 23:38:20[0m] Step: 666, Training Logs: loss_final: 1.509102, loss_mean: 1.396271, proj_loss: -0.030425, loss_mean_cls: 0.108467, deep_loss: 0.034789, grad_norm: 43.664619
[[34m2025-10-03 23:38:21[0m] Step: 667, Training Logs: loss_final: 1.509430, loss_mean: 1.397645, proj_loss: -0.031385, loss_mean_cls: 0.108880, deep_loss: 0.034290, grad_norm: 46.144039
[[34m2025-10-03 23:38:22[0m] Step: 668, Training Logs: loss_final: 1.501793, loss_mean: 1.390352, proj_loss: -0.031798, loss_mean_cls: 0.109032, deep_loss: 0.034207, grad_norm: 44.092144
[[34m2025-10-03 23:38:23[0m] Step: 669, Training Logs: loss_final: 1.522303, loss_mean: 1.409567, proj_loss: -0.030745, loss_mean_cls: 0.108561, deep_loss: 0.034919, grad_norm: 44.864632
[[34m2025-10-03 23:38:24[0m] Step: 670, Training Logs: loss_final: 1.537943, loss_mean: 1.425801, proj_loss: -0.030858, loss_mean_cls: 0.108441, deep_loss: 0.034560, grad_norm: 46.509907
[[34m2025-10-03 23:38:25[0m] Step: 671, Training Logs: loss_final: 1.511031, loss_mean: 1.399786, proj_loss: -0.031073, loss_mean_cls: 0.108144, deep_loss: 0.034174, grad_norm: 48.357712
[[34m2025-10-03 23:38:27[0m] Step: 672, Training Logs: loss_final: 1.528130, loss_mean: 1.415314, proj_loss: -0.031079, loss_mean_cls: 0.108454, deep_loss: 0.035441, grad_norm: 50.577354
[[34m2025-10-03 23:38:28[0m] Step: 673, Training Logs: loss_final: 1.530715, loss_mean: 1.420594, proj_loss: -0.032189, loss_mean_cls: 0.108082, deep_loss: 0.034228, grad_norm: 44.089592
[[34m2025-10-03 23:38:29[0m] Step: 674, Training Logs: loss_final: 1.498171, loss_mean: 1.386118, proj_loss: -0.030869, loss_mean_cls: 0.108941, deep_loss: 0.033981, grad_norm: 53.879696
[[34m2025-10-03 23:38:30[0m] Step: 675, Training Logs: loss_final: 1.493174, loss_mean: 1.381389, proj_loss: -0.031783, loss_mean_cls: 0.109106, deep_loss: 0.034462, grad_norm: 56.745899
[[34m2025-10-03 23:38:31[0m] Step: 676, Training Logs: loss_final: 1.515652, loss_mean: 1.402320, proj_loss: -0.030056, loss_mean_cls: 0.108616, deep_loss: 0.034773, grad_norm: 43.675152
[[34m2025-10-03 23:38:32[0m] Step: 677, Training Logs: loss_final: 1.498935, loss_mean: 1.387406, proj_loss: -0.030838, loss_mean_cls: 0.108962, deep_loss: 0.033404, grad_norm: 50.214737
[[34m2025-10-03 23:38:34[0m] Step: 678, Training Logs: loss_final: 1.550026, loss_mean: 1.438534, proj_loss: -0.030446, loss_mean_cls: 0.106505, deep_loss: 0.035434, grad_norm: 56.037670
[[34m2025-10-03 23:38:35[0m] Step: 679, Training Logs: loss_final: 1.549265, loss_mean: 1.438492, proj_loss: -0.031683, loss_mean_cls: 0.107177, deep_loss: 0.035279, grad_norm: 47.058712
[[34m2025-10-03 23:38:36[0m] Step: 680, Training Logs: loss_final: 1.502469, loss_mean: 1.389649, proj_loss: -0.030652, loss_mean_cls: 0.109846, deep_loss: 0.033626, grad_norm: 39.931953
[[34m2025-10-03 23:38:37[0m] Step: 681, Training Logs: loss_final: 1.534842, loss_mean: 1.423488, proj_loss: -0.030774, loss_mean_cls: 0.107316, deep_loss: 0.034812, grad_norm: 46.466476
[[34m2025-10-03 23:38:38[0m] Step: 682, Training Logs: loss_final: 1.521926, loss_mean: 1.409421, proj_loss: -0.031049, loss_mean_cls: 0.108443, deep_loss: 0.035111, grad_norm: 50.019348
[[34m2025-10-03 23:38:39[0m] Step: 683, Training Logs: loss_final: 1.520347, loss_mean: 1.407525, proj_loss: -0.028938, loss_mean_cls: 0.107662, deep_loss: 0.034097, grad_norm: 36.677071
[[34m2025-10-03 23:38:41[0m] Step: 684, Training Logs: loss_final: 1.506832, loss_mean: 1.393419, proj_loss: -0.029080, loss_mean_cls: 0.108829, deep_loss: 0.033663, grad_norm: 35.721222
[[34m2025-10-03 23:38:42[0m] Step: 685, Training Logs: loss_final: 1.514356, loss_mean: 1.403668, proj_loss: -0.031092, loss_mean_cls: 0.108541, deep_loss: 0.033240, grad_norm: 47.982193
[[34m2025-10-03 23:38:43[0m] Step: 686, Training Logs: loss_final: 1.513310, loss_mean: 1.403475, proj_loss: -0.031432, loss_mean_cls: 0.107153, deep_loss: 0.034114, grad_norm: 32.551411
[[34m2025-10-03 23:38:44[0m] Step: 687, Training Logs: loss_final: 1.491376, loss_mean: 1.379981, proj_loss: -0.031742, loss_mean_cls: 0.109463, deep_loss: 0.033674, grad_norm: 32.180374
[[34m2025-10-03 23:38:45[0m] Step: 688, Training Logs: loss_final: 1.489570, loss_mean: 1.377347, proj_loss: -0.030289, loss_mean_cls: 0.108712, deep_loss: 0.033800, grad_norm: 36.282932
[[34m2025-10-03 23:38:46[0m] Step: 689, Training Logs: loss_final: 1.477821, loss_mean: 1.367451, proj_loss: -0.030789, loss_mean_cls: 0.108276, deep_loss: 0.032882, grad_norm: 37.603024
[[34m2025-10-03 23:38:47[0m] Step: 690, Training Logs: loss_final: 1.473544, loss_mean: 1.362849, proj_loss: -0.030993, loss_mean_cls: 0.109401, deep_loss: 0.032287, grad_norm: 28.845873
[[34m2025-10-03 23:38:48[0m] Step: 691, Training Logs: loss_final: 1.491508, loss_mean: 1.381573, proj_loss: -0.031132, loss_mean_cls: 0.107758, deep_loss: 0.033308, grad_norm: 39.513542
[[34m2025-10-03 23:38:49[0m] Step: 692, Training Logs: loss_final: 1.475436, loss_mean: 1.365854, proj_loss: -0.032464, loss_mean_cls: 0.108569, deep_loss: 0.033477, grad_norm: 40.686115
[[34m2025-10-03 23:38:50[0m] Step: 693, Training Logs: loss_final: 1.491630, loss_mean: 1.382756, proj_loss: -0.032051, loss_mean_cls: 0.107488, deep_loss: 0.033438, grad_norm: 34.741398
[[34m2025-10-03 23:38:51[0m] Step: 694, Training Logs: loss_final: 1.516469, loss_mean: 1.405120, proj_loss: -0.030641, loss_mean_cls: 0.108140, deep_loss: 0.033850, grad_norm: 44.702374
[[34m2025-10-03 23:38:52[0m] Step: 695, Training Logs: loss_final: 1.496340, loss_mean: 1.382306, proj_loss: -0.028762, loss_mean_cls: 0.109314, deep_loss: 0.033481, grad_norm: 45.738331
[[34m2025-10-03 23:38:53[0m] Step: 696, Training Logs: loss_final: 1.522852, loss_mean: 1.411434, proj_loss: -0.029486, loss_mean_cls: 0.107855, deep_loss: 0.033049, grad_norm: 42.480919
[[34m2025-10-03 23:38:55[0m] Step: 697, Training Logs: loss_final: 1.519802, loss_mean: 1.408218, proj_loss: -0.030007, loss_mean_cls: 0.108631, deep_loss: 0.032959, grad_norm: 43.091434
[[34m2025-10-03 23:38:56[0m] Step: 698, Training Logs: loss_final: 1.540145, loss_mean: 1.426232, proj_loss: -0.029044, loss_mean_cls: 0.108588, deep_loss: 0.034369, grad_norm: 43.355209
[[34m2025-10-03 23:38:57[0m] Step: 699, Training Logs: loss_final: 1.529510, loss_mean: 1.416196, proj_loss: -0.030176, loss_mean_cls: 0.107782, deep_loss: 0.035708, grad_norm: 62.182819
[[34m2025-10-03 23:38:58[0m] Step: 700, Training Logs: loss_final: 1.518261, loss_mean: 1.403469, proj_loss: -0.029676, loss_mean_cls: 0.108775, deep_loss: 0.035692, grad_norm: 58.842308
[[34m2025-10-03 23:38:59[0m] Step: 701, Training Logs: loss_final: 1.460043, loss_mean: 1.345189, proj_loss: -0.029741, loss_mean_cls: 0.110212, deep_loss: 0.034383, grad_norm: 38.699211
[[34m2025-10-03 23:39:00[0m] Step: 702, Training Logs: loss_final: 1.480301, loss_mean: 1.367211, proj_loss: -0.030033, loss_mean_cls: 0.109257, deep_loss: 0.033866, grad_norm: 43.175911
[[34m2025-10-03 23:39:02[0m] Step: 703, Training Logs: loss_final: 1.517799, loss_mean: 1.405425, proj_loss: -0.029631, loss_mean_cls: 0.108153, deep_loss: 0.033851, grad_norm: 61.069874
[[34m2025-10-03 23:39:03[0m] Step: 704, Training Logs: loss_final: 1.516695, loss_mean: 1.403368, proj_loss: -0.030274, loss_mean_cls: 0.109051, deep_loss: 0.034551, grad_norm: 61.710258
[[34m2025-10-03 23:39:04[0m] Step: 705, Training Logs: loss_final: 1.542746, loss_mean: 1.429180, proj_loss: -0.031668, loss_mean_cls: 0.108647, deep_loss: 0.036586, grad_norm: 47.819420
[[34m2025-10-03 23:39:05[0m] Step: 706, Training Logs: loss_final: 1.523500, loss_mean: 1.408900, proj_loss: -0.031122, loss_mean_cls: 0.110303, deep_loss: 0.035419, grad_norm: 34.055958
[[34m2025-10-03 23:39:06[0m] Step: 707, Training Logs: loss_final: 1.550530, loss_mean: 1.435549, proj_loss: -0.031426, loss_mean_cls: 0.109898, deep_loss: 0.036509, grad_norm: 45.884911
[[34m2025-10-03 23:39:07[0m] Step: 708, Training Logs: loss_final: 1.537570, loss_mean: 1.423682, proj_loss: -0.032396, loss_mean_cls: 0.109674, deep_loss: 0.036610, grad_norm: 54.522308
[[34m2025-10-03 23:39:09[0m] Step: 709, Training Logs: loss_final: 1.544592, loss_mean: 1.428570, proj_loss: -0.031012, loss_mean_cls: 0.110780, deep_loss: 0.036254, grad_norm: 41.364155
[[34m2025-10-03 23:39:10[0m] Step: 710, Training Logs: loss_final: 1.529105, loss_mean: 1.415248, proj_loss: -0.031533, loss_mean_cls: 0.109561, deep_loss: 0.035829, grad_norm: 36.001236
[[34m2025-10-03 23:39:11[0m] Step: 711, Training Logs: loss_final: 1.544324, loss_mean: 1.430793, proj_loss: -0.031559, loss_mean_cls: 0.109275, deep_loss: 0.035816, grad_norm: 38.275402
[[34m2025-10-03 23:39:12[0m] Step: 712, Training Logs: loss_final: 1.543868, loss_mean: 1.430695, proj_loss: -0.031306, loss_mean_cls: 0.107920, deep_loss: 0.036559, grad_norm: 43.104561
[[34m2025-10-03 23:39:13[0m] Step: 713, Training Logs: loss_final: 1.555612, loss_mean: 1.443263, proj_loss: -0.032071, loss_mean_cls: 0.108285, deep_loss: 0.036135, grad_norm: 33.623962
[[34m2025-10-03 23:39:14[0m] Step: 714, Training Logs: loss_final: 1.535502, loss_mean: 1.421944, proj_loss: -0.030490, loss_mean_cls: 0.108583, deep_loss: 0.035465, grad_norm: 34.794479
[[34m2025-10-03 23:39:16[0m] Step: 715, Training Logs: loss_final: 1.541002, loss_mean: 1.426932, proj_loss: -0.030716, loss_mean_cls: 0.108319, deep_loss: 0.036468, grad_norm: 47.607483
[[34m2025-10-03 23:39:17[0m] Step: 716, Training Logs: loss_final: 1.559628, loss_mean: 1.444299, proj_loss: -0.031415, loss_mean_cls: 0.110346, deep_loss: 0.036398, grad_norm: 36.569481
[[34m2025-10-03 23:39:18[0m] Step: 717, Training Logs: loss_final: 1.530882, loss_mean: 1.416842, proj_loss: -0.030501, loss_mean_cls: 0.109287, deep_loss: 0.035255, grad_norm: 35.616467
[[34m2025-10-03 23:39:19[0m] Step: 718, Training Logs: loss_final: 1.549510, loss_mean: 1.437813, proj_loss: -0.031718, loss_mean_cls: 0.107895, deep_loss: 0.035520, grad_norm: 49.137375
[[34m2025-10-03 23:39:20[0m] Step: 719, Training Logs: loss_final: 1.535741, loss_mean: 1.419472, proj_loss: -0.029698, loss_mean_cls: 0.109843, deep_loss: 0.036124, grad_norm: 52.664841
[[34m2025-10-03 23:39:21[0m] Step: 720, Training Logs: loss_final: 1.498700, loss_mean: 1.383362, proj_loss: -0.031328, loss_mean_cls: 0.111078, deep_loss: 0.035589, grad_norm: 32.215466
[[34m2025-10-03 23:39:22[0m] Step: 721, Training Logs: loss_final: 1.516820, loss_mean: 1.403496, proj_loss: -0.030691, loss_mean_cls: 0.108326, deep_loss: 0.035688, grad_norm: 44.542004
[[34m2025-10-03 23:39:22[0m] Step: 722, Training Logs: loss_final: 1.525931, loss_mean: 1.409081, proj_loss: -0.029527, loss_mean_cls: 0.109888, deep_loss: 0.036490, grad_norm: 52.445160
[[34m2025-10-03 23:39:23[0m] Step: 723, Training Logs: loss_final: 1.529596, loss_mean: 1.415414, proj_loss: -0.031245, loss_mean_cls: 0.109254, deep_loss: 0.036172, grad_norm: 39.461147
[[34m2025-10-03 23:39:24[0m] Step: 724, Training Logs: loss_final: 1.496686, loss_mean: 1.383739, proj_loss: -0.031457, loss_mean_cls: 0.108509, deep_loss: 0.035894, grad_norm: 39.508289
[[34m2025-10-03 23:39:25[0m] Step: 725, Training Logs: loss_final: 1.512048, loss_mean: 1.399135, proj_loss: -0.031420, loss_mean_cls: 0.108977, deep_loss: 0.035355, grad_norm: 39.873917
[[34m2025-10-03 23:39:26[0m] Step: 726, Training Logs: loss_final: 1.516986, loss_mean: 1.405128, proj_loss: -0.031370, loss_mean_cls: 0.108183, deep_loss: 0.035046, grad_norm: 37.975441
[[34m2025-10-03 23:39:28[0m] Step: 727, Training Logs: loss_final: 1.513469, loss_mean: 1.401202, proj_loss: -0.031485, loss_mean_cls: 0.107872, deep_loss: 0.035880, grad_norm: 41.331444
[[34m2025-10-03 23:39:29[0m] Step: 728, Training Logs: loss_final: 1.482038, loss_mean: 1.370760, proj_loss: -0.032111, loss_mean_cls: 0.108562, deep_loss: 0.034827, grad_norm: 41.014011
[[34m2025-10-03 23:39:30[0m] Step: 729, Training Logs: loss_final: 1.505673, loss_mean: 1.393684, proj_loss: -0.031646, loss_mean_cls: 0.108187, deep_loss: 0.035448, grad_norm: 37.992836
[[34m2025-10-03 23:39:31[0m] Step: 730, Training Logs: loss_final: 1.510333, loss_mean: 1.397223, proj_loss: -0.030562, loss_mean_cls: 0.109030, deep_loss: 0.034642, grad_norm: 43.875229
[[34m2025-10-03 23:39:32[0m] Step: 731, Training Logs: loss_final: 1.515639, loss_mean: 1.403356, proj_loss: -0.030189, loss_mean_cls: 0.107650, deep_loss: 0.034822, grad_norm: 40.674725
[[34m2025-10-03 23:39:33[0m] Step: 732, Training Logs: loss_final: 1.490256, loss_mean: 1.380024, proj_loss: -0.032713, loss_mean_cls: 0.108294, deep_loss: 0.034650, grad_norm: 31.311611
[[34m2025-10-03 23:39:35[0m] Step: 733, Training Logs: loss_final: 1.500206, loss_mean: 1.388185, proj_loss: -0.031664, loss_mean_cls: 0.109646, deep_loss: 0.034039, grad_norm: 44.924580
[[34m2025-10-03 23:39:36[0m] Step: 734, Training Logs: loss_final: 1.505378, loss_mean: 1.394791, proj_loss: -0.030479, loss_mean_cls: 0.107723, deep_loss: 0.033343, grad_norm: 48.009033
[[34m2025-10-03 23:39:37[0m] Step: 735, Training Logs: loss_final: 1.508434, loss_mean: 1.396264, proj_loss: -0.030749, loss_mean_cls: 0.109841, deep_loss: 0.033078, grad_norm: 41.023315
[[34m2025-10-03 23:39:38[0m] Step: 736, Training Logs: loss_final: 1.505569, loss_mean: 1.394737, proj_loss: -0.031152, loss_mean_cls: 0.108769, deep_loss: 0.033215, grad_norm: 36.540367
[[34m2025-10-03 23:39:39[0m] Step: 737, Training Logs: loss_final: 1.475120, loss_mean: 1.365527, proj_loss: -0.031726, loss_mean_cls: 0.108691, deep_loss: 0.032629, grad_norm: 43.541428
[[34m2025-10-03 23:39:40[0m] Step: 738, Training Logs: loss_final: 1.544283, loss_mean: 1.433404, proj_loss: -0.030390, loss_mean_cls: 0.107063, deep_loss: 0.034207, grad_norm: 40.441605
[[34m2025-10-03 23:39:42[0m] Step: 739, Training Logs: loss_final: 1.513400, loss_mean: 1.401256, proj_loss: -0.030391, loss_mean_cls: 0.108314, deep_loss: 0.034221, grad_norm: 34.240246
[[34m2025-10-03 23:39:43[0m] Step: 740, Training Logs: loss_final: 1.511527, loss_mean: 1.401663, proj_loss: -0.031548, loss_mean_cls: 0.107562, deep_loss: 0.033850, grad_norm: 38.322372
[[34m2025-10-03 23:39:44[0m] Step: 741, Training Logs: loss_final: 1.530031, loss_mean: 1.418215, proj_loss: -0.030814, loss_mean_cls: 0.108522, deep_loss: 0.034109, grad_norm: 42.437180
[[34m2025-10-03 23:39:45[0m] Step: 742, Training Logs: loss_final: 1.501058, loss_mean: 1.390061, proj_loss: -0.029895, loss_mean_cls: 0.108126, deep_loss: 0.032766, grad_norm: 33.644348
[[34m2025-10-03 23:39:46[0m] Step: 743, Training Logs: loss_final: 1.494208, loss_mean: 1.384442, proj_loss: -0.032570, loss_mean_cls: 0.108493, deep_loss: 0.033843, grad_norm: 41.790676
[[34m2025-10-03 23:39:47[0m] Step: 744, Training Logs: loss_final: 1.513092, loss_mean: 1.402007, proj_loss: -0.030242, loss_mean_cls: 0.108289, deep_loss: 0.033039, grad_norm: 48.698830
[[34m2025-10-03 23:39:49[0m] Step: 745, Training Logs: loss_final: 1.493818, loss_mean: 1.381477, proj_loss: -0.031177, loss_mean_cls: 0.109283, deep_loss: 0.034235, grad_norm: 42.902859
[[34m2025-10-03 23:39:50[0m] Step: 746, Training Logs: loss_final: 1.522626, loss_mean: 1.411531, proj_loss: -0.031072, loss_mean_cls: 0.107784, deep_loss: 0.034383, grad_norm: 41.504425
[[34m2025-10-03 23:39:51[0m] Step: 747, Training Logs: loss_final: 1.517270, loss_mean: 1.406069, proj_loss: -0.030885, loss_mean_cls: 0.108816, deep_loss: 0.033269, grad_norm: 48.118465
[[34m2025-10-03 23:39:52[0m] Step: 748, Training Logs: loss_final: 1.515054, loss_mean: 1.404268, proj_loss: -0.031170, loss_mean_cls: 0.108296, deep_loss: 0.033660, grad_norm: 43.484077
[[34m2025-10-03 23:39:53[0m] Step: 749, Training Logs: loss_final: 1.514959, loss_mean: 1.405708, proj_loss: -0.032348, loss_mean_cls: 0.107783, deep_loss: 0.033815, grad_norm: 44.376095
[[34m2025-10-03 23:39:54[0m] Step: 750, Training Logs: loss_final: 1.539447, loss_mean: 1.429473, proj_loss: -0.031521, loss_mean_cls: 0.107997, deep_loss: 0.033498, grad_norm: 51.465286
[[34m2025-10-03 23:39:56[0m] Step: 751, Training Logs: loss_final: 1.512514, loss_mean: 1.403447, proj_loss: -0.031964, loss_mean_cls: 0.108315, deep_loss: 0.032716, grad_norm: 41.516319
[[34m2025-10-03 23:39:57[0m] Step: 752, Training Logs: loss_final: 1.503957, loss_mean: 1.393791, proj_loss: -0.032092, loss_mean_cls: 0.108758, deep_loss: 0.033500, grad_norm: 39.516411
[[34m2025-10-03 23:39:57[0m] Step: 753, Training Logs: loss_final: 1.523144, loss_mean: 1.412838, proj_loss: -0.030877, loss_mean_cls: 0.108120, deep_loss: 0.033064, grad_norm: 53.294563
[[34m2025-10-03 23:39:58[0m] Step: 754, Training Logs: loss_final: 1.532123, loss_mean: 1.420875, proj_loss: -0.031257, loss_mean_cls: 0.108299, deep_loss: 0.034206, grad_norm: 50.690590
[[34m2025-10-03 23:39:59[0m] Step: 755, Training Logs: loss_final: 1.519018, loss_mean: 1.407463, proj_loss: -0.030260, loss_mean_cls: 0.108884, deep_loss: 0.032930, grad_norm: 39.975212
[[34m2025-10-03 23:40:00[0m] Step: 756, Training Logs: loss_final: 1.527426, loss_mean: 1.414331, proj_loss: -0.030314, loss_mean_cls: 0.108475, deep_loss: 0.034934, grad_norm: 48.797241
[[34m2025-10-03 23:40:01[0m] Step: 757, Training Logs: loss_final: 1.546899, loss_mean: 1.437000, proj_loss: -0.032215, loss_mean_cls: 0.107200, deep_loss: 0.034914, grad_norm: 51.637409
[[34m2025-10-03 23:40:02[0m] Step: 758, Training Logs: loss_final: 1.511519, loss_mean: 1.402038, proj_loss: -0.032353, loss_mean_cls: 0.108094, deep_loss: 0.033740, grad_norm: 44.313271
[[34m2025-10-03 23:40:03[0m] Step: 759, Training Logs: loss_final: 1.530155, loss_mean: 1.420119, proj_loss: -0.030275, loss_mean_cls: 0.106928, deep_loss: 0.033383, grad_norm: 46.859432
[[34m2025-10-03 23:40:05[0m] Step: 760, Training Logs: loss_final: 1.510534, loss_mean: 1.400391, proj_loss: -0.031806, loss_mean_cls: 0.108248, deep_loss: 0.033701, grad_norm: 51.196972
[[34m2025-10-03 23:40:06[0m] Step: 761, Training Logs: loss_final: 1.531527, loss_mean: 1.421686, proj_loss: -0.031607, loss_mean_cls: 0.107992, deep_loss: 0.033456, grad_norm: 50.122330
[[34m2025-10-03 23:40:07[0m] Step: 762, Training Logs: loss_final: 1.515630, loss_mean: 1.405233, proj_loss: -0.031057, loss_mean_cls: 0.108111, deep_loss: 0.033343, grad_norm: 52.006523
[[34m2025-10-03 23:40:08[0m] Step: 763, Training Logs: loss_final: 1.501733, loss_mean: 1.389841, proj_loss: -0.030891, loss_mean_cls: 0.109569, deep_loss: 0.033215, grad_norm: 49.964512
[[34m2025-10-03 23:40:09[0m] Step: 764, Training Logs: loss_final: 1.500252, loss_mean: 1.388367, proj_loss: -0.030190, loss_mean_cls: 0.109084, deep_loss: 0.032990, grad_norm: 46.710110
[[34m2025-10-03 23:40:10[0m] Step: 765, Training Logs: loss_final: 1.503048, loss_mean: 1.393308, proj_loss: -0.032239, loss_mean_cls: 0.108156, deep_loss: 0.033824, grad_norm: 52.272709
[[34m2025-10-03 23:40:12[0m] Step: 766, Training Logs: loss_final: 1.489701, loss_mean: 1.379417, proj_loss: -0.031066, loss_mean_cls: 0.108967, deep_loss: 0.032382, grad_norm: 49.306805
[[34m2025-10-03 23:40:13[0m] Step: 767, Training Logs: loss_final: 1.497857, loss_mean: 1.387473, proj_loss: -0.031663, loss_mean_cls: 0.108800, deep_loss: 0.033247, grad_norm: 45.879513
[[34m2025-10-03 23:40:14[0m] Step: 768, Training Logs: loss_final: 1.491456, loss_mean: 1.381194, proj_loss: -0.031772, loss_mean_cls: 0.109239, deep_loss: 0.032795, grad_norm: 52.263847
[[34m2025-10-03 23:40:15[0m] Step: 769, Training Logs: loss_final: 1.463780, loss_mean: 1.354119, proj_loss: -0.031122, loss_mean_cls: 0.108484, deep_loss: 0.032299, grad_norm: 39.814320
[[34m2025-10-03 23:40:16[0m] Step: 770, Training Logs: loss_final: 1.491153, loss_mean: 1.381399, proj_loss: -0.030888, loss_mean_cls: 0.106994, deep_loss: 0.033647, grad_norm: 39.350731
[[34m2025-10-03 23:40:17[0m] Step: 771, Training Logs: loss_final: 1.505172, loss_mean: 1.396452, proj_loss: -0.030710, loss_mean_cls: 0.107376, deep_loss: 0.032055, grad_norm: 53.351105
[[34m2025-10-03 23:40:19[0m] Step: 772, Training Logs: loss_final: 1.495046, loss_mean: 1.386863, proj_loss: -0.032892, loss_mean_cls: 0.108202, deep_loss: 0.032873, grad_norm: 37.630077
[[34m2025-10-03 23:40:20[0m] Step: 773, Training Logs: loss_final: 1.493030, loss_mean: 1.384541, proj_loss: -0.031522, loss_mean_cls: 0.108018, deep_loss: 0.031993, grad_norm: 43.362278
[[34m2025-10-03 23:40:21[0m] Step: 774, Training Logs: loss_final: 1.506782, loss_mean: 1.394958, proj_loss: -0.029874, loss_mean_cls: 0.108352, deep_loss: 0.033347, grad_norm: 59.624355
[[34m2025-10-03 23:40:22[0m] Step: 775, Training Logs: loss_final: 1.491450, loss_mean: 1.380529, proj_loss: -0.031581, loss_mean_cls: 0.108999, deep_loss: 0.033503, grad_norm: 53.272785
[[34m2025-10-03 23:40:23[0m] Step: 776, Training Logs: loss_final: 1.503273, loss_mean: 1.393889, proj_loss: -0.030065, loss_mean_cls: 0.106976, deep_loss: 0.032473, grad_norm: 39.459526
[[34m2025-10-03 23:40:24[0m] Step: 777, Training Logs: loss_final: 1.493988, loss_mean: 1.386610, proj_loss: -0.032403, loss_mean_cls: 0.107052, deep_loss: 0.032729, grad_norm: 50.887260
[[34m2025-10-03 23:40:26[0m] Step: 778, Training Logs: loss_final: 1.519533, loss_mean: 1.412671, proj_loss: -0.033186, loss_mean_cls: 0.106876, deep_loss: 0.033172, grad_norm: 55.376602
[[34m2025-10-03 23:40:27[0m] Step: 779, Training Logs: loss_final: 1.467180, loss_mean: 1.359349, proj_loss: -0.032710, loss_mean_cls: 0.108569, deep_loss: 0.031972, grad_norm: 48.894543
[[34m2025-10-03 23:40:28[0m] Step: 780, Training Logs: loss_final: 1.497645, loss_mean: 1.387553, proj_loss: -0.030505, loss_mean_cls: 0.108061, deep_loss: 0.032537, grad_norm: 49.386333
[[34m2025-10-03 23:40:29[0m] Step: 781, Training Logs: loss_final: 1.530571, loss_mean: 1.421458, proj_loss: -0.030902, loss_mean_cls: 0.107135, deep_loss: 0.032880, grad_norm: 54.822441
[[34m2025-10-03 23:40:30[0m] Step: 782, Training Logs: loss_final: 1.470441, loss_mean: 1.361370, proj_loss: -0.031242, loss_mean_cls: 0.108718, deep_loss: 0.031595, grad_norm: 47.819851
[[34m2025-10-03 23:40:31[0m] Step: 783, Training Logs: loss_final: 1.482476, loss_mean: 1.372452, proj_loss: -0.030841, loss_mean_cls: 0.108323, deep_loss: 0.032542, grad_norm: 51.589272
[[34m2025-10-03 23:40:32[0m] Step: 784, Training Logs: loss_final: 1.504123, loss_mean: 1.394662, proj_loss: -0.030517, loss_mean_cls: 0.107805, deep_loss: 0.032173, grad_norm: 59.328899
[[34m2025-10-03 23:40:33[0m] Step: 785, Training Logs: loss_final: 1.501602, loss_mean: 1.391781, proj_loss: -0.030511, loss_mean_cls: 0.107624, deep_loss: 0.032708, grad_norm: 51.068466
[[34m2025-10-03 23:40:34[0m] Step: 786, Training Logs: loss_final: 1.480327, loss_mean: 1.369736, proj_loss: -0.030545, loss_mean_cls: 0.108987, deep_loss: 0.032150, grad_norm: 47.385380
[[34m2025-10-03 23:40:35[0m] Step: 787, Training Logs: loss_final: 1.500744, loss_mean: 1.391649, proj_loss: -0.031438, loss_mean_cls: 0.107684, deep_loss: 0.032849, grad_norm: 56.509598
[[34m2025-10-03 23:40:36[0m] Step: 788, Training Logs: loss_final: 1.496064, loss_mean: 1.386455, proj_loss: -0.031023, loss_mean_cls: 0.107907, deep_loss: 0.032725, grad_norm: 55.112808
[[34m2025-10-03 23:40:37[0m] Step: 789, Training Logs: loss_final: 1.526314, loss_mean: 1.415675, proj_loss: -0.030306, loss_mean_cls: 0.107878, deep_loss: 0.033067, grad_norm: 51.780582
[[34m2025-10-03 23:40:38[0m] Step: 790, Training Logs: loss_final: 1.508934, loss_mean: 1.399253, proj_loss: -0.030079, loss_mean_cls: 0.106504, deep_loss: 0.033256, grad_norm: 54.518051
[[34m2025-10-03 23:40:39[0m] Step: 791, Training Logs: loss_final: 1.502165, loss_mean: 1.393062, proj_loss: -0.031638, loss_mean_cls: 0.107726, deep_loss: 0.033015, grad_norm: 51.067982
[[34m2025-10-03 23:40:40[0m] Step: 792, Training Logs: loss_final: 1.496124, loss_mean: 1.389035, proj_loss: -0.033559, loss_mean_cls: 0.108162, deep_loss: 0.032486, grad_norm: 58.826664
[[34m2025-10-03 23:40:42[0m] Step: 793, Training Logs: loss_final: 1.515009, loss_mean: 1.407246, proj_loss: -0.032737, loss_mean_cls: 0.107166, deep_loss: 0.033335, grad_norm: 58.028805
[[34m2025-10-03 23:40:43[0m] Step: 794, Training Logs: loss_final: 1.509866, loss_mean: 1.398950, proj_loss: -0.030205, loss_mean_cls: 0.108053, deep_loss: 0.033069, grad_norm: 49.331146
[[34m2025-10-03 23:40:44[0m] Step: 795, Training Logs: loss_final: 1.511210, loss_mean: 1.404402, proj_loss: -0.032794, loss_mean_cls: 0.107754, deep_loss: 0.031848, grad_norm: 59.559425
[[34m2025-10-03 23:40:45[0m] Step: 796, Training Logs: loss_final: 1.534136, loss_mean: 1.425760, proj_loss: -0.032172, loss_mean_cls: 0.107679, deep_loss: 0.032869, grad_norm: 62.718609
[[34m2025-10-03 23:40:46[0m] Step: 797, Training Logs: loss_final: 1.529774, loss_mean: 1.420892, proj_loss: -0.031407, loss_mean_cls: 0.107729, deep_loss: 0.032559, grad_norm: 57.075436
[[34m2025-10-03 23:40:47[0m] Step: 798, Training Logs: loss_final: 1.519967, loss_mean: 1.411953, proj_loss: -0.032360, loss_mean_cls: 0.107527, deep_loss: 0.032847, grad_norm: 50.168076
[[34m2025-10-03 23:40:49[0m] Step: 799, Training Logs: loss_final: 1.520366, loss_mean: 1.409912, proj_loss: -0.029419, loss_mean_cls: 0.107062, deep_loss: 0.032812, grad_norm: 55.091721
[[34m2025-10-03 23:40:50[0m] Step: 800, Training Logs: loss_final: 1.516014, loss_mean: 1.406003, proj_loss: -0.030984, loss_mean_cls: 0.108815, deep_loss: 0.032180, grad_norm: 60.940617
[[34m2025-10-03 23:40:51[0m] Step: 801, Training Logs: loss_final: 1.517477, loss_mean: 1.407292, proj_loss: -0.031177, loss_mean_cls: 0.108410, deep_loss: 0.032952, grad_norm: 45.346493
[[34m2025-10-03 23:40:52[0m] Step: 802, Training Logs: loss_final: 1.512557, loss_mean: 1.404919, proj_loss: -0.031288, loss_mean_cls: 0.106587, deep_loss: 0.032338, grad_norm: 45.287769
[[34m2025-10-03 23:40:53[0m] Step: 803, Training Logs: loss_final: 1.513392, loss_mean: 1.405354, proj_loss: -0.032540, loss_mean_cls: 0.107709, deep_loss: 0.032869, grad_norm: 58.532310
[[34m2025-10-03 23:40:55[0m] Step: 804, Training Logs: loss_final: 1.495239, loss_mean: 1.387083, proj_loss: -0.030819, loss_mean_cls: 0.106836, deep_loss: 0.032138, grad_norm: 48.004116
[[34m2025-10-03 23:40:56[0m] Step: 805, Training Logs: loss_final: 1.469264, loss_mean: 1.360334, proj_loss: -0.030999, loss_mean_cls: 0.107898, deep_loss: 0.032031, grad_norm: 34.527195
[[34m2025-10-03 23:40:57[0m] Step: 806, Training Logs: loss_final: 1.500451, loss_mean: 1.392741, proj_loss: -0.032106, loss_mean_cls: 0.106982, deep_loss: 0.032835, grad_norm: 47.203484
[[34m2025-10-03 23:40:58[0m] Step: 807, Training Logs: loss_final: 1.495279, loss_mean: 1.387355, proj_loss: -0.031826, loss_mean_cls: 0.106694, deep_loss: 0.033056, grad_norm: 39.505787
[[34m2025-10-03 23:40:59[0m] Step: 808, Training Logs: loss_final: 1.477091, loss_mean: 1.369121, proj_loss: -0.031564, loss_mean_cls: 0.106490, deep_loss: 0.033044, grad_norm: 33.978180
[[34m2025-10-03 23:41:00[0m] Step: 809, Training Logs: loss_final: 1.497311, loss_mean: 1.387537, proj_loss: -0.030450, loss_mean_cls: 0.108095, deep_loss: 0.032128, grad_norm: 37.531376
[[34m2025-10-03 23:41:02[0m] Step: 810, Training Logs: loss_final: 1.480862, loss_mean: 1.373158, proj_loss: -0.032401, loss_mean_cls: 0.106698, deep_loss: 0.033407, grad_norm: 37.994892
[[34m2025-10-03 23:41:03[0m] Step: 811, Training Logs: loss_final: 1.491384, loss_mean: 1.382021, proj_loss: -0.030629, loss_mean_cls: 0.107598, deep_loss: 0.032395, grad_norm: 33.121792
[[34m2025-10-03 23:41:04[0m] Step: 812, Training Logs: loss_final: 1.483683, loss_mean: 1.372524, proj_loss: -0.030098, loss_mean_cls: 0.107419, deep_loss: 0.033837, grad_norm: 30.276711
[[34m2025-10-03 23:41:05[0m] Step: 813, Training Logs: loss_final: 1.457479, loss_mean: 1.346116, proj_loss: -0.029266, loss_mean_cls: 0.108157, deep_loss: 0.032472, grad_norm: 34.345993
[[34m2025-10-03 23:41:06[0m] Step: 814, Training Logs: loss_final: 1.470643, loss_mean: 1.361322, proj_loss: -0.030695, loss_mean_cls: 0.108116, deep_loss: 0.031900, grad_norm: 33.787403
[[34m2025-10-03 23:41:07[0m] Step: 815, Training Logs: loss_final: 1.492536, loss_mean: 1.383103, proj_loss: -0.030912, loss_mean_cls: 0.106681, deep_loss: 0.033664, grad_norm: 22.983416
[[34m2025-10-03 23:41:08[0m] Step: 816, Training Logs: loss_final: 1.474771, loss_mean: 1.364885, proj_loss: -0.031554, loss_mean_cls: 0.107360, deep_loss: 0.034080, grad_norm: 44.812668
[[34m2025-10-03 23:41:09[0m] Step: 817, Training Logs: loss_final: 1.495346, loss_mean: 1.381715, proj_loss: -0.030586, loss_mean_cls: 0.109058, deep_loss: 0.035159, grad_norm: 45.329208
[[34m2025-10-03 23:41:10[0m] Step: 818, Training Logs: loss_final: 1.501584, loss_mean: 1.389303, proj_loss: -0.032163, loss_mean_cls: 0.108312, deep_loss: 0.036133, grad_norm: 35.680042
[[34m2025-10-03 23:41:10[0m] Step: 819, Training Logs: loss_final: 1.507888, loss_mean: 1.393912, proj_loss: -0.030950, loss_mean_cls: 0.109921, deep_loss: 0.035005, grad_norm: 45.316090
[[34m2025-10-03 23:41:12[0m] Step: 820, Training Logs: loss_final: 1.508937, loss_mean: 1.397150, proj_loss: -0.031240, loss_mean_cls: 0.108456, deep_loss: 0.034572, grad_norm: 43.913376
[[34m2025-10-03 23:41:13[0m] Step: 821, Training Logs: loss_final: 1.501058, loss_mean: 1.390834, proj_loss: -0.033811, loss_mean_cls: 0.108180, deep_loss: 0.035856, grad_norm: 49.305199
[[34m2025-10-03 23:41:14[0m] Step: 822, Training Logs: loss_final: 1.511733, loss_mean: 1.395778, proj_loss: -0.031014, loss_mean_cls: 0.110264, deep_loss: 0.036704, grad_norm: 44.582829
[[34m2025-10-03 23:41:15[0m] Step: 823, Training Logs: loss_final: 1.511583, loss_mean: 1.399415, proj_loss: -0.031758, loss_mean_cls: 0.109179, deep_loss: 0.034747, grad_norm: 44.605568
[[34m2025-10-03 23:41:16[0m] Step: 824, Training Logs: loss_final: 1.525566, loss_mean: 1.413638, proj_loss: -0.032707, loss_mean_cls: 0.108939, deep_loss: 0.035696, grad_norm: 53.938046
[[34m2025-10-03 23:41:17[0m] Step: 825, Training Logs: loss_final: 1.495207, loss_mean: 1.378728, proj_loss: -0.031227, loss_mean_cls: 0.109507, deep_loss: 0.038199, grad_norm: 38.943550
[[34m2025-10-03 23:41:19[0m] Step: 826, Training Logs: loss_final: 1.511801, loss_mean: 1.398458, proj_loss: -0.032363, loss_mean_cls: 0.108836, deep_loss: 0.036870, grad_norm: 40.280964
[[34m2025-10-03 23:41:20[0m] Step: 827, Training Logs: loss_final: 1.503899, loss_mean: 1.390402, proj_loss: -0.031838, loss_mean_cls: 0.109337, deep_loss: 0.035998, grad_norm: 54.027756
[[34m2025-10-03 23:41:21[0m] Step: 828, Training Logs: loss_final: 1.502646, loss_mean: 1.384996, proj_loss: -0.029952, loss_mean_cls: 0.109949, deep_loss: 0.037654, grad_norm: 40.204777
[[34m2025-10-03 23:41:22[0m] Step: 829, Training Logs: loss_final: 1.513947, loss_mean: 1.401158, proj_loss: -0.030471, loss_mean_cls: 0.107075, deep_loss: 0.036186, grad_norm: 38.832993
[[34m2025-10-03 23:41:23[0m] Step: 830, Training Logs: loss_final: 1.508566, loss_mean: 1.396010, proj_loss: -0.030213, loss_mean_cls: 0.107339, deep_loss: 0.035429, grad_norm: 48.550995
[[34m2025-10-03 23:41:24[0m] Step: 831, Training Logs: loss_final: 1.497958, loss_mean: 1.381635, proj_loss: -0.029421, loss_mean_cls: 0.108748, deep_loss: 0.036995, grad_norm: 38.931267
[[34m2025-10-03 23:41:26[0m] Step: 832, Training Logs: loss_final: 1.465077, loss_mean: 1.353137, proj_loss: -0.032250, loss_mean_cls: 0.108849, deep_loss: 0.035340, grad_norm: 36.674137
[[34m2025-10-03 23:41:27[0m] Step: 833, Training Logs: loss_final: 1.483616, loss_mean: 1.373642, proj_loss: -0.033105, loss_mean_cls: 0.108342, deep_loss: 0.034737, grad_norm: 43.004711
[[34m2025-10-03 23:41:28[0m] Step: 834, Training Logs: loss_final: 1.492580, loss_mean: 1.377826, proj_loss: -0.030018, loss_mean_cls: 0.109559, deep_loss: 0.035212, grad_norm: 37.495914
[[34m2025-10-03 23:41:29[0m] Step: 835, Training Logs: loss_final: 1.455964, loss_mean: 1.342498, proj_loss: -0.029238, loss_mean_cls: 0.110224, deep_loss: 0.032481, grad_norm: 43.953876
[[34m2025-10-03 23:41:30[0m] Step: 836, Training Logs: loss_final: 1.504104, loss_mean: 1.391354, proj_loss: -0.030997, loss_mean_cls: 0.108408, deep_loss: 0.035339, grad_norm: 39.299934
[[34m2025-10-03 23:41:31[0m] Step: 837, Training Logs: loss_final: 1.487948, loss_mean: 1.373672, proj_loss: -0.029408, loss_mean_cls: 0.108526, deep_loss: 0.035157, grad_norm: 41.694603
[[34m2025-10-03 23:41:33[0m] Step: 838, Training Logs: loss_final: 1.487578, loss_mean: 1.376486, proj_loss: -0.030102, loss_mean_cls: 0.107574, deep_loss: 0.033620, grad_norm: 40.204845
[[34m2025-10-03 23:41:34[0m] Step: 839, Training Logs: loss_final: 1.488603, loss_mean: 1.374532, proj_loss: -0.028767, loss_mean_cls: 0.108766, deep_loss: 0.034073, grad_norm: 41.397251
[[34m2025-10-03 23:41:35[0m] Step: 840, Training Logs: loss_final: 1.457431, loss_mean: 1.343585, proj_loss: -0.028985, loss_mean_cls: 0.108970, deep_loss: 0.033862, grad_norm: 39.582039
[[34m2025-10-03 23:41:36[0m] Step: 841, Training Logs: loss_final: 1.493064, loss_mean: 1.382218, proj_loss: -0.030419, loss_mean_cls: 0.107682, deep_loss: 0.033583, grad_norm: 44.765572
[[34m2025-10-03 23:41:37[0m] Step: 842, Training Logs: loss_final: 1.507919, loss_mean: 1.395788, proj_loss: -0.029486, loss_mean_cls: 0.107507, deep_loss: 0.034110, grad_norm: 33.499603
[[34m2025-10-03 23:41:39[0m] Step: 843, Training Logs: loss_final: 1.499788, loss_mean: 1.388763, proj_loss: -0.029957, loss_mean_cls: 0.106978, deep_loss: 0.034004, grad_norm: 42.281631
[[34m2025-10-03 23:41:40[0m] Step: 844, Training Logs: loss_final: 1.470940, loss_mean: 1.358683, proj_loss: -0.029531, loss_mean_cls: 0.108562, deep_loss: 0.033226, grad_norm: 40.751442
[[34m2025-10-03 23:41:41[0m] Step: 845, Training Logs: loss_final: 1.476644, loss_mean: 1.365178, proj_loss: -0.030264, loss_mean_cls: 0.108457, deep_loss: 0.033273, grad_norm: 41.068733
[[34m2025-10-03 23:41:42[0m] Step: 846, Training Logs: loss_final: 1.497178, loss_mean: 1.386937, proj_loss: -0.031239, loss_mean_cls: 0.108393, deep_loss: 0.033087, grad_norm: 48.757156
[[34m2025-10-03 23:41:43[0m] Step: 847, Training Logs: loss_final: 1.484543, loss_mean: 1.374941, proj_loss: -0.032426, loss_mean_cls: 0.108014, deep_loss: 0.034013, grad_norm: 48.601948
[[34m2025-10-03 23:41:44[0m] Step: 848, Training Logs: loss_final: 1.473153, loss_mean: 1.360379, proj_loss: -0.029670, loss_mean_cls: 0.109688, deep_loss: 0.032756, grad_norm: 41.934544
[[34m2025-10-03 23:41:45[0m] Step: 849, Training Logs: loss_final: 1.458032, loss_mean: 1.346058, proj_loss: -0.030382, loss_mean_cls: 0.109436, deep_loss: 0.032918, grad_norm: 40.467503
[[34m2025-10-03 23:41:45[0m] Step: 850, Training Logs: loss_final: 1.478874, loss_mean: 1.367800, proj_loss: -0.030564, loss_mean_cls: 0.107651, deep_loss: 0.033986, grad_norm: 41.102932
[[34m2025-10-03 23:41:46[0m] Step: 851, Training Logs: loss_final: 1.467508, loss_mean: 1.357654, proj_loss: -0.031548, loss_mean_cls: 0.108595, deep_loss: 0.032807, grad_norm: 42.507259
[[34m2025-10-03 23:41:47[0m] Step: 852, Training Logs: loss_final: 1.498704, loss_mean: 1.387135, proj_loss: -0.030418, loss_mean_cls: 0.108924, deep_loss: 0.033063, grad_norm: 51.347225
[[34m2025-10-03 23:41:49[0m] Step: 853, Training Logs: loss_final: 1.502951, loss_mean: 1.394499, proj_loss: -0.032695, loss_mean_cls: 0.107418, deep_loss: 0.033729, grad_norm: 36.985435
[[34m2025-10-03 23:41:50[0m] Step: 854, Training Logs: loss_final: 1.514097, loss_mean: 1.403890, proj_loss: -0.031559, loss_mean_cls: 0.107708, deep_loss: 0.034059, grad_norm: 38.285629
[[34m2025-10-03 23:41:51[0m] Step: 855, Training Logs: loss_final: 1.493315, loss_mean: 1.380994, proj_loss: -0.030177, loss_mean_cls: 0.109227, deep_loss: 0.033272, grad_norm: 45.110550
[[34m2025-10-03 23:41:52[0m] Step: 856, Training Logs: loss_final: 1.508334, loss_mean: 1.395308, proj_loss: -0.030602, loss_mean_cls: 0.109530, deep_loss: 0.034099, grad_norm: 29.816502
[[34m2025-10-03 23:41:53[0m] Step: 857, Training Logs: loss_final: 1.504963, loss_mean: 1.396069, proj_loss: -0.032206, loss_mean_cls: 0.107884, deep_loss: 0.033216, grad_norm: 44.562878
[[34m2025-10-03 23:41:54[0m] Step: 858, Training Logs: loss_final: 1.502321, loss_mean: 1.389352, proj_loss: -0.030824, loss_mean_cls: 0.108861, deep_loss: 0.034933, grad_norm: 48.827904
[[34m2025-10-03 23:41:56[0m] Step: 859, Training Logs: loss_final: 1.492789, loss_mean: 1.381744, proj_loss: -0.031679, loss_mean_cls: 0.108663, deep_loss: 0.034061, grad_norm: 31.097921
[[34m2025-10-03 23:41:57[0m] Step: 860, Training Logs: loss_final: 1.519318, loss_mean: 1.407391, proj_loss: -0.030660, loss_mean_cls: 0.108329, deep_loss: 0.034258, grad_norm: 46.441147
[[34m2025-10-03 23:41:58[0m] Step: 861, Training Logs: loss_final: 1.513468, loss_mean: 1.397127, proj_loss: -0.030317, loss_mean_cls: 0.110536, deep_loss: 0.036123, grad_norm: 56.043827
[[34m2025-10-03 23:41:59[0m] Step: 862, Training Logs: loss_final: 1.507562, loss_mean: 1.391707, proj_loss: -0.030534, loss_mean_cls: 0.109253, deep_loss: 0.037136, grad_norm: 41.353497
[[34m2025-10-03 23:42:00[0m] Step: 863, Training Logs: loss_final: 1.505838, loss_mean: 1.393066, proj_loss: -0.031387, loss_mean_cls: 0.107926, deep_loss: 0.036234, grad_norm: 45.681705
[[34m2025-10-03 23:42:01[0m] Step: 864, Training Logs: loss_final: 1.521585, loss_mean: 1.409209, proj_loss: -0.030868, loss_mean_cls: 0.108559, deep_loss: 0.034685, grad_norm: 51.301922
[[34m2025-10-03 23:42:03[0m] Step: 865, Training Logs: loss_final: 1.541161, loss_mean: 1.427541, proj_loss: -0.029275, loss_mean_cls: 0.107431, deep_loss: 0.035463, grad_norm: 41.680504
[[34m2025-10-03 23:42:04[0m] Step: 866, Training Logs: loss_final: 1.508116, loss_mean: 1.396243, proj_loss: -0.031934, loss_mean_cls: 0.109249, deep_loss: 0.034557, grad_norm: 49.035885
[[34m2025-10-03 23:42:05[0m] Step: 867, Training Logs: loss_final: 1.524704, loss_mean: 1.410357, proj_loss: -0.030259, loss_mean_cls: 0.110773, deep_loss: 0.033833, grad_norm: 62.400463
[[34m2025-10-03 23:42:06[0m] Step: 868, Training Logs: loss_final: 1.516033, loss_mean: 1.404185, proj_loss: -0.032396, loss_mean_cls: 0.108924, deep_loss: 0.035320, grad_norm: 52.419266
[[34m2025-10-03 23:42:07[0m] Step: 869, Training Logs: loss_final: 1.520403, loss_mean: 1.408580, proj_loss: -0.031192, loss_mean_cls: 0.108581, deep_loss: 0.034434, grad_norm: 41.708858
[[34m2025-10-03 23:42:08[0m] Step: 870, Training Logs: loss_final: 1.544642, loss_mean: 1.433030, proj_loss: -0.029898, loss_mean_cls: 0.107632, deep_loss: 0.033878, grad_norm: 50.373913
[[34m2025-10-03 23:42:10[0m] Step: 871, Training Logs: loss_final: 1.518099, loss_mean: 1.407041, proj_loss: -0.032189, loss_mean_cls: 0.109352, deep_loss: 0.033896, grad_norm: 54.254051
[[34m2025-10-03 23:42:11[0m] Step: 872, Training Logs: loss_final: 1.516083, loss_mean: 1.405292, proj_loss: -0.032329, loss_mean_cls: 0.109117, deep_loss: 0.034003, grad_norm: 48.741707
[[34m2025-10-03 23:42:12[0m] Step: 873, Training Logs: loss_final: 1.518022, loss_mean: 1.405870, proj_loss: -0.030092, loss_mean_cls: 0.109396, deep_loss: 0.032849, grad_norm: 38.918411
[[34m2025-10-03 23:42:13[0m] Step: 874, Training Logs: loss_final: 1.510537, loss_mean: 1.398037, proj_loss: -0.030000, loss_mean_cls: 0.108570, deep_loss: 0.033929, grad_norm: 45.806671
[[34m2025-10-03 23:42:14[0m] Step: 875, Training Logs: loss_final: 1.508327, loss_mean: 1.399189, proj_loss: -0.031792, loss_mean_cls: 0.107951, deep_loss: 0.032978, grad_norm: 57.501945
[[34m2025-10-03 23:42:15[0m] Step: 876, Training Logs: loss_final: 1.495700, loss_mean: 1.387545, proj_loss: -0.033196, loss_mean_cls: 0.108132, deep_loss: 0.033218, grad_norm: 41.938713
[[34m2025-10-03 23:42:17[0m] Step: 877, Training Logs: loss_final: 1.473082, loss_mean: 1.361690, proj_loss: -0.030229, loss_mean_cls: 0.108759, deep_loss: 0.032861, grad_norm: 33.890656
[[34m2025-10-03 23:42:18[0m] Step: 878, Training Logs: loss_final: 1.501455, loss_mean: 1.391330, proj_loss: -0.030905, loss_mean_cls: 0.108289, deep_loss: 0.032740, grad_norm: 57.223549
[[34m2025-10-03 23:42:19[0m] Step: 879, Training Logs: loss_final: 1.490956, loss_mean: 1.378908, proj_loss: -0.031275, loss_mean_cls: 0.108863, deep_loss: 0.034460, grad_norm: 49.676685
[[34m2025-10-03 23:42:20[0m] Step: 880, Training Logs: loss_final: 1.510189, loss_mean: 1.400174, proj_loss: -0.030688, loss_mean_cls: 0.107095, deep_loss: 0.033608, grad_norm: 29.580177
[[34m2025-10-03 23:42:20[0m] Step: 881, Training Logs: loss_final: 1.495949, loss_mean: 1.386833, proj_loss: -0.032651, loss_mean_cls: 0.107381, deep_loss: 0.034386, grad_norm: 47.708336
[[34m2025-10-03 23:42:21[0m] Step: 882, Training Logs: loss_final: 1.470454, loss_mean: 1.356980, proj_loss: -0.030842, loss_mean_cls: 0.109831, deep_loss: 0.034485, grad_norm: 38.883038
[[34m2025-10-03 23:42:22[0m] Step: 883, Training Logs: loss_final: 1.460149, loss_mean: 1.348512, proj_loss: -0.031349, loss_mean_cls: 0.110709, deep_loss: 0.032277, grad_norm: 33.936668
[[34m2025-10-03 23:42:23[0m] Step: 884, Training Logs: loss_final: 1.477400, loss_mean: 1.367337, proj_loss: -0.031971, loss_mean_cls: 0.108293, deep_loss: 0.033742, grad_norm: 45.238743
[[34m2025-10-03 23:42:24[0m] Step: 885, Training Logs: loss_final: 1.469324, loss_mean: 1.357671, proj_loss: -0.031642, loss_mean_cls: 0.108458, deep_loss: 0.034838, grad_norm: 37.077221
[[34m2025-10-03 23:42:26[0m] Step: 886, Training Logs: loss_final: 1.465007, loss_mean: 1.355003, proj_loss: -0.030550, loss_mean_cls: 0.108444, deep_loss: 0.032109, grad_norm: 34.562035
[[34m2025-10-03 23:42:27[0m] Step: 887, Training Logs: loss_final: 1.470533, loss_mean: 1.359096, proj_loss: -0.030689, loss_mean_cls: 0.108690, deep_loss: 0.033437, grad_norm: 45.140759
[[34m2025-10-03 23:42:28[0m] Step: 888, Training Logs: loss_final: 1.492478, loss_mean: 1.383482, proj_loss: -0.032777, loss_mean_cls: 0.107700, deep_loss: 0.034073, grad_norm: 47.552742
[[34m2025-10-03 23:42:29[0m] Step: 889, Training Logs: loss_final: 1.464492, loss_mean: 1.352140, proj_loss: -0.030880, loss_mean_cls: 0.109214, deep_loss: 0.034017, grad_norm: 28.796238
[[34m2025-10-03 23:42:30[0m] Step: 890, Training Logs: loss_final: 1.467808, loss_mean: 1.357258, proj_loss: -0.030345, loss_mean_cls: 0.108348, deep_loss: 0.032547, grad_norm: 46.545330
[[34m2025-10-03 23:42:31[0m] Step: 891, Training Logs: loss_final: 1.492072, loss_mean: 1.378453, proj_loss: -0.030025, loss_mean_cls: 0.109584, deep_loss: 0.034060, grad_norm: 58.257786
[[34m2025-10-03 23:42:33[0m] Step: 892, Training Logs: loss_final: 1.501223, loss_mean: 1.389077, proj_loss: -0.030195, loss_mean_cls: 0.107930, deep_loss: 0.034410, grad_norm: 38.202248
[[34m2025-10-03 23:42:34[0m] Step: 893, Training Logs: loss_final: 1.472580, loss_mean: 1.362635, proj_loss: -0.031819, loss_mean_cls: 0.108966, deep_loss: 0.032797, grad_norm: 38.464653
[[34m2025-10-03 23:42:35[0m] Step: 894, Training Logs: loss_final: 1.489945, loss_mean: 1.377774, proj_loss: -0.029212, loss_mean_cls: 0.108311, deep_loss: 0.033071, grad_norm: 62.353966
[[34m2025-10-03 23:42:36[0m] Step: 895, Training Logs: loss_final: 1.488806, loss_mean: 1.375983, proj_loss: -0.030793, loss_mean_cls: 0.108815, deep_loss: 0.034802, grad_norm: 46.441265
[[34m2025-10-03 23:42:37[0m] Step: 896, Training Logs: loss_final: 1.457726, loss_mean: 1.348026, proj_loss: -0.032062, loss_mean_cls: 0.108665, deep_loss: 0.033097, grad_norm: 25.727423
[[34m2025-10-03 23:42:38[0m] Step: 897, Training Logs: loss_final: 1.488093, loss_mean: 1.378600, proj_loss: -0.032441, loss_mean_cls: 0.107700, deep_loss: 0.034234, grad_norm: 47.672440
[[34m2025-10-03 23:42:40[0m] Step: 898, Training Logs: loss_final: 1.481436, loss_mean: 1.368685, proj_loss: -0.032104, loss_mean_cls: 0.109678, deep_loss: 0.035177, grad_norm: 47.944939
[[34m2025-10-03 23:42:41[0m] Step: 899, Training Logs: loss_final: 1.482478, loss_mean: 1.368769, proj_loss: -0.030464, loss_mean_cls: 0.109737, deep_loss: 0.034436, grad_norm: 36.833656
[[34m2025-10-03 23:42:42[0m] Step: 900, Training Logs: loss_final: 1.492863, loss_mean: 1.384234, proj_loss: -0.031883, loss_mean_cls: 0.107724, deep_loss: 0.032788, grad_norm: 45.308075
[[34m2025-10-03 23:42:43[0m] Step: 901, Training Logs: loss_final: 1.497733, loss_mean: 1.389016, proj_loss: -0.032564, loss_mean_cls: 0.107209, deep_loss: 0.034072, grad_norm: 45.430370
[[34m2025-10-03 23:42:44[0m] Step: 902, Training Logs: loss_final: 1.499237, loss_mean: 1.390018, proj_loss: -0.033376, loss_mean_cls: 0.107916, deep_loss: 0.034680, grad_norm: 44.227192
[[34m2025-10-03 23:42:45[0m] Step: 903, Training Logs: loss_final: 1.509052, loss_mean: 1.396314, proj_loss: -0.029466, loss_mean_cls: 0.108206, deep_loss: 0.033998, grad_norm: 45.504757
[[34m2025-10-03 23:42:47[0m] Step: 904, Training Logs: loss_final: 1.483757, loss_mean: 1.372486, proj_loss: -0.030853, loss_mean_cls: 0.108966, deep_loss: 0.033158, grad_norm: 34.993969
[[34m2025-10-03 23:42:48[0m] Step: 905, Training Logs: loss_final: 1.499043, loss_mean: 1.390238, proj_loss: -0.031959, loss_mean_cls: 0.106967, deep_loss: 0.033797, grad_norm: 43.737679
[[34m2025-10-03 23:42:49[0m] Step: 906, Training Logs: loss_final: 1.487604, loss_mean: 1.378646, proj_loss: -0.032399, loss_mean_cls: 0.108176, deep_loss: 0.033180, grad_norm: 46.052044
[[34m2025-10-03 23:42:50[0m] Step: 907, Training Logs: loss_final: 1.513568, loss_mean: 1.402330, proj_loss: -0.031280, loss_mean_cls: 0.108863, deep_loss: 0.033655, grad_norm: 33.026188
[[34m2025-10-03 23:42:51[0m] Step: 908, Training Logs: loss_final: 1.491257, loss_mean: 1.382095, proj_loss: -0.032023, loss_mean_cls: 0.108153, deep_loss: 0.033031, grad_norm: 38.523239
[[34m2025-10-03 23:42:52[0m] Step: 909, Training Logs: loss_final: 1.516619, loss_mean: 1.406195, proj_loss: -0.030526, loss_mean_cls: 0.106888, deep_loss: 0.034062, grad_norm: 41.321404
[[34m2025-10-03 23:42:54[0m] Step: 910, Training Logs: loss_final: 1.479503, loss_mean: 1.369781, proj_loss: -0.032458, loss_mean_cls: 0.108271, deep_loss: 0.033910, grad_norm: 36.765278
[[34m2025-10-03 23:42:55[0m] Step: 911, Training Logs: loss_final: 1.471899, loss_mean: 1.360124, proj_loss: -0.030891, loss_mean_cls: 0.109728, deep_loss: 0.032938, grad_norm: 37.308853
[[34m2025-10-03 23:42:55[0m] Step: 912, Training Logs: loss_final: 1.484004, loss_mean: 1.373911, proj_loss: -0.031855, loss_mean_cls: 0.108350, deep_loss: 0.033598, grad_norm: 38.991180
[[34m2025-10-03 23:42:56[0m] Step: 913, Training Logs: loss_final: 1.491451, loss_mean: 1.382295, proj_loss: -0.032015, loss_mean_cls: 0.107541, deep_loss: 0.033629, grad_norm: 31.412182
[[34m2025-10-03 23:42:57[0m] Step: 914, Training Logs: loss_final: 1.483960, loss_mean: 1.373947, proj_loss: -0.030179, loss_mean_cls: 0.107028, deep_loss: 0.033164, grad_norm: 38.289795
[[34m2025-10-03 23:42:58[0m] Step: 915, Training Logs: loss_final: 1.471707, loss_mean: 1.362362, proj_loss: -0.031913, loss_mean_cls: 0.107819, deep_loss: 0.033439, grad_norm: 32.140224
[[34m2025-10-03 23:42:59[0m] Step: 916, Training Logs: loss_final: 1.486689, loss_mean: 1.376993, proj_loss: -0.030980, loss_mean_cls: 0.108037, deep_loss: 0.032639, grad_norm: 38.418991
[[34m2025-10-03 23:43:01[0m] Step: 917, Training Logs: loss_final: 1.470327, loss_mean: 1.359014, proj_loss: -0.031388, loss_mean_cls: 0.108832, deep_loss: 0.033870, grad_norm: 35.907795
[[34m2025-10-03 23:43:02[0m] Step: 918, Training Logs: loss_final: 1.479652, loss_mean: 1.369789, proj_loss: -0.031670, loss_mean_cls: 0.108870, deep_loss: 0.032664, grad_norm: 35.156109
[[34m2025-10-03 23:43:03[0m] Step: 919, Training Logs: loss_final: 1.481013, loss_mean: 1.371980, proj_loss: -0.031450, loss_mean_cls: 0.107325, deep_loss: 0.033157, grad_norm: 39.854343
[[34m2025-10-03 23:43:04[0m] Step: 920, Training Logs: loss_final: 1.500310, loss_mean: 1.391086, proj_loss: -0.031741, loss_mean_cls: 0.107388, deep_loss: 0.033578, grad_norm: 30.089466
[[34m2025-10-03 23:43:05[0m] Step: 921, Training Logs: loss_final: 1.486813, loss_mean: 1.376834, proj_loss: -0.031477, loss_mean_cls: 0.107877, deep_loss: 0.033579, grad_norm: 38.613781
[[34m2025-10-03 23:43:07[0m] Step: 922, Training Logs: loss_final: 1.492095, loss_mean: 1.382174, proj_loss: -0.032622, loss_mean_cls: 0.109231, deep_loss: 0.033312, grad_norm: 45.062397
[[34m2025-10-03 23:43:08[0m] Step: 923, Training Logs: loss_final: 1.489122, loss_mean: 1.378886, proj_loss: -0.032224, loss_mean_cls: 0.108151, deep_loss: 0.034309, grad_norm: 35.017185
[[34m2025-10-03 23:43:09[0m] Step: 924, Training Logs: loss_final: 1.517197, loss_mean: 1.405146, proj_loss: -0.029555, loss_mean_cls: 0.109074, deep_loss: 0.032531, grad_norm: 57.736526
[[34m2025-10-03 23:43:10[0m] Step: 925, Training Logs: loss_final: 1.529104, loss_mean: 1.418537, proj_loss: -0.032108, loss_mean_cls: 0.108854, deep_loss: 0.033821, grad_norm: 60.812332
[[34m2025-10-03 23:43:11[0m] Step: 926, Training Logs: loss_final: 1.522584, loss_mean: 1.412762, proj_loss: -0.032222, loss_mean_cls: 0.107640, deep_loss: 0.034404, grad_norm: 36.561550
[[34m2025-10-03 23:43:12[0m] Step: 927, Training Logs: loss_final: 1.491098, loss_mean: 1.381291, proj_loss: -0.032052, loss_mean_cls: 0.108306, deep_loss: 0.033552, grad_norm: 49.993717
[[34m2025-10-03 23:43:14[0m] Step: 928, Training Logs: loss_final: 1.529204, loss_mean: 1.418216, proj_loss: -0.031863, loss_mean_cls: 0.108298, deep_loss: 0.034554, grad_norm: 56.647629
[[34m2025-10-03 23:43:15[0m] Step: 929, Training Logs: loss_final: 1.496776, loss_mean: 1.386078, proj_loss: -0.032497, loss_mean_cls: 0.108644, deep_loss: 0.034551, grad_norm: 38.867702
[[34m2025-10-03 23:43:16[0m] Step: 930, Training Logs: loss_final: 1.505430, loss_mean: 1.395800, proj_loss: -0.031962, loss_mean_cls: 0.109144, deep_loss: 0.032449, grad_norm: 58.892483
[[34m2025-10-03 23:43:17[0m] Step: 931, Training Logs: loss_final: 1.537581, loss_mean: 1.429411, proj_loss: -0.033065, loss_mean_cls: 0.106461, deep_loss: 0.034774, grad_norm: 62.321915
[[34m2025-10-03 23:43:18[0m] Step: 932, Training Logs: loss_final: 1.516258, loss_mean: 1.406592, proj_loss: -0.031782, loss_mean_cls: 0.107358, deep_loss: 0.034090, grad_norm: 41.618668
[[34m2025-10-03 23:43:19[0m] Step: 933, Training Logs: loss_final: 1.507935, loss_mean: 1.399736, proj_loss: -0.032890, loss_mean_cls: 0.106916, deep_loss: 0.034173, grad_norm: 39.941242
[[34m2025-10-03 23:43:21[0m] Step: 934, Training Logs: loss_final: 1.516130, loss_mean: 1.407016, proj_loss: -0.032635, loss_mean_cls: 0.107961, deep_loss: 0.033788, grad_norm: 63.159485
[[34m2025-10-03 23:43:22[0m] Step: 935, Training Logs: loss_final: 1.512112, loss_mean: 1.399810, proj_loss: -0.032091, loss_mean_cls: 0.109591, deep_loss: 0.034803, grad_norm: 53.109009
[[34m2025-10-03 23:43:23[0m] Step: 936, Training Logs: loss_final: 1.498605, loss_mean: 1.387961, proj_loss: -0.032484, loss_mean_cls: 0.109677, deep_loss: 0.033450, grad_norm: 36.646477
[[34m2025-10-03 23:43:24[0m] Step: 937, Training Logs: loss_final: 1.512615, loss_mean: 1.401642, proj_loss: -0.030867, loss_mean_cls: 0.108698, deep_loss: 0.033143, grad_norm: 51.558979
[[34m2025-10-03 23:43:25[0m] Step: 938, Training Logs: loss_final: 1.501700, loss_mean: 1.390775, proj_loss: -0.031803, loss_mean_cls: 0.109489, deep_loss: 0.033239, grad_norm: 45.300793
[[34m2025-10-03 23:43:26[0m] Step: 939, Training Logs: loss_final: 1.489338, loss_mean: 1.378561, proj_loss: -0.031790, loss_mean_cls: 0.109410, deep_loss: 0.033158, grad_norm: 42.702744
[[34m2025-10-03 23:43:28[0m] Step: 940, Training Logs: loss_final: 1.501189, loss_mean: 1.390935, proj_loss: -0.031339, loss_mean_cls: 0.108382, deep_loss: 0.033211, grad_norm: 44.023071
[[34m2025-10-03 23:43:29[0m] Step: 941, Training Logs: loss_final: 1.514503, loss_mean: 1.404529, proj_loss: -0.031482, loss_mean_cls: 0.107445, deep_loss: 0.034011, grad_norm: 44.716236
[[34m2025-10-03 23:43:29[0m] Step: 942, Training Logs: loss_final: 1.512559, loss_mean: 1.402657, proj_loss: -0.031806, loss_mean_cls: 0.108733, deep_loss: 0.032975, grad_norm: 48.464352
[[34m2025-10-03 23:43:30[0m] Step: 943, Training Logs: loss_final: 1.495784, loss_mean: 1.386106, proj_loss: -0.031398, loss_mean_cls: 0.107494, deep_loss: 0.033582, grad_norm: 42.819096
[[34m2025-10-03 23:43:31[0m] Step: 944, Training Logs: loss_final: 1.475655, loss_mean: 1.365782, proj_loss: -0.031832, loss_mean_cls: 0.109140, deep_loss: 0.032565, grad_norm: 42.013943
[[34m2025-10-03 23:43:32[0m] Step: 945, Training Logs: loss_final: 1.475278, loss_mean: 1.365078, proj_loss: -0.031208, loss_mean_cls: 0.108443, deep_loss: 0.032965, grad_norm: 35.055016
[[34m2025-10-03 23:43:33[0m] Step: 946, Training Logs: loss_final: 1.503750, loss_mean: 1.395398, proj_loss: -0.032155, loss_mean_cls: 0.107692, deep_loss: 0.032815, grad_norm: 45.316898
[[34m2025-10-03 23:43:33[0m] Step: 947, Training Logs: loss_final: 1.501081, loss_mean: 1.392128, proj_loss: -0.032298, loss_mean_cls: 0.108017, deep_loss: 0.033234, grad_norm: 42.999741
[[34m2025-10-03 23:43:35[0m] Step: 948, Training Logs: loss_final: 1.497206, loss_mean: 1.387236, proj_loss: -0.032613, loss_mean_cls: 0.109007, deep_loss: 0.033577, grad_norm: 25.394762
[[34m2025-10-03 23:43:36[0m] Step: 949, Training Logs: loss_final: 1.482047, loss_mean: 1.373794, proj_loss: -0.032844, loss_mean_cls: 0.107120, deep_loss: 0.033978, grad_norm: 48.966988
[[34m2025-10-03 23:43:37[0m] Step: 950, Training Logs: loss_final: 1.470985, loss_mean: 1.358706, proj_loss: -0.032015, loss_mean_cls: 0.109342, deep_loss: 0.034951, grad_norm: 45.735340
[[34m2025-10-03 23:43:38[0m] Step: 951, Training Logs: loss_final: 1.499689, loss_mean: 1.389173, proj_loss: -0.032442, loss_mean_cls: 0.108788, deep_loss: 0.034169, grad_norm: 39.011795
[[34m2025-10-03 23:43:39[0m] Step: 952, Training Logs: loss_final: 1.489680, loss_mean: 1.379489, proj_loss: -0.032269, loss_mean_cls: 0.109018, deep_loss: 0.033442, grad_norm: 52.610744
[[34m2025-10-03 23:43:40[0m] Step: 953, Training Logs: loss_final: 1.491378, loss_mean: 1.381605, proj_loss: -0.032139, loss_mean_cls: 0.108953, deep_loss: 0.032959, grad_norm: 51.260262
[[34m2025-10-03 23:43:42[0m] Step: 954, Training Logs: loss_final: 1.489611, loss_mean: 1.377511, proj_loss: -0.031331, loss_mean_cls: 0.110193, deep_loss: 0.033238, grad_norm: 44.625019
[[34m2025-10-03 23:43:43[0m] Step: 955, Training Logs: loss_final: 1.517066, loss_mean: 1.409223, proj_loss: -0.034027, loss_mean_cls: 0.107192, deep_loss: 0.034679, grad_norm: 42.899052
[[34m2025-10-03 23:43:44[0m] Step: 956, Training Logs: loss_final: 1.510471, loss_mean: 1.401449, proj_loss: -0.031779, loss_mean_cls: 0.108259, deep_loss: 0.032542, grad_norm: 50.995735
[[34m2025-10-03 23:43:45[0m] Step: 957, Training Logs: loss_final: 1.499084, loss_mean: 1.389801, proj_loss: -0.032164, loss_mean_cls: 0.108152, deep_loss: 0.033295, grad_norm: 50.064102
[[34m2025-10-03 23:43:46[0m] Step: 958, Training Logs: loss_final: 1.472196, loss_mean: 1.363795, proj_loss: -0.033466, loss_mean_cls: 0.108501, deep_loss: 0.033366, grad_norm: 36.670361
[[34m2025-10-03 23:43:47[0m] Step: 959, Training Logs: loss_final: 1.483134, loss_mean: 1.374253, proj_loss: -0.032441, loss_mean_cls: 0.108124, deep_loss: 0.033198, grad_norm: 41.178940
[[34m2025-10-03 23:43:49[0m] Step: 960, Training Logs: loss_final: 1.479786, loss_mean: 1.368633, proj_loss: -0.031394, loss_mean_cls: 0.109294, deep_loss: 0.033252, grad_norm: 47.909321
[[34m2025-10-03 23:43:50[0m] Step: 961, Training Logs: loss_final: 1.507735, loss_mean: 1.397670, proj_loss: -0.032255, loss_mean_cls: 0.108575, deep_loss: 0.033746, grad_norm: 38.831135
[[34m2025-10-03 23:43:51[0m] Step: 962, Training Logs: loss_final: 1.491297, loss_mean: 1.383882, proj_loss: -0.033299, loss_mean_cls: 0.108270, deep_loss: 0.032445, grad_norm: 33.773670
[[34m2025-10-03 23:43:52[0m] Step: 963, Training Logs: loss_final: 1.519483, loss_mean: 1.409549, proj_loss: -0.032018, loss_mean_cls: 0.108041, deep_loss: 0.033911, grad_norm: 47.263439
[[34m2025-10-03 23:43:53[0m] Step: 964, Training Logs: loss_final: 1.489622, loss_mean: 1.379358, proj_loss: -0.031710, loss_mean_cls: 0.108341, deep_loss: 0.033633, grad_norm: 48.236000
[[34m2025-10-03 23:43:55[0m] Step: 965, Training Logs: loss_final: 1.461260, loss_mean: 1.354321, proj_loss: -0.034504, loss_mean_cls: 0.108045, deep_loss: 0.033398, grad_norm: 34.461430
[[34m2025-10-03 23:43:56[0m] Step: 966, Training Logs: loss_final: 1.481153, loss_mean: 1.372696, proj_loss: -0.033371, loss_mean_cls: 0.109648, deep_loss: 0.032179, grad_norm: 37.501064
[[34m2025-10-03 23:43:57[0m] Step: 967, Training Logs: loss_final: 1.507142, loss_mean: 1.398881, proj_loss: -0.033545, loss_mean_cls: 0.108037, deep_loss: 0.033769, grad_norm: 47.714031
[[34m2025-10-03 23:43:58[0m] Step: 968, Training Logs: loss_final: 1.476363, loss_mean: 1.366970, proj_loss: -0.031855, loss_mean_cls: 0.108588, deep_loss: 0.032660, grad_norm: 33.267063
[[34m2025-10-03 23:43:59[0m] Step: 969, Training Logs: loss_final: 1.474190, loss_mean: 1.366742, proj_loss: -0.033561, loss_mean_cls: 0.108012, deep_loss: 0.032998, grad_norm: 32.546749
[[34m2025-10-03 23:44:00[0m] Step: 970, Training Logs: loss_final: 1.506954, loss_mean: 1.398714, proj_loss: -0.033347, loss_mean_cls: 0.107947, deep_loss: 0.033641, grad_norm: 44.177681
[[34m2025-10-03 23:44:02[0m] Step: 971, Training Logs: loss_final: 1.504870, loss_mean: 1.395419, proj_loss: -0.031976, loss_mean_cls: 0.108760, deep_loss: 0.032667, grad_norm: 28.577425
[[34m2025-10-03 23:44:03[0m] Step: 972, Training Logs: loss_final: 1.512869, loss_mean: 1.402813, proj_loss: -0.032335, loss_mean_cls: 0.107449, deep_loss: 0.034941, grad_norm: 31.523981
[[34m2025-10-03 23:44:04[0m] Step: 973, Training Logs: loss_final: 1.504054, loss_mean: 1.393512, proj_loss: -0.031927, loss_mean_cls: 0.108725, deep_loss: 0.033743, grad_norm: 37.911446
[[34m2025-10-03 23:44:05[0m] Step: 974, Training Logs: loss_final: 1.471777, loss_mean: 1.361664, proj_loss: -0.031969, loss_mean_cls: 0.108892, deep_loss: 0.033190, grad_norm: 19.058107
[[34m2025-10-03 23:44:06[0m] Step: 975, Training Logs: loss_final: 1.512855, loss_mean: 1.403054, proj_loss: -0.033162, loss_mean_cls: 0.108541, deep_loss: 0.034422, grad_norm: 43.719482
[[34m2025-10-03 23:44:07[0m] Step: 976, Training Logs: loss_final: 1.522034, loss_mean: 1.408402, proj_loss: -0.030650, loss_mean_cls: 0.109440, deep_loss: 0.034841, grad_norm: 61.591862
[[34m2025-10-03 23:44:08[0m] Step: 977, Training Logs: loss_final: 1.511339, loss_mean: 1.396510, proj_loss: -0.030820, loss_mean_cls: 0.110215, deep_loss: 0.035435, grad_norm: 37.245399
[[34m2025-10-03 23:44:08[0m] Step: 978, Training Logs: loss_final: 1.480212, loss_mean: 1.368089, proj_loss: -0.032518, loss_mean_cls: 0.109741, deep_loss: 0.034901, grad_norm: 36.497013
[[34m2025-10-03 23:44:10[0m] Step: 979, Training Logs: loss_final: 1.499761, loss_mean: 1.389079, proj_loss: -0.032567, loss_mean_cls: 0.108299, deep_loss: 0.034951, grad_norm: 48.708649
[[34m2025-10-03 23:44:11[0m] Step: 980, Training Logs: loss_final: 1.524278, loss_mean: 1.413409, proj_loss: -0.032300, loss_mean_cls: 0.108152, deep_loss: 0.035017, grad_norm: 36.411751
[[34m2025-10-03 23:44:12[0m] Step: 981, Training Logs: loss_final: 1.477374, loss_mean: 1.365058, proj_loss: -0.031853, loss_mean_cls: 0.110163, deep_loss: 0.034006, grad_norm: 31.999308
[[34m2025-10-03 23:44:13[0m] Step: 982, Training Logs: loss_final: 1.489537, loss_mean: 1.377783, proj_loss: -0.032237, loss_mean_cls: 0.110200, deep_loss: 0.033791, grad_norm: 42.076229
[[34m2025-10-03 23:44:14[0m] Step: 983, Training Logs: loss_final: 1.479325, loss_mean: 1.368871, proj_loss: -0.032192, loss_mean_cls: 0.109074, deep_loss: 0.033572, grad_norm: 33.292030
[[34m2025-10-03 23:44:15[0m] Step: 984, Training Logs: loss_final: 1.482251, loss_mean: 1.370605, proj_loss: -0.032553, loss_mean_cls: 0.110293, deep_loss: 0.033906, grad_norm: 21.948286
[[34m2025-10-03 23:44:17[0m] Step: 985, Training Logs: loss_final: 1.489249, loss_mean: 1.378114, proj_loss: -0.030337, loss_mean_cls: 0.108494, deep_loss: 0.032978, grad_norm: 36.739761
[[34m2025-10-03 23:44:18[0m] Step: 986, Training Logs: loss_final: 1.486385, loss_mean: 1.374348, proj_loss: -0.031486, loss_mean_cls: 0.109603, deep_loss: 0.033920, grad_norm: 28.122673
[[34m2025-10-03 23:44:19[0m] Step: 987, Training Logs: loss_final: 1.492404, loss_mean: 1.380574, proj_loss: -0.031780, loss_mean_cls: 0.108820, deep_loss: 0.034790, grad_norm: 28.225315
[[34m2025-10-03 23:44:20[0m] Step: 988, Training Logs: loss_final: 1.497321, loss_mean: 1.386911, proj_loss: -0.032439, loss_mean_cls: 0.108776, deep_loss: 0.034073, grad_norm: 31.515749
[[34m2025-10-03 23:44:21[0m] Step: 989, Training Logs: loss_final: 1.471074, loss_mean: 1.360848, proj_loss: -0.032332, loss_mean_cls: 0.109544, deep_loss: 0.033013, grad_norm: 25.381847
[[34m2025-10-03 23:44:22[0m] Step: 990, Training Logs: loss_final: 1.481154, loss_mean: 1.367840, proj_loss: -0.030838, loss_mean_cls: 0.109881, deep_loss: 0.034271, grad_norm: 33.491383
[[34m2025-10-03 23:44:24[0m] Step: 991, Training Logs: loss_final: 1.471339, loss_mean: 1.361726, proj_loss: -0.032872, loss_mean_cls: 0.109613, deep_loss: 0.032872, grad_norm: 23.139017
[[34m2025-10-03 23:44:25[0m] Step: 992, Training Logs: loss_final: 1.466550, loss_mean: 1.356689, proj_loss: -0.032931, loss_mean_cls: 0.108704, deep_loss: 0.034087, grad_norm: 32.788914
[[34m2025-10-03 23:44:26[0m] Step: 993, Training Logs: loss_final: 1.460572, loss_mean: 1.348864, proj_loss: -0.032465, loss_mean_cls: 0.109712, deep_loss: 0.034460, grad_norm: 28.353127
[[34m2025-10-03 23:44:27[0m] Step: 994, Training Logs: loss_final: 1.479075, loss_mean: 1.368539, proj_loss: -0.031905, loss_mean_cls: 0.109774, deep_loss: 0.032667, grad_norm: 30.273184
[[34m2025-10-03 23:44:28[0m] Step: 995, Training Logs: loss_final: 1.460629, loss_mean: 1.349962, proj_loss: -0.032038, loss_mean_cls: 0.108225, deep_loss: 0.034480, grad_norm: 32.933399
[[34m2025-10-03 23:44:30[0m] Step: 996, Training Logs: loss_final: 1.462612, loss_mean: 1.350858, proj_loss: -0.031590, loss_mean_cls: 0.109100, deep_loss: 0.034245, grad_norm: 25.987494
[[34m2025-10-03 23:44:31[0m] Step: 997, Training Logs: loss_final: 1.463546, loss_mean: 1.351361, proj_loss: -0.030682, loss_mean_cls: 0.109769, deep_loss: 0.033098, grad_norm: 29.252548
[[34m2025-10-03 23:44:32[0m] Step: 998, Training Logs: loss_final: 1.464755, loss_mean: 1.352932, proj_loss: -0.031347, loss_mean_cls: 0.109470, deep_loss: 0.033699, grad_norm: 32.983437
[[34m2025-10-03 23:44:33[0m] Step: 999, Training Logs: loss_final: 1.483439, loss_mean: 1.371246, proj_loss: -0.030349, loss_mean_cls: 0.108148, deep_loss: 0.034395, grad_norm: 27.001347
[[34m2025-10-03 23:44:34[0m] Step: 1000, Training Logs: loss_final: 1.425683, loss_mean: 1.313535, proj_loss: -0.031288, loss_mean_cls: 0.110943, deep_loss: 0.032493, grad_norm: 34.363247
[[34m2025-10-03 23:44:35[0m] Step: 1001, Training Logs: loss_final: 1.487584, loss_mean: 1.375713, proj_loss: -0.030777, loss_mean_cls: 0.109360, deep_loss: 0.033287, grad_norm: 37.166775
[[34m2025-10-03 23:44:37[0m] Step: 1002, Training Logs: loss_final: 1.465784, loss_mean: 1.352529, proj_loss: -0.031844, loss_mean_cls: 0.110616, deep_loss: 0.034483, grad_norm: 27.083199
[[34m2025-10-03 23:44:38[0m] Step: 1003, Training Logs: loss_final: 1.448270, loss_mean: 1.337864, proj_loss: -0.031079, loss_mean_cls: 0.109129, deep_loss: 0.032356, grad_norm: 38.662426
[[34m2025-10-03 23:44:39[0m] Step: 1004, Training Logs: loss_final: 1.481956, loss_mean: 1.370819, proj_loss: -0.031510, loss_mean_cls: 0.109143, deep_loss: 0.033503, grad_norm: 31.132542
[[34m2025-10-03 23:44:40[0m] Step: 1005, Training Logs: loss_final: 1.464013, loss_mean: 1.352045, proj_loss: -0.030904, loss_mean_cls: 0.109723, deep_loss: 0.033149, grad_norm: 29.481606
[[34m2025-10-03 23:44:41[0m] Step: 1006, Training Logs: loss_final: 1.450493, loss_mean: 1.339866, proj_loss: -0.030976, loss_mean_cls: 0.109718, deep_loss: 0.031886, grad_norm: 37.555840
[[34m2025-10-03 23:44:42[0m] Step: 1007, Training Logs: loss_final: 1.444033, loss_mean: 1.332044, proj_loss: -0.031524, loss_mean_cls: 0.110161, deep_loss: 0.033353, grad_norm: 25.628063
[[34m2025-10-03 23:44:43[0m] Step: 1008, Training Logs: loss_final: 1.449414, loss_mean: 1.339588, proj_loss: -0.031814, loss_mean_cls: 0.109047, deep_loss: 0.032593, grad_norm: 35.503765
[[34m2025-10-03 23:44:43[0m] Step: 1009, Training Logs: loss_final: 1.482051, loss_mean: 1.371430, proj_loss: -0.031232, loss_mean_cls: 0.108701, deep_loss: 0.033152, grad_norm: 30.881996
[[34m2025-10-03 23:44:45[0m] Step: 1010, Training Logs: loss_final: 1.487568, loss_mean: 1.374917, proj_loss: -0.030817, loss_mean_cls: 0.109734, deep_loss: 0.033734, grad_norm: 31.587820
[[34m2025-10-03 23:44:46[0m] Step: 1011, Training Logs: loss_final: 1.454359, loss_mean: 1.343283, proj_loss: -0.031556, loss_mean_cls: 0.109966, deep_loss: 0.032666, grad_norm: 27.831793
[[34m2025-10-03 23:44:47[0m] Step: 1012, Training Logs: loss_final: 1.470895, loss_mean: 1.360067, proj_loss: -0.031718, loss_mean_cls: 0.109765, deep_loss: 0.032782, grad_norm: 38.315720
[[34m2025-10-03 23:44:48[0m] Step: 1013, Training Logs: loss_final: 1.470740, loss_mean: 1.358443, proj_loss: -0.031028, loss_mean_cls: 0.109934, deep_loss: 0.033391, grad_norm: 35.269611
[[34m2025-10-03 23:44:49[0m] Step: 1014, Training Logs: loss_final: 1.458074, loss_mean: 1.346469, proj_loss: -0.031284, loss_mean_cls: 0.109965, deep_loss: 0.032924, grad_norm: 34.586040
[[34m2025-10-03 23:44:50[0m] Step: 1015, Training Logs: loss_final: 1.450000, loss_mean: 1.339193, proj_loss: -0.032016, loss_mean_cls: 0.110501, deep_loss: 0.032323, grad_norm: 35.786282
[[34m2025-10-03 23:44:52[0m] Step: 1016, Training Logs: loss_final: 1.464132, loss_mean: 1.354439, proj_loss: -0.032164, loss_mean_cls: 0.109609, deep_loss: 0.032247, grad_norm: 36.468418
[[34m2025-10-03 23:44:53[0m] Step: 1017, Training Logs: loss_final: 1.502268, loss_mean: 1.393080, proj_loss: -0.032991, loss_mean_cls: 0.109076, deep_loss: 0.033102, grad_norm: 24.982428
[[34m2025-10-03 23:44:54[0m] Step: 1018, Training Logs: loss_final: 1.490735, loss_mean: 1.381046, proj_loss: -0.032093, loss_mean_cls: 0.109136, deep_loss: 0.032645, grad_norm: 41.027397
[[34m2025-10-03 23:44:55[0m] Step: 1019, Training Logs: loss_final: 1.475377, loss_mean: 1.364379, proj_loss: -0.030622, loss_mean_cls: 0.109199, deep_loss: 0.032422, grad_norm: 31.620670
[[34m2025-10-03 23:44:56[0m] Step: 1020, Training Logs: loss_final: 1.482053, loss_mean: 1.372577, proj_loss: -0.032693, loss_mean_cls: 0.109564, deep_loss: 0.032605, grad_norm: 30.170996
[[34m2025-10-03 23:44:57[0m] Step: 1021, Training Logs: loss_final: 1.489614, loss_mean: 1.380089, proj_loss: -0.031523, loss_mean_cls: 0.108379, deep_loss: 0.032670, grad_norm: 38.924488
[[34m2025-10-03 23:44:59[0m] Step: 1022, Training Logs: loss_final: 1.478920, loss_mean: 1.368737, proj_loss: -0.032066, loss_mean_cls: 0.109782, deep_loss: 0.032466, grad_norm: 27.908054
[[34m2025-10-03 23:45:00[0m] Step: 1023, Training Logs: loss_final: 1.504103, loss_mean: 1.392141, proj_loss: -0.030447, loss_mean_cls: 0.108494, deep_loss: 0.033915, grad_norm: 32.684494
[[34m2025-10-03 23:45:01[0m] Step: 1024, Training Logs: loss_final: 1.494897, loss_mean: 1.384972, proj_loss: -0.032225, loss_mean_cls: 0.108936, deep_loss: 0.033213, grad_norm: 38.151463
[[34m2025-10-03 23:45:02[0m] Step: 1025, Training Logs: loss_final: 1.468823, loss_mean: 1.358544, proj_loss: -0.031905, loss_mean_cls: 0.109560, deep_loss: 0.032624, grad_norm: 24.838495
[[34m2025-10-03 23:45:03[0m] Step: 1026, Training Logs: loss_final: 1.480238, loss_mean: 1.368375, proj_loss: -0.031413, loss_mean_cls: 0.109396, deep_loss: 0.033880, grad_norm: 34.515202
[[34m2025-10-03 23:45:04[0m] Step: 1027, Training Logs: loss_final: 1.478023, loss_mean: 1.366866, proj_loss: -0.032028, loss_mean_cls: 0.109603, deep_loss: 0.033581, grad_norm: 39.662376
[[34m2025-10-03 23:45:06[0m] Step: 1028, Training Logs: loss_final: 1.493878, loss_mean: 1.383559, proj_loss: -0.031430, loss_mean_cls: 0.109152, deep_loss: 0.032597, grad_norm: 22.999659
[[34m2025-10-03 23:45:07[0m] Step: 1029, Training Logs: loss_final: 1.480634, loss_mean: 1.369916, proj_loss: -0.032273, loss_mean_cls: 0.109497, deep_loss: 0.033495, grad_norm: 40.202198
[[34m2025-10-03 23:45:08[0m] Step: 1030, Training Logs: loss_final: 1.483052, loss_mean: 1.371029, proj_loss: -0.031778, loss_mean_cls: 0.110047, deep_loss: 0.033754, grad_norm: 33.734371
[[34m2025-10-03 23:45:09[0m] Step: 1031, Training Logs: loss_final: 1.469012, loss_mean: 1.359161, proj_loss: -0.032158, loss_mean_cls: 0.110015, deep_loss: 0.031993, grad_norm: 23.702034
[[34m2025-10-03 23:45:10[0m] Step: 1032, Training Logs: loss_final: 1.485782, loss_mean: 1.376600, proj_loss: -0.031925, loss_mean_cls: 0.108379, deep_loss: 0.032728, grad_norm: 39.364033
[[34m2025-10-03 23:45:11[0m] Step: 1033, Training Logs: loss_final: 1.500703, loss_mean: 1.392389, proj_loss: -0.033373, loss_mean_cls: 0.107665, deep_loss: 0.034022, grad_norm: 36.331642
[[34m2025-10-03 23:45:13[0m] Step: 1034, Training Logs: loss_final: 1.470411, loss_mean: 1.358565, proj_loss: -0.030566, loss_mean_cls: 0.109561, deep_loss: 0.032850, grad_norm: 25.804762
[[34m2025-10-03 23:45:14[0m] Step: 1035, Training Logs: loss_final: 1.492284, loss_mean: 1.383633, proj_loss: -0.032707, loss_mean_cls: 0.107491, deep_loss: 0.033867, grad_norm: 38.903675
[[34m2025-10-03 23:45:15[0m] Step: 1036, Training Logs: loss_final: 1.483510, loss_mean: 1.372246, proj_loss: -0.031026, loss_mean_cls: 0.108971, deep_loss: 0.033319, grad_norm: 36.076782
[[34m2025-10-03 23:45:16[0m] Step: 1037, Training Logs: loss_final: 1.492605, loss_mean: 1.380401, proj_loss: -0.031562, loss_mean_cls: 0.109920, deep_loss: 0.033846, grad_norm: 28.859646
[[34m2025-10-03 23:45:17[0m] Step: 1038, Training Logs: loss_final: 1.480627, loss_mean: 1.371310, proj_loss: -0.032857, loss_mean_cls: 0.109346, deep_loss: 0.032828, grad_norm: 38.811184
[[34m2025-10-03 23:45:18[0m] Step: 1039, Training Logs: loss_final: 1.511348, loss_mean: 1.402212, proj_loss: -0.032430, loss_mean_cls: 0.107369, deep_loss: 0.034197, grad_norm: 28.759850
[[34m2025-10-03 23:45:18[0m] Step: 1040, Training Logs: loss_final: 1.520782, loss_mean: 1.411023, proj_loss: -0.032393, loss_mean_cls: 0.108616, deep_loss: 0.033535, grad_norm: 31.656986
[[34m2025-10-03 23:45:20[0m] Step: 1041, Training Logs: loss_final: 1.475869, loss_mean: 1.364064, proj_loss: -0.031519, loss_mean_cls: 0.110411, deep_loss: 0.032913, grad_norm: 37.809479
[[34m2025-10-03 23:45:21[0m] Step: 1042, Training Logs: loss_final: 1.487419, loss_mean: 1.379436, proj_loss: -0.033635, loss_mean_cls: 0.108743, deep_loss: 0.032874, grad_norm: 26.743109
[[34m2025-10-03 23:45:22[0m] Step: 1043, Training Logs: loss_final: 1.470262, loss_mean: 1.360259, proj_loss: -0.031633, loss_mean_cls: 0.109149, deep_loss: 0.032488, grad_norm: 28.628910
[[34m2025-10-03 23:45:23[0m] Step: 1044, Training Logs: loss_final: 1.486964, loss_mean: 1.374563, proj_loss: -0.031264, loss_mean_cls: 0.109881, deep_loss: 0.033783, grad_norm: 38.291618
[[34m2025-10-03 23:45:24[0m] Step: 1045, Training Logs: loss_final: 1.482298, loss_mean: 1.371341, proj_loss: -0.031863, loss_mean_cls: 0.109669, deep_loss: 0.033151, grad_norm: 25.388176
[[34m2025-10-03 23:45:25[0m] Step: 1046, Training Logs: loss_final: 1.503139, loss_mean: 1.392748, proj_loss: -0.032630, loss_mean_cls: 0.108148, deep_loss: 0.034875, grad_norm: 45.100491
[[34m2025-10-03 23:45:27[0m] Step: 1047, Training Logs: loss_final: 1.514443, loss_mean: 1.401320, proj_loss: -0.031747, loss_mean_cls: 0.109813, deep_loss: 0.035056, grad_norm: 51.547676
[[34m2025-10-03 23:45:28[0m] Step: 1048, Training Logs: loss_final: 1.494552, loss_mean: 1.383158, proj_loss: -0.032989, loss_mean_cls: 0.109680, deep_loss: 0.034703, grad_norm: 22.779961
[[34m2025-10-03 23:45:29[0m] Step: 1049, Training Logs: loss_final: 1.481247, loss_mean: 1.368864, proj_loss: -0.032098, loss_mean_cls: 0.109991, deep_loss: 0.034491, grad_norm: 48.081970
[[34m2025-10-03 23:45:30[0m] Step: 1050, Training Logs: loss_final: 1.516412, loss_mean: 1.402163, proj_loss: -0.032128, loss_mean_cls: 0.109739, deep_loss: 0.036639, grad_norm: 52.840328
[[34m2025-10-03 23:45:31[0m] Step: 1051, Training Logs: loss_final: 1.465026, loss_mean: 1.346616, proj_loss: -0.032011, loss_mean_cls: 0.113575, deep_loss: 0.036845, grad_norm: 29.570770
[[34m2025-10-03 23:45:32[0m] Step: 1052, Training Logs: loss_final: 1.500380, loss_mean: 1.384383, proj_loss: -0.030068, loss_mean_cls: 0.110938, deep_loss: 0.035126, grad_norm: 51.348068
[[34m2025-10-03 23:45:34[0m] Step: 1053, Training Logs: loss_final: 1.508425, loss_mean: 1.395341, proj_loss: -0.031760, loss_mean_cls: 0.109813, deep_loss: 0.035031, grad_norm: 60.392666
[[34m2025-10-03 23:45:35[0m] Step: 1054, Training Logs: loss_final: 1.500534, loss_mean: 1.384179, proj_loss: -0.032086, loss_mean_cls: 0.111642, deep_loss: 0.036798, grad_norm: 45.787205
[[34m2025-10-03 23:45:36[0m] Step: 1055, Training Logs: loss_final: 1.498001, loss_mean: 1.383361, proj_loss: -0.031447, loss_mean_cls: 0.110458, deep_loss: 0.035628, grad_norm: 45.060116
[[34m2025-10-03 23:45:37[0m] Step: 1056, Training Logs: loss_final: 1.501117, loss_mean: 1.387678, proj_loss: -0.030523, loss_mean_cls: 0.109704, deep_loss: 0.034258, grad_norm: 51.356506
[[34m2025-10-03 23:45:38[0m] Step: 1057, Training Logs: loss_final: 1.526387, loss_mean: 1.414302, proj_loss: -0.031966, loss_mean_cls: 0.109684, deep_loss: 0.034366, grad_norm: 48.365677
[[34m2025-10-03 23:45:40[0m] Step: 1058, Training Logs: loss_final: 1.503198, loss_mean: 1.389178, proj_loss: -0.030751, loss_mean_cls: 0.109868, deep_loss: 0.034903, grad_norm: 40.306206
[[34m2025-10-03 23:45:41[0m] Step: 1059, Training Logs: loss_final: 1.509740, loss_mean: 1.396808, proj_loss: -0.031201, loss_mean_cls: 0.110003, deep_loss: 0.034129, grad_norm: 53.370468
[[34m2025-10-03 23:45:42[0m] Step: 1060, Training Logs: loss_final: 1.512082, loss_mean: 1.398547, proj_loss: -0.031137, loss_mean_cls: 0.109774, deep_loss: 0.034898, grad_norm: 60.924591
[[34m2025-10-03 23:45:43[0m] Step: 1061, Training Logs: loss_final: 1.510780, loss_mean: 1.395254, proj_loss: -0.029855, loss_mean_cls: 0.110722, deep_loss: 0.034659, grad_norm: 48.009056
[[34m2025-10-03 23:45:44[0m] Step: 1062, Training Logs: loss_final: 1.518892, loss_mean: 1.406898, proj_loss: -0.031745, loss_mean_cls: 0.108857, deep_loss: 0.034881, grad_norm: 43.592175
[[34m2025-10-03 23:45:45[0m] Step: 1063, Training Logs: loss_final: 1.507004, loss_mean: 1.395267, proj_loss: -0.031595, loss_mean_cls: 0.109635, deep_loss: 0.033697, grad_norm: 51.031464
[[34m2025-10-03 23:45:47[0m] Step: 1064, Training Logs: loss_final: 1.508341, loss_mean: 1.396066, proj_loss: -0.031694, loss_mean_cls: 0.109988, deep_loss: 0.033981, grad_norm: 61.034901
[[34m2025-10-03 23:45:48[0m] Step: 1065, Training Logs: loss_final: 1.513771, loss_mean: 1.402012, proj_loss: -0.031686, loss_mean_cls: 0.108927, deep_loss: 0.034519, grad_norm: 40.726471
[[34m2025-10-03 23:55:26[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-03 23:55:27[0m] using MLP layer as FFN
[[34m2025-10-03 23:55:31[0m] SiT Parameters: 140,221,984
[[34m2025-10-03 23:55:50[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-03 23:56:10[0m] Generating EMA samples done.
[[34m2025-10-03 23:56:10[0m] Step: 1, Training Logs: loss_final: 1.852024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:11[0m] Step: 2, Training Logs: loss_final: 1.857074, loss_mean: 1.690185, proj_loss: -0.001133, loss_mean_cls: 0.118023, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:13[0m] Step: 3, Training Logs: loss_final: 1.893526, loss_mean: 1.725911, proj_loss: -0.000692, loss_mean_cls: 0.118307, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:14[0m] Step: 4, Training Logs: loss_final: 1.863802, loss_mean: 1.696726, proj_loss: -0.000918, loss_mean_cls: 0.117994, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:15[0m] Step: 5, Training Logs: loss_final: 1.876213, loss_mean: 1.708497, proj_loss: -0.001132, loss_mean_cls: 0.118849, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:16[0m] Step: 6, Training Logs: loss_final: 1.861109, loss_mean: 1.693091, proj_loss: -0.000262, loss_mean_cls: 0.118281, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:17[0m] Step: 7, Training Logs: loss_final: 1.855225, loss_mean: 1.687699, proj_loss: -0.000456, loss_mean_cls: 0.117981, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:18[0m] Step: 8, Training Logs: loss_final: 1.865341, loss_mean: 1.697358, proj_loss: -0.000295, loss_mean_cls: 0.118278, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:20[0m] Step: 9, Training Logs: loss_final: 1.856894, loss_mean: 1.689859, proj_loss: -0.001537, loss_mean_cls: 0.118572, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:21[0m] Step: 10, Training Logs: loss_final: 1.880090, loss_mean: 1.711735, proj_loss: 0.000060, loss_mean_cls: 0.118295, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:22[0m] Step: 11, Training Logs: loss_final: 1.858421, loss_mean: 1.690543, proj_loss: -0.000472, loss_mean_cls: 0.118350, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:23[0m] Step: 12, Training Logs: loss_final: 1.876679, loss_mean: 1.708093, proj_loss: 0.000305, loss_mean_cls: 0.118281, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:24[0m] Step: 13, Training Logs: loss_final: 1.844913, loss_mean: 1.677251, proj_loss: -0.000555, loss_mean_cls: 0.118217, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:25[0m] Step: 14, Training Logs: loss_final: 1.860075, loss_mean: 1.692572, proj_loss: -0.000743, loss_mean_cls: 0.118246, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:26[0m] Step: 15, Training Logs: loss_final: 1.858943, loss_mean: 1.690525, proj_loss: 0.000005, loss_mean_cls: 0.118413, deep_loss: 0.050000, grad_norm: nan
[[34m2025-10-03 23:56:28[0m] Step: 16, Training Logs: loss_final: 1.882615, loss_mean: 1.715268, proj_loss: -0.000550, loss_mean_cls: 0.117897, deep_loss: 0.050000, grad_norm: inf
[[34m2025-10-03 23:56:29[0m] Step: 17, Training Logs: loss_final: 1.849142, loss_mean: 1.680075, proj_loss: 0.000387, loss_mean_cls: 0.118681, deep_loss: 0.050000, grad_norm: inf
[[34m2025-10-03 23:56:29[0m] Step: 18, Training Logs: loss_final: 1.846045, loss_mean: 1.678042, proj_loss: -0.000140, loss_mean_cls: 0.118143, deep_loss: 0.050000, grad_norm: 3722890.000000
[[34m2025-10-03 23:56:30[0m] Step: 19, Training Logs: loss_final: 1.857450, loss_mean: 1.695266, proj_loss: -0.001839, loss_mean_cls: 0.118697, deep_loss: 0.045326, grad_norm: 1.190755
[[34m2025-10-03 23:56:31[0m] Step: 20, Training Logs: loss_final: 1.844619, loss_mean: 1.702542, proj_loss: -0.020595, loss_mean_cls: 0.118579, deep_loss: 0.044093, grad_norm: 1.050217
[[34m2025-10-03 23:56:32[0m] Step: 21, Training Logs: loss_final: 1.800544, loss_mean: 1.677884, proj_loss: -0.039088, loss_mean_cls: 0.118060, deep_loss: 0.043688, grad_norm: 0.775306
[[34m2025-10-03 23:56:34[0m] Step: 22, Training Logs: loss_final: 1.779429, loss_mean: 1.675088, proj_loss: -0.057046, loss_mean_cls: 0.118154, deep_loss: 0.043234, grad_norm: 0.852297
[[34m2025-10-03 23:56:35[0m] Step: 23, Training Logs: loss_final: 1.779888, loss_mean: 1.684882, proj_loss: -0.066553, loss_mean_cls: 0.118888, deep_loss: 0.042671, grad_norm: 0.965044
[[34m2025-10-03 23:56:36[0m] Step: 24, Training Logs: loss_final: 1.732833, loss_mean: 1.650599, proj_loss: -0.077307, loss_mean_cls: 0.118220, deep_loss: 0.041320, grad_norm: 0.960297
[[34m2025-10-03 23:56:37[0m] Step: 25, Training Logs: loss_final: 1.747091, loss_mean: 1.675151, proj_loss: -0.083633, loss_mean_cls: 0.117665, deep_loss: 0.037908, grad_norm: 1.071090
[[34m2025-10-03 23:56:38[0m] Step: 26, Training Logs: loss_final: 1.732247, loss_mean: 1.664828, proj_loss: -0.086854, loss_mean_cls: 0.118246, deep_loss: 0.036028, grad_norm: 1.194224
[[34m2025-10-03 23:56:39[0m] Step: 27, Training Logs: loss_final: 1.740822, loss_mean: 1.677012, proj_loss: -0.090685, loss_mean_cls: 0.117870, deep_loss: 0.036625, grad_norm: 1.458827
[[34m2025-10-03 23:56:41[0m] Step: 28, Training Logs: loss_final: 1.726125, loss_mean: 1.666409, proj_loss: -0.095203, loss_mean_cls: 0.118141, deep_loss: 0.036778, grad_norm: 1.448264
[[34m2025-10-03 23:56:42[0m] Step: 29, Training Logs: loss_final: 1.709279, loss_mean: 1.651641, proj_loss: -0.096746, loss_mean_cls: 0.118754, deep_loss: 0.035630, grad_norm: 1.323612
[[34m2025-10-03 23:56:43[0m] Step: 30, Training Logs: loss_final: 1.716180, loss_mean: 1.663827, proj_loss: -0.099533, loss_mean_cls: 0.117709, deep_loss: 0.034177, grad_norm: 1.130866
[[34m2025-10-03 23:56:44[0m] Step: 31, Training Logs: loss_final: 1.678653, loss_mean: 1.629526, proj_loss: -0.103068, loss_mean_cls: 0.117498, deep_loss: 0.034697, grad_norm: 0.945440
[[34m2025-10-03 23:56:45[0m] Step: 32, Training Logs: loss_final: 1.682934, loss_mean: 1.635470, proj_loss: -0.103926, loss_mean_cls: 0.117073, deep_loss: 0.034317, grad_norm: 1.013697
[[34m2025-10-03 23:56:46[0m] Step: 33, Training Logs: loss_final: 1.691482, loss_mean: 1.647624, proj_loss: -0.107752, loss_mean_cls: 0.117478, deep_loss: 0.034132, grad_norm: 1.057782
[[34m2025-10-03 23:56:48[0m] Step: 34, Training Logs: loss_final: 1.660714, loss_mean: 1.617018, proj_loss: -0.107499, loss_mean_cls: 0.117152, deep_loss: 0.034043, grad_norm: 0.951552
[[34m2025-10-03 23:56:49[0m] Step: 35, Training Logs: loss_final: 1.684107, loss_mean: 1.645801, proj_loss: -0.112035, loss_mean_cls: 0.117595, deep_loss: 0.032746, grad_norm: 0.923220
[[34m2025-10-03 23:56:50[0m] Step: 36, Training Logs: loss_final: 1.642755, loss_mean: 1.604292, proj_loss: -0.112821, loss_mean_cls: 0.117412, deep_loss: 0.033873, grad_norm: 0.981911
[[34m2025-10-03 23:56:51[0m] Step: 37, Training Logs: loss_final: 1.644108, loss_mean: 1.610567, proj_loss: -0.116132, loss_mean_cls: 0.116967, deep_loss: 0.032705, grad_norm: 1.065663
[[34m2025-10-03 23:56:52[0m] Step: 38, Training Logs: loss_final: 1.658254, loss_mean: 1.626786, proj_loss: -0.119235, loss_mean_cls: 0.117879, deep_loss: 0.032824, grad_norm: 0.967049
[[34m2025-10-03 23:56:54[0m] Step: 39, Training Logs: loss_final: 1.624334, loss_mean: 1.596041, proj_loss: -0.121517, loss_mean_cls: 0.116626, deep_loss: 0.033183, grad_norm: 0.936775
[[34m2025-10-03 23:56:55[0m] Step: 40, Training Logs: loss_final: 1.621049, loss_mean: 1.597463, proj_loss: -0.126371, loss_mean_cls: 0.117717, deep_loss: 0.032240, grad_norm: 0.861901
[[34m2025-10-03 23:56:56[0m] Step: 41, Training Logs: loss_final: 1.600093, loss_mean: 1.578682, proj_loss: -0.127728, loss_mean_cls: 0.117250, deep_loss: 0.031888, grad_norm: 0.834176
[[34m2025-10-03 23:56:57[0m] Step: 42, Training Logs: loss_final: 1.595250, loss_mean: 1.575648, proj_loss: -0.128957, loss_mean_cls: 0.116559, deep_loss: 0.031999, grad_norm: 0.810932
[[34m2025-10-03 23:56:58[0m] Step: 43, Training Logs: loss_final: 1.575175, loss_mean: 1.557783, proj_loss: -0.131346, loss_mean_cls: 0.116795, deep_loss: 0.031943, grad_norm: 0.758303
[[34m2025-10-03 23:56:59[0m] Step: 44, Training Logs: loss_final: 1.556452, loss_mean: 1.540380, proj_loss: -0.132271, loss_mean_cls: 0.116392, deep_loss: 0.031951, grad_norm: 0.704377
[[34m2025-10-03 23:57:01[0m] Step: 45, Training Logs: loss_final: 1.542624, loss_mean: 1.526685, proj_loss: -0.133199, loss_mean_cls: 0.116787, deep_loss: 0.032351, grad_norm: 0.936443
[[34m2025-10-03 23:57:02[0m] Step: 46, Training Logs: loss_final: 1.553499, loss_mean: 1.540522, proj_loss: -0.136116, loss_mean_cls: 0.116074, deep_loss: 0.033020, grad_norm: 1.172395
[[34m2025-10-03 23:57:03[0m] Step: 47, Training Logs: loss_final: 1.524700, loss_mean: 1.512533, proj_loss: -0.136097, loss_mean_cls: 0.115781, deep_loss: 0.032483, grad_norm: 0.594672
[[34m2025-10-03 23:57:04[0m] Step: 48, Training Logs: loss_final: 1.534715, loss_mean: 1.525124, proj_loss: -0.138058, loss_mean_cls: 0.115779, deep_loss: 0.031869, grad_norm: 0.720464
[[34m2025-10-03 23:57:05[0m] Step: 49, Training Logs: loss_final: 1.535623, loss_mean: 1.526733, proj_loss: -0.138876, loss_mean_cls: 0.116408, deep_loss: 0.031357, grad_norm: 1.048578
[[34m2025-10-03 23:57:05[0m] Step: 50, Training Logs: loss_final: 1.541576, loss_mean: 1.532666, proj_loss: -0.139751, loss_mean_cls: 0.116019, deep_loss: 0.032641, grad_norm: 1.727470
[[34m2025-10-03 23:57:06[0m] Step: 51, Training Logs: loss_final: 1.526954, loss_mean: 1.519844, proj_loss: -0.139691, loss_mean_cls: 0.115795, deep_loss: 0.031007, grad_norm: 0.736862
[[34m2025-10-03 23:57:07[0m] Step: 52, Training Logs: loss_final: 1.522411, loss_mean: 1.517837, proj_loss: -0.142827, loss_mean_cls: 0.115317, deep_loss: 0.032085, grad_norm: 2.321333
[[34m2025-10-03 23:57:08[0m] Step: 53, Training Logs: loss_final: 1.528522, loss_mean: 1.523429, proj_loss: -0.141923, loss_mean_cls: 0.115068, deep_loss: 0.031949, grad_norm: 2.179204
[[34m2025-10-03 23:57:09[0m] Step: 54, Training Logs: loss_final: 1.516810, loss_mean: 1.511703, proj_loss: -0.141782, loss_mean_cls: 0.115613, deep_loss: 0.031276, grad_norm: 0.745605
[[34m2025-10-03 23:57:10[0m] Step: 55, Training Logs: loss_final: 1.493713, loss_mean: 1.490762, proj_loss: -0.144253, loss_mean_cls: 0.115735, deep_loss: 0.031469, grad_norm: 1.587335
[[34m2025-10-03 23:57:11[0m] Step: 56, Training Logs: loss_final: 1.480589, loss_mean: 1.477696, proj_loss: -0.144282, loss_mean_cls: 0.115040, deep_loss: 0.032135, grad_norm: 0.663576
[[34m2025-10-03 23:57:12[0m] Step: 57, Training Logs: loss_final: 1.509699, loss_mean: 1.508178, proj_loss: -0.145585, loss_mean_cls: 0.115396, deep_loss: 0.031710, grad_norm: 2.059987
[[34m2025-10-03 23:57:14[0m] Step: 58, Training Logs: loss_final: 1.448789, loss_mean: 1.447078, proj_loss: -0.144859, loss_mean_cls: 0.115127, deep_loss: 0.031442, grad_norm: 0.828879
[[34m2025-10-03 23:57:15[0m] Step: 59, Training Logs: loss_final: 1.419547, loss_mean: 1.417690, proj_loss: -0.144037, loss_mean_cls: 0.115208, deep_loss: 0.030687, grad_norm: 0.828922
[[34m2025-10-03 23:57:16[0m] Step: 60, Training Logs: loss_final: 1.445580, loss_mean: 1.445961, proj_loss: -0.145697, loss_mean_cls: 0.114243, deep_loss: 0.031073, grad_norm: 1.472747
[[34m2025-10-03 23:57:17[0m] Step: 61, Training Logs: loss_final: 1.473336, loss_mean: 1.474709, proj_loss: -0.147177, loss_mean_cls: 0.115304, deep_loss: 0.030500, grad_norm: 2.624000
[[34m2025-10-03 23:57:18[0m] Step: 62, Training Logs: loss_final: 1.390192, loss_mean: 1.390908, proj_loss: -0.145978, loss_mean_cls: 0.114264, deep_loss: 0.030997, grad_norm: 0.592146
[[34m2025-10-03 23:57:19[0m] Step: 63, Training Logs: loss_final: 1.435668, loss_mean: 1.435140, proj_loss: -0.145679, loss_mean_cls: 0.115724, deep_loss: 0.030483, grad_norm: 2.247913
[[34m2025-10-03 23:57:21[0m] Step: 64, Training Logs: loss_final: 1.377437, loss_mean: 1.377413, proj_loss: -0.145548, loss_mean_cls: 0.114757, deep_loss: 0.030814, grad_norm: 0.668074
[[34m2025-10-03 23:57:22[0m] Step: 65, Training Logs: loss_final: 1.461014, loss_mean: 1.462033, proj_loss: -0.147324, loss_mean_cls: 0.113329, deep_loss: 0.032976, grad_norm: 2.050485
[[34m2025-10-03 23:57:23[0m] Step: 66, Training Logs: loss_final: 1.405724, loss_mean: 1.406946, proj_loss: -0.146336, loss_mean_cls: 0.114093, deep_loss: 0.031022, grad_norm: 1.424176
[[34m2025-10-03 23:57:24[0m] Step: 67, Training Logs: loss_final: 1.374719, loss_mean: 1.377232, proj_loss: -0.147798, loss_mean_cls: 0.114253, deep_loss: 0.031032, grad_norm: 1.013231
[[34m2025-10-03 23:57:25[0m] Step: 68, Training Logs: loss_final: 1.406424, loss_mean: 1.408510, proj_loss: -0.147273, loss_mean_cls: 0.113322, deep_loss: 0.031865, grad_norm: 1.811914
[[34m2025-10-03 23:57:26[0m] Step: 69, Training Logs: loss_final: 1.383924, loss_mean: 1.384158, proj_loss: -0.145830, loss_mean_cls: 0.114079, deep_loss: 0.031517, grad_norm: 1.115738
[[34m2025-10-03 23:57:28[0m] Step: 70, Training Logs: loss_final: 1.362709, loss_mean: 1.367818, proj_loss: -0.148725, loss_mean_cls: 0.113404, deep_loss: 0.030213, grad_norm: 1.190304
[[34m2025-10-03 23:57:29[0m] Step: 71, Training Logs: loss_final: 1.427608, loss_mean: 1.430869, proj_loss: -0.148418, loss_mean_cls: 0.113308, deep_loss: 0.031848, grad_norm: 1.288794
[[34m2025-10-03 23:57:30[0m] Step: 72, Training Logs: loss_final: 1.367123, loss_mean: 1.371226, proj_loss: -0.148376, loss_mean_cls: 0.113169, deep_loss: 0.031104, grad_norm: 1.171066
[[34m2025-10-03 23:57:31[0m] Step: 73, Training Logs: loss_final: 1.371788, loss_mean: 1.374960, proj_loss: -0.147484, loss_mean_cls: 0.113780, deep_loss: 0.030532, grad_norm: 0.703961
[[34m2025-10-03 23:57:32[0m] Step: 74, Training Logs: loss_final: 1.384730, loss_mean: 1.388303, proj_loss: -0.148338, loss_mean_cls: 0.113315, deep_loss: 0.031450, grad_norm: 1.103655
[[34m2025-10-03 23:57:34[0m] Step: 75, Training Logs: loss_final: 1.367141, loss_mean: 1.369300, proj_loss: -0.147217, loss_mean_cls: 0.114179, deep_loss: 0.030878, grad_norm: 1.100881
[[34m2025-10-03 23:57:35[0m] Step: 76, Training Logs: loss_final: 1.366411, loss_mean: 1.370227, proj_loss: -0.146990, loss_mean_cls: 0.113738, deep_loss: 0.029436, grad_norm: 0.933985
[[34m2025-10-03 23:57:36[0m] Step: 77, Training Logs: loss_final: 1.356328, loss_mean: 1.359780, proj_loss: -0.146771, loss_mean_cls: 0.113466, deep_loss: 0.029852, grad_norm: 1.082046
[[34m2025-10-03 23:57:37[0m] Step: 78, Training Logs: loss_final: 1.360895, loss_mean: 1.365696, proj_loss: -0.148422, loss_mean_cls: 0.113016, deep_loss: 0.030604, grad_norm: 0.817053
[[34m2025-10-03 23:57:38[0m] Step: 79, Training Logs: loss_final: 1.369056, loss_mean: 1.372239, proj_loss: -0.147313, loss_mean_cls: 0.113260, deep_loss: 0.030870, grad_norm: 1.530940
[[34m2025-10-03 23:57:39[0m] Step: 80, Training Logs: loss_final: 1.384825, loss_mean: 1.388645, proj_loss: -0.147505, loss_mean_cls: 0.112891, deep_loss: 0.030794, grad_norm: 2.823335
[[34m2025-10-03 23:57:41[0m] Step: 81, Training Logs: loss_final: 1.440961, loss_mean: 1.444645, proj_loss: -0.147700, loss_mean_cls: 0.112266, deep_loss: 0.031750, grad_norm: 5.418567
[[34m2025-10-03 23:57:41[0m] Step: 82, Training Logs: loss_final: 1.423729, loss_mean: 1.426088, proj_loss: -0.147246, loss_mean_cls: 0.112825, deep_loss: 0.032063, grad_norm: 4.602059
[[34m2025-10-03 23:57:42[0m] Step: 83, Training Logs: loss_final: 1.461623, loss_mean: 1.465880, proj_loss: -0.149582, loss_mean_cls: 0.112458, deep_loss: 0.032867, grad_norm: 5.929204
[[34m2025-10-03 23:57:43[0m] Step: 84, Training Logs: loss_final: 1.382555, loss_mean: 1.384605, proj_loss: -0.145112, loss_mean_cls: 0.112559, deep_loss: 0.030504, grad_norm: 3.733061
[[34m2025-10-03 23:57:44[0m] Step: 85, Training Logs: loss_final: 1.403642, loss_mean: 1.406309, proj_loss: -0.145545, loss_mean_cls: 0.112321, deep_loss: 0.030557, grad_norm: 5.205406
[[34m2025-10-03 23:57:45[0m] Step: 86, Training Logs: loss_final: 1.439991, loss_mean: 1.441501, proj_loss: -0.146265, loss_mean_cls: 0.113149, deep_loss: 0.031606, grad_norm: 6.494482
[[34m2025-10-03 23:57:46[0m] Step: 87, Training Logs: loss_final: 1.443421, loss_mean: 1.446446, proj_loss: -0.146703, loss_mean_cls: 0.112418, deep_loss: 0.031260, grad_norm: 4.951910
[[34m2025-10-03 23:57:48[0m] Step: 88, Training Logs: loss_final: 1.417078, loss_mean: 1.420932, proj_loss: -0.146790, loss_mean_cls: 0.112228, deep_loss: 0.030709, grad_norm: 4.037041
[[34m2025-10-03 23:57:49[0m] Step: 89, Training Logs: loss_final: 1.387619, loss_mean: 1.391297, proj_loss: -0.146852, loss_mean_cls: 0.112696, deep_loss: 0.030478, grad_norm: 4.455070
[[34m2025-10-03 23:57:50[0m] Step: 90, Training Logs: loss_final: 1.400875, loss_mean: 1.405630, proj_loss: -0.148256, loss_mean_cls: 0.111702, deep_loss: 0.031799, grad_norm: 4.740033
[[34m2025-10-03 23:57:51[0m] Step: 91, Training Logs: loss_final: 1.392017, loss_mean: 1.395713, proj_loss: -0.147516, loss_mean_cls: 0.112161, deep_loss: 0.031659, grad_norm: 4.008111
[[34m2025-10-03 23:57:52[0m] Step: 92, Training Logs: loss_final: 1.368492, loss_mean: 1.373436, proj_loss: -0.148698, loss_mean_cls: 0.112450, deep_loss: 0.031304, grad_norm: 3.328374
[[34m2025-10-03 23:57:53[0m] Step: 93, Training Logs: loss_final: 1.432232, loss_mean: 1.435383, proj_loss: -0.145895, loss_mean_cls: 0.110843, deep_loss: 0.031901, grad_norm: 6.071809
[[34m2025-10-03 23:57:55[0m] Step: 94, Training Logs: loss_final: 1.411152, loss_mean: 1.414548, proj_loss: -0.147499, loss_mean_cls: 0.112322, deep_loss: 0.031781, grad_norm: 6.073527
[[34m2025-10-03 23:57:56[0m] Step: 95, Training Logs: loss_final: 1.390439, loss_mean: 1.394565, proj_loss: -0.146051, loss_mean_cls: 0.111306, deep_loss: 0.030620, grad_norm: 4.706511
[[34m2025-10-03 23:57:57[0m] Step: 96, Training Logs: loss_final: 1.407484, loss_mean: 1.410173, proj_loss: -0.145999, loss_mean_cls: 0.111606, deep_loss: 0.031704, grad_norm: 4.277789
[[34m2025-10-03 23:57:58[0m] Step: 97, Training Logs: loss_final: 1.405840, loss_mean: 1.409546, proj_loss: -0.147118, loss_mean_cls: 0.111485, deep_loss: 0.031927, grad_norm: 4.165754
[[34m2025-10-03 23:57:59[0m] Step: 98, Training Logs: loss_final: 1.381798, loss_mean: 1.384669, proj_loss: -0.145776, loss_mean_cls: 0.111808, deep_loss: 0.031096, grad_norm: 4.558535
[[34m2025-10-03 23:58:01[0m] Step: 99, Training Logs: loss_final: 1.395613, loss_mean: 1.399707, proj_loss: -0.146257, loss_mean_cls: 0.111265, deep_loss: 0.030898, grad_norm: 4.266659
[[34m2025-10-03 23:58:02[0m] Step: 100, Training Logs: loss_final: 1.388451, loss_mean: 1.391130, proj_loss: -0.145840, loss_mean_cls: 0.112046, deep_loss: 0.031116, grad_norm: 3.422319
[[34m2025-10-03 23:58:03[0m] Step: 101, Training Logs: loss_final: 1.377474, loss_mean: 1.379470, proj_loss: -0.144481, loss_mean_cls: 0.111824, deep_loss: 0.030660, grad_norm: 3.203753
[[34m2025-10-03 23:58:04[0m] Step: 102, Training Logs: loss_final: 1.379220, loss_mean: 1.384358, proj_loss: -0.147161, loss_mean_cls: 0.111565, deep_loss: 0.030458, grad_norm: 3.826538
[[34m2025-10-03 23:58:05[0m] Step: 103, Training Logs: loss_final: 1.394678, loss_mean: 1.399425, proj_loss: -0.148023, loss_mean_cls: 0.111715, deep_loss: 0.031562, grad_norm: 3.899208
[[34m2025-10-03 23:58:06[0m] Step: 104, Training Logs: loss_final: 1.387767, loss_mean: 1.391781, proj_loss: -0.146163, loss_mean_cls: 0.110974, deep_loss: 0.031175, grad_norm: 3.704376
[[34m2025-10-03 23:58:08[0m] Step: 105, Training Logs: loss_final: 1.375609, loss_mean: 1.378464, proj_loss: -0.145318, loss_mean_cls: 0.111275, deep_loss: 0.031188, grad_norm: 2.582890
[[34m2025-10-03 23:58:09[0m] Step: 106, Training Logs: loss_final: 1.371514, loss_mean: 1.375387, proj_loss: -0.146560, loss_mean_cls: 0.111681, deep_loss: 0.031006, grad_norm: 3.324975
[[34m2025-10-03 23:58:10[0m] Step: 107, Training Logs: loss_final: 1.376047, loss_mean: 1.378537, proj_loss: -0.144855, loss_mean_cls: 0.111290, deep_loss: 0.031076, grad_norm: 4.864861
[[34m2025-10-03 23:58:11[0m] Step: 108, Training Logs: loss_final: 1.369180, loss_mean: 1.372114, proj_loss: -0.144984, loss_mean_cls: 0.111888, deep_loss: 0.030162, grad_norm: 4.788991
[[34m2025-10-03 23:58:12[0m] Step: 109, Training Logs: loss_final: 1.420607, loss_mean: 1.420973, proj_loss: -0.144675, loss_mean_cls: 0.111140, deep_loss: 0.033168, grad_norm: 5.973158
[[34m2025-10-03 23:58:13[0m] Step: 110, Training Logs: loss_final: 1.489905, loss_mean: 1.489550, proj_loss: -0.143822, loss_mean_cls: 0.111565, deep_loss: 0.032612, grad_norm: 11.150744
[[34m2025-10-03 23:58:15[0m] Step: 111, Training Logs: loss_final: 1.412373, loss_mean: 1.411072, proj_loss: -0.143300, loss_mean_cls: 0.112307, deep_loss: 0.032294, grad_norm: 6.431697
[[34m2025-10-03 23:58:16[0m] Step: 112, Training Logs: loss_final: 1.436798, loss_mean: 1.435546, proj_loss: -0.142876, loss_mean_cls: 0.111550, deep_loss: 0.032577, grad_norm: 6.647782
[[34m2025-10-03 23:58:16[0m] Step: 113, Training Logs: loss_final: 1.423136, loss_mean: 1.420821, proj_loss: -0.142088, loss_mean_cls: 0.111497, deep_loss: 0.032906, grad_norm: 8.216562
[[34m2025-10-03 23:58:17[0m] Step: 114, Training Logs: loss_final: 1.405712, loss_mean: 1.403556, proj_loss: -0.141595, loss_mean_cls: 0.111698, deep_loss: 0.032053, grad_norm: 6.302495
[[34m2025-10-03 23:58:18[0m] Step: 115, Training Logs: loss_final: 1.419509, loss_mean: 1.414680, proj_loss: -0.138806, loss_mean_cls: 0.111954, deep_loss: 0.031682, grad_norm: 7.337322
[[34m2025-10-03 23:58:19[0m] Step: 116, Training Logs: loss_final: 1.440883, loss_mean: 1.432421, proj_loss: -0.136630, loss_mean_cls: 0.112452, deep_loss: 0.032640, grad_norm: 6.652267
[[34m2025-10-03 23:58:20[0m] Step: 117, Training Logs: loss_final: 1.446118, loss_mean: 1.440975, proj_loss: -0.139143, loss_mean_cls: 0.111480, deep_loss: 0.032805, grad_norm: 8.565838
[[34m2025-10-03 23:58:21[0m] Step: 118, Training Logs: loss_final: 1.491479, loss_mean: 1.480428, proj_loss: -0.135535, loss_mean_cls: 0.112353, deep_loss: 0.034233, grad_norm: 12.403057
[[34m2025-10-03 23:58:22[0m] Step: 119, Training Logs: loss_final: 1.439624, loss_mean: 1.432233, proj_loss: -0.136944, loss_mean_cls: 0.112559, deep_loss: 0.031777, grad_norm: 8.194375
[[34m2025-10-03 23:58:24[0m] Step: 120, Training Logs: loss_final: 1.478833, loss_mean: 1.467420, proj_loss: -0.133432, loss_mean_cls: 0.111987, deep_loss: 0.032859, grad_norm: 10.410409
[[34m2025-10-03 23:58:25[0m] Step: 121, Training Logs: loss_final: 1.441338, loss_mean: 1.434832, proj_loss: -0.137485, loss_mean_cls: 0.111407, deep_loss: 0.032583, grad_norm: 7.180409
[[34m2025-10-03 23:58:26[0m] Step: 122, Training Logs: loss_final: 1.412147, loss_mean: 1.403171, proj_loss: -0.137176, loss_mean_cls: 0.113187, deep_loss: 0.032964, grad_norm: 9.194578
[[34m2025-10-03 23:58:27[0m] Step: 123, Training Logs: loss_final: 1.430111, loss_mean: 1.423921, proj_loss: -0.138187, loss_mean_cls: 0.111753, deep_loss: 0.032624, grad_norm: 7.128738
[[34m2025-10-03 23:58:28[0m] Step: 124, Training Logs: loss_final: 1.422362, loss_mean: 1.414892, proj_loss: -0.137215, loss_mean_cls: 0.112180, deep_loss: 0.032505, grad_norm: 6.499055
[[34m2025-10-03 23:58:29[0m] Step: 125, Training Logs: loss_final: 1.454381, loss_mean: 1.446456, proj_loss: -0.137341, loss_mean_cls: 0.112470, deep_loss: 0.032796, grad_norm: 9.843773
[[34m2025-10-03 23:58:31[0m] Step: 126, Training Logs: loss_final: 1.446464, loss_mean: 1.437943, proj_loss: -0.136033, loss_mean_cls: 0.112459, deep_loss: 0.032095, grad_norm: 10.500775
[[34m2025-10-03 23:58:32[0m] Step: 127, Training Logs: loss_final: 1.422576, loss_mean: 1.414071, proj_loss: -0.136217, loss_mean_cls: 0.111604, deep_loss: 0.033118, grad_norm: 7.541412
[[34m2025-10-03 23:58:33[0m] Step: 128, Training Logs: loss_final: 1.402565, loss_mean: 1.395534, proj_loss: -0.136146, loss_mean_cls: 0.111252, deep_loss: 0.031925, grad_norm: 7.042222
[[34m2025-10-03 23:58:34[0m] Step: 129, Training Logs: loss_final: 1.456673, loss_mean: 1.447665, proj_loss: -0.134932, loss_mean_cls: 0.111787, deep_loss: 0.032152, grad_norm: 11.411099
[[34m2025-10-03 23:58:35[0m] Step: 130, Training Logs: loss_final: 1.434277, loss_mean: 1.426852, proj_loss: -0.135897, loss_mean_cls: 0.110626, deep_loss: 0.032696, grad_norm: 7.684963
[[34m2025-10-03 23:58:36[0m] Step: 131, Training Logs: loss_final: 1.435496, loss_mean: 1.423670, proj_loss: -0.133455, loss_mean_cls: 0.112058, deep_loss: 0.033223, grad_norm: 8.874546
[[34m2025-10-03 23:58:38[0m] Step: 132, Training Logs: loss_final: 1.395110, loss_mean: 1.386888, proj_loss: -0.135326, loss_mean_cls: 0.112897, deep_loss: 0.030651, grad_norm: 6.730226
[[34m2025-10-03 23:58:39[0m] Step: 133, Training Logs: loss_final: 1.426845, loss_mean: 1.415861, proj_loss: -0.133244, loss_mean_cls: 0.112302, deep_loss: 0.031927, grad_norm: 7.867723
[[34m2025-10-03 23:58:40[0m] Step: 134, Training Logs: loss_final: 1.429440, loss_mean: 1.419479, proj_loss: -0.133868, loss_mean_cls: 0.111551, deep_loss: 0.032279, grad_norm: 6.234180
[[34m2025-10-03 23:58:41[0m] Step: 135, Training Logs: loss_final: 1.421074, loss_mean: 1.410875, proj_loss: -0.133535, loss_mean_cls: 0.111781, deep_loss: 0.031953, grad_norm: 6.200508
[[34m2025-10-03 23:58:42[0m] Step: 136, Training Logs: loss_final: 1.405888, loss_mean: 1.394533, proj_loss: -0.132120, loss_mean_cls: 0.112312, deep_loss: 0.031162, grad_norm: 5.573999
[[34m2025-10-03 23:58:44[0m] Step: 137, Training Logs: loss_final: 1.396626, loss_mean: 1.383763, proj_loss: -0.131089, loss_mean_cls: 0.112886, deep_loss: 0.031066, grad_norm: 5.477999
[[34m2025-10-03 23:58:45[0m] Step: 138, Training Logs: loss_final: 1.377883, loss_mean: 1.366685, proj_loss: -0.132911, loss_mean_cls: 0.112497, deep_loss: 0.031612, grad_norm: 5.729690
[[34m2025-10-03 23:58:46[0m] Step: 139, Training Logs: loss_final: 1.377941, loss_mean: 1.366252, proj_loss: -0.132125, loss_mean_cls: 0.112426, deep_loss: 0.031389, grad_norm: 5.932611
[[34m2025-10-03 23:58:47[0m] Step: 140, Training Logs: loss_final: 1.424442, loss_mean: 1.411403, proj_loss: -0.131615, loss_mean_cls: 0.111922, deep_loss: 0.032732, grad_norm: 7.401294
[[34m2025-10-03 23:58:49[0m] Step: 141, Training Logs: loss_final: 1.402072, loss_mean: 1.389329, proj_loss: -0.132399, loss_mean_cls: 0.112886, deep_loss: 0.032256, grad_norm: 7.334338
[[34m2025-10-03 23:58:50[0m] Step: 142, Training Logs: loss_final: 1.417617, loss_mean: 1.405923, proj_loss: -0.132332, loss_mean_cls: 0.112505, deep_loss: 0.031521, grad_norm: 8.701436
[[34m2025-10-03 23:58:51[0m] Step: 143, Training Logs: loss_final: 1.409198, loss_mean: 1.398161, proj_loss: -0.133121, loss_mean_cls: 0.112000, deep_loss: 0.032159, grad_norm: 10.367977
[[34m2025-10-03 23:58:52[0m] Step: 144, Training Logs: loss_final: 1.376191, loss_mean: 1.364263, proj_loss: -0.131608, loss_mean_cls: 0.112438, deep_loss: 0.031099, grad_norm: 7.159201
[[34m2025-10-03 23:58:52[0m] Step: 145, Training Logs: loss_final: 1.408830, loss_mean: 1.394783, proj_loss: -0.130293, loss_mean_cls: 0.113039, deep_loss: 0.031302, grad_norm: 7.099374
[[34m2025-10-03 23:58:53[0m] Step: 146, Training Logs: loss_final: 1.445680, loss_mean: 1.432339, proj_loss: -0.130867, loss_mean_cls: 0.111769, deep_loss: 0.032439, grad_norm: 8.867300
[[34m2025-10-03 23:58:54[0m] Step: 147, Training Logs: loss_final: 1.401962, loss_mean: 1.387802, proj_loss: -0.129234, loss_mean_cls: 0.112444, deep_loss: 0.030950, grad_norm: 5.215583
[[34m2025-10-03 23:58:55[0m] Step: 148, Training Logs: loss_final: 1.443563, loss_mean: 1.428822, proj_loss: -0.129804, loss_mean_cls: 0.111995, deep_loss: 0.032550, grad_norm: 9.861128
[[34m2025-10-03 23:58:57[0m] Step: 149, Training Logs: loss_final: 1.386958, loss_mean: 1.371302, proj_loss: -0.127526, loss_mean_cls: 0.112432, deep_loss: 0.030749, grad_norm: 6.237093
[[34m2025-10-03 23:58:58[0m] Step: 150, Training Logs: loss_final: 1.424849, loss_mean: 1.408431, proj_loss: -0.128105, loss_mean_cls: 0.112873, deep_loss: 0.031650, grad_norm: 10.447137
[[34m2025-10-03 23:58:59[0m] Step: 151, Training Logs: loss_final: 1.413008, loss_mean: 1.396850, proj_loss: -0.128073, loss_mean_cls: 0.112050, deep_loss: 0.032181, grad_norm: 7.189882
[[34m2025-10-03 23:59:00[0m] Step: 152, Training Logs: loss_final: 1.411616, loss_mean: 1.395045, proj_loss: -0.127036, loss_mean_cls: 0.112237, deep_loss: 0.031370, grad_norm: 8.947591
[[34m2025-10-03 23:59:01[0m] Step: 153, Training Logs: loss_final: 1.406703, loss_mean: 1.390897, proj_loss: -0.128146, loss_mean_cls: 0.112557, deep_loss: 0.031395, grad_norm: 7.347679
[[34m2025-10-03 23:59:03[0m] Step: 154, Training Logs: loss_final: 1.408905, loss_mean: 1.392195, proj_loss: -0.127025, loss_mean_cls: 0.112313, deep_loss: 0.031422, grad_norm: 6.463219
[[34m2025-10-03 23:59:04[0m] Step: 155, Training Logs: loss_final: 1.410531, loss_mean: 1.393546, proj_loss: -0.125720, loss_mean_cls: 0.111791, deep_loss: 0.030914, grad_norm: 7.227612
[[34m2025-10-03 23:59:05[0m] Step: 156, Training Logs: loss_final: 1.359963, loss_mean: 1.341286, proj_loss: -0.124822, loss_mean_cls: 0.113833, deep_loss: 0.029666, grad_norm: 4.464270
[[34m2025-10-03 23:59:06[0m] Step: 157, Training Logs: loss_final: 1.410842, loss_mean: 1.391657, proj_loss: -0.123715, loss_mean_cls: 0.111946, deep_loss: 0.030954, grad_norm: 7.857898
[[34m2025-10-03 23:59:07[0m] Step: 158, Training Logs: loss_final: 1.417333, loss_mean: 1.392404, proj_loss: -0.119807, loss_mean_cls: 0.113027, deep_loss: 0.031708, grad_norm: 6.561149
[[34m2025-10-03 23:59:08[0m] Step: 159, Training Logs: loss_final: 1.384148, loss_mean: 1.362717, proj_loss: -0.122211, loss_mean_cls: 0.112500, deep_loss: 0.031142, grad_norm: 6.406652
[[34m2025-10-03 23:59:10[0m] Step: 160, Training Logs: loss_final: 1.414308, loss_mean: 1.390651, proj_loss: -0.119754, loss_mean_cls: 0.111858, deep_loss: 0.031553, grad_norm: 8.212509
[[34m2025-10-03 23:59:11[0m] Step: 161, Training Logs: loss_final: 1.381868, loss_mean: 1.356239, proj_loss: -0.117437, loss_mean_cls: 0.112438, deep_loss: 0.030627, grad_norm: 5.419571
[[34m2025-10-03 23:59:12[0m] Step: 162, Training Logs: loss_final: 1.400723, loss_mean: 1.373521, proj_loss: -0.116925, loss_mean_cls: 0.112887, deep_loss: 0.031240, grad_norm: 6.247863
[[34m2025-10-03 23:59:13[0m] Step: 163, Training Logs: loss_final: 1.386840, loss_mean: 1.361463, proj_loss: -0.117456, loss_mean_cls: 0.112219, deep_loss: 0.030614, grad_norm: 5.486972
[[34m2025-10-03 23:59:14[0m] Step: 164, Training Logs: loss_final: 1.362624, loss_mean: 1.334607, proj_loss: -0.115106, loss_mean_cls: 0.113102, deep_loss: 0.030022, grad_norm: 4.789364
[[34m2025-10-03 23:59:15[0m] Step: 165, Training Logs: loss_final: 1.424311, loss_mean: 1.393166, proj_loss: -0.113733, loss_mean_cls: 0.112744, deep_loss: 0.032134, grad_norm: 6.513977
[[34m2025-10-03 23:59:17[0m] Step: 166, Training Logs: loss_final: 1.381694, loss_mean: 1.351481, proj_loss: -0.113118, loss_mean_cls: 0.113018, deep_loss: 0.030313, grad_norm: 5.770723
[[34m2025-10-03 23:59:18[0m] Step: 167, Training Logs: loss_final: 1.391214, loss_mean: 1.357184, proj_loss: -0.109545, loss_mean_cls: 0.112940, deep_loss: 0.030635, grad_norm: 5.730629
[[34m2025-10-03 23:59:19[0m] Step: 168, Training Logs: loss_final: 1.369797, loss_mean: 1.333804, proj_loss: -0.107543, loss_mean_cls: 0.113326, deep_loss: 0.030209, grad_norm: 4.461557
[[34m2025-10-03 23:59:20[0m] Step: 169, Training Logs: loss_final: 1.396361, loss_mean: 1.359473, proj_loss: -0.107222, loss_mean_cls: 0.112657, deep_loss: 0.031453, grad_norm: 6.847363
[[34m2025-10-03 23:59:21[0m] Step: 170, Training Logs: loss_final: 1.395084, loss_mean: 1.356702, proj_loss: -0.105704, loss_mean_cls: 0.112445, deep_loss: 0.031641, grad_norm: 5.905935
[[34m2025-10-03 23:59:22[0m] Step: 171, Training Logs: loss_final: 1.407154, loss_mean: 1.367088, proj_loss: -0.104075, loss_mean_cls: 0.112391, deep_loss: 0.031750, grad_norm: 6.361753
[[34m2025-10-03 23:59:24[0m] Step: 172, Training Logs: loss_final: 1.378506, loss_mean: 1.335398, proj_loss: -0.100453, loss_mean_cls: 0.113303, deep_loss: 0.030258, grad_norm: 6.246922
[[34m2025-10-03 23:59:25[0m] Step: 173, Training Logs: loss_final: 1.397784, loss_mean: 1.355842, proj_loss: -0.102051, loss_mean_cls: 0.112696, deep_loss: 0.031297, grad_norm: 4.529147
[[34m2025-10-03 23:59:26[0m] Step: 174, Training Logs: loss_final: 1.394458, loss_mean: 1.348655, proj_loss: -0.097520, loss_mean_cls: 0.112882, deep_loss: 0.030441, grad_norm: 6.856174
[[34m2025-10-03 23:59:27[0m] Step: 175, Training Logs: loss_final: 1.396561, loss_mean: 1.350042, proj_loss: -0.096812, loss_mean_cls: 0.112171, deep_loss: 0.031161, grad_norm: 6.055427
[[34m2025-10-03 23:59:27[0m] Step: 176, Training Logs: loss_final: 1.393936, loss_mean: 1.344277, proj_loss: -0.092767, loss_mean_cls: 0.111954, deep_loss: 0.030472, grad_norm: 4.578010
[[34m2025-10-03 23:59:28[0m] Step: 177, Training Logs: loss_final: 1.407187, loss_mean: 1.358467, proj_loss: -0.094500, loss_mean_cls: 0.111844, deep_loss: 0.031377, grad_norm: 4.872346
[[34m2025-10-03 23:59:30[0m] Step: 178, Training Logs: loss_final: 1.413449, loss_mean: 1.359782, proj_loss: -0.090747, loss_mean_cls: 0.112908, deep_loss: 0.031506, grad_norm: 6.977461
[[34m2025-10-03 23:59:31[0m] Step: 179, Training Logs: loss_final: 1.393180, loss_mean: 1.336624, proj_loss: -0.086836, loss_mean_cls: 0.112596, deep_loss: 0.030796, grad_norm: 6.054224
[[34m2025-10-03 23:59:32[0m] Step: 180, Training Logs: loss_final: 1.409787, loss_mean: 1.349648, proj_loss: -0.084255, loss_mean_cls: 0.112639, deep_loss: 0.031755, grad_norm: 5.548503
[[34m2025-10-03 23:59:33[0m] Step: 181, Training Logs: loss_final: 1.410015, loss_mean: 1.345552, proj_loss: -0.080815, loss_mean_cls: 0.113632, deep_loss: 0.031646, grad_norm: 8.639910
[[34m2025-10-03 23:59:34[0m] Step: 182, Training Logs: loss_final: 1.375903, loss_mean: 1.312915, proj_loss: -0.080217, loss_mean_cls: 0.112743, deep_loss: 0.030462, grad_norm: 6.827496
[[34m2025-10-03 23:59:35[0m] Step: 183, Training Logs: loss_final: 1.433541, loss_mean: 1.368788, proj_loss: -0.078893, loss_mean_cls: 0.112426, deep_loss: 0.031219, grad_norm: 9.117872
[[34m2025-10-03 23:59:37[0m] Step: 184, Training Logs: loss_final: 1.420725, loss_mean: 1.353951, proj_loss: -0.077892, loss_mean_cls: 0.113283, deep_loss: 0.031382, grad_norm: 9.188498
[[34m2025-10-03 23:59:38[0m] Step: 185, Training Logs: loss_final: 1.393178, loss_mean: 1.326488, proj_loss: -0.076787, loss_mean_cls: 0.112381, deep_loss: 0.031096, grad_norm: 7.302738
[[34m2025-10-03 23:59:39[0m] Step: 186, Training Logs: loss_final: 1.448046, loss_mean: 1.378248, proj_loss: -0.074542, loss_mean_cls: 0.111723, deep_loss: 0.032617, grad_norm: 6.681391
[[34m2025-10-03 23:59:40[0m] Step: 187, Training Logs: loss_final: 1.417116, loss_mean: 1.345590, proj_loss: -0.073379, loss_mean_cls: 0.112819, deep_loss: 0.032086, grad_norm: 9.665709
[[34m2025-10-03 23:59:41[0m] Step: 188, Training Logs: loss_final: 1.428227, loss_mean: 1.350910, proj_loss: -0.068667, loss_mean_cls: 0.112884, deep_loss: 0.033100, grad_norm: 11.876671
[[34m2025-10-03 23:59:42[0m] Step: 189, Training Logs: loss_final: 1.410329, loss_mean: 1.336902, proj_loss: -0.070178, loss_mean_cls: 0.112835, deep_loss: 0.030770, grad_norm: 7.677266
[[34m2025-10-03 23:59:44[0m] Step: 190, Training Logs: loss_final: 1.447120, loss_mean: 1.371817, proj_loss: -0.067764, loss_mean_cls: 0.111447, deep_loss: 0.031619, grad_norm: 9.667538
[[34m2025-10-03 23:59:45[0m] Step: 191, Training Logs: loss_final: 1.457509, loss_mean: 1.372502, proj_loss: -0.060177, loss_mean_cls: 0.112643, deep_loss: 0.032541, grad_norm: 13.947860
[[34m2025-10-03 23:59:46[0m] Step: 192, Training Logs: loss_final: 1.465032, loss_mean: 1.385329, proj_loss: -0.064874, loss_mean_cls: 0.113250, deep_loss: 0.031327, grad_norm: 12.857370
[[34m2025-10-03 23:59:47[0m] Step: 193, Training Logs: loss_final: 1.446572, loss_mean: 1.366657, proj_loss: -0.064651, loss_mean_cls: 0.112145, deep_loss: 0.032422, grad_norm: 10.097665
[[34m2025-10-03 23:59:48[0m] Step: 194, Training Logs: loss_final: 1.465610, loss_mean: 1.381295, proj_loss: -0.062022, loss_mean_cls: 0.113452, deep_loss: 0.032884, grad_norm: 8.984549
[[34m2025-10-03 23:59:49[0m] Step: 195, Training Logs: loss_final: 1.465387, loss_mean: 1.384788, proj_loss: -0.063458, loss_mean_cls: 0.112423, deep_loss: 0.031634, grad_norm: 11.188003
[[34m2025-10-03 23:59:51[0m] Step: 196, Training Logs: loss_final: 1.452893, loss_mean: 1.368255, proj_loss: -0.061490, loss_mean_cls: 0.113011, deep_loss: 0.033116, grad_norm: 7.697748
[[34m2025-10-03 23:59:52[0m] Step: 197, Training Logs: loss_final: 1.485050, loss_mean: 1.402481, proj_loss: -0.064406, loss_mean_cls: 0.111985, deep_loss: 0.034990, grad_norm: 11.089597
[[34m2025-10-03 23:59:53[0m] Step: 198, Training Logs: loss_final: 1.499332, loss_mean: 1.415736, proj_loss: -0.062798, loss_mean_cls: 0.112913, deep_loss: 0.033481, grad_norm: 12.553448
[[34m2025-10-03 23:59:54[0m] Step: 199, Training Logs: loss_final: 1.462065, loss_mean: 1.378173, proj_loss: -0.061002, loss_mean_cls: 0.111897, deep_loss: 0.032997, grad_norm: 9.609369
[[34m2025-10-03 23:59:55[0m] Step: 200, Training Logs: loss_final: 1.506419, loss_mean: 1.418261, proj_loss: -0.058287, loss_mean_cls: 0.112496, deep_loss: 0.033949, grad_norm: 11.246774
[[34m2025-10-03 23:59:56[0m] Step: 201, Training Logs: loss_final: 1.482757, loss_mean: 1.394521, proj_loss: -0.059321, loss_mean_cls: 0.112806, deep_loss: 0.034751, grad_norm: 6.722470
[[34m2025-10-03 23:59:58[0m] Step: 202, Training Logs: loss_final: 1.485340, loss_mean: 1.397267, proj_loss: -0.059460, loss_mean_cls: 0.113285, deep_loss: 0.034248, grad_norm: 9.721070
[[34m2025-10-03 23:59:59[0m] Step: 203, Training Logs: loss_final: 1.496475, loss_mean: 1.406298, proj_loss: -0.058448, loss_mean_cls: 0.113607, deep_loss: 0.035018, grad_norm: 10.310780
[[34m2025-10-04 00:00:00[0m] Step: 204, Training Logs: loss_final: 1.495700, loss_mean: 1.404198, proj_loss: -0.056719, loss_mean_cls: 0.113248, deep_loss: 0.034974, grad_norm: 12.249331
[[34m2025-10-04 00:00:01[0m] Step: 205, Training Logs: loss_final: 1.481592, loss_mean: 1.388447, proj_loss: -0.054812, loss_mean_cls: 0.113172, deep_loss: 0.034785, grad_norm: 6.713975
[[34m2025-10-04 00:00:02[0m] Step: 206, Training Logs: loss_final: 1.590514, loss_mean: 1.498266, proj_loss: -0.055825, loss_mean_cls: 0.112420, deep_loss: 0.035653, grad_norm: 22.741882
[[34m2025-10-04 00:00:03[0m] Step: 207, Training Logs: loss_final: 1.516349, loss_mean: 1.420927, proj_loss: -0.052497, loss_mean_cls: 0.113763, deep_loss: 0.034155, grad_norm: 14.589533
[[34m2025-10-04 00:00:04[0m] Step: 208, Training Logs: loss_final: 1.605371, loss_mean: 1.505115, proj_loss: -0.050028, loss_mean_cls: 0.113521, deep_loss: 0.036763, grad_norm: 25.181852
[[34m2025-10-04 00:00:05[0m] Step: 209, Training Logs: loss_final: 1.561224, loss_mean: 1.464035, proj_loss: -0.052321, loss_mean_cls: 0.113356, deep_loss: 0.036153, grad_norm: 22.096781
[[34m2025-10-04 00:00:06[0m] Step: 210, Training Logs: loss_final: 1.536958, loss_mean: 1.444499, proj_loss: -0.056563, loss_mean_cls: 0.113399, deep_loss: 0.035623, grad_norm: 13.399783
[[34m2025-10-04 00:00:07[0m] Step: 211, Training Logs: loss_final: 1.572848, loss_mean: 1.479635, proj_loss: -0.057573, loss_mean_cls: 0.113042, deep_loss: 0.037744, grad_norm: 16.920336
[[34m2025-10-04 00:00:08[0m] Step: 212, Training Logs: loss_final: 1.611732, loss_mean: 1.516915, proj_loss: -0.056145, loss_mean_cls: 0.112849, deep_loss: 0.038113, grad_norm: 21.541216
[[34m2025-10-04 00:00:09[0m] Step: 213, Training Logs: loss_final: 1.568796, loss_mean: 1.474875, proj_loss: -0.055329, loss_mean_cls: 0.112563, deep_loss: 0.036687, grad_norm: 16.601238
[[34m2025-10-04 00:00:11[0m] Step: 214, Training Logs: loss_final: 1.527432, loss_mean: 1.433729, proj_loss: -0.053393, loss_mean_cls: 0.112799, deep_loss: 0.034296, grad_norm: 10.013838
[[34m2025-10-04 00:00:12[0m] Step: 215, Training Logs: loss_final: 1.535105, loss_mean: 1.440672, proj_loss: -0.052512, loss_mean_cls: 0.112802, deep_loss: 0.034143, grad_norm: 12.634494
[[34m2025-10-04 00:00:13[0m] Step: 216, Training Logs: loss_final: 1.542804, loss_mean: 1.445286, proj_loss: -0.051112, loss_mean_cls: 0.113320, deep_loss: 0.035310, grad_norm: 16.406132
[[34m2025-10-04 00:00:14[0m] Step: 217, Training Logs: loss_final: 1.504296, loss_mean: 1.406374, proj_loss: -0.050816, loss_mean_cls: 0.113160, deep_loss: 0.035577, grad_norm: 13.944531
[[34m2025-10-04 00:00:15[0m] Step: 218, Training Logs: loss_final: 1.506860, loss_mean: 1.409108, proj_loss: -0.052729, loss_mean_cls: 0.113742, deep_loss: 0.036740, grad_norm: 9.723689
[[34m2025-10-04 00:00:16[0m] Step: 219, Training Logs: loss_final: 1.525250, loss_mean: 1.427677, proj_loss: -0.050801, loss_mean_cls: 0.113182, deep_loss: 0.035192, grad_norm: 12.925051
[[34m2025-10-04 00:00:18[0m] Step: 220, Training Logs: loss_final: 1.512050, loss_mean: 1.411733, proj_loss: -0.047451, loss_mean_cls: 0.113525, deep_loss: 0.034243, grad_norm: 14.678536
[[34m2025-10-04 00:00:19[0m] Step: 221, Training Logs: loss_final: 1.497947, loss_mean: 1.397530, proj_loss: -0.046798, loss_mean_cls: 0.112750, deep_loss: 0.034465, grad_norm: 12.756423
[[34m2025-10-04 00:00:20[0m] Step: 222, Training Logs: loss_final: 1.514544, loss_mean: 1.416298, proj_loss: -0.049143, loss_mean_cls: 0.112057, deep_loss: 0.035332, grad_norm: 11.839172
[[34m2025-10-04 00:00:21[0m] Step: 223, Training Logs: loss_final: 1.536991, loss_mean: 1.437549, proj_loss: -0.048127, loss_mean_cls: 0.112205, deep_loss: 0.035363, grad_norm: 14.733963
[[34m2025-10-04 00:00:22[0m] Step: 224, Training Logs: loss_final: 1.489426, loss_mean: 1.388379, proj_loss: -0.046305, loss_mean_cls: 0.113786, deep_loss: 0.033566, grad_norm: 12.905499
[[34m2025-10-04 00:00:24[0m] Step: 225, Training Logs: loss_final: 1.480745, loss_mean: 1.379720, proj_loss: -0.045850, loss_mean_cls: 0.113652, deep_loss: 0.033223, grad_norm: 9.556414
[[34m2025-10-04 00:00:25[0m] Step: 226, Training Logs: loss_final: 1.700772, loss_mean: 1.590117, proj_loss: -0.039366, loss_mean_cls: 0.113161, deep_loss: 0.036861, grad_norm: 37.674541
[[34m2025-10-04 00:00:26[0m] Step: 227, Training Logs: loss_final: 1.585356, loss_mean: 1.479231, proj_loss: -0.041500, loss_mean_cls: 0.113484, deep_loss: 0.034140, grad_norm: 29.914881
[[34m2025-10-04 00:00:27[0m] Step: 228, Training Logs: loss_final: 1.552070, loss_mean: 1.440713, proj_loss: -0.038213, loss_mean_cls: 0.113545, deep_loss: 0.036026, grad_norm: 23.086067
[[34m2025-10-04 00:00:28[0m] Step: 229, Training Logs: loss_final: 1.515741, loss_mean: 1.406838, proj_loss: -0.039850, loss_mean_cls: 0.114133, deep_loss: 0.034620, grad_norm: 20.487936
[[34m2025-10-04 00:00:29[0m] Step: 230, Training Logs: loss_final: 1.555305, loss_mean: 1.449013, proj_loss: -0.041558, loss_mean_cls: 0.112969, deep_loss: 0.034882, grad_norm: 23.826315
[[34m2025-10-04 00:00:31[0m] Step: 231, Training Logs: loss_final: 1.537135, loss_mean: 1.432882, proj_loss: -0.043106, loss_mean_cls: 0.112422, deep_loss: 0.034937, grad_norm: 17.656729
[[34m2025-10-04 00:00:32[0m] Step: 232, Training Logs: loss_final: 1.515692, loss_mean: 1.410612, proj_loss: -0.041647, loss_mean_cls: 0.113710, deep_loss: 0.033018, grad_norm: 11.418549
[[34m2025-10-04 00:00:33[0m] Step: 233, Training Logs: loss_final: 1.542351, loss_mean: 1.440377, proj_loss: -0.044151, loss_mean_cls: 0.112825, deep_loss: 0.033300, grad_norm: 16.794102
[[34m2025-10-04 00:00:34[0m] Step: 234, Training Logs: loss_final: 1.548454, loss_mean: 1.444128, proj_loss: -0.041931, loss_mean_cls: 0.112372, deep_loss: 0.033885, grad_norm: 18.610651
[[34m2025-10-04 00:00:35[0m] Step: 235, Training Logs: loss_final: 1.509960, loss_mean: 1.404552, proj_loss: -0.041548, loss_mean_cls: 0.113630, deep_loss: 0.033327, grad_norm: 13.737331
[[34m2025-10-04 00:00:36[0m] Step: 236, Training Logs: loss_final: 1.493414, loss_mean: 1.388713, proj_loss: -0.042842, loss_mean_cls: 0.113555, deep_loss: 0.033987, grad_norm: 9.608197
[[34m2025-10-04 00:00:37[0m] Step: 237, Training Logs: loss_final: 1.513816, loss_mean: 1.407120, proj_loss: -0.039396, loss_mean_cls: 0.113081, deep_loss: 0.033012, grad_norm: 13.420139
[[34m2025-10-04 00:00:38[0m] Step: 238, Training Logs: loss_final: 1.502005, loss_mean: 1.398197, proj_loss: -0.042878, loss_mean_cls: 0.112636, deep_loss: 0.034050, grad_norm: 11.620801
[[34m2025-10-04 00:00:38[0m] Step: 239, Training Logs: loss_final: 1.518381, loss_mean: 1.411531, proj_loss: -0.041192, loss_mean_cls: 0.113645, deep_loss: 0.034397, grad_norm: 10.207598
[[34m2025-10-04 00:00:39[0m] Step: 240, Training Logs: loss_final: 1.495004, loss_mean: 1.388736, proj_loss: -0.041229, loss_mean_cls: 0.113674, deep_loss: 0.033824, grad_norm: 11.958281
[[34m2025-10-04 00:00:41[0m] Step: 241, Training Logs: loss_final: 1.501503, loss_mean: 1.394577, proj_loss: -0.039384, loss_mean_cls: 0.113178, deep_loss: 0.033131, grad_norm: 11.301592
[[34m2025-10-04 00:00:42[0m] Step: 242, Training Logs: loss_final: 1.489620, loss_mean: 1.383183, proj_loss: -0.039675, loss_mean_cls: 0.113334, deep_loss: 0.032779, grad_norm: 8.430005
[[34m2025-10-04 00:00:43[0m] Step: 243, Training Logs: loss_final: 1.489606, loss_mean: 1.381562, proj_loss: -0.038521, loss_mean_cls: 0.113197, deep_loss: 0.033369, grad_norm: 11.273098
[[34m2025-10-04 00:00:44[0m] Step: 244, Training Logs: loss_final: 1.506708, loss_mean: 1.396147, proj_loss: -0.037329, loss_mean_cls: 0.113333, deep_loss: 0.034558, grad_norm: 10.974840
[[34m2025-10-04 00:00:45[0m] Step: 245, Training Logs: loss_final: 1.479867, loss_mean: 1.370654, proj_loss: -0.037355, loss_mean_cls: 0.113306, deep_loss: 0.033262, grad_norm: 8.312143
[[34m2025-10-04 00:00:47[0m] Step: 246, Training Logs: loss_final: 1.497836, loss_mean: 1.388960, proj_loss: -0.036989, loss_mean_cls: 0.113246, deep_loss: 0.032619, grad_norm: 9.789226
[[34m2025-10-04 00:00:48[0m] Step: 247, Training Logs: loss_final: 1.484354, loss_mean: 1.376573, proj_loss: -0.037848, loss_mean_cls: 0.113287, deep_loss: 0.032342, grad_norm: 8.084336
[[34m2025-10-04 00:00:49[0m] Step: 248, Training Logs: loss_final: 1.489785, loss_mean: 1.378052, proj_loss: -0.033973, loss_mean_cls: 0.113493, deep_loss: 0.032213, grad_norm: 15.765607
[[34m2025-10-04 00:00:50[0m] Step: 249, Training Logs: loss_final: 1.480053, loss_mean: 1.370050, proj_loss: -0.035465, loss_mean_cls: 0.113266, deep_loss: 0.032202, grad_norm: 13.202952
[[34m2025-10-04 00:00:51[0m] Step: 250, Training Logs: loss_final: 1.508657, loss_mean: 1.396592, proj_loss: -0.034008, loss_mean_cls: 0.113402, deep_loss: 0.032670, grad_norm: 12.582973
[[34m2025-10-04 00:00:52[0m] Step: 251, Training Logs: loss_final: 1.488651, loss_mean: 1.376224, proj_loss: -0.033314, loss_mean_cls: 0.113512, deep_loss: 0.032228, grad_norm: 16.626486
[[34m2025-10-04 00:00:54[0m] Step: 252, Training Logs: loss_final: 1.494564, loss_mean: 1.381594, proj_loss: -0.033378, loss_mean_cls: 0.114116, deep_loss: 0.032233, grad_norm: 9.675166
[[34m2025-10-04 00:00:55[0m] Step: 253, Training Logs: loss_final: 1.481219, loss_mean: 1.368936, proj_loss: -0.032498, loss_mean_cls: 0.112809, deep_loss: 0.031972, grad_norm: 10.234221
[[34m2025-10-04 00:00:56[0m] Step: 254, Training Logs: loss_final: 1.537043, loss_mean: 1.419349, proj_loss: -0.028747, loss_mean_cls: 0.112510, deep_loss: 0.033931, grad_norm: 19.957294
[[34m2025-10-04 00:00:57[0m] Step: 255, Training Logs: loss_final: 1.470961, loss_mean: 1.353189, proj_loss: -0.029015, loss_mean_cls: 0.113658, deep_loss: 0.033129, grad_norm: 9.148400
[[34m2025-10-04 00:00:58[0m] Step: 256, Training Logs: loss_final: 1.506682, loss_mean: 1.392900, proj_loss: -0.031231, loss_mean_cls: 0.113077, deep_loss: 0.031936, grad_norm: 14.493100
[[34m2025-10-04 00:00:59[0m] Step: 257, Training Logs: loss_final: 1.496227, loss_mean: 1.378411, proj_loss: -0.028626, loss_mean_cls: 0.113790, deep_loss: 0.032652, grad_norm: 15.839291
[[34m2025-10-04 00:01:01[0m] Step: 258, Training Logs: loss_final: 1.504386, loss_mean: 1.387373, proj_loss: -0.030003, loss_mean_cls: 0.113554, deep_loss: 0.033463, grad_norm: 9.679765
[[34m2025-10-04 00:01:02[0m] Step: 259, Training Logs: loss_final: 1.497849, loss_mean: 1.380134, proj_loss: -0.028079, loss_mean_cls: 0.113736, deep_loss: 0.032058, grad_norm: 13.755634
[[34m2025-10-04 00:01:03[0m] Step: 260, Training Logs: loss_final: 1.498679, loss_mean: 1.377752, proj_loss: -0.026731, loss_mean_cls: 0.114223, deep_loss: 0.033434, grad_norm: 16.615538
[[34m2025-10-04 00:01:04[0m] Step: 261, Training Logs: loss_final: 1.476081, loss_mean: 1.354855, proj_loss: -0.026120, loss_mean_cls: 0.114161, deep_loss: 0.033185, grad_norm: 10.414713
[[34m2025-10-04 00:01:05[0m] Step: 262, Training Logs: loss_final: 1.503132, loss_mean: 1.384804, proj_loss: -0.027009, loss_mean_cls: 0.113233, deep_loss: 0.032105, grad_norm: 13.938799
[[34m2025-10-04 00:01:06[0m] Step: 263, Training Logs: loss_final: 1.526273, loss_mean: 1.405298, proj_loss: -0.024811, loss_mean_cls: 0.112724, deep_loss: 0.033063, grad_norm: 15.882940
[[34m2025-10-04 00:01:08[0m] Step: 264, Training Logs: loss_final: 1.486235, loss_mean: 1.363357, proj_loss: -0.023318, loss_mean_cls: 0.113278, deep_loss: 0.032918, grad_norm: 12.346762
[[34m2025-10-04 00:01:09[0m] Step: 265, Training Logs: loss_final: 1.511631, loss_mean: 1.387627, proj_loss: -0.022388, loss_mean_cls: 0.113838, deep_loss: 0.032553, grad_norm: 12.980903
[[34m2025-10-04 00:01:10[0m] Step: 266, Training Logs: loss_final: 1.490881, loss_mean: 1.366722, proj_loss: -0.022865, loss_mean_cls: 0.114052, deep_loss: 0.032972, grad_norm: 16.657892
[[34m2025-10-04 00:01:11[0m] Step: 267, Training Logs: loss_final: 1.515429, loss_mean: 1.392913, proj_loss: -0.024616, loss_mean_cls: 0.113457, deep_loss: 0.033675, grad_norm: 22.823265
[[34m2025-10-04 00:01:12[0m] Step: 268, Training Logs: loss_final: 1.491355, loss_mean: 1.366347, proj_loss: -0.022268, loss_mean_cls: 0.114727, deep_loss: 0.032549, grad_norm: 15.961018
[[34m2025-10-04 00:01:13[0m] Step: 269, Training Logs: loss_final: 1.535552, loss_mean: 1.410004, proj_loss: -0.021562, loss_mean_cls: 0.114373, deep_loss: 0.032738, grad_norm: 18.019751
[[34m2025-10-04 00:01:14[0m] Step: 270, Training Logs: loss_final: 1.514897, loss_mean: 1.390702, proj_loss: -0.022315, loss_mean_cls: 0.114449, deep_loss: 0.032062, grad_norm: 22.169029
[[34m2025-10-04 00:01:15[0m] Step: 271, Training Logs: loss_final: 1.517434, loss_mean: 1.393446, proj_loss: -0.023266, loss_mean_cls: 0.114089, deep_loss: 0.033165, grad_norm: 14.388453
[[34m2025-10-04 00:01:16[0m] Step: 272, Training Logs: loss_final: 1.556383, loss_mean: 1.429536, proj_loss: -0.021139, loss_mean_cls: 0.113771, deep_loss: 0.034216, grad_norm: 31.574945
[[34m2025-10-04 00:01:17[0m] Step: 273, Training Logs: loss_final: 1.516803, loss_mean: 1.392817, proj_loss: -0.023070, loss_mean_cls: 0.114041, deep_loss: 0.033014, grad_norm: 20.045990
[[34m2025-10-04 00:01:18[0m] Step: 274, Training Logs: loss_final: 1.602940, loss_mean: 1.480426, proj_loss: -0.024512, loss_mean_cls: 0.113376, deep_loss: 0.033650, grad_norm: 37.495258
[[34m2025-10-04 00:01:19[0m] Step: 275, Training Logs: loss_final: 1.547673, loss_mean: 1.425473, proj_loss: -0.023216, loss_mean_cls: 0.113335, deep_loss: 0.032081, grad_norm: 26.649580
[[34m2025-10-04 00:01:21[0m] Step: 276, Training Logs: loss_final: 1.588347, loss_mean: 1.459191, proj_loss: -0.019087, loss_mean_cls: 0.113699, deep_loss: 0.034543, grad_norm: 35.085270
[[34m2025-10-04 00:01:22[0m] Step: 277, Training Logs: loss_final: 1.578205, loss_mean: 1.451703, proj_loss: -0.021396, loss_mean_cls: 0.113665, deep_loss: 0.034234, grad_norm: 33.061680
[[34m2025-10-04 00:01:23[0m] Step: 278, Training Logs: loss_final: 1.522954, loss_mean: 1.395616, proj_loss: -0.020897, loss_mean_cls: 0.114911, deep_loss: 0.033325, grad_norm: 22.777102
[[34m2025-10-04 00:01:24[0m] Step: 279, Training Logs: loss_final: 1.566353, loss_mean: 1.440760, proj_loss: -0.022025, loss_mean_cls: 0.113892, deep_loss: 0.033726, grad_norm: 28.398235
[[34m2025-10-04 00:01:25[0m] Step: 280, Training Logs: loss_final: 1.532213, loss_mean: 1.404957, proj_loss: -0.020558, loss_mean_cls: 0.113551, deep_loss: 0.034262, grad_norm: 24.295212
[[34m2025-10-04 00:01:26[0m] Step: 281, Training Logs: loss_final: 1.567079, loss_mean: 1.440541, proj_loss: -0.020688, loss_mean_cls: 0.113707, deep_loss: 0.033519, grad_norm: 25.372869
[[34m2025-10-04 00:01:28[0m] Step: 282, Training Logs: loss_final: 1.523965, loss_mean: 1.397926, proj_loss: -0.021482, loss_mean_cls: 0.113911, deep_loss: 0.033610, grad_norm: 25.899128
[[34m2025-10-04 00:01:29[0m] Step: 283, Training Logs: loss_final: 1.515940, loss_mean: 1.391395, proj_loss: -0.023334, loss_mean_cls: 0.114408, deep_loss: 0.033472, grad_norm: 22.017365
[[34m2025-10-04 00:01:30[0m] Step: 284, Training Logs: loss_final: 1.520935, loss_mean: 1.396101, proj_loss: -0.021215, loss_mean_cls: 0.114030, deep_loss: 0.032019, grad_norm: 23.860312
[[34m2025-10-04 00:01:31[0m] Step: 285, Training Logs: loss_final: 1.529778, loss_mean: 1.404525, proj_loss: -0.021997, loss_mean_cls: 0.114167, deep_loss: 0.033083, grad_norm: 24.760592
[[34m2025-10-04 00:01:32[0m] Step: 286, Training Logs: loss_final: 1.490120, loss_mean: 1.364351, proj_loss: -0.021684, loss_mean_cls: 0.113885, deep_loss: 0.033568, grad_norm: 17.754305
[[34m2025-10-04 00:01:34[0m] Step: 287, Training Logs: loss_final: 1.520792, loss_mean: 1.394716, proj_loss: -0.020822, loss_mean_cls: 0.113330, deep_loss: 0.033567, grad_norm: 25.407948
[[34m2025-10-04 00:01:35[0m] Step: 288, Training Logs: loss_final: 1.505246, loss_mean: 1.379451, proj_loss: -0.021309, loss_mean_cls: 0.113822, deep_loss: 0.033282, grad_norm: 20.060091
[[34m2025-10-04 00:01:36[0m] Step: 289, Training Logs: loss_final: 1.520754, loss_mean: 1.394506, proj_loss: -0.020699, loss_mean_cls: 0.113363, deep_loss: 0.033583, grad_norm: 23.813856
[[34m2025-10-04 00:01:37[0m] Step: 290, Training Logs: loss_final: 1.501038, loss_mean: 1.372461, proj_loss: -0.019828, loss_mean_cls: 0.114605, deep_loss: 0.033800, grad_norm: 24.600000
[[34m2025-10-04 00:01:38[0m] Step: 291, Training Logs: loss_final: 1.489681, loss_mean: 1.364052, proj_loss: -0.021009, loss_mean_cls: 0.113395, deep_loss: 0.033242, grad_norm: 20.380802
[[34m2025-10-04 00:01:39[0m] Step: 292, Training Logs: loss_final: 1.490947, loss_mean: 1.364897, proj_loss: -0.019264, loss_mean_cls: 0.113976, deep_loss: 0.031338, grad_norm: 21.464661
[[34m2025-10-04 00:01:41[0m] Step: 293, Training Logs: loss_final: 1.486654, loss_mean: 1.361781, proj_loss: -0.020857, loss_mean_cls: 0.114051, deep_loss: 0.031679, grad_norm: 19.369986
[[34m2025-10-04 00:01:42[0m] Step: 294, Training Logs: loss_final: 1.494255, loss_mean: 1.368402, proj_loss: -0.020634, loss_mean_cls: 0.114470, deep_loss: 0.032017, grad_norm: 18.789242
[[34m2025-10-04 00:01:43[0m] Step: 295, Training Logs: loss_final: 1.493494, loss_mean: 1.367714, proj_loss: -0.019533, loss_mean_cls: 0.113981, deep_loss: 0.031332, grad_norm: 20.591784
[[34m2025-10-04 00:01:44[0m] Step: 296, Training Logs: loss_final: 1.485747, loss_mean: 1.360312, proj_loss: -0.020816, loss_mean_cls: 0.114490, deep_loss: 0.031762, grad_norm: 20.299358
[[34m2025-10-04 00:01:45[0m] Step: 297, Training Logs: loss_final: 1.472398, loss_mean: 1.347249, proj_loss: -0.020640, loss_mean_cls: 0.113972, deep_loss: 0.031818, grad_norm: 16.748493
[[34m2025-10-04 00:01:46[0m] Step: 298, Training Logs: loss_final: 1.484197, loss_mean: 1.357608, proj_loss: -0.019457, loss_mean_cls: 0.113587, deep_loss: 0.032458, grad_norm: 16.646702
[[34m2025-10-04 00:01:47[0m] Step: 299, Training Logs: loss_final: 1.499766, loss_mean: 1.373363, proj_loss: -0.020121, loss_mean_cls: 0.112957, deep_loss: 0.033566, grad_norm: 17.076757
[[34m2025-10-04 00:01:48[0m] Step: 300, Training Logs: loss_final: 1.498100, loss_mean: 1.369914, proj_loss: -0.019033, loss_mean_cls: 0.113734, deep_loss: 0.033485, grad_norm: 14.593293
[[34m2025-10-04 00:01:49[0m] Step: 301, Training Logs: loss_final: 1.478163, loss_mean: 1.351223, proj_loss: -0.018479, loss_mean_cls: 0.113043, deep_loss: 0.032375, grad_norm: 15.840508
[[34m2025-10-04 00:01:50[0m] Step: 302, Training Logs: loss_final: 1.483417, loss_mean: 1.357499, proj_loss: -0.019347, loss_mean_cls: 0.113492, deep_loss: 0.031773, grad_norm: 15.855126
[[34m2025-10-04 00:01:51[0m] Step: 303, Training Logs: loss_final: 1.472959, loss_mean: 1.347602, proj_loss: -0.020071, loss_mean_cls: 0.113628, deep_loss: 0.031800, grad_norm: 17.599674
[[34m2025-10-04 00:01:52[0m] Step: 304, Training Logs: loss_final: 1.461567, loss_mean: 1.334384, proj_loss: -0.018507, loss_mean_cls: 0.114218, deep_loss: 0.031472, grad_norm: 16.692451
[[34m2025-10-04 00:01:54[0m] Step: 305, Training Logs: loss_final: 1.426913, loss_mean: 1.301425, proj_loss: -0.019515, loss_mean_cls: 0.114953, deep_loss: 0.030049, grad_norm: 12.558902
[[34m2025-10-04 00:01:55[0m] Step: 306, Training Logs: loss_final: 1.451946, loss_mean: 1.324548, proj_loss: -0.017926, loss_mean_cls: 0.114325, deep_loss: 0.030999, grad_norm: 19.797266
[[34m2025-10-04 00:01:56[0m] Step: 307, Training Logs: loss_final: 1.448058, loss_mean: 1.324421, proj_loss: -0.020854, loss_mean_cls: 0.112974, deep_loss: 0.031517, grad_norm: 12.957324
[[34m2025-10-04 00:01:57[0m] Step: 308, Training Logs: loss_final: 1.486079, loss_mean: 1.358872, proj_loss: -0.017996, loss_mean_cls: 0.114464, deep_loss: 0.030738, grad_norm: 16.578459
[[34m2025-10-04 00:01:58[0m] Step: 309, Training Logs: loss_final: 1.482473, loss_mean: 1.355267, proj_loss: -0.018633, loss_mean_cls: 0.113344, deep_loss: 0.032495, grad_norm: 16.492201
[[34m2025-10-04 00:01:59[0m] Step: 310, Training Logs: loss_final: 1.472436, loss_mean: 1.346327, proj_loss: -0.018676, loss_mean_cls: 0.113612, deep_loss: 0.031174, grad_norm: 15.824360
[[34m2025-10-04 00:02:01[0m] Step: 311, Training Logs: loss_final: 1.477899, loss_mean: 1.351443, proj_loss: -0.019364, loss_mean_cls: 0.114172, deep_loss: 0.031647, grad_norm: 15.346721
[[34m2025-10-04 00:02:02[0m] Step: 312, Training Logs: loss_final: 1.479717, loss_mean: 1.352821, proj_loss: -0.019300, loss_mean_cls: 0.114138, deep_loss: 0.032058, grad_norm: 16.135174
[[34m2025-10-04 00:02:03[0m] Step: 313, Training Logs: loss_final: 1.480949, loss_mean: 1.355920, proj_loss: -0.019741, loss_mean_cls: 0.113853, deep_loss: 0.030917, grad_norm: 21.291897
[[34m2025-10-04 00:02:04[0m] Step: 314, Training Logs: loss_final: 1.476107, loss_mean: 1.348221, proj_loss: -0.018249, loss_mean_cls: 0.113457, deep_loss: 0.032678, grad_norm: 15.607379
[[34m2025-10-04 00:02:05[0m] Step: 315, Training Logs: loss_final: 1.491853, loss_mean: 1.364885, proj_loss: -0.018400, loss_mean_cls: 0.113757, deep_loss: 0.031611, grad_norm: 24.198902
[[34m2025-10-04 00:02:06[0m] Step: 316, Training Logs: loss_final: 1.469839, loss_mean: 1.340480, proj_loss: -0.018374, loss_mean_cls: 0.114468, deep_loss: 0.033265, grad_norm: 18.348715
[[34m2025-10-04 00:02:06[0m] Step: 317, Training Logs: loss_final: 1.478709, loss_mean: 1.347166, proj_loss: -0.017133, loss_mean_cls: 0.114367, deep_loss: 0.034309, grad_norm: 18.451750
[[34m2025-10-04 00:02:07[0m] Step: 318, Training Logs: loss_final: 1.479475, loss_mean: 1.351990, proj_loss: -0.018319, loss_mean_cls: 0.114273, deep_loss: 0.031532, grad_norm: 18.206621
[[34m2025-10-04 00:02:08[0m] Step: 319, Training Logs: loss_final: 1.466677, loss_mean: 1.337478, proj_loss: -0.017375, loss_mean_cls: 0.114569, deep_loss: 0.032006, grad_norm: 12.756605
[[34m2025-10-04 00:02:09[0m] Step: 320, Training Logs: loss_final: 1.481272, loss_mean: 1.353057, proj_loss: -0.016683, loss_mean_cls: 0.113820, deep_loss: 0.031078, grad_norm: 12.131826
[[34m2025-10-04 00:02:10[0m] Step: 321, Training Logs: loss_final: 1.482063, loss_mean: 1.353427, proj_loss: -0.017577, loss_mean_cls: 0.113858, deep_loss: 0.032356, grad_norm: 16.901064
[[34m2025-10-04 00:02:10[0m] Step: 322, Training Logs: loss_final: 1.468284, loss_mean: 1.338601, proj_loss: -0.017097, loss_mean_cls: 0.114065, deep_loss: 0.032715, grad_norm: 19.061077
[[34m2025-10-04 00:02:11[0m] Step: 323, Training Logs: loss_final: 1.467701, loss_mean: 1.339785, proj_loss: -0.017221, loss_mean_cls: 0.113703, deep_loss: 0.031434, grad_norm: 15.044535
[[34m2025-10-04 00:02:12[0m] Step: 324, Training Logs: loss_final: 1.492593, loss_mean: 1.362203, proj_loss: -0.017339, loss_mean_cls: 0.113999, deep_loss: 0.033730, grad_norm: 23.138632
[[34m2025-10-04 00:02:13[0m] Step: 325, Training Logs: loss_final: 1.465597, loss_mean: 1.334458, proj_loss: -0.018023, loss_mean_cls: 0.115061, deep_loss: 0.034102, grad_norm: 15.704202
[[34m2025-10-04 00:02:14[0m] Step: 326, Training Logs: loss_final: 1.484318, loss_mean: 1.357820, proj_loss: -0.020201, loss_mean_cls: 0.113446, deep_loss: 0.033253, grad_norm: 17.061953
[[34m2025-10-04 00:02:14[0m] Step: 327, Training Logs: loss_final: 1.473607, loss_mean: 1.346298, proj_loss: -0.018306, loss_mean_cls: 0.113384, deep_loss: 0.032231, grad_norm: 20.854256
[[34m2025-10-04 00:02:15[0m] Step: 328, Training Logs: loss_final: 1.454642, loss_mean: 1.326366, proj_loss: -0.018416, loss_mean_cls: 0.114623, deep_loss: 0.032069, grad_norm: 20.543154
[[34m2025-10-04 00:02:15[0m] Step: 329, Training Logs: loss_final: 1.494852, loss_mean: 1.366862, proj_loss: -0.019083, loss_mean_cls: 0.114151, deep_loss: 0.032923, grad_norm: 25.496435
[[34m2025-10-04 00:02:16[0m] Step: 330, Training Logs: loss_final: 1.496405, loss_mean: 1.368711, proj_loss: -0.019789, loss_mean_cls: 0.114479, deep_loss: 0.033003, grad_norm: 31.644299
[[34m2025-10-04 00:02:16[0m] Step: 331, Training Logs: loss_final: 1.496275, loss_mean: 1.368919, proj_loss: -0.019698, loss_mean_cls: 0.114073, deep_loss: 0.032981, grad_norm: 16.196733
[[34m2025-10-04 00:02:16[0m] Step: 332, Training Logs: loss_final: 1.466106, loss_mean: 1.339400, proj_loss: -0.018678, loss_mean_cls: 0.114211, deep_loss: 0.031174, grad_norm: 12.200967
[[34m2025-10-04 00:02:17[0m] Step: 333, Training Logs: loss_final: 1.467105, loss_mean: 1.339161, proj_loss: -0.018675, loss_mean_cls: 0.114209, deep_loss: 0.032409, grad_norm: 19.654465
[[34m2025-10-04 00:02:17[0m] Step: 334, Training Logs: loss_final: 1.480204, loss_mean: 1.348546, proj_loss: -0.016502, loss_mean_cls: 0.114260, deep_loss: 0.033899, grad_norm: 21.208923
[[34m2025-10-04 00:02:18[0m] Step: 335, Training Logs: loss_final: 1.451811, loss_mean: 1.322311, proj_loss: -0.017734, loss_mean_cls: 0.114774, deep_loss: 0.032459, grad_norm: 18.707586
[[34m2025-10-04 00:02:18[0m] Step: 336, Training Logs: loss_final: 1.483221, loss_mean: 1.355764, proj_loss: -0.019795, loss_mean_cls: 0.115016, deep_loss: 0.032236, grad_norm: 15.328529
[[34m2025-10-04 00:02:18[0m] Step: 337, Training Logs: loss_final: 1.475537, loss_mean: 1.347917, proj_loss: -0.018404, loss_mean_cls: 0.114359, deep_loss: 0.031664, grad_norm: 21.445913
[[34m2025-10-04 00:02:19[0m] Step: 338, Training Logs: loss_final: 1.456101, loss_mean: 1.330111, proj_loss: -0.020237, loss_mean_cls: 0.114016, deep_loss: 0.032210, grad_norm: 21.471798
[[34m2025-10-04 00:02:20[0m] Step: 339, Training Logs: loss_final: 1.469818, loss_mean: 1.341701, proj_loss: -0.019563, loss_mean_cls: 0.114029, deep_loss: 0.033650, grad_norm: 16.315407
[[34m2025-10-04 00:02:21[0m] Step: 340, Training Logs: loss_final: 1.480402, loss_mean: 1.352878, proj_loss: -0.019022, loss_mean_cls: 0.114174, deep_loss: 0.032372, grad_norm: 20.967762
[[34m2025-10-04 00:02:22[0m] Step: 341, Training Logs: loss_final: 1.476682, loss_mean: 1.350532, proj_loss: -0.020276, loss_mean_cls: 0.114544, deep_loss: 0.031882, grad_norm: 17.618948
[[34m2025-10-04 00:02:22[0m] Step: 342, Training Logs: loss_final: 1.481841, loss_mean: 1.357788, proj_loss: -0.021804, loss_mean_cls: 0.113699, deep_loss: 0.032158, grad_norm: 14.769582
[[34m2025-10-04 00:02:23[0m] Step: 343, Training Logs: loss_final: 1.456461, loss_mean: 1.330327, proj_loss: -0.020389, loss_mean_cls: 0.114205, deep_loss: 0.032318, grad_norm: 12.197879
[[34m2025-10-04 00:02:24[0m] Step: 344, Training Logs: loss_final: 1.478870, loss_mean: 1.353946, proj_loss: -0.021523, loss_mean_cls: 0.114452, deep_loss: 0.031994, grad_norm: 15.340162
[[34m2025-10-04 00:02:25[0m] Step: 345, Training Logs: loss_final: 1.459162, loss_mean: 1.333305, proj_loss: -0.021023, loss_mean_cls: 0.114268, deep_loss: 0.032612, grad_norm: 11.471508
[[34m2025-10-04 00:02:26[0m] Step: 346, Training Logs: loss_final: 1.466556, loss_mean: 1.340403, proj_loss: -0.019987, loss_mean_cls: 0.114935, deep_loss: 0.031206, grad_norm: 13.826324
[[34m2025-10-04 00:02:27[0m] Step: 347, Training Logs: loss_final: 1.459570, loss_mean: 1.333027, proj_loss: -0.020092, loss_mean_cls: 0.114105, deep_loss: 0.032530, grad_norm: 15.430247
[[34m2025-10-04 00:02:27[0m] Step: 348, Training Logs: loss_final: 1.438280, loss_mean: 1.312395, proj_loss: -0.019633, loss_mean_cls: 0.114627, deep_loss: 0.030892, grad_norm: 16.558569
[[34m2025-10-04 00:02:28[0m] Step: 349, Training Logs: loss_final: 1.463277, loss_mean: 1.337708, proj_loss: -0.020986, loss_mean_cls: 0.114561, deep_loss: 0.031994, grad_norm: 13.752559
[[34m2025-10-04 00:02:29[0m] Step: 350, Training Logs: loss_final: 1.472646, loss_mean: 1.346859, proj_loss: -0.020758, loss_mean_cls: 0.114725, deep_loss: 0.031821, grad_norm: 14.261764
[[34m2025-10-04 00:02:30[0m] Step: 351, Training Logs: loss_final: 1.467713, loss_mean: 1.342558, proj_loss: -0.020262, loss_mean_cls: 0.114033, deep_loss: 0.031384, grad_norm: 20.463120
[[34m2025-10-04 00:02:31[0m] Step: 352, Training Logs: loss_final: 1.476389, loss_mean: 1.349679, proj_loss: -0.020066, loss_mean_cls: 0.114924, deep_loss: 0.031852, grad_norm: 16.003492
[[34m2025-10-04 00:02:31[0m] Step: 353, Training Logs: loss_final: 1.487018, loss_mean: 1.362194, proj_loss: -0.021585, loss_mean_cls: 0.114367, deep_loss: 0.032043, grad_norm: 24.261734
[[34m2025-10-04 00:02:32[0m] Step: 354, Training Logs: loss_final: 1.474058, loss_mean: 1.347259, proj_loss: -0.020756, loss_mean_cls: 0.114701, deep_loss: 0.032854, grad_norm: 22.181639
[[34m2025-10-04 00:02:33[0m] Step: 355, Training Logs: loss_final: 1.477540, loss_mean: 1.350744, proj_loss: -0.019848, loss_mean_cls: 0.114457, deep_loss: 0.032186, grad_norm: 19.408991
[[34m2025-10-04 00:02:34[0m] Step: 356, Training Logs: loss_final: 1.452446, loss_mean: 1.327597, proj_loss: -0.020775, loss_mean_cls: 0.114740, deep_loss: 0.030884, grad_norm: 22.007442
[[34m2025-10-04 00:02:35[0m] Step: 357, Training Logs: loss_final: 1.483307, loss_mean: 1.360289, proj_loss: -0.022371, loss_mean_cls: 0.113962, deep_loss: 0.031427, grad_norm: 21.307915
[[34m2025-10-04 00:02:35[0m] Step: 358, Training Logs: loss_final: 1.464167, loss_mean: 1.340276, proj_loss: -0.021893, loss_mean_cls: 0.114173, deep_loss: 0.031611, grad_norm: 20.573494
[[34m2025-10-04 00:02:36[0m] Step: 359, Training Logs: loss_final: 1.474596, loss_mean: 1.348570, proj_loss: -0.020337, loss_mean_cls: 0.114609, deep_loss: 0.031754, grad_norm: 19.660053
[[34m2025-10-04 00:02:37[0m] Step: 360, Training Logs: loss_final: 1.436620, loss_mean: 1.311786, proj_loss: -0.020770, loss_mean_cls: 0.115031, deep_loss: 0.030573, grad_norm: 18.955379
[[34m2025-10-04 00:02:38[0m] Step: 361, Training Logs: loss_final: 1.478996, loss_mean: 1.352193, proj_loss: -0.020816, loss_mean_cls: 0.115356, deep_loss: 0.032262, grad_norm: 17.793285
[[34m2025-10-04 00:02:39[0m] Step: 362, Training Logs: loss_final: 1.455333, loss_mean: 1.331166, proj_loss: -0.021568, loss_mean_cls: 0.114897, deep_loss: 0.030839, grad_norm: 14.839175
[[34m2025-10-04 00:02:40[0m] Step: 363, Training Logs: loss_final: 1.464990, loss_mean: 1.339762, proj_loss: -0.021004, loss_mean_cls: 0.114457, deep_loss: 0.031775, grad_norm: 18.542713
[[34m2025-10-04 00:02:40[0m] Step: 364, Training Logs: loss_final: 1.472923, loss_mean: 1.347859, proj_loss: -0.021429, loss_mean_cls: 0.114445, deep_loss: 0.032047, grad_norm: 16.893154
[[34m2025-10-04 00:02:41[0m] Step: 365, Training Logs: loss_final: 1.468290, loss_mean: 1.342656, proj_loss: -0.020770, loss_mean_cls: 0.114803, deep_loss: 0.031601, grad_norm: 18.317446
[[34m2025-10-04 11:40:15[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:41:17[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:41:18[0m] using MLP layer as FFN
[[34m2025-10-04 11:42:32[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:42:32[0m] using MLP layer as FFN
[[34m2025-10-04 11:43:25[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:43:26[0m] using MLP layer as FFN
[[34m2025-10-04 11:44:26[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:44:27[0m] using MLP layer as FFN
[[34m2025-10-04 11:44:31[0m] SiT Parameters: 140,504,592
[[34m2025-10-04 11:45:00[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-04 11:50:04[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:50:05[0m] using MLP layer as FFN
[[34m2025-10-04 11:50:10[0m] SiT Parameters: 140,504,592
[[34m2025-10-04 11:50:37[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-04 11:54:42[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:54:43[0m] using MLP layer as FFN
[[34m2025-10-04 11:54:48[0m] SiT Parameters: 139,618,704
[[34m2025-10-04 11:55:15[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-04 11:55:38[0m] Generating EMA samples done.
[[34m2025-10-04 11:55:38[0m] Step: 1, Training Logs: loss_final: 1.795647, loss_mean: 1.689609, loss_mean_cls: 0.106038, grad_norm: 1.174266
[[34m2025-10-04 11:55:38[0m] Step: 2, Training Logs: loss_final: 1.785069, loss_mean: 1.678836, loss_mean_cls: 0.106234, grad_norm: 0.901399
[[34m2025-10-04 11:55:38[0m] Step: 3, Training Logs: loss_final: 1.821233, loss_mean: 1.717687, loss_mean_cls: 0.103546, grad_norm: 0.796063
[[34m2025-10-04 11:55:39[0m] Step: 4, Training Logs: loss_final: 1.778000, loss_mean: 1.672744, loss_mean_cls: 0.105256, grad_norm: 0.684705
[[34m2025-10-04 11:55:39[0m] Step: 5, Training Logs: loss_final: 1.786037, loss_mean: 1.684995, loss_mean_cls: 0.101042, grad_norm: 0.812753
[[34m2025-10-04 11:55:39[0m] Step: 6, Training Logs: loss_final: 1.766339, loss_mean: 1.666939, loss_mean_cls: 0.099399, grad_norm: 0.754432
[[34m2025-10-04 11:55:39[0m] Step: 7, Training Logs: loss_final: 1.754195, loss_mean: 1.651879, loss_mean_cls: 0.102316, grad_norm: 0.825753
[[34m2025-10-04 11:55:40[0m] Step: 8, Training Logs: loss_final: 1.751540, loss_mean: 1.652565, loss_mean_cls: 0.098975, grad_norm: 0.830366
[[34m2025-10-04 11:55:40[0m] Step: 9, Training Logs: loss_final: 1.738420, loss_mean: 1.636014, loss_mean_cls: 0.102406, grad_norm: 0.858504
[[34m2025-10-04 11:55:40[0m] Step: 10, Training Logs: loss_final: 1.736533, loss_mean: 1.639352, loss_mean_cls: 0.097181, grad_norm: 1.043699
[[34m2025-10-04 11:55:41[0m] Step: 11, Training Logs: loss_final: 1.709282, loss_mean: 1.608108, loss_mean_cls: 0.101174, grad_norm: 1.076937
[[34m2025-10-04 11:55:41[0m] Step: 12, Training Logs: loss_final: 1.707039, loss_mean: 1.611309, loss_mean_cls: 0.095730, grad_norm: 1.105962
[[34m2025-10-04 11:55:41[0m] Step: 13, Training Logs: loss_final: 1.671346, loss_mean: 1.574493, loss_mean_cls: 0.096853, grad_norm: 0.938797
[[34m2025-10-04 11:55:41[0m] Step: 14, Training Logs: loss_final: 1.659532, loss_mean: 1.562734, loss_mean_cls: 0.096798, grad_norm: 0.746807
[[34m2025-10-04 11:55:42[0m] Step: 15, Training Logs: loss_final: 1.653482, loss_mean: 1.557485, loss_mean_cls: 0.095997, grad_norm: 0.753487
[[34m2025-10-04 11:55:42[0m] Step: 16, Training Logs: loss_final: 1.668430, loss_mean: 1.573485, loss_mean_cls: 0.094945, grad_norm: 1.121276
[[34m2025-10-04 11:55:42[0m] Step: 17, Training Logs: loss_final: 1.622319, loss_mean: 1.525881, loss_mean_cls: 0.096438, grad_norm: 0.825314
[[34m2025-10-04 11:55:43[0m] Step: 18, Training Logs: loss_final: 1.619625, loss_mean: 1.526897, loss_mean_cls: 0.092728, grad_norm: 0.887003
[[34m2025-10-04 11:55:43[0m] Step: 19, Training Logs: loss_final: 1.612384, loss_mean: 1.517632, loss_mean_cls: 0.094752, grad_norm: 0.850046
[[34m2025-10-04 11:55:43[0m] Step: 20, Training Logs: loss_final: 1.615233, loss_mean: 1.523051, loss_mean_cls: 0.092182, grad_norm: 0.884068
[[34m2025-10-04 11:55:43[0m] Step: 21, Training Logs: loss_final: 1.591438, loss_mean: 1.496822, loss_mean_cls: 0.094616, grad_norm: 0.907214
[[34m2025-10-04 11:55:44[0m] Step: 22, Training Logs: loss_final: 1.577800, loss_mean: 1.487948, loss_mean_cls: 0.089852, grad_norm: 0.827451
[[34m2025-10-04 11:55:44[0m] Step: 23, Training Logs: loss_final: 1.564300, loss_mean: 1.471926, loss_mean_cls: 0.092373, grad_norm: 0.895760
[[34m2025-10-04 11:55:44[0m] Step: 24, Training Logs: loss_final: 1.544717, loss_mean: 1.452857, loss_mean_cls: 0.091861, grad_norm: 0.818182
[[34m2025-10-04 11:55:45[0m] Step: 25, Training Logs: loss_final: 1.546056, loss_mean: 1.454019, loss_mean_cls: 0.092037, grad_norm: 0.739996
[[34m2025-10-04 11:55:45[0m] Step: 26, Training Logs: loss_final: 1.531811, loss_mean: 1.439312, loss_mean_cls: 0.092500, grad_norm: 0.670497
[[34m2025-10-04 11:55:45[0m] Step: 27, Training Logs: loss_final: 1.507270, loss_mean: 1.414881, loss_mean_cls: 0.092389, grad_norm: 0.758096
[[34m2025-10-04 11:55:45[0m] Step: 28, Training Logs: loss_final: 1.519139, loss_mean: 1.429240, loss_mean_cls: 0.089898, grad_norm: 0.610969
[[34m2025-10-04 11:55:46[0m] Step: 29, Training Logs: loss_final: 1.499356, loss_mean: 1.407856, loss_mean_cls: 0.091500, grad_norm: 0.716992
[[34m2025-10-04 11:55:46[0m] Step: 30, Training Logs: loss_final: 1.483597, loss_mean: 1.393857, loss_mean_cls: 0.089740, grad_norm: 0.649494
[[34m2025-10-04 11:55:46[0m] Step: 31, Training Logs: loss_final: 1.477721, loss_mean: 1.388299, loss_mean_cls: 0.089422, grad_norm: 0.829575
[[34m2025-10-04 11:55:47[0m] Step: 32, Training Logs: loss_final: 1.488826, loss_mean: 1.401185, loss_mean_cls: 0.087641, grad_norm: 0.938077
[[34m2025-10-04 11:55:47[0m] Step: 33, Training Logs: loss_final: 1.479225, loss_mean: 1.392005, loss_mean_cls: 0.087220, grad_norm: 0.871170
[[34m2025-10-04 11:55:47[0m] Step: 34, Training Logs: loss_final: 1.478034, loss_mean: 1.389407, loss_mean_cls: 0.088627, grad_norm: 0.668033
[[34m2025-10-04 11:55:47[0m] Step: 35, Training Logs: loss_final: 1.466846, loss_mean: 1.379078, loss_mean_cls: 0.087767, grad_norm: 0.999028
[[34m2025-10-04 11:55:48[0m] Step: 36, Training Logs: loss_final: 1.428662, loss_mean: 1.341124, loss_mean_cls: 0.087539, grad_norm: 0.792635
[[34m2025-10-04 11:55:48[0m] Step: 37, Training Logs: loss_final: 1.445827, loss_mean: 1.359899, loss_mean_cls: 0.085929, grad_norm: 1.028910
[[34m2025-10-04 11:55:48[0m] Step: 38, Training Logs: loss_final: 1.443310, loss_mean: 1.355693, loss_mean_cls: 0.087617, grad_norm: 0.735829
[[34m2025-10-04 11:55:49[0m] Step: 39, Training Logs: loss_final: 1.421320, loss_mean: 1.335914, loss_mean_cls: 0.085405, grad_norm: 0.781313
[[34m2025-10-04 11:55:49[0m] Step: 40, Training Logs: loss_final: 1.428990, loss_mean: 1.345265, loss_mean_cls: 0.083725, grad_norm: 0.614093
[[34m2025-10-04 11:55:49[0m] Step: 41, Training Logs: loss_final: 1.413832, loss_mean: 1.326742, loss_mean_cls: 0.087090, grad_norm: 0.728805
[[34m2025-10-04 11:55:49[0m] Step: 42, Training Logs: loss_final: 1.416154, loss_mean: 1.330340, loss_mean_cls: 0.085814, grad_norm: 0.865837
[[34m2025-10-04 11:55:50[0m] Step: 43, Training Logs: loss_final: 1.426973, loss_mean: 1.342601, loss_mean_cls: 0.084372, grad_norm: 1.212790
[[34m2025-10-04 11:55:50[0m] Step: 44, Training Logs: loss_final: 1.417312, loss_mean: 1.335625, loss_mean_cls: 0.081687, grad_norm: 1.149147
[[34m2025-10-04 11:55:50[0m] Step: 45, Training Logs: loss_final: 1.374274, loss_mean: 1.291313, loss_mean_cls: 0.082960, grad_norm: 0.669741
[[34m2025-10-04 11:55:51[0m] Step: 46, Training Logs: loss_final: 1.377141, loss_mean: 1.294317, loss_mean_cls: 0.082824, grad_norm: 0.976353
[[34m2025-10-04 11:55:51[0m] Step: 47, Training Logs: loss_final: 1.380575, loss_mean: 1.298036, loss_mean_cls: 0.082539, grad_norm: 1.360397
[[34m2025-10-04 11:55:51[0m] Step: 48, Training Logs: loss_final: 1.373814, loss_mean: 1.291798, loss_mean_cls: 0.082016, grad_norm: 1.120300
[[34m2025-10-04 11:55:51[0m] Step: 49, Training Logs: loss_final: 1.355728, loss_mean: 1.274397, loss_mean_cls: 0.081331, grad_norm: 1.069529
[[34m2025-10-04 11:55:52[0m] Step: 50, Training Logs: loss_final: 1.365051, loss_mean: 1.282728, loss_mean_cls: 0.082323, grad_norm: 0.960896
[[34m2025-10-04 11:55:52[0m] Step: 51, Training Logs: loss_final: 1.352253, loss_mean: 1.271365, loss_mean_cls: 0.080888, grad_norm: 1.036798
[[34m2025-10-04 11:55:52[0m] Step: 52, Training Logs: loss_final: 1.309304, loss_mean: 1.229391, loss_mean_cls: 0.079913, grad_norm: 1.098624
[[34m2025-10-04 11:55:53[0m] Step: 53, Training Logs: loss_final: 1.342698, loss_mean: 1.261277, loss_mean_cls: 0.081422, grad_norm: 0.829682
[[34m2025-10-04 11:55:53[0m] Step: 54, Training Logs: loss_final: 1.307747, loss_mean: 1.231277, loss_mean_cls: 0.076470, grad_norm: 1.049511
[[34m2025-10-04 11:55:53[0m] Step: 55, Training Logs: loss_final: 1.315868, loss_mean: 1.237365, loss_mean_cls: 0.078503, grad_norm: 2.268262
[[34m2025-10-04 11:55:53[0m] Step: 56, Training Logs: loss_final: 1.286505, loss_mean: 1.207611, loss_mean_cls: 0.078894, grad_norm: 0.808640
[[34m2025-10-04 11:55:54[0m] Step: 57, Training Logs: loss_final: 1.275219, loss_mean: 1.195886, loss_mean_cls: 0.079333, grad_norm: 1.666968
[[34m2025-10-04 11:55:54[0m] Step: 58, Training Logs: loss_final: 1.328512, loss_mean: 1.249691, loss_mean_cls: 0.078821, grad_norm: 1.804855
[[34m2025-10-04 11:55:54[0m] Step: 59, Training Logs: loss_final: 1.271576, loss_mean: 1.194121, loss_mean_cls: 0.077455, grad_norm: 1.254132
[[34m2025-10-04 11:55:55[0m] Step: 60, Training Logs: loss_final: 1.284966, loss_mean: 1.206485, loss_mean_cls: 0.078480, grad_norm: 1.378370
[[34m2025-10-04 11:55:55[0m] Step: 61, Training Logs: loss_final: 1.257621, loss_mean: 1.180187, loss_mean_cls: 0.077434, grad_norm: 1.085317
[[34m2025-10-04 11:55:55[0m] Step: 62, Training Logs: loss_final: 1.256328, loss_mean: 1.181461, loss_mean_cls: 0.074867, grad_norm: 0.763753
[[34m2025-10-04 11:55:55[0m] Step: 63, Training Logs: loss_final: 1.256204, loss_mean: 1.179373, loss_mean_cls: 0.076832, grad_norm: 1.680212
[[34m2025-10-04 11:55:56[0m] Step: 64, Training Logs: loss_final: 1.218861, loss_mean: 1.143166, loss_mean_cls: 0.075695, grad_norm: 1.070058
[[34m2025-10-04 11:55:56[0m] Step: 65, Training Logs: loss_final: 1.266388, loss_mean: 1.189377, loss_mean_cls: 0.077011, grad_norm: 1.162077
[[34m2025-10-04 11:55:56[0m] Step: 66, Training Logs: loss_final: 1.240091, loss_mean: 1.164940, loss_mean_cls: 0.075151, grad_norm: 0.779917
[[34m2025-10-04 11:55:56[0m] Step: 67, Training Logs: loss_final: 1.213773, loss_mean: 1.138098, loss_mean_cls: 0.075676, grad_norm: 0.959115
[[34m2025-10-04 11:55:57[0m] Step: 68, Training Logs: loss_final: 1.251102, loss_mean: 1.177503, loss_mean_cls: 0.073598, grad_norm: 1.095073
[[34m2025-10-04 11:55:57[0m] Step: 69, Training Logs: loss_final: 1.224620, loss_mean: 1.150177, loss_mean_cls: 0.074442, grad_norm: 0.864246
[[34m2025-10-04 11:55:57[0m] Step: 70, Training Logs: loss_final: 1.220434, loss_mean: 1.148310, loss_mean_cls: 0.072124, grad_norm: 1.022914
[[34m2025-10-04 11:55:58[0m] Step: 71, Training Logs: loss_final: 1.244501, loss_mean: 1.170995, loss_mean_cls: 0.073507, grad_norm: 1.333435
[[34m2025-10-04 11:55:58[0m] Step: 72, Training Logs: loss_final: 1.211727, loss_mean: 1.138361, loss_mean_cls: 0.073366, grad_norm: 1.030856
[[34m2025-10-04 11:55:58[0m] Step: 73, Training Logs: loss_final: 1.180672, loss_mean: 1.106680, loss_mean_cls: 0.073992, grad_norm: 1.019569
[[34m2025-10-04 11:55:58[0m] Step: 74, Training Logs: loss_final: 1.212634, loss_mean: 1.140465, loss_mean_cls: 0.072169, grad_norm: 1.491407
[[34m2025-10-04 11:55:59[0m] Step: 75, Training Logs: loss_final: 1.224154, loss_mean: 1.153368, loss_mean_cls: 0.070785, grad_norm: 1.140900
[[34m2025-10-04 11:55:59[0m] Step: 76, Training Logs: loss_final: 1.209434, loss_mean: 1.136663, loss_mean_cls: 0.072772, grad_norm: 1.220662
[[34m2025-10-04 11:55:59[0m] Step: 77, Training Logs: loss_final: 1.213073, loss_mean: 1.141849, loss_mean_cls: 0.071224, grad_norm: 1.227434
[[34m2025-10-04 11:56:00[0m] Step: 78, Training Logs: loss_final: 1.207920, loss_mean: 1.136820, loss_mean_cls: 0.071100, grad_norm: 1.200321
[[34m2025-10-04 11:56:00[0m] Step: 79, Training Logs: loss_final: 1.212982, loss_mean: 1.142960, loss_mean_cls: 0.070022, grad_norm: 1.182392
[[34m2025-10-04 11:56:00[0m] Step: 80, Training Logs: loss_final: 1.189428, loss_mean: 1.120346, loss_mean_cls: 0.069081, grad_norm: 1.556226
[[34m2025-10-04 11:56:01[0m] Step: 81, Training Logs: loss_final: 1.210733, loss_mean: 1.140425, loss_mean_cls: 0.070307, grad_norm: 1.449676
[[34m2025-10-04 11:56:01[0m] Step: 82, Training Logs: loss_final: 1.201556, loss_mean: 1.131593, loss_mean_cls: 0.069963, grad_norm: 1.056618
[[34m2025-10-04 11:56:01[0m] Step: 83, Training Logs: loss_final: 1.181162, loss_mean: 1.111683, loss_mean_cls: 0.069479, grad_norm: 1.822040
[[34m2025-10-04 11:56:01[0m] Step: 84, Training Logs: loss_final: 1.176741, loss_mean: 1.105825, loss_mean_cls: 0.070916, grad_norm: 0.839267
[[34m2025-10-04 11:56:02[0m] Step: 85, Training Logs: loss_final: 1.179919, loss_mean: 1.111011, loss_mean_cls: 0.068909, grad_norm: 1.329496
[[34m2025-10-04 11:56:02[0m] Step: 86, Training Logs: loss_final: 1.132134, loss_mean: 1.062311, loss_mean_cls: 0.069823, grad_norm: 1.298198
[[34m2025-10-04 11:56:02[0m] Step: 87, Training Logs: loss_final: 1.175014, loss_mean: 1.106182, loss_mean_cls: 0.068832, grad_norm: 1.236820
[[34m2025-10-04 11:56:03[0m] Step: 88, Training Logs: loss_final: 1.159912, loss_mean: 1.091611, loss_mean_cls: 0.068301, grad_norm: 2.044524
[[34m2025-10-04 11:56:03[0m] Step: 89, Training Logs: loss_final: 1.186359, loss_mean: 1.118595, loss_mean_cls: 0.067764, grad_norm: 1.303310
[[34m2025-10-04 11:56:03[0m] Step: 90, Training Logs: loss_final: 1.141944, loss_mean: 1.074987, loss_mean_cls: 0.066957, grad_norm: 1.522194
[[34m2025-10-04 11:56:03[0m] Step: 91, Training Logs: loss_final: 1.144631, loss_mean: 1.076173, loss_mean_cls: 0.068458, grad_norm: 1.189894
[[34m2025-10-04 11:56:04[0m] Step: 92, Training Logs: loss_final: 1.150659, loss_mean: 1.082561, loss_mean_cls: 0.068098, grad_norm: 1.226912
[[34m2025-10-04 11:56:04[0m] Step: 93, Training Logs: loss_final: 1.186440, loss_mean: 1.117766, loss_mean_cls: 0.068675, grad_norm: 1.049817
[[34m2025-10-04 11:56:04[0m] Step: 94, Training Logs: loss_final: 1.161872, loss_mean: 1.093169, loss_mean_cls: 0.068703, grad_norm: 0.826174
[[34m2025-10-04 11:56:05[0m] Step: 95, Training Logs: loss_final: 1.162671, loss_mean: 1.094248, loss_mean_cls: 0.068423, grad_norm: 1.167366
[[34m2025-10-04 11:56:05[0m] Step: 96, Training Logs: loss_final: 1.173846, loss_mean: 1.107471, loss_mean_cls: 0.066374, grad_norm: 0.948181
[[34m2025-10-04 11:56:05[0m] Step: 97, Training Logs: loss_final: 1.164604, loss_mean: 1.096872, loss_mean_cls: 0.067733, grad_norm: 1.184624
[[34m2025-10-04 11:56:05[0m] Step: 98, Training Logs: loss_final: 1.131846, loss_mean: 1.065462, loss_mean_cls: 0.066384, grad_norm: 0.780388
[[34m2025-10-04 11:56:06[0m] Step: 99, Training Logs: loss_final: 1.140858, loss_mean: 1.072921, loss_mean_cls: 0.067937, grad_norm: 1.180982
[[34m2025-10-04 11:56:06[0m] Step: 100, Training Logs: loss_final: 1.142393, loss_mean: 1.075491, loss_mean_cls: 0.066902, grad_norm: 0.708727
[[34m2025-10-04 11:56:06[0m] Step: 101, Training Logs: loss_final: 1.153363, loss_mean: 1.088083, loss_mean_cls: 0.065279, grad_norm: 1.479051
[[34m2025-10-04 11:56:07[0m] Step: 102, Training Logs: loss_final: 1.154621, loss_mean: 1.087304, loss_mean_cls: 0.067317, grad_norm: 1.199495
[[34m2025-10-04 11:56:07[0m] Step: 103, Training Logs: loss_final: 1.138569, loss_mean: 1.071632, loss_mean_cls: 0.066937, grad_norm: 0.915438
[[34m2025-10-04 11:56:07[0m] Step: 104, Training Logs: loss_final: 1.118974, loss_mean: 1.054033, loss_mean_cls: 0.064941, grad_norm: 1.313585
[[34m2025-10-04 11:56:07[0m] Step: 105, Training Logs: loss_final: 1.148571, loss_mean: 1.082632, loss_mean_cls: 0.065938, grad_norm: 0.894477
[[34m2025-10-04 11:56:08[0m] Step: 106, Training Logs: loss_final: 1.154166, loss_mean: 1.088406, loss_mean_cls: 0.065761, grad_norm: 1.079737
[[34m2025-10-04 11:56:08[0m] Step: 107, Training Logs: loss_final: 1.137953, loss_mean: 1.072160, loss_mean_cls: 0.065793, grad_norm: 1.484596
[[34m2025-10-04 11:56:08[0m] Step: 108, Training Logs: loss_final: 1.159239, loss_mean: 1.094395, loss_mean_cls: 0.064844, grad_norm: 1.045299
[[34m2025-10-04 11:56:09[0m] Step: 109, Training Logs: loss_final: 1.135452, loss_mean: 1.068693, loss_mean_cls: 0.066759, grad_norm: 1.101148
[[34m2025-10-04 11:56:09[0m] Step: 110, Training Logs: loss_final: 1.144132, loss_mean: 1.078086, loss_mean_cls: 0.066046, grad_norm: 1.034104
[[34m2025-10-04 11:56:09[0m] Step: 111, Training Logs: loss_final: 1.109139, loss_mean: 1.042265, loss_mean_cls: 0.066874, grad_norm: 0.824648
[[34m2025-10-04 11:56:09[0m] Step: 112, Training Logs: loss_final: 1.132567, loss_mean: 1.066315, loss_mean_cls: 0.066253, grad_norm: 1.263528
[[34m2025-10-04 11:56:10[0m] Step: 113, Training Logs: loss_final: 1.124068, loss_mean: 1.057839, loss_mean_cls: 0.066229, grad_norm: 1.411090
[[34m2025-10-04 11:56:10[0m] Step: 114, Training Logs: loss_final: 1.134740, loss_mean: 1.070144, loss_mean_cls: 0.064595, grad_norm: 0.937598
[[34m2025-10-04 11:56:10[0m] Step: 115, Training Logs: loss_final: 1.126991, loss_mean: 1.061400, loss_mean_cls: 0.065590, grad_norm: 1.196780
[[34m2025-10-04 11:56:11[0m] Step: 116, Training Logs: loss_final: 1.137667, loss_mean: 1.072809, loss_mean_cls: 0.064859, grad_norm: 1.393651
[[34m2025-10-04 11:56:11[0m] Step: 117, Training Logs: loss_final: 1.147022, loss_mean: 1.080750, loss_mean_cls: 0.066272, grad_norm: 1.020705
[[34m2025-10-04 11:56:11[0m] Step: 118, Training Logs: loss_final: 1.109673, loss_mean: 1.043185, loss_mean_cls: 0.066489, grad_norm: 1.225523
[[34m2025-10-04 11:56:12[0m] Step: 119, Training Logs: loss_final: 1.119401, loss_mean: 1.054886, loss_mean_cls: 0.064515, grad_norm: 0.908479
[[34m2025-10-04 11:56:12[0m] Step: 120, Training Logs: loss_final: 1.130745, loss_mean: 1.065965, loss_mean_cls: 0.064780, grad_norm: 1.369089
[[34m2025-10-04 11:56:12[0m] Step: 121, Training Logs: loss_final: 1.141380, loss_mean: 1.075447, loss_mean_cls: 0.065933, grad_norm: 0.926562
[[34m2025-10-04 11:56:12[0m] Step: 122, Training Logs: loss_final: 1.110147, loss_mean: 1.044951, loss_mean_cls: 0.065195, grad_norm: 1.066342
[[34m2025-10-04 11:56:13[0m] Step: 123, Training Logs: loss_final: 1.149160, loss_mean: 1.085045, loss_mean_cls: 0.064115, grad_norm: 1.005233
[[34m2025-10-04 11:56:13[0m] Step: 124, Training Logs: loss_final: 1.143576, loss_mean: 1.079296, loss_mean_cls: 0.064279, grad_norm: 1.121734
[[34m2025-10-04 11:56:13[0m] Step: 125, Training Logs: loss_final: 1.113838, loss_mean: 1.049010, loss_mean_cls: 0.064829, grad_norm: 1.061054
[[34m2025-10-04 11:56:14[0m] Step: 126, Training Logs: loss_final: 1.106887, loss_mean: 1.043169, loss_mean_cls: 0.063718, grad_norm: 0.880969
[[34m2025-10-04 11:56:14[0m] Step: 127, Training Logs: loss_final: 1.106058, loss_mean: 1.042661, loss_mean_cls: 0.063396, grad_norm: 0.957814
[[34m2025-10-04 11:56:14[0m] Step: 128, Training Logs: loss_final: 1.122327, loss_mean: 1.057975, loss_mean_cls: 0.064352, grad_norm: 0.737015
[[34m2025-10-04 11:56:14[0m] Step: 129, Training Logs: loss_final: 1.148317, loss_mean: 1.084772, loss_mean_cls: 0.063545, grad_norm: 0.927350
[[34m2025-10-04 11:56:15[0m] Step: 130, Training Logs: loss_final: 1.134899, loss_mean: 1.071214, loss_mean_cls: 0.063686, grad_norm: 0.973368
[[34m2025-10-04 11:56:15[0m] Step: 131, Training Logs: loss_final: 1.136020, loss_mean: 1.072167, loss_mean_cls: 0.063853, grad_norm: 1.149637
[[34m2025-10-04 11:56:15[0m] Step: 132, Training Logs: loss_final: 1.082798, loss_mean: 1.018941, loss_mean_cls: 0.063857, grad_norm: 0.812827
[[34m2025-10-04 11:56:16[0m] Step: 133, Training Logs: loss_final: 1.107146, loss_mean: 1.041739, loss_mean_cls: 0.065407, grad_norm: 0.959704
[[34m2025-10-04 11:56:16[0m] Step: 134, Training Logs: loss_final: 1.112836, loss_mean: 1.049722, loss_mean_cls: 0.063114, grad_norm: 1.553830
[[34m2025-10-04 11:56:16[0m] Step: 135, Training Logs: loss_final: 1.108448, loss_mean: 1.044287, loss_mean_cls: 0.064161, grad_norm: 0.767877
[[34m2025-10-04 11:56:16[0m] Step: 136, Training Logs: loss_final: 1.113887, loss_mean: 1.049805, loss_mean_cls: 0.064082, grad_norm: 1.239066
[[34m2025-10-04 11:56:17[0m] Step: 137, Training Logs: loss_final: 1.078130, loss_mean: 1.013259, loss_mean_cls: 0.064871, grad_norm: 0.826535
[[34m2025-10-04 11:56:17[0m] Step: 138, Training Logs: loss_final: 1.119535, loss_mean: 1.054763, loss_mean_cls: 0.064772, grad_norm: 0.990605
[[34m2025-10-04 11:56:17[0m] Step: 139, Training Logs: loss_final: 1.082140, loss_mean: 1.017380, loss_mean_cls: 0.064760, grad_norm: 0.831741
[[34m2025-10-04 11:56:18[0m] Step: 140, Training Logs: loss_final: 1.090272, loss_mean: 1.026423, loss_mean_cls: 0.063849, grad_norm: 0.930927
[[34m2025-10-04 11:56:18[0m] Step: 141, Training Logs: loss_final: 1.123071, loss_mean: 1.058900, loss_mean_cls: 0.064171, grad_norm: 1.092066
[[34m2025-10-04 11:56:18[0m] Step: 142, Training Logs: loss_final: 1.091323, loss_mean: 1.028510, loss_mean_cls: 0.062813, grad_norm: 0.869009
[[34m2025-10-04 11:56:18[0m] Step: 143, Training Logs: loss_final: 1.058624, loss_mean: 0.994504, loss_mean_cls: 0.064120, grad_norm: 0.930199
[[34m2025-10-04 11:56:19[0m] Step: 144, Training Logs: loss_final: 1.083332, loss_mean: 1.019257, loss_mean_cls: 0.064075, grad_norm: 0.787457
[[34m2025-10-04 11:56:19[0m] Step: 145, Training Logs: loss_final: 1.065985, loss_mean: 1.001327, loss_mean_cls: 0.064658, grad_norm: 0.912915
[[34m2025-10-04 11:56:19[0m] Step: 146, Training Logs: loss_final: 1.091722, loss_mean: 1.028316, loss_mean_cls: 0.063405, grad_norm: 0.818442
[[34m2025-10-04 11:56:20[0m] Step: 147, Training Logs: loss_final: 1.096863, loss_mean: 1.032724, loss_mean_cls: 0.064139, grad_norm: 1.291899
[[34m2025-10-04 11:56:20[0m] Step: 148, Training Logs: loss_final: 1.104063, loss_mean: 1.040382, loss_mean_cls: 0.063681, grad_norm: 0.965531
[[34m2025-10-04 11:56:20[0m] Step: 149, Training Logs: loss_final: 1.109504, loss_mean: 1.046762, loss_mean_cls: 0.062741, grad_norm: 1.136224
[[34m2025-10-04 11:56:21[0m] Step: 150, Training Logs: loss_final: 1.112775, loss_mean: 1.050696, loss_mean_cls: 0.062078, grad_norm: 1.234653
[[34m2025-10-04 11:56:21[0m] Step: 151, Training Logs: loss_final: 1.057041, loss_mean: 0.992438, loss_mean_cls: 0.064603, grad_norm: 0.712097
[[34m2025-10-04 11:56:21[0m] Step: 152, Training Logs: loss_final: 1.122152, loss_mean: 1.058710, loss_mean_cls: 0.063442, grad_norm: 1.261345
[[34m2025-10-04 11:56:21[0m] Step: 153, Training Logs: loss_final: 1.095499, loss_mean: 1.033966, loss_mean_cls: 0.061533, grad_norm: 0.981268
[[34m2025-10-04 11:56:22[0m] Step: 154, Training Logs: loss_final: 1.094931, loss_mean: 1.032169, loss_mean_cls: 0.062763, grad_norm: 0.946518
[[34m2025-10-04 11:56:22[0m] Step: 155, Training Logs: loss_final: 1.093915, loss_mean: 1.029880, loss_mean_cls: 0.064035, grad_norm: 0.864749
[[34m2025-10-04 11:56:22[0m] Step: 156, Training Logs: loss_final: 1.092364, loss_mean: 1.029691, loss_mean_cls: 0.062673, grad_norm: 1.202510
[[34m2025-10-04 11:56:23[0m] Step: 157, Training Logs: loss_final: 1.077755, loss_mean: 1.016313, loss_mean_cls: 0.061441, grad_norm: 1.026806
[[34m2025-10-04 11:56:23[0m] Step: 158, Training Logs: loss_final: 1.095664, loss_mean: 1.032472, loss_mean_cls: 0.063193, grad_norm: 0.990058
[[34m2025-10-04 11:56:23[0m] Step: 159, Training Logs: loss_final: 1.082216, loss_mean: 1.020891, loss_mean_cls: 0.061325, grad_norm: 0.973974
[[34m2025-10-04 11:56:23[0m] Step: 160, Training Logs: loss_final: 1.076460, loss_mean: 1.012586, loss_mean_cls: 0.063874, grad_norm: 0.941470
[[34m2025-10-04 11:56:24[0m] Step: 161, Training Logs: loss_final: 1.101123, loss_mean: 1.037103, loss_mean_cls: 0.064020, grad_norm: 0.750021
[[34m2025-10-04 11:56:24[0m] Step: 162, Training Logs: loss_final: 1.088160, loss_mean: 1.025598, loss_mean_cls: 0.062561, grad_norm: 1.324303
[[34m2025-10-04 11:56:24[0m] Step: 163, Training Logs: loss_final: 1.085729, loss_mean: 1.023157, loss_mean_cls: 0.062572, grad_norm: 0.887782
[[34m2025-10-04 11:56:25[0m] Step: 164, Training Logs: loss_final: 1.090119, loss_mean: 1.026854, loss_mean_cls: 0.063265, grad_norm: 0.926017
[[34m2025-10-04 11:56:25[0m] Step: 165, Training Logs: loss_final: 1.083936, loss_mean: 1.021814, loss_mean_cls: 0.062121, grad_norm: 0.890201
[[34m2025-10-04 11:56:25[0m] Step: 166, Training Logs: loss_final: 1.051654, loss_mean: 0.987716, loss_mean_cls: 0.063937, grad_norm: 0.784934
[[34m2025-10-04 11:56:25[0m] Step: 167, Training Logs: loss_final: 1.103188, loss_mean: 1.038917, loss_mean_cls: 0.064271, grad_norm: 1.007990
[[34m2025-10-04 11:56:26[0m] Step: 168, Training Logs: loss_final: 1.089051, loss_mean: 1.025753, loss_mean_cls: 0.063297, grad_norm: 1.009032
[[34m2025-10-04 11:56:26[0m] Step: 169, Training Logs: loss_final: 1.071231, loss_mean: 1.008423, loss_mean_cls: 0.062808, grad_norm: 1.028093
[[34m2025-10-04 11:56:26[0m] Step: 170, Training Logs: loss_final: 1.084263, loss_mean: 1.021411, loss_mean_cls: 0.062852, grad_norm: 0.963162
[[34m2025-10-04 11:56:27[0m] Step: 171, Training Logs: loss_final: 1.100405, loss_mean: 1.038397, loss_mean_cls: 0.062008, grad_norm: 1.341097
[[34m2025-10-04 11:56:27[0m] Step: 172, Training Logs: loss_final: 1.065575, loss_mean: 1.003677, loss_mean_cls: 0.061898, grad_norm: 0.862727
[[34m2025-10-04 11:56:27[0m] Step: 173, Training Logs: loss_final: 1.061090, loss_mean: 0.997548, loss_mean_cls: 0.063542, grad_norm: 1.809757
[[34m2025-10-04 11:56:28[0m] Step: 174, Training Logs: loss_final: 1.084081, loss_mean: 1.022197, loss_mean_cls: 0.061884, grad_norm: 1.089422
[[34m2025-10-04 11:56:28[0m] Step: 175, Training Logs: loss_final: 1.046785, loss_mean: 0.984504, loss_mean_cls: 0.062281, grad_norm: 0.953080
[[34m2025-10-04 11:56:28[0m] Step: 176, Training Logs: loss_final: 1.080198, loss_mean: 1.016293, loss_mean_cls: 0.063905, grad_norm: 1.124304
[[34m2025-10-04 11:56:28[0m] Step: 177, Training Logs: loss_final: 1.047183, loss_mean: 0.984632, loss_mean_cls: 0.062551, grad_norm: 0.747689
[[34m2025-10-04 11:56:29[0m] Step: 178, Training Logs: loss_final: 1.076812, loss_mean: 1.013928, loss_mean_cls: 0.062883, grad_norm: 1.305068
[[34m2025-10-04 11:56:29[0m] Step: 179, Training Logs: loss_final: 1.080147, loss_mean: 1.017831, loss_mean_cls: 0.062316, grad_norm: 0.921939
[[34m2025-10-04 11:56:29[0m] Step: 180, Training Logs: loss_final: 1.047495, loss_mean: 0.986111, loss_mean_cls: 0.061384, grad_norm: 1.064507
[[34m2025-10-04 11:56:30[0m] Step: 181, Training Logs: loss_final: 1.089074, loss_mean: 1.026894, loss_mean_cls: 0.062180, grad_norm: 1.505542
[[34m2025-10-04 11:56:30[0m] Step: 182, Training Logs: loss_final: 1.047513, loss_mean: 0.985052, loss_mean_cls: 0.062462, grad_norm: 0.723676
[[34m2025-10-04 11:56:30[0m] Step: 183, Training Logs: loss_final: 1.081072, loss_mean: 1.019323, loss_mean_cls: 0.061749, grad_norm: 1.214268
[[34m2025-10-04 11:56:30[0m] Step: 184, Training Logs: loss_final: 1.053654, loss_mean: 0.991626, loss_mean_cls: 0.062028, grad_norm: 0.847241
[[34m2025-10-04 11:56:31[0m] Step: 185, Training Logs: loss_final: 1.063487, loss_mean: 1.001717, loss_mean_cls: 0.061770, grad_norm: 0.999229
[[34m2025-10-04 11:56:31[0m] Step: 186, Training Logs: loss_final: 1.063850, loss_mean: 1.002696, loss_mean_cls: 0.061154, grad_norm: 1.096784
[[34m2025-10-04 11:56:31[0m] Step: 187, Training Logs: loss_final: 1.053910, loss_mean: 0.991632, loss_mean_cls: 0.062278, grad_norm: 0.799485
[[34m2025-10-04 11:56:32[0m] Step: 188, Training Logs: loss_final: 1.096554, loss_mean: 1.035187, loss_mean_cls: 0.061367, grad_norm: 1.064910
[[34m2025-10-04 11:56:32[0m] Step: 189, Training Logs: loss_final: 1.044127, loss_mean: 0.981126, loss_mean_cls: 0.063002, grad_norm: 1.220169
[[34m2025-10-04 11:56:32[0m] Step: 190, Training Logs: loss_final: 1.088271, loss_mean: 1.026305, loss_mean_cls: 0.061967, grad_norm: 0.938965
[[34m2025-10-04 11:56:33[0m] Step: 191, Training Logs: loss_final: 1.037370, loss_mean: 0.975902, loss_mean_cls: 0.061468, grad_norm: 0.687807
[[34m2025-10-04 11:56:33[0m] Step: 192, Training Logs: loss_final: 1.049437, loss_mean: 0.987374, loss_mean_cls: 0.062063, grad_norm: 1.146187
[[34m2025-10-04 11:56:33[0m] Step: 193, Training Logs: loss_final: 1.051530, loss_mean: 0.990455, loss_mean_cls: 0.061075, grad_norm: 1.063150
[[34m2025-10-04 11:56:33[0m] Step: 194, Training Logs: loss_final: 1.064426, loss_mean: 1.002752, loss_mean_cls: 0.061674, grad_norm: 0.679976
[[34m2025-10-04 11:56:34[0m] Step: 195, Training Logs: loss_final: 1.065199, loss_mean: 1.002915, loss_mean_cls: 0.062284, grad_norm: 1.040725
[[34m2025-10-04 11:56:34[0m] Step: 196, Training Logs: loss_final: 1.054080, loss_mean: 0.989846, loss_mean_cls: 0.064234, grad_norm: 1.407995
[[34m2025-10-04 11:56:34[0m] Step: 197, Training Logs: loss_final: 1.090921, loss_mean: 1.030619, loss_mean_cls: 0.060302, grad_norm: 0.752677
[[34m2025-10-04 11:56:35[0m] Step: 198, Training Logs: loss_final: 1.062793, loss_mean: 1.001559, loss_mean_cls: 0.061234, grad_norm: 1.413319
[[34m2025-10-04 11:56:35[0m] Step: 199, Training Logs: loss_final: 1.070892, loss_mean: 1.010875, loss_mean_cls: 0.060018, grad_norm: 0.922436
[[34m2025-10-04 11:56:35[0m] Step: 200, Training Logs: loss_final: 1.065669, loss_mean: 1.003739, loss_mean_cls: 0.061930, grad_norm: 0.821615
[[34m2025-10-04 11:56:35[0m] Step: 201, Training Logs: loss_final: 1.041703, loss_mean: 0.980502, loss_mean_cls: 0.061201, grad_norm: 0.912067
[[34m2025-10-04 11:56:36[0m] Step: 202, Training Logs: loss_final: 1.055266, loss_mean: 0.994752, loss_mean_cls: 0.060514, grad_norm: 1.177199
[[34m2025-10-04 11:56:36[0m] Step: 203, Training Logs: loss_final: 1.062017, loss_mean: 1.000923, loss_mean_cls: 0.061094, grad_norm: 1.122271
[[34m2025-10-04 11:56:36[0m] Step: 204, Training Logs: loss_final: 1.066347, loss_mean: 1.004688, loss_mean_cls: 0.061658, grad_norm: 0.839102
[[34m2025-10-04 11:56:37[0m] Step: 205, Training Logs: loss_final: 1.075681, loss_mean: 1.014168, loss_mean_cls: 0.061513, grad_norm: 0.925992
[[34m2025-10-04 11:56:37[0m] Step: 206, Training Logs: loss_final: 1.073047, loss_mean: 1.012701, loss_mean_cls: 0.060346, grad_norm: 1.353738
[[34m2025-10-04 11:56:37[0m] Step: 207, Training Logs: loss_final: 1.057695, loss_mean: 0.996870, loss_mean_cls: 0.060825, grad_norm: 0.975990
[[34m2025-10-04 11:56:38[0m] Step: 208, Training Logs: loss_final: 1.043735, loss_mean: 0.982253, loss_mean_cls: 0.061482, grad_norm: 0.730484
[[34m2025-10-04 11:56:38[0m] Step: 209, Training Logs: loss_final: 1.054044, loss_mean: 0.993048, loss_mean_cls: 0.060995, grad_norm: 0.969638
[[34m2025-10-04 11:56:38[0m] Step: 210, Training Logs: loss_final: 1.064327, loss_mean: 1.003960, loss_mean_cls: 0.060368, grad_norm: 0.805270
[[34m2025-10-04 11:56:38[0m] Step: 211, Training Logs: loss_final: 1.057443, loss_mean: 0.996940, loss_mean_cls: 0.060504, grad_norm: 0.852624
[[34m2025-10-04 11:56:39[0m] Step: 212, Training Logs: loss_final: 1.050336, loss_mean: 0.991059, loss_mean_cls: 0.059278, grad_norm: 1.228341
[[34m2025-10-04 11:56:39[0m] Step: 213, Training Logs: loss_final: 1.074598, loss_mean: 1.014764, loss_mean_cls: 0.059835, grad_norm: 0.891025
[[34m2025-10-04 11:56:39[0m] Step: 214, Training Logs: loss_final: 1.060872, loss_mean: 1.000547, loss_mean_cls: 0.060324, grad_norm: 0.937589
[[34m2025-10-04 11:56:40[0m] Step: 215, Training Logs: loss_final: 1.060896, loss_mean: 0.999992, loss_mean_cls: 0.060903, grad_norm: 1.233064
[[34m2025-10-04 11:56:40[0m] Step: 216, Training Logs: loss_final: 1.071736, loss_mean: 1.010778, loss_mean_cls: 0.060959, grad_norm: 0.941281
[[34m2025-10-04 11:56:40[0m] Step: 217, Training Logs: loss_final: 1.064877, loss_mean: 1.004710, loss_mean_cls: 0.060167, grad_norm: 1.662890
[[34m2025-10-04 11:56:41[0m] Step: 218, Training Logs: loss_final: 1.028721, loss_mean: 0.967238, loss_mean_cls: 0.061484, grad_norm: 0.984590
[[34m2025-10-04 11:56:41[0m] Step: 219, Training Logs: loss_final: 1.047692, loss_mean: 0.986557, loss_mean_cls: 0.061135, grad_norm: 1.321867
[[34m2025-10-04 11:56:41[0m] Step: 220, Training Logs: loss_final: 1.036246, loss_mean: 0.975751, loss_mean_cls: 0.060495, grad_norm: 0.946382
[[34m2025-10-04 11:56:41[0m] Step: 221, Training Logs: loss_final: 1.093608, loss_mean: 1.033581, loss_mean_cls: 0.060027, grad_norm: 1.528314
[[34m2025-10-04 11:56:42[0m] Step: 222, Training Logs: loss_final: 1.044980, loss_mean: 0.984346, loss_mean_cls: 0.060633, grad_norm: 1.377258
[[34m2025-10-04 11:56:42[0m] Step: 223, Training Logs: loss_final: 1.059383, loss_mean: 0.999171, loss_mean_cls: 0.060212, grad_norm: 1.285617
[[34m2025-10-04 11:56:42[0m] Step: 224, Training Logs: loss_final: 1.066991, loss_mean: 1.007832, loss_mean_cls: 0.059160, grad_norm: 1.125822
[[34m2025-10-04 11:56:43[0m] Step: 225, Training Logs: loss_final: 1.060088, loss_mean: 0.999623, loss_mean_cls: 0.060465, grad_norm: 0.878518
[[34m2025-10-04 11:57:29[0m] Experiment directory created at your_path/reg_xlarge_dinov2_base_align_4_cls/linear-dinov2-b-enc4
[[34m2025-10-04 11:57:30[0m] using MLP layer as FFN
[[34m2025-10-04 11:57:34[0m] SiT Parameters: 139,618,704
[[34m2025-10-04 11:58:00[0m] Dataset contains 1,280,238 images (/mnt/nvme-fast/datasets)
[[34m2025-10-04 11:58:22[0m] Generating EMA samples done.
[[34m2025-10-04 11:58:22[0m] Step: 1, Training Logs: loss_final: 1.795647, loss_mean: 1.689609, loss_mean_cls: 0.106038, grad_norm: 1.174266
[[34m2025-10-04 11:58:22[0m] Step: 2, Training Logs: loss_final: 1.785069, loss_mean: 1.678836, loss_mean_cls: 0.106234, grad_norm: 0.901399
[[34m2025-10-04 11:58:23[0m] Step: 3, Training Logs: loss_final: 1.821233, loss_mean: 1.717687, loss_mean_cls: 0.103546, grad_norm: 0.796063
[[34m2025-10-04 11:58:23[0m] Step: 4, Training Logs: loss_final: 1.778000, loss_mean: 1.672744, loss_mean_cls: 0.105256, grad_norm: 0.684705
[[34m2025-10-04 11:58:23[0m] Step: 5, Training Logs: loss_final: 1.786037, loss_mean: 1.684995, loss_mean_cls: 0.101042, grad_norm: 0.812753
[[34m2025-10-04 11:58:23[0m] Step: 6, Training Logs: loss_final: 1.766339, loss_mean: 1.666939, loss_mean_cls: 0.099399, grad_norm: 0.754432
[[34m2025-10-04 11:58:24[0m] Step: 7, Training Logs: loss_final: 1.754195, loss_mean: 1.651879, loss_mean_cls: 0.102316, grad_norm: 0.825753
[[34m2025-10-04 11:58:24[0m] Step: 8, Training Logs: loss_final: 1.751540, loss_mean: 1.652565, loss_mean_cls: 0.098975, grad_norm: 0.830367
[[34m2025-10-04 11:58:24[0m] Step: 9, Training Logs: loss_final: 1.738420, loss_mean: 1.636014, loss_mean_cls: 0.102406, grad_norm: 0.858504
[[34m2025-10-04 11:58:25[0m] Step: 10, Training Logs: loss_final: 1.736533, loss_mean: 1.639352, loss_mean_cls: 0.097181, grad_norm: 1.043700
[[34m2025-10-04 11:58:25[0m] Step: 11, Training Logs: loss_final: 1.709282, loss_mean: 1.608108, loss_mean_cls: 0.101174, grad_norm: 1.076937
[[34m2025-10-04 11:58:25[0m] Step: 12, Training Logs: loss_final: 1.707039, loss_mean: 1.611309, loss_mean_cls: 0.095730, grad_norm: 1.105959
[[34m2025-10-04 11:58:25[0m] Step: 13, Training Logs: loss_final: 1.671346, loss_mean: 1.574493, loss_mean_cls: 0.096853, grad_norm: 0.938799
[[34m2025-10-04 11:58:26[0m] Step: 14, Training Logs: loss_final: 1.659532, loss_mean: 1.562734, loss_mean_cls: 0.096798, grad_norm: 0.746807
[[34m2025-10-04 11:58:26[0m] Step: 15, Training Logs: loss_final: 1.653482, loss_mean: 1.557485, loss_mean_cls: 0.095997, grad_norm: 0.753487
[[34m2025-10-04 11:58:26[0m] Step: 16, Training Logs: loss_final: 1.668429, loss_mean: 1.573484, loss_mean_cls: 0.094945, grad_norm: 1.121265
[[34m2025-10-04 11:58:27[0m] Step: 17, Training Logs: loss_final: 1.622318, loss_mean: 1.525880, loss_mean_cls: 0.096438, grad_norm: 0.825314
[[34m2025-10-04 11:58:27[0m] Step: 18, Training Logs: loss_final: 1.619625, loss_mean: 1.526897, loss_mean_cls: 0.092728, grad_norm: 0.887005
[[34m2025-10-04 11:58:27[0m] Step: 19, Training Logs: loss_final: 1.612384, loss_mean: 1.517632, loss_mean_cls: 0.094752, grad_norm: 0.850046
[[34m2025-10-04 11:58:27[0m] Step: 20, Training Logs: loss_final: 1.615233, loss_mean: 1.523052, loss_mean_cls: 0.092182, grad_norm: 0.884069
[[34m2025-10-04 11:58:28[0m] Step: 21, Training Logs: loss_final: 1.591437, loss_mean: 1.496822, loss_mean_cls: 0.094616, grad_norm: 0.907211
[[34m2025-10-04 11:58:28[0m] Step: 22, Training Logs: loss_final: 1.577800, loss_mean: 1.487948, loss_mean_cls: 0.089852, grad_norm: 0.827453
[[34m2025-10-04 11:58:28[0m] Step: 23, Training Logs: loss_final: 1.564300, loss_mean: 1.471926, loss_mean_cls: 0.092373, grad_norm: 0.895761
[[34m2025-10-04 11:58:29[0m] Step: 24, Training Logs: loss_final: 1.544718, loss_mean: 1.452857, loss_mean_cls: 0.091861, grad_norm: 0.818182
[[34m2025-10-04 11:58:29[0m] Step: 25, Training Logs: loss_final: 1.546056, loss_mean: 1.454019, loss_mean_cls: 0.092037, grad_norm: 0.739992
[[34m2025-10-04 11:58:29[0m] Step: 26, Training Logs: loss_final: 1.531811, loss_mean: 1.439312, loss_mean_cls: 0.092500, grad_norm: 0.670495
[[34m2025-10-04 11:58:29[0m] Step: 27, Training Logs: loss_final: 1.507269, loss_mean: 1.414880, loss_mean_cls: 0.092389, grad_norm: 0.758085
[[34m2025-10-04 11:58:30[0m] Step: 28, Training Logs: loss_final: 1.519138, loss_mean: 1.429240, loss_mean_cls: 0.089898, grad_norm: 0.610955
[[34m2025-10-04 11:58:30[0m] Step: 29, Training Logs: loss_final: 1.499355, loss_mean: 1.407855, loss_mean_cls: 0.091500, grad_norm: 0.716972
[[34m2025-10-04 11:58:30[0m] Step: 30, Training Logs: loss_final: 1.483597, loss_mean: 1.393857, loss_mean_cls: 0.089740, grad_norm: 0.649487
[[34m2025-10-04 11:58:31[0m] Step: 31, Training Logs: loss_final: 1.477716, loss_mean: 1.388294, loss_mean_cls: 0.089422, grad_norm: 0.829367
[[34m2025-10-04 11:58:31[0m] Step: 32, Training Logs: loss_final: 1.488828, loss_mean: 1.401187, loss_mean_cls: 0.087641, grad_norm: 0.938168
[[34m2025-10-04 11:58:31[0m] Step: 33, Training Logs: loss_final: 1.479227, loss_mean: 1.392007, loss_mean_cls: 0.087220, grad_norm: 0.871349
[[34m2025-10-04 11:58:31[0m] Step: 34, Training Logs: loss_final: 1.478032, loss_mean: 1.389406, loss_mean_cls: 0.088627, grad_norm: 0.667966
[[34m2025-10-04 11:58:32[0m] Step: 35, Training Logs: loss_final: 1.466846, loss_mean: 1.379078, loss_mean_cls: 0.087767, grad_norm: 0.999040
[[34m2025-10-04 11:58:32[0m] Step: 36, Training Logs: loss_final: 1.428661, loss_mean: 1.341122, loss_mean_cls: 0.087538, grad_norm: 0.792584
[[34m2025-10-04 11:58:32[0m] Step: 37, Training Logs: loss_final: 1.445827, loss_mean: 1.359899, loss_mean_cls: 0.085929, grad_norm: 1.028947
[[34m2025-10-04 11:58:33[0m] Step: 38, Training Logs: loss_final: 1.443309, loss_mean: 1.355692, loss_mean_cls: 0.087617, grad_norm: 0.735793
[[34m2025-10-04 11:58:33[0m] Step: 39, Training Logs: loss_final: 1.421322, loss_mean: 1.335917, loss_mean_cls: 0.085405, grad_norm: 0.781432
[[34m2025-10-04 11:58:33[0m] Step: 40, Training Logs: loss_final: 1.428990, loss_mean: 1.345265, loss_mean_cls: 0.083725, grad_norm: 0.614151
[[34m2025-10-04 11:58:33[0m] Step: 41, Training Logs: loss_final: 1.413831, loss_mean: 1.326741, loss_mean_cls: 0.087090, grad_norm: 0.728774
[[34m2025-10-04 11:58:34[0m] Step: 42, Training Logs: loss_final: 1.416155, loss_mean: 1.330340, loss_mean_cls: 0.085814, grad_norm: 0.865924
[[34m2025-10-04 11:58:34[0m] Step: 43, Training Logs: loss_final: 1.426980, loss_mean: 1.342608, loss_mean_cls: 0.084372, grad_norm: 1.213044
[[34m2025-10-04 11:58:34[0m] Step: 44, Training Logs: loss_final: 1.417303, loss_mean: 1.335616, loss_mean_cls: 0.081687, grad_norm: 1.148594
[[34m2025-10-04 11:58:35[0m] Step: 45, Training Logs: loss_final: 1.374272, loss_mean: 1.291312, loss_mean_cls: 0.082960, grad_norm: 0.669817
[[34m2025-10-04 11:58:35[0m] Step: 46, Training Logs: loss_final: 1.377121, loss_mean: 1.294297, loss_mean_cls: 0.082824, grad_norm: 0.975531
[[34m2025-10-04 11:58:35[0m] Step: 47, Training Logs: loss_final: 1.380542, loss_mean: 1.298003, loss_mean_cls: 0.082539, grad_norm: 1.359640
[[34m2025-10-04 11:58:35[0m] Step: 48, Training Logs: loss_final: 1.373809, loss_mean: 1.291792, loss_mean_cls: 0.082016, grad_norm: 1.120516
[[34m2025-10-04 11:58:36[0m] Step: 49, Training Logs: loss_final: 1.355756, loss_mean: 1.274424, loss_mean_cls: 0.081331, grad_norm: 1.070781
[[34m2025-10-04 11:58:36[0m] Step: 50, Training Logs: loss_final: 1.365003, loss_mean: 1.282681, loss_mean_cls: 0.082322, grad_norm: 0.959041
[[34m2025-10-04 11:58:36[0m] Step: 51, Training Logs: loss_final: 1.352158, loss_mean: 1.271270, loss_mean_cls: 0.080888, grad_norm: 1.033821
[[34m2025-10-04 11:58:37[0m] Step: 52, Training Logs: loss_final: 1.309095, loss_mean: 1.229182, loss_mean_cls: 0.079913, grad_norm: 1.091182
[[34m2025-10-04 11:58:37[0m] Step: 53, Training Logs: loss_final: 1.342832, loss_mean: 1.261409, loss_mean_cls: 0.081422, grad_norm: 0.836610
[[34m2025-10-04 11:58:37[0m] Step: 54, Training Logs: loss_final: 1.308012, loss_mean: 1.231543, loss_mean_cls: 0.076469, grad_norm: 1.063546
[[34m2025-10-04 11:58:38[0m] Step: 55, Training Logs: loss_final: 1.315091, loss_mean: 1.236589, loss_mean_cls: 0.078502, grad_norm: 2.240750
[[34m2025-10-04 11:58:38[0m] Step: 56, Training Logs: loss_final: 1.286936, loss_mean: 1.208036, loss_mean_cls: 0.078900, grad_norm: 0.838871
[[34m2025-10-04 11:58:38[0m] Step: 57, Training Logs: loss_final: 1.272975, loss_mean: 1.193654, loss_mean_cls: 0.079321, grad_norm: 1.586586
[[34m2025-10-04 11:58:38[0m] Step: 58, Training Logs: loss_final: 1.325151, loss_mean: 1.246340, loss_mean_cls: 0.078811, grad_norm: 1.656412
[[34m2025-10-04 11:58:39[0m] Step: 59, Training Logs: loss_final: 1.270622, loss_mean: 1.193167, loss_mean_cls: 0.077456, grad_norm: 1.205681
[[34m2025-10-04 11:58:39[0m] Step: 60, Training Logs: loss_final: 1.283479, loss_mean: 1.205036, loss_mean_cls: 0.078444, grad_norm: 1.382442
[[34m2025-10-04 11:58:39[0m] Step: 61, Training Logs: loss_final: 1.255970, loss_mean: 1.178587, loss_mean_cls: 0.077383, grad_norm: 1.058620
[[34m2025-10-04 11:58:40[0m] Step: 62, Training Logs: loss_final: 1.257552, loss_mean: 1.182739, loss_mean_cls: 0.074813, grad_norm: 0.873871
[[34m2025-10-04 11:58:40[0m] Step: 63, Training Logs: loss_final: 1.253753, loss_mean: 1.176965, loss_mean_cls: 0.076789, grad_norm: 1.645415
[[34m2025-10-04 11:58:40[0m] Step: 64, Training Logs: loss_final: 1.215299, loss_mean: 1.139653, loss_mean_cls: 0.075647, grad_norm: 1.055909
[[34m2025-10-04 11:58:40[0m] Step: 65, Training Logs: loss_final: 1.263942, loss_mean: 1.187037, loss_mean_cls: 0.076905, grad_norm: 1.124856
[[34m2025-10-04 11:58:41[0m] Step: 66, Training Logs: loss_final: 1.239204, loss_mean: 1.164142, loss_mean_cls: 0.075062, grad_norm: 0.871376
[[34m2025-10-04 11:58:41[0m] Step: 67, Training Logs: loss_final: 1.210094, loss_mean: 1.134537, loss_mean_cls: 0.075557, grad_norm: 0.926383
[[34m2025-10-04 11:58:41[0m] Step: 68, Training Logs: loss_final: 1.245885, loss_mean: 1.172380, loss_mean_cls: 0.073505, grad_norm: 0.998089
[[34m2025-10-04 11:58:42[0m] Step: 69, Training Logs: loss_final: 1.222483, loss_mean: 1.148157, loss_mean_cls: 0.074326, grad_norm: 0.835955
[[34m2025-10-04 11:58:42[0m] Step: 70, Training Logs: loss_final: 1.218089, loss_mean: 1.146074, loss_mean_cls: 0.072015, grad_norm: 0.929099
[[34m2025-10-04 11:58:42[0m] Step: 71, Training Logs: loss_final: 1.244082, loss_mean: 1.170653, loss_mean_cls: 0.073430, grad_norm: 1.489280
[[34m2025-10-04 11:58:42[0m] Step: 72, Training Logs: loss_final: 1.207598, loss_mean: 1.134306, loss_mean_cls: 0.073292, grad_norm: 0.932517
[[34m2025-10-04 11:58:43[0m] Step: 73, Training Logs: loss_final: 1.181768, loss_mean: 1.107891, loss_mean_cls: 0.073878, grad_norm: 1.283269
[[34m2025-10-04 11:58:43[0m] Step: 74, Training Logs: loss_final: 1.209371, loss_mean: 1.137290, loss_mean_cls: 0.072081, grad_norm: 1.301948
[[34m2025-10-04 11:58:43[0m] Step: 75, Training Logs: loss_final: 1.220604, loss_mean: 1.149832, loss_mean_cls: 0.070772, grad_norm: 0.997420
[[34m2025-10-04 11:58:44[0m] Step: 76, Training Logs: loss_final: 1.209661, loss_mean: 1.136858, loss_mean_cls: 0.072803, grad_norm: 1.404692
[[34m2025-10-04 11:58:44[0m] Step: 77, Training Logs: loss_final: 1.208287, loss_mean: 1.137162, loss_mean_cls: 0.071125, grad_norm: 0.815032
[[34m2025-10-04 11:58:44[0m] Step: 78, Training Logs: loss_final: 1.212492, loss_mean: 1.141473, loss_mean_cls: 0.071019, grad_norm: 1.342152
[[34m2025-10-04 11:58:44[0m] Step: 79, Training Logs: loss_final: 1.216359, loss_mean: 1.146272, loss_mean_cls: 0.070087, grad_norm: 1.453270
[[34m2025-10-04 11:58:45[0m] Step: 80, Training Logs: loss_final: 1.179088, loss_mean: 1.110210, loss_mean_cls: 0.068878, grad_norm: 1.049633
[[34m2025-10-04 11:58:45[0m] Step: 81, Training Logs: loss_final: 1.225874, loss_mean: 1.155485, loss_mean_cls: 0.070389, grad_norm: 2.606600
[[34m2025-10-04 11:58:45[0m] Step: 82, Training Logs: loss_final: 1.217092, loss_mean: 1.147198, loss_mean_cls: 0.069894, grad_norm: 1.769640
[[34m2025-10-04 11:58:46[0m] Step: 83, Training Logs: loss_final: 1.186351, loss_mean: 1.116974, loss_mean_cls: 0.069377, grad_norm: 1.784245
[[34m2025-10-04 11:58:46[0m] Step: 84, Training Logs: loss_final: 1.189900, loss_mean: 1.118911, loss_mean_cls: 0.070989, grad_norm: 1.635399
[[34m2025-10-04 11:58:46[0m] Step: 85, Training Logs: loss_final: 1.185666, loss_mean: 1.116685, loss_mean_cls: 0.068981, grad_norm: 1.403618
[[34m2025-10-04 11:58:47[0m] Step: 86, Training Logs: loss_final: 1.146943, loss_mean: 1.077051, loss_mean_cls: 0.069892, grad_norm: 2.097867
[[34m2025-10-04 11:58:47[0m] Step: 87, Training Logs: loss_final: 1.193006, loss_mean: 1.123862, loss_mean_cls: 0.069145, grad_norm: 1.299907
[[34m2025-10-04 11:58:47[0m] Step: 88, Training Logs: loss_final: 1.156297, loss_mean: 1.087725, loss_mean_cls: 0.068572, grad_norm: 1.627457
[[34m2025-10-04 11:58:47[0m] Step: 89, Training Logs: loss_final: 1.200314, loss_mean: 1.132090, loss_mean_cls: 0.068224, grad_norm: 1.380630
[[34m2025-10-04 11:58:48[0m] Step: 90, Training Logs: loss_final: 1.155200, loss_mean: 1.087793, loss_mean_cls: 0.067407, grad_norm: 1.678809
[[34m2025-10-04 11:58:48[0m] Step: 91, Training Logs: loss_final: 1.147881, loss_mean: 1.078900, loss_mean_cls: 0.068982, grad_norm: 0.911546
[[34m2025-10-04 11:58:48[0m] Step: 92, Training Logs: loss_final: 1.157635, loss_mean: 1.089134, loss_mean_cls: 0.068502, grad_norm: 1.804483
[[34m2025-10-04 11:58:49[0m] Step: 93, Training Logs: loss_final: 1.185192, loss_mean: 1.116255, loss_mean_cls: 0.068937, grad_norm: 0.748166
[[34m2025-10-04 11:58:49[0m] Step: 94, Training Logs: loss_final: 1.173838, loss_mean: 1.104753, loss_mean_cls: 0.069085, grad_norm: 1.239238
[[34m2025-10-04 11:58:49[0m] Step: 95, Training Logs: loss_final: 1.161858, loss_mean: 1.093089, loss_mean_cls: 0.068769, grad_norm: 0.744298
[[34m2025-10-04 11:58:50[0m] Step: 96, Training Logs: loss_final: 1.178396, loss_mean: 1.111730, loss_mean_cls: 0.066667, grad_norm: 0.897914
[[34m2025-10-04 11:58:50[0m] Step: 97, Training Logs: loss_final: 1.162250, loss_mean: 1.094257, loss_mean_cls: 0.067993, grad_norm: 0.709594
[[34m2025-10-04 11:58:50[0m] Step: 98, Training Logs: loss_final: 1.137692, loss_mean: 1.071057, loss_mean_cls: 0.066635, grad_norm: 1.172933
[[34m2025-10-04 11:58:50[0m] Step: 99, Training Logs: loss_final: 1.138081, loss_mean: 1.069899, loss_mean_cls: 0.068183, grad_norm: 0.774389
[[34m2025-10-04 11:58:51[0m] Step: 100, Training Logs: loss_final: 1.152895, loss_mean: 1.085724, loss_mean_cls: 0.067172, grad_norm: 1.087725
[[34m2025-10-04 11:58:51[0m] Step: 101, Training Logs: loss_final: 1.159549, loss_mean: 1.094029, loss_mean_cls: 0.065520, grad_norm: 1.553042
[[34m2025-10-04 11:58:51[0m] Step: 102, Training Logs: loss_final: 1.160604, loss_mean: 1.092967, loss_mean_cls: 0.067636, grad_norm: 1.162378
[[34m2025-10-04 11:58:52[0m] Step: 103, Training Logs: loss_final: 1.144908, loss_mean: 1.077739, loss_mean_cls: 0.067169, grad_norm: 1.119771
[[34m2025-10-04 11:58:52[0m] Step: 104, Training Logs: loss_final: 1.125550, loss_mean: 1.060457, loss_mean_cls: 0.065093, grad_norm: 1.471360
[[34m2025-10-04 11:58:52[0m] Step: 105, Training Logs: loss_final: 1.159310, loss_mean: 1.093179, loss_mean_cls: 0.066131, grad_norm: 1.545331
[[34m2025-10-04 11:58:52[0m] Step: 106, Training Logs: loss_final: 1.158241, loss_mean: 1.092270, loss_mean_cls: 0.065971, grad_norm: 0.889555
[[34m2025-10-04 11:58:53[0m] Step: 107, Training Logs: loss_final: 1.137724, loss_mean: 1.071675, loss_mean_cls: 0.066049, grad_norm: 0.914894
[[34m2025-10-04 11:58:53[0m] Step: 108, Training Logs: loss_final: 1.164804, loss_mean: 1.099778, loss_mean_cls: 0.065027, grad_norm: 0.929008
[[34m2025-10-04 11:58:53[0m] Step: 109, Training Logs: loss_final: 1.143355, loss_mean: 1.076445, loss_mean_cls: 0.066910, grad_norm: 1.209500
[[34m2025-10-04 11:58:54[0m] Step: 110, Training Logs: loss_final: 1.143968, loss_mean: 1.077758, loss_mean_cls: 0.066210, grad_norm: 0.751902
[[34m2025-10-04 11:58:54[0m] Step: 111, Training Logs: loss_final: 1.110958, loss_mean: 1.043878, loss_mean_cls: 0.067080, grad_norm: 0.783049
[[34m2025-10-04 11:58:54[0m] Step: 112, Training Logs: loss_final: 1.130311, loss_mean: 1.063896, loss_mean_cls: 0.066415, grad_norm: 0.821997
[[34m2025-10-04 11:58:54[0m] Step: 113, Training Logs: loss_final: 1.115051, loss_mean: 1.048683, loss_mean_cls: 0.066368, grad_norm: 0.805253
[[34m2025-10-04 11:58:55[0m] Step: 114, Training Logs: loss_final: 1.148364, loss_mean: 1.083573, loss_mean_cls: 0.064791, grad_norm: 1.915238
[[34m2025-10-04 11:58:55[0m] Step: 115, Training Logs: loss_final: 1.127321, loss_mean: 1.061635, loss_mean_cls: 0.065686, grad_norm: 1.188713
[[34m2025-10-04 11:58:55[0m] Step: 116, Training Logs: loss_final: 1.157699, loss_mean: 1.092663, loss_mean_cls: 0.065036, grad_norm: 2.045234
[[34m2025-10-04 11:58:56[0m] Step: 117, Training Logs: loss_final: 1.161479, loss_mean: 1.095012, loss_mean_cls: 0.066467, grad_norm: 1.294130
[[34m2025-10-04 11:58:56[0m] Step: 118, Training Logs: loss_final: 1.112753, loss_mean: 1.046154, loss_mean_cls: 0.066600, grad_norm: 1.279721
[[34m2025-10-04 11:58:56[0m] Step: 119, Training Logs: loss_final: 1.135281, loss_mean: 1.070652, loss_mean_cls: 0.064629, grad_norm: 1.446000
[[34m2025-10-04 11:58:56[0m] Step: 120, Training Logs: loss_final: 1.124041, loss_mean: 1.059114, loss_mean_cls: 0.064927, grad_norm: 0.976336
[[34m2025-10-04 11:58:57[0m] Step: 121, Training Logs: loss_final: 1.151919, loss_mean: 1.085931, loss_mean_cls: 0.065988, grad_norm: 1.263715
[[34m2025-10-04 11:58:57[0m] Step: 122, Training Logs: loss_final: 1.104317, loss_mean: 1.039116, loss_mean_cls: 0.065201, grad_norm: 0.845004
[[34m2025-10-04 11:58:57[0m] Step: 123, Training Logs: loss_final: 1.152336, loss_mean: 1.088170, loss_mean_cls: 0.064166, grad_norm: 1.076913
[[34m2025-10-04 11:58:58[0m] Step: 124, Training Logs: loss_final: 1.139302, loss_mean: 1.074941, loss_mean_cls: 0.064361, grad_norm: 0.802603
[[34m2025-10-04 11:58:58[0m] Step: 125, Training Logs: loss_final: 1.117040, loss_mean: 1.052078, loss_mean_cls: 0.064962, grad_norm: 1.045692
[[34m2025-10-04 11:58:58[0m] Step: 126, Training Logs: loss_final: 1.110948, loss_mean: 1.047124, loss_mean_cls: 0.063824, grad_norm: 1.023891
[[34m2025-10-04 11:58:58[0m] Step: 127, Training Logs: loss_final: 1.105299, loss_mean: 1.041788, loss_mean_cls: 0.063510, grad_norm: 0.713374
[[34m2025-10-04 11:58:59[0m] Step: 128, Training Logs: loss_final: 1.130095, loss_mean: 1.065627, loss_mean_cls: 0.064469, grad_norm: 1.065424
[[34m2025-10-04 11:58:59[0m] Step: 129, Training Logs: loss_final: 1.146949, loss_mean: 1.083305, loss_mean_cls: 0.063643, grad_norm: 0.794002
[[34m2025-10-04 11:58:59[0m] Step: 130, Training Logs: loss_final: 1.136735, loss_mean: 1.072915, loss_mean_cls: 0.063820, grad_norm: 0.932061
[[34m2025-10-04 11:59:00[0m] Step: 131, Training Logs: loss_final: 1.133647, loss_mean: 1.069658, loss_mean_cls: 0.063989, grad_norm: 0.600076
[[34m2025-10-04 11:59:00[0m] Step: 132, Training Logs: loss_final: 1.080663, loss_mean: 1.016690, loss_mean_cls: 0.063973, grad_norm: 0.629335
[[34m2025-10-04 11:59:00[0m] Step: 133, Training Logs: loss_final: 1.105346, loss_mean: 1.039862, loss_mean_cls: 0.065484, grad_norm: 0.643454
[[34m2025-10-04 11:59:00[0m] Step: 134, Training Logs: loss_final: 1.100369, loss_mean: 1.037168, loss_mean_cls: 0.063201, grad_norm: 0.563212
[[34m2025-10-04 11:59:01[0m] Step: 135, Training Logs: loss_final: 1.105932, loss_mean: 1.041672, loss_mean_cls: 0.064261, grad_norm: 0.622289
[[34m2025-10-04 11:59:01[0m] Step: 136, Training Logs: loss_final: 1.110836, loss_mean: 1.046662, loss_mean_cls: 0.064174, grad_norm: 0.698803
[[34m2025-10-04 11:59:01[0m] Step: 137, Training Logs: loss_final: 1.076723, loss_mean: 1.011783, loss_mean_cls: 0.064940, grad_norm: 0.707218
[[34m2025-10-04 11:59:02[0m] Step: 138, Training Logs: loss_final: 1.120642, loss_mean: 1.055791, loss_mean_cls: 0.064851, grad_norm: 0.994787
[[34m2025-10-04 11:59:02[0m] Step: 139, Training Logs: loss_final: 1.087007, loss_mean: 1.022182, loss_mean_cls: 0.064826, grad_norm: 1.046595
[[34m2025-10-04 11:59:02[0m] Step: 140, Training Logs: loss_final: 1.093595, loss_mean: 1.029660, loss_mean_cls: 0.063935, grad_norm: 1.089518
[[34m2025-10-04 11:59:03[0m] Step: 141, Training Logs: loss_final: 1.123184, loss_mean: 1.058955, loss_mean_cls: 0.064229, grad_norm: 1.226431
[[34m2025-10-04 11:59:03[0m] Step: 142, Training Logs: loss_final: 1.099040, loss_mean: 1.036157, loss_mean_cls: 0.062882, grad_norm: 1.219716
[[34m2025-10-04 11:59:03[0m] Step: 143, Training Logs: loss_final: 1.055038, loss_mean: 0.990889, loss_mean_cls: 0.064149, grad_norm: 0.605489
[[34m2025-10-04 11:59:03[0m] Step: 144, Training Logs: loss_final: 1.091070, loss_mean: 1.026933, loss_mean_cls: 0.064137, grad_norm: 1.270204
[[34m2025-10-04 11:59:04[0m] Step: 145, Training Logs: loss_final: 1.068309, loss_mean: 1.003605, loss_mean_cls: 0.064705, grad_norm: 1.111048
[[34m2025-10-04 11:59:04[0m] Step: 146, Training Logs: loss_final: 1.097403, loss_mean: 1.033930, loss_mean_cls: 0.063474, grad_norm: 0.946707
[[34m2025-10-04 11:59:04[0m] Step: 147, Training Logs: loss_final: 1.095557, loss_mean: 1.031377, loss_mean_cls: 0.064180, grad_norm: 1.267769
[[34m2025-10-04 11:59:05[0m] Step: 148, Training Logs: loss_final: 1.109137, loss_mean: 1.045402, loss_mean_cls: 0.063735, grad_norm: 1.166205
[[34m2025-10-04 11:59:05[0m] Step: 149, Training Logs: loss_final: 1.110693, loss_mean: 1.047938, loss_mean_cls: 0.062756, grad_norm: 1.223423
[[34m2025-10-04 11:59:05[0m] Step: 150, Training Logs: loss_final: 1.108818, loss_mean: 1.046710, loss_mean_cls: 0.062108, grad_norm: 0.936960
[[34m2025-10-04 11:59:05[0m] Step: 151, Training Logs: loss_final: 1.059399, loss_mean: 0.994788, loss_mean_cls: 0.064611, grad_norm: 0.846941
[[34m2025-10-04 11:59:06[0m] Step: 152, Training Logs: loss_final: 1.121889, loss_mean: 1.058429, loss_mean_cls: 0.063460, grad_norm: 1.020130
[[34m2025-10-04 11:59:06[0m] Step: 153, Training Logs: loss_final: 1.095939, loss_mean: 1.034368, loss_mean_cls: 0.061571, grad_norm: 1.148290
[[34m2025-10-04 11:59:06[0m] Step: 154, Training Logs: loss_final: 1.094089, loss_mean: 1.031305, loss_mean_cls: 0.062784, grad_norm: 0.649710
[[34m2025-10-04 11:59:07[0m] Step: 155, Training Logs: loss_final: 1.098525, loss_mean: 1.034432, loss_mean_cls: 0.064093, grad_norm: 1.210493
[[34m2025-10-04 11:59:07[0m] Step: 156, Training Logs: loss_final: 1.091479, loss_mean: 1.028702, loss_mean_cls: 0.062777, grad_norm: 1.007981
[[34m2025-10-04 11:59:07[0m] Step: 157, Training Logs: loss_final: 1.082099, loss_mean: 1.020590, loss_mean_cls: 0.061508, grad_norm: 1.185494
[[34m2025-10-04 11:59:07[0m] Step: 158, Training Logs: loss_final: 1.100116, loss_mean: 1.036934, loss_mean_cls: 0.063181, grad_norm: 1.049936
[[34m2025-10-04 11:59:08[0m] Step: 159, Training Logs: loss_final: 1.080206, loss_mean: 1.018823, loss_mean_cls: 0.061383, grad_norm: 0.724169
[[34m2025-10-04 11:59:08[0m] Step: 160, Training Logs: loss_final: 1.082649, loss_mean: 1.018734, loss_mean_cls: 0.063915, grad_norm: 1.187783
[[34m2025-10-04 11:59:08[0m] Step: 161, Training Logs: loss_final: 1.101871, loss_mean: 1.037806, loss_mean_cls: 0.064065, grad_norm: 0.812157
[[34m2025-10-04 11:59:09[0m] Step: 162, Training Logs: loss_final: 1.090401, loss_mean: 1.027790, loss_mean_cls: 0.062612, grad_norm: 1.317237
[[34m2025-10-04 11:59:09[0m] Step: 163, Training Logs: loss_final: 1.092381, loss_mean: 1.029764, loss_mean_cls: 0.062617, grad_norm: 1.294814
[[34m2025-10-04 11:59:09[0m] Step: 164, Training Logs: loss_final: 1.088589, loss_mean: 1.025306, loss_mean_cls: 0.063283, grad_norm: 0.899731
[[34m2025-10-04 11:59:09[0m] Step: 165, Training Logs: loss_final: 1.089273, loss_mean: 1.027073, loss_mean_cls: 0.062200, grad_norm: 1.093026
[[34m2025-10-04 11:59:10[0m] Step: 166, Training Logs: loss_final: 1.053399, loss_mean: 0.989408, loss_mean_cls: 0.063992, grad_norm: 0.855233
[[34m2025-10-04 11:59:10[0m] Step: 167, Training Logs: loss_final: 1.110872, loss_mean: 1.046482, loss_mean_cls: 0.064389, grad_norm: 1.277748
[[34m2025-10-04 11:59:10[0m] Step: 168, Training Logs: loss_final: 1.088963, loss_mean: 1.025599, loss_mean_cls: 0.063364, grad_norm: 0.880995
[[34m2025-10-04 11:59:11[0m] Step: 169, Training Logs: loss_final: 1.070628, loss_mean: 1.007739, loss_mean_cls: 0.062889, grad_norm: 0.920139
[[34m2025-10-04 11:59:11[0m] Step: 170, Training Logs: loss_final: 1.082061, loss_mean: 1.019171, loss_mean_cls: 0.062890, grad_norm: 0.868008
[[34m2025-10-04 11:59:11[0m] Step: 171, Training Logs: loss_final: 1.097022, loss_mean: 1.034947, loss_mean_cls: 0.062075, grad_norm: 1.069641
[[34m2025-10-04 11:59:11[0m] Step: 172, Training Logs: loss_final: 1.063415, loss_mean: 1.001423, loss_mean_cls: 0.061992, grad_norm: 0.822438
[[34m2025-10-04 11:59:12[0m] Step: 173, Training Logs: loss_final: 1.045943, loss_mean: 0.982353, loss_mean_cls: 0.063590, grad_norm: 0.881901
[[34m2025-10-04 11:59:12[0m] Step: 174, Training Logs: loss_final: 1.078992, loss_mean: 1.017144, loss_mean_cls: 0.061848, grad_norm: 0.923457
[[34m2025-10-04 11:59:12[0m] Step: 175, Training Logs: loss_final: 1.045215, loss_mean: 0.982871, loss_mean_cls: 0.062343, grad_norm: 1.179002
[[34m2025-10-04 11:59:13[0m] Step: 176, Training Logs: loss_final: 1.075766, loss_mean: 1.011802, loss_mean_cls: 0.063964, grad_norm: 0.855685
[[34m2025-10-04 11:59:13[0m] Step: 177, Training Logs: loss_final: 1.050661, loss_mean: 0.987994, loss_mean_cls: 0.062667, grad_norm: 1.013143
[[34m2025-10-04 11:59:13[0m] Step: 178, Training Logs: loss_final: 1.070028, loss_mean: 1.007090, loss_mean_cls: 0.062938, grad_norm: 0.863631
[[34m2025-10-04 11:59:13[0m] Step: 179, Training Logs: loss_final: 1.078369, loss_mean: 1.015977, loss_mean_cls: 0.062392, grad_norm: 0.799556
[[34m2025-10-04 11:59:14[0m] Step: 180, Training Logs: loss_final: 1.044156, loss_mean: 0.982728, loss_mean_cls: 0.061428, grad_norm: 0.979666
[[34m2025-10-04 11:59:14[0m] Step: 181, Training Logs: loss_final: 1.084331, loss_mean: 1.022099, loss_mean_cls: 0.062231, grad_norm: 1.378204
[[34m2025-10-04 11:59:14[0m] Step: 182, Training Logs: loss_final: 1.050454, loss_mean: 0.987956, loss_mean_cls: 0.062498, grad_norm: 0.845689
[[34m2025-10-04 11:59:15[0m] Step: 183, Training Logs: loss_final: 1.080031, loss_mean: 1.018179, loss_mean_cls: 0.061852, grad_norm: 1.231650
[[34m2025-10-04 11:59:15[0m] Step: 184, Training Logs: loss_final: 1.059551, loss_mean: 0.997448, loss_mean_cls: 0.062104, grad_norm: 1.116566
[[34m2025-10-04 11:59:15[0m] Step: 185, Training Logs: loss_final: 1.064238, loss_mean: 1.002420, loss_mean_cls: 0.061818, grad_norm: 0.871099
[[34m2025-10-04 11:59:15[0m] Step: 186, Training Logs: loss_final: 1.061370, loss_mean: 1.000179, loss_mean_cls: 0.061191, grad_norm: 1.114910
[[34m2025-10-04 11:59:16[0m] Step: 187, Training Logs: loss_final: 1.055591, loss_mean: 0.993270, loss_mean_cls: 0.062321, grad_norm: 0.863193
[[34m2025-10-04 11:59:16[0m] Step: 188, Training Logs: loss_final: 1.102974, loss_mean: 1.041552, loss_mean_cls: 0.061422, grad_norm: 1.363935
[[34m2025-10-04 11:59:16[0m] Step: 189, Training Logs: loss_final: 1.044736, loss_mean: 0.981661, loss_mean_cls: 0.063075, grad_norm: 0.957310
[[34m2025-10-04 11:59:17[0m] Step: 190, Training Logs: loss_final: 1.091679, loss_mean: 1.029676, loss_mean_cls: 0.062002, grad_norm: 1.115942
[[34m2025-10-04 11:59:17[0m] Step: 191, Training Logs: loss_final: 1.040212, loss_mean: 0.978650, loss_mean_cls: 0.061562, grad_norm: 0.846537
[[34m2025-10-04 11:59:17[0m] Step: 192, Training Logs: loss_final: 1.047045, loss_mean: 0.984966, loss_mean_cls: 0.062079, grad_norm: 0.881012
[[34m2025-10-04 11:59:17[0m] Step: 193, Training Logs: loss_final: 1.063240, loss_mean: 1.002117, loss_mean_cls: 0.061123, grad_norm: 1.449064
[[34m2025-10-04 11:59:18[0m] Step: 194, Training Logs: loss_final: 1.068559, loss_mean: 1.006875, loss_mean_cls: 0.061684, grad_norm: 0.821755
[[34m2025-10-04 11:59:18[0m] Step: 195, Training Logs: loss_final: 1.077656, loss_mean: 1.015350, loss_mean_cls: 0.062305, grad_norm: 1.585069
[[34m2025-10-04 11:59:18[0m] Step: 196, Training Logs: loss_final: 1.041453, loss_mean: 0.977241, loss_mean_cls: 0.064212, grad_norm: 0.797770
[[34m2025-10-04 11:59:19[0m] Step: 197, Training Logs: loss_final: 1.094154, loss_mean: 1.033823, loss_mean_cls: 0.060330, grad_norm: 1.014875
[[34m2025-10-04 11:59:19[0m] Step: 198, Training Logs: loss_final: 1.058999, loss_mean: 0.997704, loss_mean_cls: 0.061295, grad_norm: 1.202015
[[34m2025-10-04 11:59:19[0m] Step: 199, Training Logs: loss_final: 1.073846, loss_mean: 1.013786, loss_mean_cls: 0.060060, grad_norm: 1.062728
[[34m2025-10-04 11:59:20[0m] Step: 200, Training Logs: loss_final: 1.068950, loss_mean: 1.007000, loss_mean_cls: 0.061949, grad_norm: 1.038326
[[34m2025-10-04 11:59:20[0m] Step: 201, Training Logs: loss_final: 1.039293, loss_mean: 0.978058, loss_mean_cls: 0.061235, grad_norm: 0.692390
[[34m2025-10-04 11:59:20[0m] Step: 202, Training Logs: loss_final: 1.054545, loss_mean: 0.993963, loss_mean_cls: 0.060582, grad_norm: 0.979375
[[34m2025-10-04 11:59:20[0m] Step: 203, Training Logs: loss_final: 1.063964, loss_mean: 1.002774, loss_mean_cls: 0.061190, grad_norm: 1.226263
[[34m2025-10-04 11:59:21[0m] Step: 204, Training Logs: loss_final: 1.068907, loss_mean: 1.007209, loss_mean_cls: 0.061698, grad_norm: 1.009795
[[34m2025-10-04 11:59:21[0m] Step: 205, Training Logs: loss_final: 1.075126, loss_mean: 1.013581, loss_mean_cls: 0.061545, grad_norm: 0.868994
[[34m2025-10-04 11:59:21[0m] Step: 206, Training Logs: loss_final: 1.074482, loss_mean: 1.014164, loss_mean_cls: 0.060319, grad_norm: 1.324093
[[34m2025-10-04 11:59:22[0m] Step: 207, Training Logs: loss_final: 1.057914, loss_mean: 0.997108, loss_mean_cls: 0.060806, grad_norm: 1.008509
[[34m2025-10-04 11:59:22[0m] Step: 208, Training Logs: loss_final: 1.044524, loss_mean: 0.982995, loss_mean_cls: 0.061529, grad_norm: 0.756854
[[34m2025-10-04 11:59:22[0m] Step: 209, Training Logs: loss_final: 1.058371, loss_mean: 0.997310, loss_mean_cls: 0.061061, grad_norm: 1.001887
[[34m2025-10-04 11:59:23[0m] Step: 210, Training Logs: loss_final: 1.063412, loss_mean: 1.003053, loss_mean_cls: 0.060359, grad_norm: 0.786596
[[34m2025-10-04 11:59:23[0m] Step: 211, Training Logs: loss_final: 1.057284, loss_mean: 0.996740, loss_mean_cls: 0.060544, grad_norm: 0.711001
[[34m2025-10-04 11:59:23[0m] Step: 212, Training Logs: loss_final: 1.056001, loss_mean: 0.996604, loss_mean_cls: 0.059398, grad_norm: 1.461260
[[34m2025-10-04 11:59:23[0m] Step: 213, Training Logs: loss_final: 1.073707, loss_mean: 1.013813, loss_mean_cls: 0.059894, grad_norm: 0.708135
[[34m2025-10-04 11:59:24[0m] Step: 214, Training Logs: loss_final: 1.072841, loss_mean: 1.012409, loss_mean_cls: 0.060433, grad_norm: 1.319640
[[34m2025-10-04 11:59:24[0m] Step: 215, Training Logs: loss_final: 1.057741, loss_mean: 0.996811, loss_mean_cls: 0.060930, grad_norm: 0.979530
[[34m2025-10-04 11:59:24[0m] Step: 216, Training Logs: loss_final: 1.070524, loss_mean: 1.009519, loss_mean_cls: 0.061005, grad_norm: 0.869351
[[34m2025-10-04 11:59:25[0m] Step: 217, Training Logs: loss_final: 1.060650, loss_mean: 1.000460, loss_mean_cls: 0.060190, grad_norm: 1.262853
[[34m2025-10-04 11:59:25[0m] Step: 218, Training Logs: loss_final: 1.026598, loss_mean: 0.965076, loss_mean_cls: 0.061522, grad_norm: 0.999598
[[34m2025-10-04 11:59:25[0m] Step: 219, Training Logs: loss_final: 1.049094, loss_mean: 0.987929, loss_mean_cls: 0.061165, grad_norm: 1.374824
[[34m2025-10-04 11:59:26[0m] Step: 220, Training Logs: loss_final: 1.036040, loss_mean: 0.975466, loss_mean_cls: 0.060574, grad_norm: 1.094213
[[34m2025-10-04 11:59:26[0m] Step: 221, Training Logs: loss_final: 1.086760, loss_mean: 1.026719, loss_mean_cls: 0.060041, grad_norm: 1.093327
[[34m2025-10-04 11:59:26[0m] Step: 222, Training Logs: loss_final: 1.038916, loss_mean: 0.978318, loss_mean_cls: 0.060598, grad_norm: 1.068303
[[34m2025-10-04 11:59:26[0m] Step: 223, Training Logs: loss_final: 1.057428, loss_mean: 0.997229, loss_mean_cls: 0.060198, grad_norm: 1.065031
[[34m2025-10-04 11:59:27[0m] Step: 224, Training Logs: loss_final: 1.067340, loss_mean: 1.008277, loss_mean_cls: 0.059063, grad_norm: 1.163527
[[34m2025-10-04 11:59:27[0m] Step: 225, Training Logs: loss_final: 1.064210, loss_mean: 1.003725, loss_mean_cls: 0.060484, grad_norm: 1.058786
[[34m2025-10-04 11:59:27[0m] Step: 226, Training Logs: loss_final: 1.045886, loss_mean: 0.986143, loss_mean_cls: 0.059743, grad_norm: 1.153386
[[34m2025-10-04 11:59:28[0m] Step: 227, Training Logs: loss_final: 1.034164, loss_mean: 0.973530, loss_mean_cls: 0.060634, grad_norm: 0.976899
[[34m2025-10-04 11:59:28[0m] Step: 228, Training Logs: loss_final: 1.059143, loss_mean: 0.998691, loss_mean_cls: 0.060452, grad_norm: 1.149375
[[34m2025-10-04 11:59:28[0m] Step: 229, Training Logs: loss_final: 1.029826, loss_mean: 0.970765, loss_mean_cls: 0.059061, grad_norm: 1.116368
[[34m2025-10-04 11:59:29[0m] Step: 230, Training Logs: loss_final: 1.025412, loss_mean: 0.964530, loss_mean_cls: 0.060883, grad_norm: 1.519690
[[34m2025-10-04 11:59:29[0m] Step: 231, Training Logs: loss_final: 1.060463, loss_mean: 1.000738, loss_mean_cls: 0.059725, grad_norm: 1.156793
[[34m2025-10-04 11:59:29[0m] Step: 232, Training Logs: loss_final: 1.052135, loss_mean: 0.992876, loss_mean_cls: 0.059259, grad_norm: 1.080449
[[34m2025-10-04 11:59:29[0m] Step: 233, Training Logs: loss_final: 1.049033, loss_mean: 0.990299, loss_mean_cls: 0.058734, grad_norm: 1.123685
[[34m2025-10-04 11:59:30[0m] Step: 234, Training Logs: loss_final: 1.063928, loss_mean: 1.004589, loss_mean_cls: 0.059339, grad_norm: 1.342039
[[34m2025-10-04 11:59:30[0m] Step: 235, Training Logs: loss_final: 1.038271, loss_mean: 0.977799, loss_mean_cls: 0.060472, grad_norm: 0.908680
[[34m2025-10-04 11:59:30[0m] Step: 236, Training Logs: loss_final: 1.047474, loss_mean: 0.987739, loss_mean_cls: 0.059735, grad_norm: 1.005320
[[34m2025-10-04 11:59:31[0m] Step: 237, Training Logs: loss_final: 1.023801, loss_mean: 0.963113, loss_mean_cls: 0.060688, grad_norm: 1.016588
[[34m2025-10-04 11:59:31[0m] Step: 238, Training Logs: loss_final: 1.035403, loss_mean: 0.975971, loss_mean_cls: 0.059432, grad_norm: 1.196049
[[34m2025-10-04 11:59:31[0m] Step: 239, Training Logs: loss_final: 1.036942, loss_mean: 0.976687, loss_mean_cls: 0.060256, grad_norm: 0.857842
[[34m2025-10-04 11:59:31[0m] Step: 240, Training Logs: loss_final: 1.022371, loss_mean: 0.961824, loss_mean_cls: 0.060547, grad_norm: 1.372671
[[34m2025-10-04 11:59:32[0m] Step: 241, Training Logs: loss_final: 1.035283, loss_mean: 0.976663, loss_mean_cls: 0.058621, grad_norm: 1.023927
[[34m2025-10-04 11:59:32[0m] Step: 242, Training Logs: loss_final: 1.033531, loss_mean: 0.973893, loss_mean_cls: 0.059638, grad_norm: 0.840458
[[34m2025-10-04 11:59:32[0m] Step: 243, Training Logs: loss_final: 1.056961, loss_mean: 0.996270, loss_mean_cls: 0.060691, grad_norm: 1.204095
[[34m2025-10-04 11:59:33[0m] Step: 244, Training Logs: loss_final: 1.031632, loss_mean: 0.971930, loss_mean_cls: 0.059702, grad_norm: 0.793920
[[34m2025-10-04 11:59:33[0m] Step: 245, Training Logs: loss_final: 1.033391, loss_mean: 0.972911, loss_mean_cls: 0.060480, grad_norm: 1.475015
[[34m2025-10-04 11:59:33[0m] Step: 246, Training Logs: loss_final: 1.056824, loss_mean: 0.997597, loss_mean_cls: 0.059227, grad_norm: 0.918792
[[34m2025-10-04 11:59:34[0m] Step: 247, Training Logs: loss_final: 1.026028, loss_mean: 0.967065, loss_mean_cls: 0.058962, grad_norm: 0.866246
[[34m2025-10-04 11:59:34[0m] Step: 248, Training Logs: loss_final: 1.066033, loss_mean: 1.007041, loss_mean_cls: 0.058992, grad_norm: 0.883244
[[34m2025-10-04 11:59:34[0m] Step: 249, Training Logs: loss_final: 1.040200, loss_mean: 0.980144, loss_mean_cls: 0.060056, grad_norm: 1.024802
[[34m2025-10-04 11:59:35[0m] Step: 250, Training Logs: loss_final: 1.028762, loss_mean: 0.969286, loss_mean_cls: 0.059477, grad_norm: 0.768439
[[34m2025-10-04 11:59:35[0m] Step: 251, Training Logs: loss_final: 1.023879, loss_mean: 0.965186, loss_mean_cls: 0.058693, grad_norm: 0.993195
[[34m2025-10-04 11:59:35[0m] Step: 252, Training Logs: loss_final: 1.048741, loss_mean: 0.987827, loss_mean_cls: 0.060914, grad_norm: 1.146387
[[34m2025-10-04 11:59:35[0m] Step: 253, Training Logs: loss_final: 1.018461, loss_mean: 0.958760, loss_mean_cls: 0.059701, grad_norm: 0.895699
[[34m2025-10-04 11:59:36[0m] Step: 254, Training Logs: loss_final: 1.033269, loss_mean: 0.972717, loss_mean_cls: 0.060552, grad_norm: 1.572865
[[34m2025-10-04 11:59:36[0m] Step: 255, Training Logs: loss_final: 1.052704, loss_mean: 0.994115, loss_mean_cls: 0.058589, grad_norm: 1.067278
[[34m2025-10-04 11:59:36[0m] Step: 256, Training Logs: loss_final: 1.033268, loss_mean: 0.974695, loss_mean_cls: 0.058573, grad_norm: 1.141338
[[34m2025-10-04 11:59:37[0m] Step: 257, Training Logs: loss_final: 1.020905, loss_mean: 0.962658, loss_mean_cls: 0.058247, grad_norm: 0.801576
[[34m2025-10-04 11:59:37[0m] Step: 258, Training Logs: loss_final: 1.069934, loss_mean: 1.011987, loss_mean_cls: 0.057947, grad_norm: 1.034377
[[34m2025-10-04 11:59:37[0m] Step: 259, Training Logs: loss_final: 1.038039, loss_mean: 0.978559, loss_mean_cls: 0.059479, grad_norm: 0.946777
[[34m2025-10-04 11:59:37[0m] Step: 260, Training Logs: loss_final: 1.044884, loss_mean: 0.986085, loss_mean_cls: 0.058799, grad_norm: 0.861927
[[34m2025-10-04 11:59:38[0m] Step: 261, Training Logs: loss_final: 1.056149, loss_mean: 0.997148, loss_mean_cls: 0.059001, grad_norm: 1.018608
[[34m2025-10-04 11:59:38[0m] Step: 262, Training Logs: loss_final: 1.019789, loss_mean: 0.961388, loss_mean_cls: 0.058400, grad_norm: 1.054557
[[34m2025-10-04 11:59:38[0m] Step: 263, Training Logs: loss_final: 1.049937, loss_mean: 0.991498, loss_mean_cls: 0.058439, grad_norm: 0.757994
[[34m2025-10-04 11:59:39[0m] Step: 264, Training Logs: loss_final: 1.047902, loss_mean: 0.989353, loss_mean_cls: 0.058549, grad_norm: 1.401295
[[34m2025-10-04 11:59:39[0m] Step: 265, Training Logs: loss_final: 1.017465, loss_mean: 0.958300, loss_mean_cls: 0.059164, grad_norm: 0.842247
[[34m2025-10-04 11:59:39[0m] Step: 266, Training Logs: loss_final: 1.038961, loss_mean: 0.980237, loss_mean_cls: 0.058724, grad_norm: 1.094010
[[34m2025-10-04 11:59:39[0m] Step: 267, Training Logs: loss_final: 1.034260, loss_mean: 0.976709, loss_mean_cls: 0.057551, grad_norm: 1.022727
[[34m2025-10-04 11:59:40[0m] Step: 268, Training Logs: loss_final: 1.019988, loss_mean: 0.961462, loss_mean_cls: 0.058526, grad_norm: 0.888695
[[34m2025-10-04 11:59:40[0m] Step: 269, Training Logs: loss_final: 1.040647, loss_mean: 0.981753, loss_mean_cls: 0.058894, grad_norm: 0.903536
[[34m2025-10-04 11:59:40[0m] Step: 270, Training Logs: loss_final: 1.027176, loss_mean: 0.967842, loss_mean_cls: 0.059333, grad_norm: 0.874025
[[34m2025-10-04 11:59:41[0m] Step: 271, Training Logs: loss_final: 1.048959, loss_mean: 0.989350, loss_mean_cls: 0.059609, grad_norm: 0.803761
[[34m2025-10-04 11:59:41[0m] Step: 272, Training Logs: loss_final: 1.015719, loss_mean: 0.958181, loss_mean_cls: 0.057537, grad_norm: 1.248416
[[34m2025-10-04 11:59:41[0m] Step: 273, Training Logs: loss_final: 1.031381, loss_mean: 0.972615, loss_mean_cls: 0.058765, grad_norm: 0.928918
[[34m2025-10-04 11:59:41[0m] Step: 274, Training Logs: loss_final: 1.024593, loss_mean: 0.966410, loss_mean_cls: 0.058183, grad_norm: 0.966539
[[34m2025-10-04 11:59:42[0m] Step: 275, Training Logs: loss_final: 1.032919, loss_mean: 0.974401, loss_mean_cls: 0.058517, grad_norm: 0.843899
[[34m2025-10-04 11:59:42[0m] Step: 276, Training Logs: loss_final: 1.033738, loss_mean: 0.975844, loss_mean_cls: 0.057894, grad_norm: 0.904493
[[34m2025-10-04 11:59:42[0m] Step: 277, Training Logs: loss_final: 1.022608, loss_mean: 0.964248, loss_mean_cls: 0.058360, grad_norm: 0.866951
[[34m2025-10-04 11:59:43[0m] Step: 278, Training Logs: loss_final: 1.036414, loss_mean: 0.978575, loss_mean_cls: 0.057839, grad_norm: 0.707303
[[34m2025-10-04 11:59:43[0m] Step: 279, Training Logs: loss_final: 1.028003, loss_mean: 0.970761, loss_mean_cls: 0.057243, grad_norm: 0.905739
[[34m2025-10-04 11:59:43[0m] Step: 280, Training Logs: loss_final: 1.024691, loss_mean: 0.966403, loss_mean_cls: 0.058288, grad_norm: 0.997895
[[34m2025-10-04 11:59:43[0m] Step: 281, Training Logs: loss_final: 1.038646, loss_mean: 0.980543, loss_mean_cls: 0.058102, grad_norm: 0.821466
[[34m2025-10-04 11:59:44[0m] Step: 282, Training Logs: loss_final: 1.011075, loss_mean: 0.952894, loss_mean_cls: 0.058181, grad_norm: 0.671294
[[34m2025-10-04 11:59:44[0m] Step: 283, Training Logs: loss_final: 1.019086, loss_mean: 0.961182, loss_mean_cls: 0.057904, grad_norm: 1.114947
[[34m2025-10-04 11:59:44[0m] Step: 284, Training Logs: loss_final: 1.029109, loss_mean: 0.970442, loss_mean_cls: 0.058667, grad_norm: 0.666842
[[34m2025-10-04 11:59:45[0m] Step: 285, Training Logs: loss_final: 1.041883, loss_mean: 0.983726, loss_mean_cls: 0.058158, grad_norm: 1.000216
[[34m2025-10-04 11:59:45[0m] Step: 286, Training Logs: loss_final: 1.011892, loss_mean: 0.954671, loss_mean_cls: 0.057221, grad_norm: 0.742658
[[34m2025-10-04 11:59:45[0m] Step: 287, Training Logs: loss_final: 1.012007, loss_mean: 0.955451, loss_mean_cls: 0.056557, grad_norm: 0.977658
[[34m2025-10-04 11:59:45[0m] Step: 288, Training Logs: loss_final: 1.019883, loss_mean: 0.962468, loss_mean_cls: 0.057415, grad_norm: 0.742889
[[34m2025-10-04 11:59:46[0m] Step: 289, Training Logs: loss_final: 1.017251, loss_mean: 0.959054, loss_mean_cls: 0.058197, grad_norm: 0.871899
[[34m2025-10-04 11:59:46[0m] Step: 290, Training Logs: loss_final: 1.014231, loss_mean: 0.956765, loss_mean_cls: 0.057466, grad_norm: 1.069149
[[34m2025-10-04 11:59:46[0m] Step: 291, Training Logs: loss_final: 1.012971, loss_mean: 0.955729, loss_mean_cls: 0.057241, grad_norm: 0.948060
[[34m2025-10-04 11:59:47[0m] Step: 292, Training Logs: loss_final: 0.988343, loss_mean: 0.930338, loss_mean_cls: 0.058006, grad_norm: 0.654610
[[34m2025-10-04 11:59:47[0m] Step: 293, Training Logs: loss_final: 1.008259, loss_mean: 0.951601, loss_mean_cls: 0.056658, grad_norm: 0.772584
[[34m2025-10-04 11:59:47[0m] Step: 294, Training Logs: loss_final: 1.035720, loss_mean: 0.977890, loss_mean_cls: 0.057830, grad_norm: 0.808702
[[34m2025-10-04 11:59:47[0m] Step: 295, Training Logs: loss_final: 1.006164, loss_mean: 0.947338, loss_mean_cls: 0.058826, grad_norm: 0.790169
[[34m2025-10-04 11:59:48[0m] Step: 296, Training Logs: loss_final: 0.995307, loss_mean: 0.937201, loss_mean_cls: 0.058106, grad_norm: 0.773530
[[34m2025-10-04 11:59:48[0m] Step: 297, Training Logs: loss_final: 1.018198, loss_mean: 0.960209, loss_mean_cls: 0.057988, grad_norm: 0.778248
[[34m2025-10-04 11:59:48[0m] Step: 298, Training Logs: loss_final: 1.017756, loss_mean: 0.959511, loss_mean_cls: 0.058246, grad_norm: 0.963301
[[34m2025-10-04 11:59:49[0m] Step: 299, Training Logs: loss_final: 1.029318, loss_mean: 0.970967, loss_mean_cls: 0.058351, grad_norm: 0.937274
[[34m2025-10-04 11:59:49[0m] Step: 300, Training Logs: loss_final: 1.007811, loss_mean: 0.949522, loss_mean_cls: 0.058289, grad_norm: 0.630913
[[34m2025-10-04 11:59:49[0m] Step: 301, Training Logs: loss_final: 1.028834, loss_mean: 0.971115, loss_mean_cls: 0.057719, grad_norm: 0.926601
[[34m2025-10-04 11:59:50[0m] Step: 302, Training Logs: loss_final: 1.030642, loss_mean: 0.973916, loss_mean_cls: 0.056726, grad_norm: 1.545798
[[34m2025-10-04 11:59:50[0m] Step: 303, Training Logs: loss_final: 1.024089, loss_mean: 0.966090, loss_mean_cls: 0.057998, grad_norm: 0.793828
[[34m2025-10-04 11:59:50[0m] Step: 304, Training Logs: loss_final: 1.026446, loss_mean: 0.968071, loss_mean_cls: 0.058375, grad_norm: 0.844596
[[34m2025-10-04 11:59:50[0m] Step: 305, Training Logs: loss_final: 1.004808, loss_mean: 0.947714, loss_mean_cls: 0.057094, grad_norm: 0.770304
[[34m2025-10-04 11:59:51[0m] Step: 306, Training Logs: loss_final: 1.006524, loss_mean: 0.948498, loss_mean_cls: 0.058026, grad_norm: 0.978765
[[34m2025-10-04 11:59:51[0m] Step: 307, Training Logs: loss_final: 1.023667, loss_mean: 0.966769, loss_mean_cls: 0.056897, grad_norm: 1.071971
[[34m2025-10-04 11:59:51[0m] Step: 308, Training Logs: loss_final: 1.013839, loss_mean: 0.955427, loss_mean_cls: 0.058412, grad_norm: 1.254986
[[34m2025-10-04 11:59:52[0m] Step: 309, Training Logs: loss_final: 1.025017, loss_mean: 0.967974, loss_mean_cls: 0.057043, grad_norm: 1.156484
[[34m2025-10-04 11:59:52[0m] Step: 310, Training Logs: loss_final: 1.006072, loss_mean: 0.947475, loss_mean_cls: 0.058597, grad_norm: 1.213351
[[34m2025-10-04 11:59:52[0m] Step: 311, Training Logs: loss_final: 1.026765, loss_mean: 0.969551, loss_mean_cls: 0.057215, grad_norm: 1.922248
[[34m2025-10-04 11:59:52[0m] Step: 312, Training Logs: loss_final: 1.014698, loss_mean: 0.957144, loss_mean_cls: 0.057554, grad_norm: 1.485842
[[34m2025-10-04 11:59:53[0m] Step: 313, Training Logs: loss_final: 1.014127, loss_mean: 0.956727, loss_mean_cls: 0.057400, grad_norm: 1.422783
[[34m2025-10-04 11:59:53[0m] Step: 314, Training Logs: loss_final: 1.001512, loss_mean: 0.943434, loss_mean_cls: 0.058078, grad_norm: 1.408204
[[34m2025-10-04 11:59:53[0m] Step: 315, Training Logs: loss_final: 1.019137, loss_mean: 0.961218, loss_mean_cls: 0.057919, grad_norm: 1.425473
[[34m2025-10-04 11:59:54[0m] Step: 316, Training Logs: loss_final: 1.005824, loss_mean: 0.947277, loss_mean_cls: 0.058548, grad_norm: 1.028567
[[34m2025-10-04 11:59:54[0m] Step: 317, Training Logs: loss_final: 1.048406, loss_mean: 0.992274, loss_mean_cls: 0.056133, grad_norm: 1.178803
[[34m2025-10-04 11:59:54[0m] Step: 318, Training Logs: loss_final: 1.015677, loss_mean: 0.958566, loss_mean_cls: 0.057111, grad_norm: 1.145043
[[34m2025-10-04 11:59:55[0m] Step: 319, Training Logs: loss_final: 1.015932, loss_mean: 0.959430, loss_mean_cls: 0.056502, grad_norm: 1.110621
[[34m2025-10-04 11:59:55[0m] Step: 320, Training Logs: loss_final: 1.018037, loss_mean: 0.960450, loss_mean_cls: 0.057587, grad_norm: 1.310447
[[34m2025-10-04 11:59:55[0m] Step: 321, Training Logs: loss_final: 1.017647, loss_mean: 0.959925, loss_mean_cls: 0.057721, grad_norm: 0.899477
[[34m2025-10-04 11:59:55[0m] Step: 322, Training Logs: loss_final: 1.011532, loss_mean: 0.953662, loss_mean_cls: 0.057869, grad_norm: 1.179859
[[34m2025-10-04 11:59:56[0m] Step: 323, Training Logs: loss_final: 1.037956, loss_mean: 0.981995, loss_mean_cls: 0.055962, grad_norm: 1.165558
[[34m2025-10-04 11:59:56[0m] Step: 324, Training Logs: loss_final: 1.026602, loss_mean: 0.968773, loss_mean_cls: 0.057829, grad_norm: 1.161447
[[34m2025-10-04 11:59:56[0m] Step: 325, Training Logs: loss_final: 1.005603, loss_mean: 0.948553, loss_mean_cls: 0.057050, grad_norm: 1.094988
[[34m2025-10-04 11:59:57[0m] Step: 326, Training Logs: loss_final: 1.014215, loss_mean: 0.957177, loss_mean_cls: 0.057039, grad_norm: 1.100006
[[34m2025-10-04 11:59:57[0m] Step: 327, Training Logs: loss_final: 1.019311, loss_mean: 0.961141, loss_mean_cls: 0.058170, grad_norm: 0.816990
[[34m2025-10-04 11:59:57[0m] Step: 328, Training Logs: loss_final: 1.006825, loss_mean: 0.949626, loss_mean_cls: 0.057199, grad_norm: 1.118074
[[34m2025-10-04 11:59:57[0m] Step: 329, Training Logs: loss_final: 1.002619, loss_mean: 0.945480, loss_mean_cls: 0.057139, grad_norm: 0.813555
[[34m2025-10-04 11:59:58[0m] Step: 330, Training Logs: loss_final: 1.017564, loss_mean: 0.961813, loss_mean_cls: 0.055751, grad_norm: 1.070704
[[34m2025-10-04 11:59:58[0m] Step: 331, Training Logs: loss_final: 1.028385, loss_mean: 0.971684, loss_mean_cls: 0.056701, grad_norm: 1.124968
[[34m2025-10-04 11:59:58[0m] Step: 332, Training Logs: loss_final: 1.017041, loss_mean: 0.960833, loss_mean_cls: 0.056208, grad_norm: 0.727091
[[34m2025-10-04 11:59:59[0m] Step: 333, Training Logs: loss_final: 1.017956, loss_mean: 0.961653, loss_mean_cls: 0.056303, grad_norm: 0.962668
[[34m2025-10-04 11:59:59[0m] Step: 334, Training Logs: loss_final: 1.012183, loss_mean: 0.955372, loss_mean_cls: 0.056812, grad_norm: 0.781692
[[34m2025-10-04 11:59:59[0m] Step: 335, Training Logs: loss_final: 1.020072, loss_mean: 0.962697, loss_mean_cls: 0.057375, grad_norm: 0.895319
[[34m2025-10-04 11:59:59[0m] Step: 336, Training Logs: loss_final: 1.030388, loss_mean: 0.973848, loss_mean_cls: 0.056540, grad_norm: 1.023325
[[34m2025-10-04 12:00:00[0m] Step: 337, Training Logs: loss_final: 1.007360, loss_mean: 0.949032, loss_mean_cls: 0.058328, grad_norm: 0.989024
[[34m2025-10-04 12:00:00[0m] Step: 338, Training Logs: loss_final: 0.997454, loss_mean: 0.940297, loss_mean_cls: 0.057157, grad_norm: 0.714587
[[34m2025-10-04 12:00:00[0m] Step: 339, Training Logs: loss_final: 1.042962, loss_mean: 0.986831, loss_mean_cls: 0.056131, grad_norm: 0.853947
[[34m2025-10-04 12:00:01[0m] Step: 340, Training Logs: loss_final: 1.012410, loss_mean: 0.955630, loss_mean_cls: 0.056780, grad_norm: 1.113705
[[34m2025-10-04 12:00:01[0m] Step: 341, Training Logs: loss_final: 1.002821, loss_mean: 0.946296, loss_mean_cls: 0.056525, grad_norm: 0.670107
[[34m2025-10-04 12:00:01[0m] Step: 342, Training Logs: loss_final: 0.990770, loss_mean: 0.934778, loss_mean_cls: 0.055992, grad_norm: 0.727091
[[34m2025-10-04 12:00:01[0m] Step: 343, Training Logs: loss_final: 1.019219, loss_mean: 0.962952, loss_mean_cls: 0.056267, grad_norm: 0.697787
[[34m2025-10-04 12:00:02[0m] Step: 344, Training Logs: loss_final: 1.006486, loss_mean: 0.950090, loss_mean_cls: 0.056396, grad_norm: 0.710124
[[34m2025-10-04 12:00:02[0m] Step: 345, Training Logs: loss_final: 0.992759, loss_mean: 0.936511, loss_mean_cls: 0.056248, grad_norm: 0.668432
[[34m2025-10-04 12:00:02[0m] Step: 346, Training Logs: loss_final: 1.021784, loss_mean: 0.965745, loss_mean_cls: 0.056039, grad_norm: 0.674241
[[34m2025-10-04 12:00:03[0m] Step: 347, Training Logs: loss_final: 1.013461, loss_mean: 0.957676, loss_mean_cls: 0.055785, grad_norm: 0.870181
[[34m2025-10-04 12:00:03[0m] Step: 348, Training Logs: loss_final: 1.003865, loss_mean: 0.947640, loss_mean_cls: 0.056225, grad_norm: 0.593953
[[34m2025-10-04 12:00:03[0m] Step: 349, Training Logs: loss_final: 1.011898, loss_mean: 0.955633, loss_mean_cls: 0.056265, grad_norm: 0.770111
[[34m2025-10-04 12:00:04[0m] Step: 350, Training Logs: loss_final: 1.030655, loss_mean: 0.974350, loss_mean_cls: 0.056305, grad_norm: 0.822437
[[34m2025-10-04 12:00:04[0m] Step: 351, Training Logs: loss_final: 1.023380, loss_mean: 0.967276, loss_mean_cls: 0.056104, grad_norm: 0.949585
[[34m2025-10-04 12:00:04[0m] Step: 352, Training Logs: loss_final: 1.030451, loss_mean: 0.973910, loss_mean_cls: 0.056542, grad_norm: 0.939556
[[34m2025-10-04 12:00:04[0m] Step: 353, Training Logs: loss_final: 1.003234, loss_mean: 0.947138, loss_mean_cls: 0.056095, grad_norm: 0.647791
[[34m2025-10-04 12:00:05[0m] Step: 354, Training Logs: loss_final: 1.027248, loss_mean: 0.971350, loss_mean_cls: 0.055899, grad_norm: 0.809754
[[34m2025-10-04 12:00:05[0m] Step: 355, Training Logs: loss_final: 1.030000, loss_mean: 0.974122, loss_mean_cls: 0.055878, grad_norm: 0.868476
[[34m2025-10-04 12:00:05[0m] Step: 356, Training Logs: loss_final: 1.006185, loss_mean: 0.950726, loss_mean_cls: 0.055458, grad_norm: 0.773658
[[34m2025-10-04 12:00:06[0m] Step: 357, Training Logs: loss_final: 0.989470, loss_mean: 0.932569, loss_mean_cls: 0.056900, grad_norm: 1.061478
[[34m2025-10-04 12:00:06[0m] Step: 358, Training Logs: loss_final: 1.039738, loss_mean: 0.985678, loss_mean_cls: 0.054060, grad_norm: 0.916775
[[34m2025-10-04 12:00:06[0m] Step: 359, Training Logs: loss_final: 1.009598, loss_mean: 0.954062, loss_mean_cls: 0.055536, grad_norm: 1.072015
[[34m2025-10-04 12:00:06[0m] Step: 360, Training Logs: loss_final: 1.000474, loss_mean: 0.943805, loss_mean_cls: 0.056670, grad_norm: 1.079769
[[34m2025-10-04 12:00:07[0m] Step: 361, Training Logs: loss_final: 1.001710, loss_mean: 0.945982, loss_mean_cls: 0.055729, grad_norm: 1.026698
[[34m2025-10-04 12:00:07[0m] Step: 362, Training Logs: loss_final: 1.019222, loss_mean: 0.962967, loss_mean_cls: 0.056255, grad_norm: 0.823947
[[34m2025-10-04 12:00:07[0m] Step: 363, Training Logs: loss_final: 0.978830, loss_mean: 0.922125, loss_mean_cls: 0.056705, grad_norm: 0.759089
[[34m2025-10-04 12:00:08[0m] Step: 364, Training Logs: loss_final: 1.023972, loss_mean: 0.968192, loss_mean_cls: 0.055780, grad_norm: 1.014554
[[34m2025-10-04 12:00:08[0m] Step: 365, Training Logs: loss_final: 1.027517, loss_mean: 0.971162, loss_mean_cls: 0.056355, grad_norm: 0.915990
[[34m2025-10-04 12:00:08[0m] Step: 366, Training Logs: loss_final: 0.996688, loss_mean: 0.940544, loss_mean_cls: 0.056145, grad_norm: 0.895093
[[34m2025-10-04 12:00:09[0m] Step: 367, Training Logs: loss_final: 1.000296, loss_mean: 0.942490, loss_mean_cls: 0.057806, grad_norm: 0.804379
[[34m2025-10-04 12:00:09[0m] Step: 368, Training Logs: loss_final: 1.004688, loss_mean: 0.949173, loss_mean_cls: 0.055515, grad_norm: 0.820034
[[34m2025-10-04 12:00:09[0m] Step: 369, Training Logs: loss_final: 1.012252, loss_mean: 0.956468, loss_mean_cls: 0.055784, grad_norm: 0.830876
[[34m2025-10-04 12:00:09[0m] Step: 370, Training Logs: loss_final: 1.013357, loss_mean: 0.958213, loss_mean_cls: 0.055144, grad_norm: 0.933957
[[34m2025-10-04 12:00:10[0m] Step: 371, Training Logs: loss_final: 0.989026, loss_mean: 0.932706, loss_mean_cls: 0.056320, grad_norm: 1.047417
[[34m2025-10-04 12:00:10[0m] Step: 372, Training Logs: loss_final: 1.010681, loss_mean: 0.955098, loss_mean_cls: 0.055583, grad_norm: 1.313547
[[34m2025-10-04 12:00:10[0m] Step: 373, Training Logs: loss_final: 1.006794, loss_mean: 0.949628, loss_mean_cls: 0.057166, grad_norm: 1.350664
[[34m2025-10-04 12:00:11[0m] Step: 374, Training Logs: loss_final: 1.010429, loss_mean: 0.954434, loss_mean_cls: 0.055994, grad_norm: 0.737399
[[34m2025-10-04 12:00:11[0m] Step: 375, Training Logs: loss_final: 1.009079, loss_mean: 0.953585, loss_mean_cls: 0.055494, grad_norm: 1.493816
[[34m2025-10-04 12:00:11[0m] Step: 376, Training Logs: loss_final: 1.003095, loss_mean: 0.947150, loss_mean_cls: 0.055945, grad_norm: 0.864676
[[34m2025-10-04 12:00:11[0m] Step: 377, Training Logs: loss_final: 1.014235, loss_mean: 0.958486, loss_mean_cls: 0.055749, grad_norm: 1.118281
[[34m2025-10-04 12:00:12[0m] Step: 378, Training Logs: loss_final: 1.015619, loss_mean: 0.959227, loss_mean_cls: 0.056392, grad_norm: 0.801415
[[34m2025-10-04 12:00:12[0m] Step: 379, Training Logs: loss_final: 0.997607, loss_mean: 0.941587, loss_mean_cls: 0.056020, grad_norm: 0.956774
[[34m2025-10-04 12:00:12[0m] Step: 380, Training Logs: loss_final: 1.000465, loss_mean: 0.944448, loss_mean_cls: 0.056017, grad_norm: 1.133845
[[34m2025-10-04 12:00:13[0m] Step: 381, Training Logs: loss_final: 1.003357, loss_mean: 0.947993, loss_mean_cls: 0.055364, grad_norm: 0.632612
[[34m2025-10-04 12:00:13[0m] Step: 382, Training Logs: loss_final: 1.018127, loss_mean: 0.961792, loss_mean_cls: 0.056335, grad_norm: 0.897196
[[34m2025-10-04 12:00:13[0m] Step: 383, Training Logs: loss_final: 0.996385, loss_mean: 0.940190, loss_mean_cls: 0.056195, grad_norm: 0.816502
[[34m2025-10-04 12:00:14[0m] Step: 384, Training Logs: loss_final: 1.034108, loss_mean: 0.978310, loss_mean_cls: 0.055798, grad_norm: 0.723791
[[34m2025-10-04 12:00:14[0m] Step: 385, Training Logs: loss_final: 1.033584, loss_mean: 0.977347, loss_mean_cls: 0.056237, grad_norm: 0.924487
[[34m2025-10-04 12:00:14[0m] Step: 386, Training Logs: loss_final: 1.015645, loss_mean: 0.959512, loss_mean_cls: 0.056133, grad_norm: 0.879853
[[34m2025-10-04 12:00:14[0m] Step: 387, Training Logs: loss_final: 1.024759, loss_mean: 0.968850, loss_mean_cls: 0.055908, grad_norm: 0.870078
[[34m2025-10-04 12:00:15[0m] Step: 388, Training Logs: loss_final: 1.033858, loss_mean: 0.979204, loss_mean_cls: 0.054654, grad_norm: 0.843862
[[34m2025-10-04 12:00:15[0m] Step: 389, Training Logs: loss_final: 1.015918, loss_mean: 0.959129, loss_mean_cls: 0.056788, grad_norm: 1.179595
[[34m2025-10-04 12:00:15[0m] Step: 390, Training Logs: loss_final: 1.015237, loss_mean: 0.960166, loss_mean_cls: 0.055071, grad_norm: 1.223019
[[34m2025-10-04 12:00:16[0m] Step: 391, Training Logs: loss_final: 1.011278, loss_mean: 0.955054, loss_mean_cls: 0.056224, grad_norm: 1.008732
[[34m2025-10-04 12:00:16[0m] Step: 392, Training Logs: loss_final: 1.007877, loss_mean: 0.952177, loss_mean_cls: 0.055700, grad_norm: 1.468787
[[34m2025-10-04 12:00:16[0m] Step: 393, Training Logs: loss_final: 1.028091, loss_mean: 0.971670, loss_mean_cls: 0.056422, grad_norm: 1.216125
[[34m2025-10-04 12:00:17[0m] Step: 394, Training Logs: loss_final: 1.005784, loss_mean: 0.950423, loss_mean_cls: 0.055361, grad_norm: 1.221492
[[34m2025-10-04 12:00:17[0m] Step: 395, Training Logs: loss_final: 1.020835, loss_mean: 0.965560, loss_mean_cls: 0.055275, grad_norm: 1.180369
[[34m2025-10-04 12:00:17[0m] Step: 396, Training Logs: loss_final: 1.031556, loss_mean: 0.976386, loss_mean_cls: 0.055170, grad_norm: 1.287244
[[34m2025-10-04 12:00:17[0m] Step: 397, Training Logs: loss_final: 1.006815, loss_mean: 0.951301, loss_mean_cls: 0.055514, grad_norm: 0.711647
[[34m2025-10-04 12:00:18[0m] Step: 398, Training Logs: loss_final: 1.011769, loss_mean: 0.954665, loss_mean_cls: 0.057104, grad_norm: 1.168815
[[34m2025-10-04 12:00:18[0m] Step: 399, Training Logs: loss_final: 1.010131, loss_mean: 0.955937, loss_mean_cls: 0.054194, grad_norm: 0.625627
[[34m2025-10-04 12:00:18[0m] Step: 400, Training Logs: loss_final: 1.011679, loss_mean: 0.957600, loss_mean_cls: 0.054079, grad_norm: 0.962819
[[34m2025-10-04 12:00:19[0m] Step: 401, Training Logs: loss_final: 1.019100, loss_mean: 0.963040, loss_mean_cls: 0.056060, grad_norm: 0.862213
[[34m2025-10-04 12:00:19[0m] Step: 402, Training Logs: loss_final: 1.007013, loss_mean: 0.952415, loss_mean_cls: 0.054598, grad_norm: 0.770093
[[34m2025-10-04 12:00:19[0m] Step: 403, Training Logs: loss_final: 1.024811, loss_mean: 0.970215, loss_mean_cls: 0.054595, grad_norm: 0.622951
[[34m2025-10-04 12:00:19[0m] Step: 404, Training Logs: loss_final: 0.978612, loss_mean: 0.922353, loss_mean_cls: 0.056259, grad_norm: 0.938018
[[34m2025-10-04 12:00:20[0m] Step: 405, Training Logs: loss_final: 0.987597, loss_mean: 0.931907, loss_mean_cls: 0.055690, grad_norm: 1.193448
[[34m2025-10-04 12:00:20[0m] Step: 406, Training Logs: loss_final: 0.983072, loss_mean: 0.927845, loss_mean_cls: 0.055227, grad_norm: 0.837814
[[34m2025-10-04 12:00:20[0m] Step: 407, Training Logs: loss_final: 1.000954, loss_mean: 0.944893, loss_mean_cls: 0.056060, grad_norm: 0.804386
[[34m2025-10-04 12:00:21[0m] Step: 408, Training Logs: loss_final: 1.002301, loss_mean: 0.947588, loss_mean_cls: 0.054714, grad_norm: 0.955987
[[34m2025-10-04 12:00:21[0m] Step: 409, Training Logs: loss_final: 0.986754, loss_mean: 0.931201, loss_mean_cls: 0.055552, grad_norm: 1.193363
[[34m2025-10-04 12:00:21[0m] Step: 410, Training Logs: loss_final: 1.026610, loss_mean: 0.970849, loss_mean_cls: 0.055761, grad_norm: 1.282919
[[34m2025-10-04 12:00:22[0m] Step: 411, Training Logs: loss_final: 1.000803, loss_mean: 0.945889, loss_mean_cls: 0.054914, grad_norm: 0.730243
[[34m2025-10-04 12:00:22[0m] Step: 412, Training Logs: loss_final: 0.985495, loss_mean: 0.928923, loss_mean_cls: 0.056571, grad_norm: 1.318953
[[34m2025-10-04 12:00:22[0m] Step: 413, Training Logs: loss_final: 1.002850, loss_mean: 0.948482, loss_mean_cls: 0.054368, grad_norm: 0.911973
[[34m2025-10-04 12:00:22[0m] Step: 414, Training Logs: loss_final: 1.022554, loss_mean: 0.968386, loss_mean_cls: 0.054168, grad_norm: 0.879483
[[34m2025-10-04 12:00:23[0m] Step: 415, Training Logs: loss_final: 1.025498, loss_mean: 0.970613, loss_mean_cls: 0.054885, grad_norm: 1.148207
[[34m2025-10-04 12:00:23[0m] Step: 416, Training Logs: loss_final: 1.008926, loss_mean: 0.954041, loss_mean_cls: 0.054885, grad_norm: 0.658885
[[34m2025-10-04 12:00:23[0m] Step: 417, Training Logs: loss_final: 1.003184, loss_mean: 0.948576, loss_mean_cls: 0.054608, grad_norm: 0.922058
[[34m2025-10-04 12:00:24[0m] Step: 418, Training Logs: loss_final: 0.987677, loss_mean: 0.932462, loss_mean_cls: 0.055215, grad_norm: 0.857271
[[34m2025-10-04 12:00:24[0m] Step: 419, Training Logs: loss_final: 0.995183, loss_mean: 0.939418, loss_mean_cls: 0.055765, grad_norm: 0.660870
[[34m2025-10-04 12:00:24[0m] Step: 420, Training Logs: loss_final: 1.017854, loss_mean: 0.963506, loss_mean_cls: 0.054348, grad_norm: 1.159370
[[34m2025-10-04 12:00:24[0m] Step: 421, Training Logs: loss_final: 0.964760, loss_mean: 0.909388, loss_mean_cls: 0.055372, grad_norm: 1.007287
[[34m2025-10-04 12:00:25[0m] Step: 422, Training Logs: loss_final: 0.995844, loss_mean: 0.941701, loss_mean_cls: 0.054143, grad_norm: 0.725723
[[34m2025-10-04 12:00:25[0m] Step: 423, Training Logs: loss_final: 0.990591, loss_mean: 0.934518, loss_mean_cls: 0.056073, grad_norm: 0.675674
[[34m2025-10-04 12:00:25[0m] Step: 424, Training Logs: loss_final: 0.993835, loss_mean: 0.939767, loss_mean_cls: 0.054069, grad_norm: 0.697286
[[34m2025-10-04 12:00:26[0m] Step: 425, Training Logs: loss_final: 0.999495, loss_mean: 0.944048, loss_mean_cls: 0.055447, grad_norm: 0.911225
[[34m2025-10-04 12:00:26[0m] Step: 426, Training Logs: loss_final: 0.984709, loss_mean: 0.929703, loss_mean_cls: 0.055006, grad_norm: 1.111531
[[34m2025-10-04 12:00:26[0m] Step: 427, Training Logs: loss_final: 1.006395, loss_mean: 0.950819, loss_mean_cls: 0.055576, grad_norm: 0.869586
[[34m2025-10-04 12:00:26[0m] Step: 428, Training Logs: loss_final: 0.979592, loss_mean: 0.924516, loss_mean_cls: 0.055076, grad_norm: 1.108585
[[34m2025-10-04 12:00:27[0m] Step: 429, Training Logs: loss_final: 0.993751, loss_mean: 0.938265, loss_mean_cls: 0.055485, grad_norm: 0.924058
[[34m2025-10-04 12:00:27[0m] Step: 430, Training Logs: loss_final: 1.007397, loss_mean: 0.953144, loss_mean_cls: 0.054254, grad_norm: 0.788452
[[34m2025-10-04 12:00:27[0m] Step: 431, Training Logs: loss_final: 0.997223, loss_mean: 0.941542, loss_mean_cls: 0.055681, grad_norm: 0.927470
[[34m2025-10-04 12:00:28[0m] Step: 432, Training Logs: loss_final: 0.989193, loss_mean: 0.934087, loss_mean_cls: 0.055106, grad_norm: 0.988340
[[34m2025-10-04 12:00:28[0m] Step: 433, Training Logs: loss_final: 0.995613, loss_mean: 0.940913, loss_mean_cls: 0.054700, grad_norm: 1.040382
[[34m2025-10-04 12:00:28[0m] Step: 434, Training Logs: loss_final: 0.999114, loss_mean: 0.944281, loss_mean_cls: 0.054833, grad_norm: 1.017655
[[34m2025-10-04 12:00:28[0m] Step: 435, Training Logs: loss_final: 1.005615, loss_mean: 0.950825, loss_mean_cls: 0.054790, grad_norm: 0.755184
[[34m2025-10-04 12:00:29[0m] Step: 436, Training Logs: loss_final: 0.986392, loss_mean: 0.929166, loss_mean_cls: 0.057226, grad_norm: 0.959395
[[34m2025-10-04 12:00:29[0m] Step: 437, Training Logs: loss_final: 0.993217, loss_mean: 0.939777, loss_mean_cls: 0.053441, grad_norm: 0.825583
[[34m2025-10-04 12:00:29[0m] Step: 438, Training Logs: loss_final: 1.000774, loss_mean: 0.945150, loss_mean_cls: 0.055624, grad_norm: 1.238045
[[34m2025-10-04 12:00:30[0m] Step: 439, Training Logs: loss_final: 1.006701, loss_mean: 0.952430, loss_mean_cls: 0.054270, grad_norm: 1.170266
[[34m2025-10-04 12:00:30[0m] Step: 440, Training Logs: loss_final: 0.979258, loss_mean: 0.923434, loss_mean_cls: 0.055825, grad_norm: 0.856436
[[34m2025-10-04 12:00:30[0m] Step: 441, Training Logs: loss_final: 0.972088, loss_mean: 0.916473, loss_mean_cls: 0.055615, grad_norm: 1.086883
[[34m2025-10-04 12:00:30[0m] Step: 442, Training Logs: loss_final: 0.994793, loss_mean: 0.939265, loss_mean_cls: 0.055529, grad_norm: 0.990261
[[34m2025-10-04 12:00:31[0m] Step: 443, Training Logs: loss_final: 0.989937, loss_mean: 0.934574, loss_mean_cls: 0.055363, grad_norm: 1.057715
[[34m2025-10-04 12:00:31[0m] Step: 444, Training Logs: loss_final: 1.012712, loss_mean: 0.958835, loss_mean_cls: 0.053877, grad_norm: 0.989066
[[34m2025-10-04 12:00:31[0m] Step: 445, Training Logs: loss_final: 0.995511, loss_mean: 0.940289, loss_mean_cls: 0.055222, grad_norm: 1.077678
[[34m2025-10-04 12:00:32[0m] Step: 446, Training Logs: loss_final: 1.011709, loss_mean: 0.957225, loss_mean_cls: 0.054483, grad_norm: 0.927165
[[34m2025-10-04 12:00:32[0m] Step: 447, Training Logs: loss_final: 1.008272, loss_mean: 0.952860, loss_mean_cls: 0.055412, grad_norm: 0.913883
[[34m2025-10-04 12:00:32[0m] Step: 448, Training Logs: loss_final: 1.004685, loss_mean: 0.949558, loss_mean_cls: 0.055128, grad_norm: 1.095881
[[34m2025-10-04 12:00:33[0m] Step: 449, Training Logs: loss_final: 1.002906, loss_mean: 0.948237, loss_mean_cls: 0.054669, grad_norm: 0.905726
[[34m2025-10-04 12:00:33[0m] Step: 450, Training Logs: loss_final: 1.004995, loss_mean: 0.951234, loss_mean_cls: 0.053762, grad_norm: 0.885046
[[34m2025-10-04 12:00:33[0m] Step: 451, Training Logs: loss_final: 0.991154, loss_mean: 0.936500, loss_mean_cls: 0.054654, grad_norm: 0.919261
[[34m2025-10-04 12:00:33[0m] Step: 452, Training Logs: loss_final: 1.026853, loss_mean: 0.972290, loss_mean_cls: 0.054563, grad_norm: 0.767228
[[34m2025-10-04 12:00:34[0m] Step: 453, Training Logs: loss_final: 0.998238, loss_mean: 0.943752, loss_mean_cls: 0.054486, grad_norm: 1.013422
[[34m2025-10-04 12:00:34[0m] Step: 454, Training Logs: loss_final: 0.987465, loss_mean: 0.933170, loss_mean_cls: 0.054295, grad_norm: 0.770540
[[34m2025-10-04 12:00:34[0m] Step: 455, Training Logs: loss_final: 0.995840, loss_mean: 0.941094, loss_mean_cls: 0.054746, grad_norm: 0.687837
[[34m2025-10-04 12:00:35[0m] Step: 456, Training Logs: loss_final: 1.003476, loss_mean: 0.948893, loss_mean_cls: 0.054583, grad_norm: 0.865557
[[34m2025-10-04 12:00:35[0m] Step: 457, Training Logs: loss_final: 1.014030, loss_mean: 0.959732, loss_mean_cls: 0.054298, grad_norm: 0.984038
[[34m2025-10-04 12:00:35[0m] Step: 458, Training Logs: loss_final: 1.002995, loss_mean: 0.948012, loss_mean_cls: 0.054983, grad_norm: 0.857231
[[34m2025-10-04 12:00:35[0m] Step: 459, Training Logs: loss_final: 0.988443, loss_mean: 0.932348, loss_mean_cls: 0.056095, grad_norm: 0.647200
[[34m2025-10-04 12:00:36[0m] Step: 460, Training Logs: loss_final: 0.998782, loss_mean: 0.944167, loss_mean_cls: 0.054615, grad_norm: 1.050833
[[34m2025-10-04 12:00:36[0m] Step: 461, Training Logs: loss_final: 0.998407, loss_mean: 0.943375, loss_mean_cls: 0.055033, grad_norm: 1.338582
[[34m2025-10-04 12:00:36[0m] Step: 462, Training Logs: loss_final: 0.991443, loss_mean: 0.936050, loss_mean_cls: 0.055393, grad_norm: 0.810606
[[34m2025-10-04 12:00:37[0m] Step: 463, Training Logs: loss_final: 0.996331, loss_mean: 0.942263, loss_mean_cls: 0.054068, grad_norm: 1.209988
[[34m2025-10-04 12:00:37[0m] Step: 464, Training Logs: loss_final: 0.979170, loss_mean: 0.923971, loss_mean_cls: 0.055199, grad_norm: 0.871957
[[34m2025-10-04 12:00:37[0m] Step: 465, Training Logs: loss_final: 0.993433, loss_mean: 0.937918, loss_mean_cls: 0.055515, grad_norm: 1.259403
[[34m2025-10-04 12:00:38[0m] Step: 466, Training Logs: loss_final: 0.990126, loss_mean: 0.935338, loss_mean_cls: 0.054788, grad_norm: 1.059872
[[34m2025-10-04 12:00:38[0m] Step: 467, Training Logs: loss_final: 1.000183, loss_mean: 0.944157, loss_mean_cls: 0.056027, grad_norm: 0.759858
[[34m2025-10-04 12:00:38[0m] Step: 468, Training Logs: loss_final: 0.987979, loss_mean: 0.933146, loss_mean_cls: 0.054833, grad_norm: 1.002604
[[34m2025-10-04 12:00:38[0m] Step: 469, Training Logs: loss_final: 0.996939, loss_mean: 0.942523, loss_mean_cls: 0.054416, grad_norm: 1.214589
[[34m2025-10-04 12:00:39[0m] Step: 470, Training Logs: loss_final: 0.990584, loss_mean: 0.935496, loss_mean_cls: 0.055088, grad_norm: 0.654742
[[34m2025-10-04 12:00:39[0m] Step: 471, Training Logs: loss_final: 1.000215, loss_mean: 0.944981, loss_mean_cls: 0.055233, grad_norm: 1.080070
[[34m2025-10-04 12:00:39[0m] Step: 472, Training Logs: loss_final: 0.971847, loss_mean: 0.916881, loss_mean_cls: 0.054966, grad_norm: 0.709129
[[34m2025-10-04 12:00:40[0m] Step: 473, Training Logs: loss_final: 0.984783, loss_mean: 0.929778, loss_mean_cls: 0.055006, grad_norm: 0.648309
[[34m2025-10-04 12:00:40[0m] Step: 474, Training Logs: loss_final: 1.005942, loss_mean: 0.951939, loss_mean_cls: 0.054004, grad_norm: 0.704776
[[34m2025-10-04 12:00:40[0m] Step: 475, Training Logs: loss_final: 1.008255, loss_mean: 0.953731, loss_mean_cls: 0.054524, grad_norm: 0.750459
[[34m2025-10-04 12:00:40[0m] Step: 476, Training Logs: loss_final: 0.987007, loss_mean: 0.932002, loss_mean_cls: 0.055005, grad_norm: 0.633024
[[34m2025-10-04 12:00:41[0m] Step: 477, Training Logs: loss_final: 0.996081, loss_mean: 0.942009, loss_mean_cls: 0.054072, grad_norm: 0.912140
[[34m2025-10-04 12:00:41[0m] Step: 478, Training Logs: loss_final: 1.003468, loss_mean: 0.949648, loss_mean_cls: 0.053819, grad_norm: 0.948200
[[34m2025-10-04 12:00:41[0m] Step: 479, Training Logs: loss_final: 0.989333, loss_mean: 0.934802, loss_mean_cls: 0.054530, grad_norm: 0.856204
[[34m2025-10-04 12:00:42[0m] Step: 480, Training Logs: loss_final: 1.016146, loss_mean: 0.962288, loss_mean_cls: 0.053858, grad_norm: 1.079028
[[34m2025-10-04 12:00:42[0m] Step: 481, Training Logs: loss_final: 0.987604, loss_mean: 0.933091, loss_mean_cls: 0.054513, grad_norm: 1.199734
[[34m2025-10-04 12:00:42[0m] Step: 482, Training Logs: loss_final: 1.022662, loss_mean: 0.970036, loss_mean_cls: 0.052626, grad_norm: 1.063554
[[34m2025-10-04 12:00:43[0m] Step: 483, Training Logs: loss_final: 1.006252, loss_mean: 0.952739, loss_mean_cls: 0.053513, grad_norm: 0.945146
[[34m2025-10-04 12:00:43[0m] Step: 484, Training Logs: loss_final: 1.022115, loss_mean: 0.966810, loss_mean_cls: 0.055305, grad_norm: 1.588359
[[34m2025-10-04 12:00:43[0m] Step: 485, Training Logs: loss_final: 1.011734, loss_mean: 0.957149, loss_mean_cls: 0.054585, grad_norm: 1.363770
[[34m2025-10-04 12:00:43[0m] Step: 486, Training Logs: loss_final: 0.987845, loss_mean: 0.933396, loss_mean_cls: 0.054449, grad_norm: 1.080097
[[34m2025-10-04 12:00:44[0m] Step: 487, Training Logs: loss_final: 1.008948, loss_mean: 0.954830, loss_mean_cls: 0.054118, grad_norm: 1.096223
[[34m2025-10-04 12:00:44[0m] Step: 488, Training Logs: loss_final: 1.000842, loss_mean: 0.945284, loss_mean_cls: 0.055558, grad_norm: 0.942002
[[34m2025-10-04 12:00:44[0m] Step: 489, Training Logs: loss_final: 0.999237, loss_mean: 0.944043, loss_mean_cls: 0.055195, grad_norm: 1.017520
[[34m2025-10-04 12:00:45[0m] Step: 490, Training Logs: loss_final: 0.992415, loss_mean: 0.937684, loss_mean_cls: 0.054731, grad_norm: 0.830637
[[34m2025-10-04 12:00:45[0m] Step: 491, Training Logs: loss_final: 0.970360, loss_mean: 0.915000, loss_mean_cls: 0.055360, grad_norm: 0.886243
[[34m2025-10-04 12:00:45[0m] Step: 492, Training Logs: loss_final: 0.996066, loss_mean: 0.942019, loss_mean_cls: 0.054047, grad_norm: 1.317507
[[34m2025-10-04 12:00:46[0m] Step: 493, Training Logs: loss_final: 0.996952, loss_mean: 0.942120, loss_mean_cls: 0.054832, grad_norm: 1.280403
[[34m2025-10-04 12:00:46[0m] Step: 494, Training Logs: loss_final: 1.009964, loss_mean: 0.955285, loss_mean_cls: 0.054678, grad_norm: 1.049575
[[34m2025-10-04 12:00:46[0m] Step: 495, Training Logs: loss_final: 0.999082, loss_mean: 0.944110, loss_mean_cls: 0.054972, grad_norm: 1.017423
[[34m2025-10-04 12:00:46[0m] Step: 496, Training Logs: loss_final: 0.978462, loss_mean: 0.925393, loss_mean_cls: 0.053069, grad_norm: 0.746040
[[34m2025-10-04 12:00:47[0m] Step: 497, Training Logs: loss_final: 0.990604, loss_mean: 0.936985, loss_mean_cls: 0.053618, grad_norm: 1.027572
[[34m2025-10-04 12:00:47[0m] Step: 498, Training Logs: loss_final: 0.980911, loss_mean: 0.925600, loss_mean_cls: 0.055311, grad_norm: 0.849716
[[34m2025-10-04 12:00:47[0m] Step: 499, Training Logs: loss_final: 0.974514, loss_mean: 0.919706, loss_mean_cls: 0.054809, grad_norm: 0.895420
[[34m2025-10-04 12:00:48[0m] Step: 500, Training Logs: loss_final: 0.986058, loss_mean: 0.931212, loss_mean_cls: 0.054846, grad_norm: 0.865233
[[34m2025-10-04 12:00:48[0m] Step: 501, Training Logs: loss_final: 0.991593, loss_mean: 0.939107, loss_mean_cls: 0.052486, grad_norm: 0.847381
[[34m2025-10-04 12:00:48[0m] Step: 502, Training Logs: loss_final: 0.991013, loss_mean: 0.935982, loss_mean_cls: 0.055030, grad_norm: 0.768402
[[34m2025-10-04 12:00:48[0m] Step: 503, Training Logs: loss_final: 1.003642, loss_mean: 0.948265, loss_mean_cls: 0.055376, grad_norm: 1.050453
[[34m2025-10-04 12:00:49[0m] Step: 504, Training Logs: loss_final: 0.959056, loss_mean: 0.904598, loss_mean_cls: 0.054458, grad_norm: 0.877419
[[34m2025-10-04 12:00:49[0m] Step: 505, Training Logs: loss_final: 0.968382, loss_mean: 0.914410, loss_mean_cls: 0.053972, grad_norm: 0.747306
[[34m2025-10-04 12:00:49[0m] Step: 506, Training Logs: loss_final: 1.001376, loss_mean: 0.945980, loss_mean_cls: 0.055396, grad_norm: 0.689536
[[34m2025-10-04 12:00:50[0m] Step: 507, Training Logs: loss_final: 0.981100, loss_mean: 0.927362, loss_mean_cls: 0.053738, grad_norm: 0.599909
[[34m2025-10-04 12:00:50[0m] Step: 508, Training Logs: loss_final: 0.971605, loss_mean: 0.916467, loss_mean_cls: 0.055138, grad_norm: 0.687968
[[34m2025-10-04 12:00:50[0m] Step: 509, Training Logs: loss_final: 0.994076, loss_mean: 0.939242, loss_mean_cls: 0.054834, grad_norm: 0.841816
[[34m2025-10-04 12:00:50[0m] Step: 510, Training Logs: loss_final: 0.991164, loss_mean: 0.936995, loss_mean_cls: 0.054169, grad_norm: 0.715230
[[34m2025-10-04 12:00:51[0m] Step: 511, Training Logs: loss_final: 0.968390, loss_mean: 0.913573, loss_mean_cls: 0.054817, grad_norm: 0.826192
[[34m2025-10-04 12:00:51[0m] Step: 512, Training Logs: loss_final: 0.970056, loss_mean: 0.915950, loss_mean_cls: 0.054106, grad_norm: 0.851192
[[34m2025-10-04 12:00:51[0m] Step: 513, Training Logs: loss_final: 1.008229, loss_mean: 0.954650, loss_mean_cls: 0.053579, grad_norm: 0.926745
[[34m2025-10-04 12:00:52[0m] Step: 514, Training Logs: loss_final: 0.987310, loss_mean: 0.932575, loss_mean_cls: 0.054735, grad_norm: 0.726518
[[34m2025-10-04 12:00:52[0m] Step: 515, Training Logs: loss_final: 0.991054, loss_mean: 0.936149, loss_mean_cls: 0.054904, grad_norm: 0.922199
[[34m2025-10-04 12:00:52[0m] Step: 516, Training Logs: loss_final: 0.993860, loss_mean: 0.938766, loss_mean_cls: 0.055094, grad_norm: 0.957317
[[34m2025-10-04 12:00:53[0m] Step: 517, Training Logs: loss_final: 0.988181, loss_mean: 0.933872, loss_mean_cls: 0.054309, grad_norm: 0.855280
[[34m2025-10-04 12:00:53[0m] Step: 518, Training Logs: loss_final: 0.983174, loss_mean: 0.929785, loss_mean_cls: 0.053389, grad_norm: 0.807987
[[34m2025-10-04 12:00:53[0m] Step: 519, Training Logs: loss_final: 1.000734, loss_mean: 0.947956, loss_mean_cls: 0.052779, grad_norm: 1.038084
[[34m2025-10-04 12:00:53[0m] Step: 520, Training Logs: loss_final: 0.997998, loss_mean: 0.944368, loss_mean_cls: 0.053630, grad_norm: 0.745950
[[34m2025-10-04 12:00:54[0m] Step: 521, Training Logs: loss_final: 0.997011, loss_mean: 0.944802, loss_mean_cls: 0.052209, grad_norm: 0.644181
[[34m2025-10-04 12:00:54[0m] Step: 522, Training Logs: loss_final: 1.011762, loss_mean: 0.956705, loss_mean_cls: 0.055057, grad_norm: 0.940572
[[34m2025-10-04 12:00:54[0m] Step: 523, Training Logs: loss_final: 0.997351, loss_mean: 0.944456, loss_mean_cls: 0.052895, grad_norm: 0.716175
[[34m2025-10-04 12:00:55[0m] Step: 524, Training Logs: loss_final: 1.005644, loss_mean: 0.951087, loss_mean_cls: 0.054557, grad_norm: 0.909430
[[34m2025-10-04 12:00:55[0m] Step: 525, Training Logs: loss_final: 0.973045, loss_mean: 0.918732, loss_mean_cls: 0.054313, grad_norm: 1.059683
[[34m2025-10-04 12:00:55[0m] Step: 526, Training Logs: loss_final: 0.959253, loss_mean: 0.905856, loss_mean_cls: 0.053397, grad_norm: 1.084696
[[34m2025-10-04 12:00:55[0m] Step: 527, Training Logs: loss_final: 1.002514, loss_mean: 0.949213, loss_mean_cls: 0.053302, grad_norm: 0.868751
[[34m2025-10-04 12:00:56[0m] Step: 528, Training Logs: loss_final: 0.963379, loss_mean: 0.909904, loss_mean_cls: 0.053476, grad_norm: 0.913590
[[34m2025-10-04 12:00:56[0m] Step: 529, Training Logs: loss_final: 0.980821, loss_mean: 0.925604, loss_mean_cls: 0.055217, grad_norm: 1.164788
[[34m2025-10-04 12:00:56[0m] Step: 530, Training Logs: loss_final: 0.961861, loss_mean: 0.906336, loss_mean_cls: 0.055525, grad_norm: 1.369612
[[34m2025-10-04 12:00:57[0m] Step: 531, Training Logs: loss_final: 0.974848, loss_mean: 0.921076, loss_mean_cls: 0.053772, grad_norm: 0.974419
[[34m2025-10-04 12:00:57[0m] Step: 532, Training Logs: loss_final: 0.994899, loss_mean: 0.941147, loss_mean_cls: 0.053752, grad_norm: 0.826493
[[34m2025-10-04 12:00:57[0m] Step: 533, Training Logs: loss_final: 1.005734, loss_mean: 0.951745, loss_mean_cls: 0.053989, grad_norm: 1.110824
[[34m2025-10-04 12:00:57[0m] Step: 534, Training Logs: loss_final: 0.980358, loss_mean: 0.925976, loss_mean_cls: 0.054382, grad_norm: 0.796904
[[34m2025-10-04 12:00:58[0m] Step: 535, Training Logs: loss_final: 0.995208, loss_mean: 0.939577, loss_mean_cls: 0.055632, grad_norm: 0.934465
[[34m2025-10-04 12:00:58[0m] Step: 536, Training Logs: loss_final: 0.981790, loss_mean: 0.928030, loss_mean_cls: 0.053761, grad_norm: 0.924943
[[34m2025-10-04 12:00:58[0m] Step: 537, Training Logs: loss_final: 1.004477, loss_mean: 0.950191, loss_mean_cls: 0.054285, grad_norm: 0.907178
[[34m2025-10-04 12:00:59[0m] Step: 538, Training Logs: loss_final: 0.984933, loss_mean: 0.930674, loss_mean_cls: 0.054259, grad_norm: 1.045782
[[34m2025-10-04 12:00:59[0m] Step: 539, Training Logs: loss_final: 0.995701, loss_mean: 0.942388, loss_mean_cls: 0.053313, grad_norm: 0.699608
[[34m2025-10-04 12:00:59[0m] Step: 540, Training Logs: loss_final: 0.989849, loss_mean: 0.935808, loss_mean_cls: 0.054042, grad_norm: 0.911029
[[34m2025-10-04 12:01:00[0m] Step: 541, Training Logs: loss_final: 0.975565, loss_mean: 0.921534, loss_mean_cls: 0.054031, grad_norm: 0.913433
[[34m2025-10-04 12:01:00[0m] Step: 542, Training Logs: loss_final: 1.018245, loss_mean: 0.963800, loss_mean_cls: 0.054444, grad_norm: 0.936662
[[34m2025-10-04 12:01:00[0m] Step: 543, Training Logs: loss_final: 0.974255, loss_mean: 0.919980, loss_mean_cls: 0.054276, grad_norm: 0.895698
[[34m2025-10-04 12:01:00[0m] Step: 544, Training Logs: loss_final: 1.002212, loss_mean: 0.948945, loss_mean_cls: 0.053267, grad_norm: 0.921276
[[34m2025-10-04 12:01:01[0m] Step: 545, Training Logs: loss_final: 1.001756, loss_mean: 0.947494, loss_mean_cls: 0.054262, grad_norm: 0.873632
[[34m2025-10-04 12:01:01[0m] Step: 546, Training Logs: loss_final: 0.979096, loss_mean: 0.925980, loss_mean_cls: 0.053116, grad_norm: 0.703369
[[34m2025-10-04 12:01:01[0m] Step: 547, Training Logs: loss_final: 0.973249, loss_mean: 0.920241, loss_mean_cls: 0.053008, grad_norm: 0.861590
[[34m2025-10-04 12:01:02[0m] Step: 548, Training Logs: loss_final: 0.964863, loss_mean: 0.910865, loss_mean_cls: 0.053997, grad_norm: 0.881582
[[34m2025-10-04 12:01:02[0m] Step: 549, Training Logs: loss_final: 1.003454, loss_mean: 0.949887, loss_mean_cls: 0.053567, grad_norm: 0.663728
[[34m2025-10-04 12:01:02[0m] Step: 550, Training Logs: loss_final: 0.974991, loss_mean: 0.921052, loss_mean_cls: 0.053939, grad_norm: 0.770830
[[34m2025-10-04 12:01:03[0m] Step: 551, Training Logs: loss_final: 1.010335, loss_mean: 0.956188, loss_mean_cls: 0.054147, grad_norm: 1.028425
[[34m2025-10-04 12:01:03[0m] Step: 552, Training Logs: loss_final: 0.965281, loss_mean: 0.911280, loss_mean_cls: 0.054001, grad_norm: 0.944879
[[34m2025-10-04 12:01:03[0m] Step: 553, Training Logs: loss_final: 0.990795, loss_mean: 0.937236, loss_mean_cls: 0.053559, grad_norm: 0.896866
[[34m2025-10-04 12:01:03[0m] Step: 554, Training Logs: loss_final: 0.980882, loss_mean: 0.927205, loss_mean_cls: 0.053677, grad_norm: 1.128723
[[34m2025-10-04 12:01:04[0m] Step: 555, Training Logs: loss_final: 0.977680, loss_mean: 0.923543, loss_mean_cls: 0.054138, grad_norm: 0.985705
[[34m2025-10-04 12:01:04[0m] Step: 556, Training Logs: loss_final: 0.993574, loss_mean: 0.938991, loss_mean_cls: 0.054583, grad_norm: 0.875481
[[34m2025-10-04 12:01:04[0m] Step: 557, Training Logs: loss_final: 0.982534, loss_mean: 0.929274, loss_mean_cls: 0.053260, grad_norm: 0.754565
[[34m2025-10-04 12:01:05[0m] Step: 558, Training Logs: loss_final: 1.000600, loss_mean: 0.947927, loss_mean_cls: 0.052674, grad_norm: 0.782882
[[34m2025-10-04 12:01:05[0m] Step: 559, Training Logs: loss_final: 0.998924, loss_mean: 0.946478, loss_mean_cls: 0.052446, grad_norm: 0.792628
[[34m2025-10-04 12:01:05[0m] Step: 560, Training Logs: loss_final: 0.991673, loss_mean: 0.938618, loss_mean_cls: 0.053055, grad_norm: 0.856975
[[34m2025-10-04 12:01:05[0m] Step: 561, Training Logs: loss_final: 0.985878, loss_mean: 0.931853, loss_mean_cls: 0.054025, grad_norm: 0.739940
[[34m2025-10-04 12:01:06[0m] Step: 562, Training Logs: loss_final: 0.969387, loss_mean: 0.915793, loss_mean_cls: 0.053594, grad_norm: 0.885723
[[34m2025-10-04 12:01:06[0m] Step: 563, Training Logs: loss_final: 0.994261, loss_mean: 0.942149, loss_mean_cls: 0.052112, grad_norm: 0.929412
[[34m2025-10-04 12:01:06[0m] Step: 564, Training Logs: loss_final: 0.992740, loss_mean: 0.938077, loss_mean_cls: 0.054663, grad_norm: 0.830403
[[34m2025-10-04 12:01:07[0m] Step: 565, Training Logs: loss_final: 0.994293, loss_mean: 0.940567, loss_mean_cls: 0.053726, grad_norm: 0.565613
[[34m2025-10-04 12:01:07[0m] Step: 566, Training Logs: loss_final: 0.961615, loss_mean: 0.908284, loss_mean_cls: 0.053330, grad_norm: 0.872983
[[34m2025-10-04 12:01:07[0m] Step: 567, Training Logs: loss_final: 0.987344, loss_mean: 0.932606, loss_mean_cls: 0.054738, grad_norm: 0.673015
[[34m2025-10-04 12:01:08[0m] Step: 568, Training Logs: loss_final: 0.991988, loss_mean: 0.939668, loss_mean_cls: 0.052320, grad_norm: 1.026703
[[34m2025-10-04 12:01:08[0m] Step: 569, Training Logs: loss_final: 0.984886, loss_mean: 0.931311, loss_mean_cls: 0.053576, grad_norm: 0.769391
[[34m2025-10-04 12:01:08[0m] Step: 570, Training Logs: loss_final: 0.980186, loss_mean: 0.927497, loss_mean_cls: 0.052689, grad_norm: 0.925735
[[34m2025-10-04 12:01:08[0m] Step: 571, Training Logs: loss_final: 0.992060, loss_mean: 0.938856, loss_mean_cls: 0.053203, grad_norm: 0.990612
[[34m2025-10-04 12:01:09[0m] Step: 572, Training Logs: loss_final: 0.989363, loss_mean: 0.935935, loss_mean_cls: 0.053428, grad_norm: 1.157046
[[34m2025-10-04 12:01:09[0m] Step: 573, Training Logs: loss_final: 1.001754, loss_mean: 0.949422, loss_mean_cls: 0.052332, grad_norm: 0.849727
[[34m2025-10-04 12:01:09[0m] Step: 574, Training Logs: loss_final: 0.980400, loss_mean: 0.926626, loss_mean_cls: 0.053774, grad_norm: 1.366369
[[34m2025-10-04 12:01:10[0m] Step: 575, Training Logs: loss_final: 0.979004, loss_mean: 0.925289, loss_mean_cls: 0.053715, grad_norm: 0.901814
[[34m2025-10-04 12:01:10[0m] Step: 576, Training Logs: loss_final: 0.996702, loss_mean: 0.943603, loss_mean_cls: 0.053099, grad_norm: 1.328864
[[34m2025-10-04 12:01:10[0m] Step: 577, Training Logs: loss_final: 1.010773, loss_mean: 0.958760, loss_mean_cls: 0.052013, grad_norm: 1.078045
[[34m2025-10-04 12:01:10[0m] Step: 578, Training Logs: loss_final: 1.001675, loss_mean: 0.947973, loss_mean_cls: 0.053702, grad_norm: 1.067576
[[34m2025-10-04 12:01:11[0m] Step: 579, Training Logs: loss_final: 0.996301, loss_mean: 0.942468, loss_mean_cls: 0.053833, grad_norm: 1.273611
[[34m2025-10-04 12:01:11[0m] Step: 580, Training Logs: loss_final: 0.982963, loss_mean: 0.929303, loss_mean_cls: 0.053660, grad_norm: 0.932373
[[34m2025-10-04 12:01:11[0m] Step: 581, Training Logs: loss_final: 0.957495, loss_mean: 0.902788, loss_mean_cls: 0.054707, grad_norm: 1.169626
[[34m2025-10-04 12:01:12[0m] Step: 582, Training Logs: loss_final: 0.989791, loss_mean: 0.937410, loss_mean_cls: 0.052381, grad_norm: 0.908793
[[34m2025-10-04 12:01:12[0m] Step: 583, Training Logs: loss_final: 0.986052, loss_mean: 0.932556, loss_mean_cls: 0.053496, grad_norm: 0.986844
[[34m2025-10-04 12:01:12[0m] Step: 584, Training Logs: loss_final: 0.979230, loss_mean: 0.925045, loss_mean_cls: 0.054184, grad_norm: 1.029321
[[34m2025-10-04 12:01:13[0m] Step: 585, Training Logs: loss_final: 0.997848, loss_mean: 0.943585, loss_mean_cls: 0.054263, grad_norm: 0.627264
[[34m2025-10-04 12:01:13[0m] Step: 586, Training Logs: loss_final: 1.004995, loss_mean: 0.951668, loss_mean_cls: 0.053326, grad_norm: 0.921223
[[34m2025-10-04 12:01:13[0m] Step: 587, Training Logs: loss_final: 0.995342, loss_mean: 0.941398, loss_mean_cls: 0.053943, grad_norm: 1.037146
[[34m2025-10-04 12:01:13[0m] Step: 588, Training Logs: loss_final: 0.970393, loss_mean: 0.915740, loss_mean_cls: 0.054653, grad_norm: 0.933025
[[34m2025-10-04 12:01:14[0m] Step: 589, Training Logs: loss_final: 0.988966, loss_mean: 0.936780, loss_mean_cls: 0.052185, grad_norm: 0.871867
[[34m2025-10-04 12:01:14[0m] Step: 590, Training Logs: loss_final: 0.973206, loss_mean: 0.919281, loss_mean_cls: 0.053925, grad_norm: 0.866129
[[34m2025-10-04 12:01:14[0m] Step: 591, Training Logs: loss_final: 0.998556, loss_mean: 0.945044, loss_mean_cls: 0.053512, grad_norm: 1.012362
[[34m2025-10-04 12:01:15[0m] Step: 592, Training Logs: loss_final: 0.982907, loss_mean: 0.928934, loss_mean_cls: 0.053972, grad_norm: 0.626959
[[34m2025-10-04 12:01:15[0m] Step: 593, Training Logs: loss_final: 0.983390, loss_mean: 0.928883, loss_mean_cls: 0.054507, grad_norm: 0.725376
[[34m2025-10-04 12:01:15[0m] Step: 594, Training Logs: loss_final: 0.974139, loss_mean: 0.920049, loss_mean_cls: 0.054090, grad_norm: 0.572389
[[34m2025-10-04 12:01:15[0m] Step: 595, Training Logs: loss_final: 0.959054, loss_mean: 0.905541, loss_mean_cls: 0.053513, grad_norm: 0.805328
[[34m2025-10-04 12:01:16[0m] Step: 596, Training Logs: loss_final: 0.986410, loss_mean: 0.934753, loss_mean_cls: 0.051658, grad_norm: 0.702381
[[34m2025-10-04 12:01:16[0m] Step: 597, Training Logs: loss_final: 0.988200, loss_mean: 0.934375, loss_mean_cls: 0.053825, grad_norm: 1.376437
[[34m2025-10-04 12:01:16[0m] Step: 598, Training Logs: loss_final: 0.972168, loss_mean: 0.919080, loss_mean_cls: 0.053088, grad_norm: 0.839867
[[34m2025-10-04 12:01:17[0m] Step: 599, Training Logs: loss_final: 0.995490, loss_mean: 0.942975, loss_mean_cls: 0.052515, grad_norm: 1.064027
[[34m2025-10-04 12:01:17[0m] Step: 600, Training Logs: loss_final: 0.951409, loss_mean: 0.897207, loss_mean_cls: 0.054203, grad_norm: 1.050509
[[34m2025-10-04 12:01:17[0m] Step: 601, Training Logs: loss_final: 0.985655, loss_mean: 0.934107, loss_mean_cls: 0.051548, grad_norm: 1.009597
[[34m2025-10-04 12:01:18[0m] Step: 602, Training Logs: loss_final: 0.974501, loss_mean: 0.920067, loss_mean_cls: 0.054433, grad_norm: 1.099109
[[34m2025-10-04 12:01:18[0m] Step: 603, Training Logs: loss_final: 0.978082, loss_mean: 0.925303, loss_mean_cls: 0.052779, grad_norm: 0.905367
[[34m2025-10-04 12:01:18[0m] Step: 604, Training Logs: loss_final: 1.013389, loss_mean: 0.961599, loss_mean_cls: 0.051789, grad_norm: 1.067720
[[34m2025-10-04 12:01:19[0m] Step: 605, Training Logs: loss_final: 0.981424, loss_mean: 0.928174, loss_mean_cls: 0.053251, grad_norm: 0.786584
[[34m2025-10-04 12:01:19[0m] Step: 606, Training Logs: loss_final: 0.950801, loss_mean: 0.897298, loss_mean_cls: 0.053503, grad_norm: 0.860505
[[34m2025-10-04 12:01:19[0m] Step: 607, Training Logs: loss_final: 0.992037, loss_mean: 0.938331, loss_mean_cls: 0.053706, grad_norm: 0.955718
[[34m2025-10-04 12:01:19[0m] Step: 608, Training Logs: loss_final: 0.968819, loss_mean: 0.915315, loss_mean_cls: 0.053504, grad_norm: 0.703996
[[34m2025-10-04 12:01:20[0m] Step: 609, Training Logs: loss_final: 0.985377, loss_mean: 0.932836, loss_mean_cls: 0.052541, grad_norm: 0.820067
[[34m2025-10-04 12:01:20[0m] Step: 610, Training Logs: loss_final: 0.981164, loss_mean: 0.927087, loss_mean_cls: 0.054077, grad_norm: 0.657678
[[34m2025-10-04 12:01:20[0m] Step: 611, Training Logs: loss_final: 0.980346, loss_mean: 0.927160, loss_mean_cls: 0.053186, grad_norm: 0.809386
[[34m2025-10-04 12:01:21[0m] Step: 612, Training Logs: loss_final: 0.992562, loss_mean: 0.940733, loss_mean_cls: 0.051830, grad_norm: 0.808507
[[34m2025-10-04 12:01:21[0m] Step: 613, Training Logs: loss_final: 0.992437, loss_mean: 0.939173, loss_mean_cls: 0.053265, grad_norm: 0.803922
[[34m2025-10-04 12:01:21[0m] Step: 614, Training Logs: loss_final: 0.983601, loss_mean: 0.931496, loss_mean_cls: 0.052105, grad_norm: 0.763974
[[34m2025-10-04 12:01:21[0m] Step: 615, Training Logs: loss_final: 1.008889, loss_mean: 0.956308, loss_mean_cls: 0.052580, grad_norm: 0.781672
[[34m2025-10-04 12:01:22[0m] Step: 616, Training Logs: loss_final: 0.983065, loss_mean: 0.929853, loss_mean_cls: 0.053213, grad_norm: 0.756252
[[34m2025-10-04 12:01:22[0m] Step: 617, Training Logs: loss_final: 0.995046, loss_mean: 0.942011, loss_mean_cls: 0.053035, grad_norm: 0.814620
[[34m2025-10-04 12:01:22[0m] Step: 618, Training Logs: loss_final: 0.984749, loss_mean: 0.931723, loss_mean_cls: 0.053026, grad_norm: 1.010564
[[34m2025-10-04 12:01:23[0m] Step: 619, Training Logs: loss_final: 0.959958, loss_mean: 0.905895, loss_mean_cls: 0.054062, grad_norm: 0.639704
[[34m2025-10-04 12:01:23[0m] Step: 620, Training Logs: loss_final: 0.973835, loss_mean: 0.920499, loss_mean_cls: 0.053336, grad_norm: 0.961852
[[34m2025-10-04 12:01:23[0m] Step: 621, Training Logs: loss_final: 0.986593, loss_mean: 0.933399, loss_mean_cls: 0.053194, grad_norm: 1.071848
[[34m2025-10-04 12:01:23[0m] Step: 622, Training Logs: loss_final: 0.974018, loss_mean: 0.920603, loss_mean_cls: 0.053416, grad_norm: 1.010227
[[34m2025-10-04 12:01:24[0m] Step: 623, Training Logs: loss_final: 0.985503, loss_mean: 0.932636, loss_mean_cls: 0.052867, grad_norm: 0.780205
[[34m2025-10-04 12:01:24[0m] Step: 624, Training Logs: loss_final: 0.961289, loss_mean: 0.909798, loss_mean_cls: 0.051491, grad_norm: 1.030632
[[34m2025-10-04 12:01:24[0m] Step: 625, Training Logs: loss_final: 0.979538, loss_mean: 0.925779, loss_mean_cls: 0.053759, grad_norm: 0.885046
[[34m2025-10-04 12:01:25[0m] Step: 626, Training Logs: loss_final: 0.963253, loss_mean: 0.909140, loss_mean_cls: 0.054112, grad_norm: 0.735960
[[34m2025-10-04 12:01:25[0m] Step: 627, Training Logs: loss_final: 0.960829, loss_mean: 0.906679, loss_mean_cls: 0.054150, grad_norm: 0.817122
[[34m2025-10-04 12:01:25[0m] Step: 628, Training Logs: loss_final: 0.988322, loss_mean: 0.934607, loss_mean_cls: 0.053715, grad_norm: 1.030184
[[34m2025-10-04 12:01:25[0m] Step: 629, Training Logs: loss_final: 0.989154, loss_mean: 0.935378, loss_mean_cls: 0.053776, grad_norm: 0.863831
[[34m2025-10-04 12:01:26[0m] Step: 630, Training Logs: loss_final: 0.990678, loss_mean: 0.939555, loss_mean_cls: 0.051123, grad_norm: 0.670397
[[34m2025-10-04 12:01:26[0m] Step: 631, Training Logs: loss_final: 0.970550, loss_mean: 0.916055, loss_mean_cls: 0.054495, grad_norm: 0.842277
[[34m2025-10-04 12:01:26[0m] Step: 632, Training Logs: loss_final: 0.985116, loss_mean: 0.931824, loss_mean_cls: 0.053293, grad_norm: 0.794634
[[34m2025-10-04 12:01:27[0m] Step: 633, Training Logs: loss_final: 0.977029, loss_mean: 0.924301, loss_mean_cls: 0.052728, grad_norm: 0.696938
[[34m2025-10-04 12:01:27[0m] Step: 634, Training Logs: loss_final: 0.966891, loss_mean: 0.913038, loss_mean_cls: 0.053852, grad_norm: 0.840819
[[34m2025-10-04 12:01:27[0m] Step: 635, Training Logs: loss_final: 0.978255, loss_mean: 0.925326, loss_mean_cls: 0.052929, grad_norm: 0.684833
[[34m2025-10-04 12:01:27[0m] Step: 636, Training Logs: loss_final: 0.997306, loss_mean: 0.944979, loss_mean_cls: 0.052327, grad_norm: 0.569814
[[34m2025-10-04 12:01:28[0m] Step: 637, Training Logs: loss_final: 0.986003, loss_mean: 0.933962, loss_mean_cls: 0.052041, grad_norm: 0.629132
[[34m2025-10-04 12:01:28[0m] Step: 638, Training Logs: loss_final: 0.977986, loss_mean: 0.924568, loss_mean_cls: 0.053418, grad_norm: 0.834447
[[34m2025-10-04 12:01:28[0m] Step: 639, Training Logs: loss_final: 0.974248, loss_mean: 0.921593, loss_mean_cls: 0.052655, grad_norm: 0.815015
[[34m2025-10-04 12:01:29[0m] Step: 640, Training Logs: loss_final: 0.979081, loss_mean: 0.925899, loss_mean_cls: 0.053183, grad_norm: 0.867944
[[34m2025-10-04 12:01:29[0m] Step: 641, Training Logs: loss_final: 0.973480, loss_mean: 0.921997, loss_mean_cls: 0.051482, grad_norm: 0.837682
[[34m2025-10-04 12:01:29[0m] Step: 642, Training Logs: loss_final: 0.981054, loss_mean: 0.927696, loss_mean_cls: 0.053358, grad_norm: 1.010673
[[34m2025-10-04 12:01:29[0m] Step: 643, Training Logs: loss_final: 0.964233, loss_mean: 0.912096, loss_mean_cls: 0.052138, grad_norm: 0.913894
[[34m2025-10-04 12:01:30[0m] Step: 644, Training Logs: loss_final: 0.966652, loss_mean: 0.913329, loss_mean_cls: 0.053322, grad_norm: 0.963048
[[34m2025-10-04 12:01:30[0m] Step: 645, Training Logs: loss_final: 0.985065, loss_mean: 0.932924, loss_mean_cls: 0.052141, grad_norm: 0.492808
[[34m2025-10-04 12:01:30[0m] Step: 646, Training Logs: loss_final: 1.007385, loss_mean: 0.954281, loss_mean_cls: 0.053104, grad_norm: 0.913238
[[34m2025-10-04 12:01:31[0m] Step: 647, Training Logs: loss_final: 0.960099, loss_mean: 0.907860, loss_mean_cls: 0.052239, grad_norm: 0.757487
[[34m2025-10-04 12:01:31[0m] Step: 648, Training Logs: loss_final: 0.960651, loss_mean: 0.907285, loss_mean_cls: 0.053366, grad_norm: 0.841861
[[34m2025-10-04 12:01:31[0m] Step: 649, Training Logs: loss_final: 0.975815, loss_mean: 0.922689, loss_mean_cls: 0.053126, grad_norm: 1.250819
[[34m2025-10-04 12:01:31[0m] Step: 650, Training Logs: loss_final: 0.985691, loss_mean: 0.933202, loss_mean_cls: 0.052489, grad_norm: 0.703883
[[34m2025-10-04 12:01:32[0m] Step: 651, Training Logs: loss_final: 0.968969, loss_mean: 0.915429, loss_mean_cls: 0.053540, grad_norm: 1.221732
[[34m2025-10-04 12:01:32[0m] Step: 652, Training Logs: loss_final: 0.987949, loss_mean: 0.934046, loss_mean_cls: 0.053902, grad_norm: 1.026252
[[34m2025-10-04 12:01:32[0m] Step: 653, Training Logs: loss_final: 0.985123, loss_mean: 0.932587, loss_mean_cls: 0.052537, grad_norm: 0.995581
[[34m2025-10-04 12:01:33[0m] Step: 654, Training Logs: loss_final: 0.974954, loss_mean: 0.921914, loss_mean_cls: 0.053040, grad_norm: 1.067892
[[34m2025-10-04 12:01:33[0m] Step: 655, Training Logs: loss_final: 0.990792, loss_mean: 0.937470, loss_mean_cls: 0.053321, grad_norm: 0.856675
[[34m2025-10-04 12:01:33[0m] Step: 656, Training Logs: loss_final: 0.988475, loss_mean: 0.935093, loss_mean_cls: 0.053382, grad_norm: 0.901040
[[34m2025-10-04 12:01:34[0m] Step: 657, Training Logs: loss_final: 0.986314, loss_mean: 0.934278, loss_mean_cls: 0.052035, grad_norm: 1.162784
[[34m2025-10-04 12:01:34[0m] Step: 658, Training Logs: loss_final: 0.981315, loss_mean: 0.929314, loss_mean_cls: 0.052000, grad_norm: 0.911981
[[34m2025-10-04 12:01:34[0m] Step: 659, Training Logs: loss_final: 0.988230, loss_mean: 0.934154, loss_mean_cls: 0.054076, grad_norm: 1.196713
[[34m2025-10-04 12:01:34[0m] Step: 660, Training Logs: loss_final: 0.998024, loss_mean: 0.944459, loss_mean_cls: 0.053565, grad_norm: 1.263652
[[34m2025-10-04 12:01:35[0m] Step: 661, Training Logs: loss_final: 0.981654, loss_mean: 0.928561, loss_mean_cls: 0.053093, grad_norm: 0.779195
[[34m2025-10-04 12:01:35[0m] Step: 662, Training Logs: loss_final: 0.992864, loss_mean: 0.940176, loss_mean_cls: 0.052688, grad_norm: 1.041341
[[34m2025-10-04 12:01:35[0m] Step: 663, Training Logs: loss_final: 0.979650, loss_mean: 0.925677, loss_mean_cls: 0.053973, grad_norm: 0.855053
[[34m2025-10-04 12:01:36[0m] Step: 664, Training Logs: loss_final: 0.963726, loss_mean: 0.909985, loss_mean_cls: 0.053741, grad_norm: 0.630291
[[34m2025-10-04 12:01:36[0m] Step: 665, Training Logs: loss_final: 0.974084, loss_mean: 0.920477, loss_mean_cls: 0.053607, grad_norm: 0.712146
[[34m2025-10-04 12:01:36[0m] Step: 666, Training Logs: loss_final: 0.985820, loss_mean: 0.932787, loss_mean_cls: 0.053033, grad_norm: 0.713454
[[34m2025-10-04 12:01:37[0m] Step: 667, Training Logs: loss_final: 0.964570, loss_mean: 0.911145, loss_mean_cls: 0.053425, grad_norm: 0.823347
[[34m2025-10-04 12:01:37[0m] Step: 668, Training Logs: loss_final: 0.999585, loss_mean: 0.948404, loss_mean_cls: 0.051181, grad_norm: 0.746507
[[34m2025-10-04 12:01:37[0m] Step: 669, Training Logs: loss_final: 0.963043, loss_mean: 0.909337, loss_mean_cls: 0.053707, grad_norm: 1.059900
[[34m2025-10-04 12:01:37[0m] Step: 670, Training Logs: loss_final: 0.992572, loss_mean: 0.939172, loss_mean_cls: 0.053401, grad_norm: 0.859342
[[34m2025-10-04 12:01:38[0m] Step: 671, Training Logs: loss_final: 0.976105, loss_mean: 0.923500, loss_mean_cls: 0.052605, grad_norm: 0.897353
[[34m2025-10-04 12:01:38[0m] Step: 672, Training Logs: loss_final: 0.969507, loss_mean: 0.917142, loss_mean_cls: 0.052365, grad_norm: 0.896936
[[34m2025-10-04 12:01:38[0m] Step: 673, Training Logs: loss_final: 1.009779, loss_mean: 0.957438, loss_mean_cls: 0.052341, grad_norm: 1.090678
[[34m2025-10-04 12:01:39[0m] Step: 674, Training Logs: loss_final: 0.967671, loss_mean: 0.914629, loss_mean_cls: 0.053042, grad_norm: 0.789310
[[34m2025-10-04 12:01:39[0m] Step: 675, Training Logs: loss_final: 0.967618, loss_mean: 0.914831, loss_mean_cls: 0.052788, grad_norm: 0.926533
[[34m2025-10-04 12:01:39[0m] Step: 676, Training Logs: loss_final: 0.963485, loss_mean: 0.909798, loss_mean_cls: 0.053687, grad_norm: 0.709549
[[34m2025-10-04 12:01:40[0m] Step: 677, Training Logs: loss_final: 0.975444, loss_mean: 0.922269, loss_mean_cls: 0.053176, grad_norm: 0.904314
[[34m2025-10-04 12:01:40[0m] Step: 678, Training Logs: loss_final: 0.983771, loss_mean: 0.931261, loss_mean_cls: 0.052510, grad_norm: 0.883976
[[34m2025-10-04 12:01:40[0m] Step: 679, Training Logs: loss_final: 0.977819, loss_mean: 0.925063, loss_mean_cls: 0.052756, grad_norm: 0.880781
[[34m2025-10-04 12:01:40[0m] Step: 680, Training Logs: loss_final: 0.964318, loss_mean: 0.911763, loss_mean_cls: 0.052555, grad_norm: 1.002317
[[34m2025-10-04 12:01:41[0m] Step: 681, Training Logs: loss_final: 0.986372, loss_mean: 0.934655, loss_mean_cls: 0.051717, grad_norm: 0.762239
[[34m2025-10-04 12:01:41[0m] Step: 682, Training Logs: loss_final: 0.988814, loss_mean: 0.935871, loss_mean_cls: 0.052943, grad_norm: 1.122320
[[34m2025-10-04 12:01:41[0m] Step: 683, Training Logs: loss_final: 0.963087, loss_mean: 0.910057, loss_mean_cls: 0.053030, grad_norm: 0.661079
[[34m2025-10-04 12:01:42[0m] Step: 684, Training Logs: loss_final: 0.979973, loss_mean: 0.927859, loss_mean_cls: 0.052114, grad_norm: 0.992508
[[34m2025-10-04 12:01:42[0m] Step: 685, Training Logs: loss_final: 0.957639, loss_mean: 0.905232, loss_mean_cls: 0.052408, grad_norm: 0.655719
[[34m2025-10-04 12:01:42[0m] Step: 686, Training Logs: loss_final: 0.965263, loss_mean: 0.911788, loss_mean_cls: 0.053475, grad_norm: 1.013656
[[34m2025-10-04 12:01:42[0m] Step: 687, Training Logs: loss_final: 0.964604, loss_mean: 0.911136, loss_mean_cls: 0.053468, grad_norm: 0.840385
[[34m2025-10-04 12:01:43[0m] Step: 688, Training Logs: loss_final: 0.966884, loss_mean: 0.914379, loss_mean_cls: 0.052505, grad_norm: 1.038725
[[34m2025-10-04 12:01:43[0m] Step: 689, Training Logs: loss_final: 0.964678, loss_mean: 0.911222, loss_mean_cls: 0.053457, grad_norm: 0.822245
[[34m2025-10-04 12:01:43[0m] Step: 690, Training Logs: loss_final: 0.981920, loss_mean: 0.928237, loss_mean_cls: 0.053683, grad_norm: 0.962020
[[34m2025-10-04 12:01:44[0m] Step: 691, Training Logs: loss_final: 0.990761, loss_mean: 0.939176, loss_mean_cls: 0.051585, grad_norm: 0.623875
[[34m2025-10-04 12:01:44[0m] Step: 692, Training Logs: loss_final: 0.978326, loss_mean: 0.924531, loss_mean_cls: 0.053794, grad_norm: 0.897502
[[34m2025-10-04 12:01:44[0m] Step: 693, Training Logs: loss_final: 0.966566, loss_mean: 0.913117, loss_mean_cls: 0.053449, grad_norm: 0.836238
[[34m2025-10-04 12:01:44[0m] Step: 694, Training Logs: loss_final: 0.986319, loss_mean: 0.934809, loss_mean_cls: 0.051511, grad_norm: 0.671842
[[34m2025-10-04 12:01:45[0m] Step: 695, Training Logs: loss_final: 0.969130, loss_mean: 0.917048, loss_mean_cls: 0.052083, grad_norm: 0.755802
[[34m2025-10-04 12:01:45[0m] Step: 696, Training Logs: loss_final: 0.974070, loss_mean: 0.921094, loss_mean_cls: 0.052976, grad_norm: 0.751344
[[34m2025-10-04 12:01:45[0m] Step: 697, Training Logs: loss_final: 0.967992, loss_mean: 0.915630, loss_mean_cls: 0.052362, grad_norm: 0.773546
[[34m2025-10-04 12:01:46[0m] Step: 698, Training Logs: loss_final: 0.984124, loss_mean: 0.932453, loss_mean_cls: 0.051671, grad_norm: 0.712688
[[34m2025-10-04 12:01:46[0m] Step: 699, Training Logs: loss_final: 0.964554, loss_mean: 0.911812, loss_mean_cls: 0.052742, grad_norm: 0.919555
[[34m2025-10-04 12:01:46[0m] Step: 700, Training Logs: loss_final: 0.974999, loss_mean: 0.921431, loss_mean_cls: 0.053569, grad_norm: 0.752022
[[34m2025-10-04 12:01:47[0m] Step: 701, Training Logs: loss_final: 0.974935, loss_mean: 0.921670, loss_mean_cls: 0.053265, grad_norm: 1.118094
[[34m2025-10-04 12:01:47[0m] Step: 702, Training Logs: loss_final: 0.972694, loss_mean: 0.919622, loss_mean_cls: 0.053072, grad_norm: 0.489744
[[34m2025-10-04 12:01:47[0m] Step: 703, Training Logs: loss_final: 0.963905, loss_mean: 0.911388, loss_mean_cls: 0.052517, grad_norm: 0.988565
[[34m2025-10-04 12:01:47[0m] Step: 704, Training Logs: loss_final: 0.973287, loss_mean: 0.919849, loss_mean_cls: 0.053438, grad_norm: 0.659727
[[34m2025-10-04 12:01:48[0m] Step: 705, Training Logs: loss_final: 0.983573, loss_mean: 0.931911, loss_mean_cls: 0.051662, grad_norm: 0.837954
[[34m2025-10-04 12:01:48[0m] Step: 706, Training Logs: loss_final: 0.975360, loss_mean: 0.923878, loss_mean_cls: 0.051482, grad_norm: 0.682025
[[34m2025-10-04 12:01:48[0m] Step: 707, Training Logs: loss_final: 0.974497, loss_mean: 0.921876, loss_mean_cls: 0.052621, grad_norm: 0.628338
[[34m2025-10-04 12:01:49[0m] Step: 708, Training Logs: loss_final: 0.979395, loss_mean: 0.926623, loss_mean_cls: 0.052772, grad_norm: 1.016923
[[34m2025-10-04 12:01:49[0m] Step: 709, Training Logs: loss_final: 0.955855, loss_mean: 0.902375, loss_mean_cls: 0.053479, grad_norm: 0.786072
[[34m2025-10-04 12:01:49[0m] Step: 710, Training Logs: loss_final: 0.977699, loss_mean: 0.925396, loss_mean_cls: 0.052303, grad_norm: 1.025629
[[34m2025-10-04 12:01:50[0m] Step: 711, Training Logs: loss_final: 0.986101, loss_mean: 0.934679, loss_mean_cls: 0.051422, grad_norm: 0.529214
[[34m2025-10-04 12:01:50[0m] Step: 712, Training Logs: loss_final: 0.961851, loss_mean: 0.908947, loss_mean_cls: 0.052904, grad_norm: 1.126538
[[34m2025-10-04 12:01:50[0m] Step: 713, Training Logs: loss_final: 0.967989, loss_mean: 0.915884, loss_mean_cls: 0.052105, grad_norm: 0.570872
[[34m2025-10-04 12:01:50[0m] Step: 714, Training Logs: loss_final: 0.976052, loss_mean: 0.922689, loss_mean_cls: 0.053362, grad_norm: 0.725376
[[34m2025-10-04 12:01:51[0m] Step: 715, Training Logs: loss_final: 0.954122, loss_mean: 0.901122, loss_mean_cls: 0.053000, grad_norm: 0.689121
[[34m2025-10-04 12:01:51[0m] Step: 716, Training Logs: loss_final: 0.971372, loss_mean: 0.920224, loss_mean_cls: 0.051148, grad_norm: 0.584641
[[34m2025-10-04 12:01:51[0m] Step: 717, Training Logs: loss_final: 0.981429, loss_mean: 0.929379, loss_mean_cls: 0.052050, grad_norm: 0.599706
[[34m2025-10-04 12:01:52[0m] Step: 718, Training Logs: loss_final: 0.980889, loss_mean: 0.927961, loss_mean_cls: 0.052928, grad_norm: 0.626850
[[34m2025-10-04 12:01:52[0m] Step: 719, Training Logs: loss_final: 0.980685, loss_mean: 0.928075, loss_mean_cls: 0.052611, grad_norm: 0.831293
[[34m2025-10-04 12:01:52[0m] Step: 720, Training Logs: loss_final: 0.979403, loss_mean: 0.927485, loss_mean_cls: 0.051918, grad_norm: 1.057953
[[34m2025-10-04 12:01:53[0m] Step: 721, Training Logs: loss_final: 0.957312, loss_mean: 0.905319, loss_mean_cls: 0.051993, grad_norm: 0.915263
[[34m2025-10-04 12:01:53[0m] Step: 722, Training Logs: loss_final: 0.961932, loss_mean: 0.909523, loss_mean_cls: 0.052409, grad_norm: 0.820495
[[34m2025-10-04 12:01:53[0m] Step: 723, Training Logs: loss_final: 0.966274, loss_mean: 0.914726, loss_mean_cls: 0.051548, grad_norm: 0.634492
[[34m2025-10-04 12:01:53[0m] Step: 724, Training Logs: loss_final: 0.961426, loss_mean: 0.909262, loss_mean_cls: 0.052165, grad_norm: 1.289459
[[34m2025-10-04 12:01:54[0m] Step: 725, Training Logs: loss_final: 0.973900, loss_mean: 0.921991, loss_mean_cls: 0.051908, grad_norm: 0.668727
[[34m2025-10-04 12:01:54[0m] Step: 726, Training Logs: loss_final: 0.968858, loss_mean: 0.916857, loss_mean_cls: 0.052001, grad_norm: 1.034051
[[34m2025-10-04 12:01:54[0m] Step: 727, Training Logs: loss_final: 0.972533, loss_mean: 0.919913, loss_mean_cls: 0.052620, grad_norm: 0.830707
[[34m2025-10-04 12:01:55[0m] Step: 728, Training Logs: loss_final: 0.953806, loss_mean: 0.902099, loss_mean_cls: 0.051707, grad_norm: 1.125119
[[34m2025-10-04 12:01:55[0m] Step: 729, Training Logs: loss_final: 0.964629, loss_mean: 0.911834, loss_mean_cls: 0.052796, grad_norm: 0.842877
[[34m2025-10-04 12:01:55[0m] Step: 730, Training Logs: loss_final: 0.981535, loss_mean: 0.929549, loss_mean_cls: 0.051987, grad_norm: 0.851333
[[34m2025-10-04 12:01:55[0m] Step: 731, Training Logs: loss_final: 0.976356, loss_mean: 0.923692, loss_mean_cls: 0.052664, grad_norm: 0.945533
[[34m2025-10-04 12:01:56[0m] Step: 732, Training Logs: loss_final: 0.971670, loss_mean: 0.920569, loss_mean_cls: 0.051102, grad_norm: 0.617120
[[34m2025-10-04 12:01:56[0m] Step: 733, Training Logs: loss_final: 0.983314, loss_mean: 0.931092, loss_mean_cls: 0.052222, grad_norm: 1.017807
[[34m2025-10-04 12:01:56[0m] Step: 734, Training Logs: loss_final: 0.945983, loss_mean: 0.892244, loss_mean_cls: 0.053739, grad_norm: 0.653407
[[34m2025-10-04 12:01:57[0m] Step: 735, Training Logs: loss_final: 0.959668, loss_mean: 0.907390, loss_mean_cls: 0.052278, grad_norm: 0.964349
[[34m2025-10-04 12:01:57[0m] Step: 736, Training Logs: loss_final: 0.966586, loss_mean: 0.914433, loss_mean_cls: 0.052152, grad_norm: 0.784617
[[34m2025-10-04 12:01:57[0m] Step: 737, Training Logs: loss_final: 0.973060, loss_mean: 0.920788, loss_mean_cls: 0.052272, grad_norm: 0.907380
[[34m2025-10-04 12:01:58[0m] Step: 738, Training Logs: loss_final: 0.969252, loss_mean: 0.916341, loss_mean_cls: 0.052911, grad_norm: 0.975792
[[34m2025-10-04 12:01:58[0m] Step: 739, Training Logs: loss_final: 0.972473, loss_mean: 0.920038, loss_mean_cls: 0.052435, grad_norm: 0.757870
[[34m2025-10-04 12:01:58[0m] Step: 740, Training Logs: loss_final: 0.965962, loss_mean: 0.913262, loss_mean_cls: 0.052701, grad_norm: 1.084061
[[34m2025-10-04 12:01:58[0m] Step: 741, Training Logs: loss_final: 0.973211, loss_mean: 0.921543, loss_mean_cls: 0.051668, grad_norm: 0.804535
[[34m2025-10-04 12:01:59[0m] Step: 742, Training Logs: loss_final: 0.945798, loss_mean: 0.893293, loss_mean_cls: 0.052505, grad_norm: 1.118770
[[34m2025-10-04 12:01:59[0m] Step: 743, Training Logs: loss_final: 0.966719, loss_mean: 0.913707, loss_mean_cls: 0.053011, grad_norm: 0.693874
[[34m2025-10-04 12:01:59[0m] Step: 744, Training Logs: loss_final: 0.970917, loss_mean: 0.917698, loss_mean_cls: 0.053219, grad_norm: 0.980308
[[34m2025-10-04 12:02:00[0m] Step: 745, Training Logs: loss_final: 0.991208, loss_mean: 0.939566, loss_mean_cls: 0.051642, grad_norm: 0.676542
[[34m2025-10-04 12:02:00[0m] Step: 746, Training Logs: loss_final: 0.968403, loss_mean: 0.916332, loss_mean_cls: 0.052071, grad_norm: 0.825244
[[34m2025-10-04 12:02:00[0m] Step: 747, Training Logs: loss_final: 0.976332, loss_mean: 0.923937, loss_mean_cls: 0.052395, grad_norm: 0.747882
[[34m2025-10-04 12:02:01[0m] Step: 748, Training Logs: loss_final: 0.965154, loss_mean: 0.912587, loss_mean_cls: 0.052567, grad_norm: 0.664177
[[34m2025-10-04 12:02:01[0m] Step: 749, Training Logs: loss_final: 0.936861, loss_mean: 0.884489, loss_mean_cls: 0.052373, grad_norm: 0.881509
[[34m2025-10-04 12:02:01[0m] Step: 750, Training Logs: loss_final: 0.973156, loss_mean: 0.920836, loss_mean_cls: 0.052319, grad_norm: 0.820339
[[34m2025-10-04 12:02:01[0m] Step: 751, Training Logs: loss_final: 0.983364, loss_mean: 0.931575, loss_mean_cls: 0.051789, grad_norm: 0.923502
[[34m2025-10-04 12:02:02[0m] Step: 752, Training Logs: loss_final: 0.974805, loss_mean: 0.922486, loss_mean_cls: 0.052318, grad_norm: 1.292153
[[34m2025-10-04 12:02:02[0m] Step: 753, Training Logs: loss_final: 0.980649, loss_mean: 0.929442, loss_mean_cls: 0.051206, grad_norm: 1.025801
[[34m2025-10-04 12:02:02[0m] Step: 754, Training Logs: loss_final: 0.968417, loss_mean: 0.915074, loss_mean_cls: 0.053343, grad_norm: 1.020645
[[34m2025-10-04 12:02:03[0m] Step: 755, Training Logs: loss_final: 0.959142, loss_mean: 0.907199, loss_mean_cls: 0.051944, grad_norm: 1.322918
[[34m2025-10-04 12:02:03[0m] Step: 756, Training Logs: loss_final: 0.998059, loss_mean: 0.945706, loss_mean_cls: 0.052354, grad_norm: 1.061881
[[34m2025-10-04 12:02:03[0m] Step: 757, Training Logs: loss_final: 0.978374, loss_mean: 0.926738, loss_mean_cls: 0.051636, grad_norm: 1.303026
[[34m2025-10-04 12:02:03[0m] Step: 758, Training Logs: loss_final: 0.981828, loss_mean: 0.929368, loss_mean_cls: 0.052460, grad_norm: 0.848423
[[34m2025-10-04 12:02:04[0m] Step: 759, Training Logs: loss_final: 0.981580, loss_mean: 0.929798, loss_mean_cls: 0.051782, grad_norm: 0.722830
[[34m2025-10-04 12:02:04[0m] Step: 760, Training Logs: loss_final: 0.956470, loss_mean: 0.903551, loss_mean_cls: 0.052919, grad_norm: 0.982377
[[34m2025-10-04 12:02:04[0m] Step: 761, Training Logs: loss_final: 0.956876, loss_mean: 0.904891, loss_mean_cls: 0.051985, grad_norm: 0.593068
[[34m2025-10-04 12:02:05[0m] Step: 762, Training Logs: loss_final: 0.965086, loss_mean: 0.912936, loss_mean_cls: 0.052150, grad_norm: 0.731906
[[34m2025-10-04 12:02:05[0m] Step: 763, Training Logs: loss_final: 0.955065, loss_mean: 0.902063, loss_mean_cls: 0.053002, grad_norm: 0.672260
[[34m2025-10-04 12:02:05[0m] Step: 764, Training Logs: loss_final: 0.947762, loss_mean: 0.894832, loss_mean_cls: 0.052930, grad_norm: 0.733826
[[34m2025-10-04 12:02:05[0m] Step: 765, Training Logs: loss_final: 0.964411, loss_mean: 0.912737, loss_mean_cls: 0.051674, grad_norm: 0.670622
[[34m2025-10-04 12:02:06[0m] Step: 766, Training Logs: loss_final: 0.952678, loss_mean: 0.900975, loss_mean_cls: 0.051703, grad_norm: 0.850374
[[34m2025-10-04 12:02:06[0m] Step: 767, Training Logs: loss_final: 0.957979, loss_mean: 0.905561, loss_mean_cls: 0.052419, grad_norm: 0.643618
[[34m2025-10-04 12:02:06[0m] Step: 768, Training Logs: loss_final: 0.975491, loss_mean: 0.923272, loss_mean_cls: 0.052219, grad_norm: 0.857794
[[34m2025-10-04 12:02:07[0m] Step: 769, Training Logs: loss_final: 0.981045, loss_mean: 0.928783, loss_mean_cls: 0.052262, grad_norm: 1.171940
[[34m2025-10-04 12:02:07[0m] Step: 770, Training Logs: loss_final: 0.953849, loss_mean: 0.901590, loss_mean_cls: 0.052259, grad_norm: 0.770442
[[34m2025-10-04 12:02:07[0m] Step: 771, Training Logs: loss_final: 0.980195, loss_mean: 0.928837, loss_mean_cls: 0.051357, grad_norm: 0.879199
[[34m2025-10-04 12:02:08[0m] Step: 772, Training Logs: loss_final: 0.958803, loss_mean: 0.907397, loss_mean_cls: 0.051407, grad_norm: 1.096577
[[34m2025-10-04 12:02:08[0m] Step: 773, Training Logs: loss_final: 0.971049, loss_mean: 0.919626, loss_mean_cls: 0.051423, grad_norm: 0.744647
[[34m2025-10-04 12:02:08[0m] Step: 774, Training Logs: loss_final: 0.988424, loss_mean: 0.937489, loss_mean_cls: 0.050935, grad_norm: 1.164398
[[34m2025-10-04 12:02:08[0m] Step: 775, Training Logs: loss_final: 0.991084, loss_mean: 0.938435, loss_mean_cls: 0.052649, grad_norm: 0.668903
[[34m2025-10-04 12:02:09[0m] Step: 776, Training Logs: loss_final: 0.969824, loss_mean: 0.917464, loss_mean_cls: 0.052360, grad_norm: 0.883962
[[34m2025-10-04 12:02:09[0m] Step: 777, Training Logs: loss_final: 0.964755, loss_mean: 0.912949, loss_mean_cls: 0.051806, grad_norm: 0.969122
[[34m2025-10-04 12:02:09[0m] Step: 778, Training Logs: loss_final: 0.978751, loss_mean: 0.927011, loss_mean_cls: 0.051740, grad_norm: 0.887001
[[34m2025-10-04 12:02:10[0m] Step: 779, Training Logs: loss_final: 0.969450, loss_mean: 0.917784, loss_mean_cls: 0.051666, grad_norm: 1.045190
[[34m2025-10-04 12:02:10[0m] Step: 780, Training Logs: loss_final: 0.957532, loss_mean: 0.905509, loss_mean_cls: 0.052023, grad_norm: 0.594833
[[34m2025-10-04 12:02:10[0m] Step: 781, Training Logs: loss_final: 0.965337, loss_mean: 0.913120, loss_mean_cls: 0.052217, grad_norm: 0.936862
[[34m2025-10-04 12:02:10[0m] Step: 782, Training Logs: loss_final: 0.954111, loss_mean: 0.902906, loss_mean_cls: 0.051206, grad_norm: 0.757581
[[34m2025-10-04 12:02:11[0m] Step: 783, Training Logs: loss_final: 0.954835, loss_mean: 0.902617, loss_mean_cls: 0.052218, grad_norm: 0.711586
[[34m2025-10-04 12:02:11[0m] Step: 784, Training Logs: loss_final: 0.945110, loss_mean: 0.892747, loss_mean_cls: 0.052363, grad_norm: 0.801743
[[34m2025-10-04 12:02:11[0m] Step: 785, Training Logs: loss_final: 0.967115, loss_mean: 0.914123, loss_mean_cls: 0.052992, grad_norm: 0.655975
[[34m2025-10-04 12:02:12[0m] Step: 786, Training Logs: loss_final: 0.965095, loss_mean: 0.913066, loss_mean_cls: 0.052029, grad_norm: 0.743107
[[34m2025-10-04 12:02:12[0m] Step: 787, Training Logs: loss_final: 0.967056, loss_mean: 0.916227, loss_mean_cls: 0.050828, grad_norm: 0.711353
[[34m2025-10-04 12:02:12[0m] Step: 788, Training Logs: loss_final: 0.968453, loss_mean: 0.916680, loss_mean_cls: 0.051773, grad_norm: 0.681926
[[34m2025-10-04 12:02:12[0m] Step: 789, Training Logs: loss_final: 0.979086, loss_mean: 0.927791, loss_mean_cls: 0.051295, grad_norm: 0.634352
[[34m2025-10-04 12:02:13[0m] Step: 790, Training Logs: loss_final: 0.991006, loss_mean: 0.939214, loss_mean_cls: 0.051792, grad_norm: 0.713060
[[34m2025-10-04 12:02:13[0m] Step: 791, Training Logs: loss_final: 0.984596, loss_mean: 0.933299, loss_mean_cls: 0.051298, grad_norm: 0.687856
[[34m2025-10-04 12:02:13[0m] Step: 792, Training Logs: loss_final: 0.952450, loss_mean: 0.900912, loss_mean_cls: 0.051538, grad_norm: 0.549229
[[34m2025-10-04 12:02:14[0m] Step: 793, Training Logs: loss_final: 0.948666, loss_mean: 0.896760, loss_mean_cls: 0.051906, grad_norm: 0.537744
[[34m2025-10-04 12:02:14[0m] Step: 794, Training Logs: loss_final: 0.942759, loss_mean: 0.889617, loss_mean_cls: 0.053142, grad_norm: 0.595778
[[34m2025-10-04 12:02:14[0m] Step: 795, Training Logs: loss_final: 0.979395, loss_mean: 0.927292, loss_mean_cls: 0.052104, grad_norm: 0.595155
[[34m2025-10-04 12:02:15[0m] Step: 796, Training Logs: loss_final: 0.958994, loss_mean: 0.907595, loss_mean_cls: 0.051399, grad_norm: 0.585857
[[34m2025-10-04 12:02:15[0m] Step: 797, Training Logs: loss_final: 0.960853, loss_mean: 0.909101, loss_mean_cls: 0.051752, grad_norm: 0.540420
[[34m2025-10-04 12:02:15[0m] Step: 798, Training Logs: loss_final: 0.962071, loss_mean: 0.910215, loss_mean_cls: 0.051856, grad_norm: 0.624417
[[34m2025-10-04 12:02:15[0m] Step: 799, Training Logs: loss_final: 0.954444, loss_mean: 0.902290, loss_mean_cls: 0.052154, grad_norm: 0.591339
[[34m2025-10-04 12:02:16[0m] Step: 800, Training Logs: loss_final: 0.956422, loss_mean: 0.903338, loss_mean_cls: 0.053084, grad_norm: 0.649889
[[34m2025-10-04 12:02:16[0m] Step: 801, Training Logs: loss_final: 0.947058, loss_mean: 0.894374, loss_mean_cls: 0.052684, grad_norm: 0.416590
[[34m2025-10-04 12:02:16[0m] Step: 802, Training Logs: loss_final: 0.964364, loss_mean: 0.911513, loss_mean_cls: 0.052851, grad_norm: 0.682757
[[34m2025-10-04 12:02:17[0m] Step: 803, Training Logs: loss_final: 0.942834, loss_mean: 0.890813, loss_mean_cls: 0.052022, grad_norm: 0.673440
[[34m2025-10-04 12:02:17[0m] Step: 804, Training Logs: loss_final: 0.954837, loss_mean: 0.903065, loss_mean_cls: 0.051772, grad_norm: 0.722591
[[34m2025-10-04 12:02:17[0m] Step: 805, Training Logs: loss_final: 0.973624, loss_mean: 0.920674, loss_mean_cls: 0.052950, grad_norm: 0.772176
[[34m2025-10-04 12:02:17[0m] Step: 806, Training Logs: loss_final: 0.986281, loss_mean: 0.934942, loss_mean_cls: 0.051339, grad_norm: 0.706920
[[34m2025-10-04 12:02:18[0m] Step: 807, Training Logs: loss_final: 0.986348, loss_mean: 0.934520, loss_mean_cls: 0.051828, grad_norm: 0.540067
[[34m2025-10-04 12:02:18[0m] Step: 808, Training Logs: loss_final: 0.965548, loss_mean: 0.913793, loss_mean_cls: 0.051755, grad_norm: 0.617809
[[34m2025-10-04 12:02:18[0m] Step: 809, Training Logs: loss_final: 0.942054, loss_mean: 0.890201, loss_mean_cls: 0.051853, grad_norm: 0.696388
[[34m2025-10-04 12:02:19[0m] Step: 810, Training Logs: loss_final: 0.949880, loss_mean: 0.898149, loss_mean_cls: 0.051732, grad_norm: 0.762894
[[34m2025-10-04 12:02:19[0m] Step: 811, Training Logs: loss_final: 0.976630, loss_mean: 0.924872, loss_mean_cls: 0.051757, grad_norm: 0.704564
[[34m2025-10-04 12:02:19[0m] Step: 812, Training Logs: loss_final: 0.952965, loss_mean: 0.900065, loss_mean_cls: 0.052901, grad_norm: 0.711658
[[34m2025-10-04 12:02:20[0m] Step: 813, Training Logs: loss_final: 0.956573, loss_mean: 0.904496, loss_mean_cls: 0.052077, grad_norm: 0.824464
[[34m2025-10-04 12:02:20[0m] Step: 814, Training Logs: loss_final: 0.941201, loss_mean: 0.888102, loss_mean_cls: 0.053099, grad_norm: 0.758180
[[34m2025-10-04 12:02:20[0m] Step: 815, Training Logs: loss_final: 0.972631, loss_mean: 0.920520, loss_mean_cls: 0.052111, grad_norm: 0.797905
[[34m2025-10-04 12:02:20[0m] Step: 816, Training Logs: loss_final: 0.955967, loss_mean: 0.903894, loss_mean_cls: 0.052073, grad_norm: 0.790934
[[34m2025-10-04 12:02:21[0m] Step: 817, Training Logs: loss_final: 0.961205, loss_mean: 0.908783, loss_mean_cls: 0.052422, grad_norm: 0.707114
[[34m2025-10-04 12:02:21[0m] Step: 818, Training Logs: loss_final: 0.966950, loss_mean: 0.915085, loss_mean_cls: 0.051865, grad_norm: 0.701299
[[34m2025-10-04 12:02:21[0m] Step: 819, Training Logs: loss_final: 0.952104, loss_mean: 0.900191, loss_mean_cls: 0.051913, grad_norm: 0.698019
[[34m2025-10-04 12:02:22[0m] Step: 820, Training Logs: loss_final: 0.973638, loss_mean: 0.922078, loss_mean_cls: 0.051560, grad_norm: 0.864035
[[34m2025-10-04 12:02:22[0m] Step: 821, Training Logs: loss_final: 0.950138, loss_mean: 0.899773, loss_mean_cls: 0.050366, grad_norm: 0.957282
[[34m2025-10-04 12:02:22[0m] Step: 822, Training Logs: loss_final: 0.983043, loss_mean: 0.930751, loss_mean_cls: 0.052292, grad_norm: 0.812803
[[34m2025-10-04 12:02:23[0m] Step: 823, Training Logs: loss_final: 0.968005, loss_mean: 0.915967, loss_mean_cls: 0.052038, grad_norm: 1.123860
[[34m2025-10-04 12:02:23[0m] Step: 824, Training Logs: loss_final: 0.983212, loss_mean: 0.931514, loss_mean_cls: 0.051698, grad_norm: 1.125050
[[34m2025-10-04 12:02:23[0m] Step: 825, Training Logs: loss_final: 0.970054, loss_mean: 0.918330, loss_mean_cls: 0.051724, grad_norm: 0.861172
[[34m2025-10-04 12:02:23[0m] Step: 826, Training Logs: loss_final: 0.965841, loss_mean: 0.914126, loss_mean_cls: 0.051715, grad_norm: 1.119926
[[34m2025-10-04 12:02:24[0m] Step: 827, Training Logs: loss_final: 0.979438, loss_mean: 0.928385, loss_mean_cls: 0.051053, grad_norm: 0.762913
[[34m2025-10-04 12:02:24[0m] Step: 828, Training Logs: loss_final: 0.988403, loss_mean: 0.936990, loss_mean_cls: 0.051413, grad_norm: 0.865914
[[34m2025-10-04 12:02:24[0m] Step: 829, Training Logs: loss_final: 0.964162, loss_mean: 0.912004, loss_mean_cls: 0.052158, grad_norm: 0.771502
[[34m2025-10-04 12:02:25[0m] Step: 830, Training Logs: loss_final: 0.953791, loss_mean: 0.901942, loss_mean_cls: 0.051849, grad_norm: 0.737742
[[34m2025-10-04 12:02:25[0m] Step: 831, Training Logs: loss_final: 0.959018, loss_mean: 0.905668, loss_mean_cls: 0.053350, grad_norm: 0.759419
[[34m2025-10-04 12:02:25[0m] Step: 832, Training Logs: loss_final: 0.953551, loss_mean: 0.901901, loss_mean_cls: 0.051650, grad_norm: 1.055126
[[34m2025-10-04 12:02:25[0m] Step: 833, Training Logs: loss_final: 0.953150, loss_mean: 0.901576, loss_mean_cls: 0.051574, grad_norm: 0.809905
[[34m2025-10-04 12:02:26[0m] Step: 834, Training Logs: loss_final: 0.970741, loss_mean: 0.919592, loss_mean_cls: 0.051149, grad_norm: 0.761911
[[34m2025-10-04 12:02:26[0m] Step: 835, Training Logs: loss_final: 0.958720, loss_mean: 0.906325, loss_mean_cls: 0.052395, grad_norm: 0.592898
[[34m2025-10-04 12:02:26[0m] Step: 836, Training Logs: loss_final: 0.975662, loss_mean: 0.923449, loss_mean_cls: 0.052212, grad_norm: 0.796609
[[34m2025-10-04 12:02:27[0m] Step: 837, Training Logs: loss_final: 0.977816, loss_mean: 0.926105, loss_mean_cls: 0.051710, grad_norm: 0.611423
[[34m2025-10-04 12:02:27[0m] Step: 838, Training Logs: loss_final: 0.948080, loss_mean: 0.896311, loss_mean_cls: 0.051769, grad_norm: 0.715833
[[34m2025-10-04 12:02:27[0m] Step: 839, Training Logs: loss_final: 0.977883, loss_mean: 0.925029, loss_mean_cls: 0.052854, grad_norm: 0.474057
[[34m2025-10-04 12:02:28[0m] Step: 840, Training Logs: loss_final: 0.952322, loss_mean: 0.900559, loss_mean_cls: 0.051763, grad_norm: 0.687763
[[34m2025-10-04 12:02:28[0m] Step: 841, Training Logs: loss_final: 0.944343, loss_mean: 0.891062, loss_mean_cls: 0.053281, grad_norm: 0.698033
[[34m2025-10-04 12:02:28[0m] Step: 842, Training Logs: loss_final: 0.939499, loss_mean: 0.887457, loss_mean_cls: 0.052042, grad_norm: 0.763151
[[34m2025-10-04 12:02:28[0m] Step: 843, Training Logs: loss_final: 0.953266, loss_mean: 0.902053, loss_mean_cls: 0.051213, grad_norm: 0.675099
[[34m2025-10-04 12:02:29[0m] Step: 844, Training Logs: loss_final: 0.954231, loss_mean: 0.902391, loss_mean_cls: 0.051840, grad_norm: 0.649972
[[34m2025-10-04 12:02:29[0m] Step: 845, Training Logs: loss_final: 0.941488, loss_mean: 0.889628, loss_mean_cls: 0.051860, grad_norm: 0.491706
[[34m2025-10-04 12:02:29[0m] Step: 846, Training Logs: loss_final: 0.959858, loss_mean: 0.907661, loss_mean_cls: 0.052198, grad_norm: 0.651640
[[34m2025-10-04 12:02:30[0m] Step: 847, Training Logs: loss_final: 0.943050, loss_mean: 0.890738, loss_mean_cls: 0.052311, grad_norm: 0.554164
[[34m2025-10-04 12:02:30[0m] Step: 848, Training Logs: loss_final: 0.973843, loss_mean: 0.920637, loss_mean_cls: 0.053205, grad_norm: 0.727234
[[34m2025-10-04 12:02:30[0m] Step: 849, Training Logs: loss_final: 0.959059, loss_mean: 0.905394, loss_mean_cls: 0.053665, grad_norm: 0.753257
[[34m2025-10-04 12:02:30[0m] Step: 850, Training Logs: loss_final: 0.976593, loss_mean: 0.925458, loss_mean_cls: 0.051135, grad_norm: 0.709283
[[34m2025-10-04 12:02:31[0m] Step: 851, Training Logs: loss_final: 0.969847, loss_mean: 0.918637, loss_mean_cls: 0.051211, grad_norm: 0.822740
[[34m2025-10-04 12:02:31[0m] Step: 852, Training Logs: loss_final: 0.961292, loss_mean: 0.909624, loss_mean_cls: 0.051668, grad_norm: 0.622706
[[34m2025-10-04 12:02:31[0m] Step: 853, Training Logs: loss_final: 0.951721, loss_mean: 0.900376, loss_mean_cls: 0.051345, grad_norm: 0.709485
[[34m2025-10-04 12:02:32[0m] Step: 854, Training Logs: loss_final: 0.957471, loss_mean: 0.905643, loss_mean_cls: 0.051828, grad_norm: 0.680633
[[34m2025-10-04 12:02:32[0m] Step: 855, Training Logs: loss_final: 0.971297, loss_mean: 0.919362, loss_mean_cls: 0.051934, grad_norm: 0.710261
[[34m2025-10-04 12:02:32[0m] Step: 856, Training Logs: loss_final: 0.985502, loss_mean: 0.933915, loss_mean_cls: 0.051587, grad_norm: 0.687994
[[34m2025-10-04 12:02:33[0m] Step: 857, Training Logs: loss_final: 0.953967, loss_mean: 0.901973, loss_mean_cls: 0.051993, grad_norm: 0.764332
[[34m2025-10-04 12:02:33[0m] Step: 858, Training Logs: loss_final: 0.948724, loss_mean: 0.897000, loss_mean_cls: 0.051725, grad_norm: 0.725760
[[34m2025-10-04 12:02:33[0m] Step: 859, Training Logs: loss_final: 0.954996, loss_mean: 0.903763, loss_mean_cls: 0.051232, grad_norm: 0.685527
[[34m2025-10-04 12:02:33[0m] Step: 860, Training Logs: loss_final: 0.958901, loss_mean: 0.907946, loss_mean_cls: 0.050955, grad_norm: 0.903041
[[34m2025-10-04 12:02:34[0m] Step: 861, Training Logs: loss_final: 0.967275, loss_mean: 0.916446, loss_mean_cls: 0.050829, grad_norm: 0.661979
[[34m2025-10-04 12:02:34[0m] Step: 862, Training Logs: loss_final: 0.951438, loss_mean: 0.900635, loss_mean_cls: 0.050803, grad_norm: 0.757274
[[34m2025-10-04 12:02:34[0m] Step: 863, Training Logs: loss_final: 0.959825, loss_mean: 0.908946, loss_mean_cls: 0.050879, grad_norm: 0.839385
[[34m2025-10-04 12:02:35[0m] Step: 864, Training Logs: loss_final: 0.975085, loss_mean: 0.923547, loss_mean_cls: 0.051537, grad_norm: 0.883591
[[34m2025-10-04 12:02:35[0m] Step: 865, Training Logs: loss_final: 0.954159, loss_mean: 0.901722, loss_mean_cls: 0.052437, grad_norm: 0.910991
[[34m2025-10-04 12:02:35[0m] Step: 866, Training Logs: loss_final: 0.964229, loss_mean: 0.913283, loss_mean_cls: 0.050945, grad_norm: 0.649073
[[34m2025-10-04 12:02:35[0m] Step: 867, Training Logs: loss_final: 0.977661, loss_mean: 0.926208, loss_mean_cls: 0.051453, grad_norm: 0.846767
[[34m2025-10-04 12:02:36[0m] Step: 868, Training Logs: loss_final: 0.965206, loss_mean: 0.914087, loss_mean_cls: 0.051119, grad_norm: 0.722881
[[34m2025-10-04 12:02:36[0m] Step: 869, Training Logs: loss_final: 0.954289, loss_mean: 0.902036, loss_mean_cls: 0.052253, grad_norm: 0.844850
[[34m2025-10-04 12:02:36[0m] Step: 870, Training Logs: loss_final: 0.957401, loss_mean: 0.904898, loss_mean_cls: 0.052503, grad_norm: 0.790943
[[34m2025-10-04 12:02:37[0m] Step: 871, Training Logs: loss_final: 0.949833, loss_mean: 0.897880, loss_mean_cls: 0.051952, grad_norm: 0.836808
[[34m2025-10-04 12:02:37[0m] Step: 872, Training Logs: loss_final: 0.931747, loss_mean: 0.879486, loss_mean_cls: 0.052261, grad_norm: 0.760089
[[34m2025-10-04 12:02:37[0m] Step: 873, Training Logs: loss_final: 0.957795, loss_mean: 0.905312, loss_mean_cls: 0.052483, grad_norm: 0.820292
[[34m2025-10-04 12:02:37[0m] Step: 874, Training Logs: loss_final: 0.966246, loss_mean: 0.914290, loss_mean_cls: 0.051957, grad_norm: 1.032078
[[34m2025-10-04 12:02:38[0m] Step: 875, Training Logs: loss_final: 0.965747, loss_mean: 0.912835, loss_mean_cls: 0.052913, grad_norm: 0.934596
[[34m2025-10-04 12:02:38[0m] Step: 876, Training Logs: loss_final: 0.971799, loss_mean: 0.919588, loss_mean_cls: 0.052211, grad_norm: 1.190470
[[34m2025-10-04 12:02:38[0m] Step: 877, Training Logs: loss_final: 0.974316, loss_mean: 0.923972, loss_mean_cls: 0.050344, grad_norm: 1.125610
[[34m2025-10-04 12:02:39[0m] Step: 878, Training Logs: loss_final: 0.945796, loss_mean: 0.893314, loss_mean_cls: 0.052482, grad_norm: 0.820855
[[34m2025-10-04 12:02:39[0m] Step: 879, Training Logs: loss_final: 0.957911, loss_mean: 0.906569, loss_mean_cls: 0.051342, grad_norm: 1.018627
[[34m2025-10-04 12:02:39[0m] Step: 880, Training Logs: loss_final: 0.984346, loss_mean: 0.932880, loss_mean_cls: 0.051466, grad_norm: 1.032194
[[34m2025-10-04 12:02:40[0m] Step: 881, Training Logs: loss_final: 0.950351, loss_mean: 0.898543, loss_mean_cls: 0.051808, grad_norm: 0.927001
[[34m2025-10-04 12:02:40[0m] Step: 882, Training Logs: loss_final: 0.985678, loss_mean: 0.934307, loss_mean_cls: 0.051371, grad_norm: 0.766275
[[34m2025-10-04 12:02:40[0m] Step: 883, Training Logs: loss_final: 0.956765, loss_mean: 0.905657, loss_mean_cls: 0.051107, grad_norm: 0.888899
[[34m2025-10-04 12:02:40[0m] Step: 884, Training Logs: loss_final: 0.958963, loss_mean: 0.908810, loss_mean_cls: 0.050153, grad_norm: 1.017892
[[34m2025-10-04 12:02:41[0m] Step: 885, Training Logs: loss_final: 0.961018, loss_mean: 0.909435, loss_mean_cls: 0.051583, grad_norm: 0.671972
[[34m2025-10-04 12:02:41[0m] Step: 886, Training Logs: loss_final: 0.963856, loss_mean: 0.912150, loss_mean_cls: 0.051706, grad_norm: 1.088599
[[34m2025-10-04 12:02:41[0m] Step: 887, Training Logs: loss_final: 0.962673, loss_mean: 0.911469, loss_mean_cls: 0.051204, grad_norm: 0.841847
[[34m2025-10-04 12:02:42[0m] Step: 888, Training Logs: loss_final: 0.972849, loss_mean: 0.921844, loss_mean_cls: 0.051005, grad_norm: 1.318228
[[34m2025-10-04 12:02:42[0m] Step: 889, Training Logs: loss_final: 0.966245, loss_mean: 0.914582, loss_mean_cls: 0.051663, grad_norm: 0.703370
[[34m2025-10-04 12:02:42[0m] Step: 890, Training Logs: loss_final: 0.971573, loss_mean: 0.921356, loss_mean_cls: 0.050217, grad_norm: 0.825966
[[34m2025-10-04 12:02:42[0m] Step: 891, Training Logs: loss_final: 0.975344, loss_mean: 0.924409, loss_mean_cls: 0.050935, grad_norm: 0.740681
[[34m2025-10-04 12:02:43[0m] Step: 892, Training Logs: loss_final: 0.966430, loss_mean: 0.915185, loss_mean_cls: 0.051244, grad_norm: 0.755520
[[34m2025-10-04 12:02:43[0m] Step: 893, Training Logs: loss_final: 0.981844, loss_mean: 0.931921, loss_mean_cls: 0.049923, grad_norm: 1.104218
[[34m2025-10-04 12:02:43[0m] Step: 894, Training Logs: loss_final: 0.958305, loss_mean: 0.906386, loss_mean_cls: 0.051919, grad_norm: 0.531793
[[34m2025-10-04 12:02:44[0m] Step: 895, Training Logs: loss_final: 0.955678, loss_mean: 0.903032, loss_mean_cls: 0.052646, grad_norm: 0.986687
[[34m2025-10-04 12:02:44[0m] Step: 896, Training Logs: loss_final: 0.942982, loss_mean: 0.891177, loss_mean_cls: 0.051805, grad_norm: 0.676017
[[34m2025-10-04 12:02:44[0m] Step: 897, Training Logs: loss_final: 0.959182, loss_mean: 0.907134, loss_mean_cls: 0.052048, grad_norm: 1.038439
[[34m2025-10-04 12:02:45[0m] Step: 898, Training Logs: loss_final: 0.948075, loss_mean: 0.896484, loss_mean_cls: 0.051591, grad_norm: 0.634547
[[34m2025-10-04 12:02:45[0m] Step: 899, Training Logs: loss_final: 0.964049, loss_mean: 0.912082, loss_mean_cls: 0.051968, grad_norm: 0.711714
[[34m2025-10-04 12:02:45[0m] Step: 900, Training Logs: loss_final: 0.956528, loss_mean: 0.905087, loss_mean_cls: 0.051441, grad_norm: 0.666437
[[34m2025-10-04 12:02:45[0m] Step: 901, Training Logs: loss_final: 0.955924, loss_mean: 0.904348, loss_mean_cls: 0.051576, grad_norm: 0.597876
[[34m2025-10-04 12:02:46[0m] Step: 902, Training Logs: loss_final: 0.951350, loss_mean: 0.900354, loss_mean_cls: 0.050995, grad_norm: 0.544858
[[34m2025-10-04 12:02:46[0m] Step: 903, Training Logs: loss_final: 0.969087, loss_mean: 0.917385, loss_mean_cls: 0.051702, grad_norm: 0.485884
[[34m2025-10-04 12:02:46[0m] Step: 904, Training Logs: loss_final: 0.965271, loss_mean: 0.913524, loss_mean_cls: 0.051747, grad_norm: 0.678896
[[34m2025-10-04 12:02:47[0m] Step: 905, Training Logs: loss_final: 0.950107, loss_mean: 0.897349, loss_mean_cls: 0.052758, grad_norm: 0.730740
[[34m2025-10-04 12:02:47[0m] Step: 906, Training Logs: loss_final: 0.973519, loss_mean: 0.921971, loss_mean_cls: 0.051549, grad_norm: 0.722693
[[34m2025-10-04 12:02:47[0m] Step: 907, Training Logs: loss_final: 0.968120, loss_mean: 0.916033, loss_mean_cls: 0.052088, grad_norm: 0.707088
[[34m2025-10-04 12:02:48[0m] Step: 908, Training Logs: loss_final: 0.955002, loss_mean: 0.902944, loss_mean_cls: 0.052058, grad_norm: 0.391422
[[34m2025-10-04 12:02:48[0m] Step: 909, Training Logs: loss_final: 0.952435, loss_mean: 0.901075, loss_mean_cls: 0.051360, grad_norm: 0.844889
[[34m2025-10-04 12:02:48[0m] Step: 910, Training Logs: loss_final: 0.951978, loss_mean: 0.900610, loss_mean_cls: 0.051368, grad_norm: 0.671533
[[34m2025-10-04 12:02:48[0m] Step: 911, Training Logs: loss_final: 0.942495, loss_mean: 0.890550, loss_mean_cls: 0.051944, grad_norm: 0.768052
[[34m2025-10-04 12:02:49[0m] Step: 912, Training Logs: loss_final: 0.926007, loss_mean: 0.873813, loss_mean_cls: 0.052194, grad_norm: 0.552234
[[34m2025-10-04 12:02:49[0m] Step: 913, Training Logs: loss_final: 0.960141, loss_mean: 0.907682, loss_mean_cls: 0.052459, grad_norm: 0.711394
[[34m2025-10-04 12:02:49[0m] Step: 914, Training Logs: loss_final: 0.965242, loss_mean: 0.913863, loss_mean_cls: 0.051379, grad_norm: 0.708281
[[34m2025-10-04 12:02:50[0m] Step: 915, Training Logs: loss_final: 0.958575, loss_mean: 0.907164, loss_mean_cls: 0.051411, grad_norm: 0.762262
[[34m2025-10-04 12:02:50[0m] Step: 916, Training Logs: loss_final: 0.972986, loss_mean: 0.921637, loss_mean_cls: 0.051349, grad_norm: 0.833954
[[34m2025-10-04 12:02:50[0m] Step: 917, Training Logs: loss_final: 0.958531, loss_mean: 0.907183, loss_mean_cls: 0.051348, grad_norm: 1.000453
[[34m2025-10-04 12:02:51[0m] Step: 918, Training Logs: loss_final: 0.935950, loss_mean: 0.883512, loss_mean_cls: 0.052437, grad_norm: 0.994595
[[34m2025-10-04 12:02:51[0m] Step: 919, Training Logs: loss_final: 0.956432, loss_mean: 0.904088, loss_mean_cls: 0.052343, grad_norm: 0.867237
[[34m2025-10-04 12:02:51[0m] Step: 920, Training Logs: loss_final: 0.951395, loss_mean: 0.899635, loss_mean_cls: 0.051760, grad_norm: 0.786022
[[34m2025-10-04 12:02:51[0m] Step: 921, Training Logs: loss_final: 0.972320, loss_mean: 0.921628, loss_mean_cls: 0.050692, grad_norm: 0.995515
[[34m2025-10-04 12:02:52[0m] Step: 922, Training Logs: loss_final: 0.968319, loss_mean: 0.917926, loss_mean_cls: 0.050393, grad_norm: 0.893840
[[34m2025-10-04 12:02:52[0m] Step: 923, Training Logs: loss_final: 0.965020, loss_mean: 0.914293, loss_mean_cls: 0.050727, grad_norm: 0.957563
[[34m2025-10-04 12:02:52[0m] Step: 924, Training Logs: loss_final: 0.959468, loss_mean: 0.907811, loss_mean_cls: 0.051657, grad_norm: 0.935897
[[34m2025-10-04 12:02:53[0m] Step: 925, Training Logs: loss_final: 0.970225, loss_mean: 0.920930, loss_mean_cls: 0.049295, grad_norm: 0.731390
[[34m2025-10-04 12:02:53[0m] Step: 926, Training Logs: loss_final: 0.962555, loss_mean: 0.910895, loss_mean_cls: 0.051660, grad_norm: 0.897457
[[34m2025-10-04 12:02:53[0m] Step: 927, Training Logs: loss_final: 0.950953, loss_mean: 0.898717, loss_mean_cls: 0.052236, grad_norm: 0.925227
[[34m2025-10-04 12:02:54[0m] Step: 928, Training Logs: loss_final: 0.956807, loss_mean: 0.904571, loss_mean_cls: 0.052235, grad_norm: 0.712262
[[34m2025-10-04 12:02:54[0m] Step: 929, Training Logs: loss_final: 0.947485, loss_mean: 0.894461, loss_mean_cls: 0.053024, grad_norm: 0.864226
[[34m2025-10-04 12:02:54[0m] Step: 930, Training Logs: loss_final: 0.955783, loss_mean: 0.904837, loss_mean_cls: 0.050947, grad_norm: 0.643347
[[34m2025-10-04 12:02:54[0m] Step: 931, Training Logs: loss_final: 0.961135, loss_mean: 0.910432, loss_mean_cls: 0.050703, grad_norm: 0.869257
[[34m2025-10-04 12:02:55[0m] Step: 932, Training Logs: loss_final: 0.965228, loss_mean: 0.913243, loss_mean_cls: 0.051985, grad_norm: 1.048659
[[34m2025-10-04 12:02:55[0m] Step: 933, Training Logs: loss_final: 0.952504, loss_mean: 0.900798, loss_mean_cls: 0.051706, grad_norm: 0.575406
[[34m2025-10-04 12:02:55[0m] Step: 934, Training Logs: loss_final: 0.959880, loss_mean: 0.908885, loss_mean_cls: 0.050995, grad_norm: 0.793674
[[34m2025-10-04 12:02:56[0m] Step: 935, Training Logs: loss_final: 0.952075, loss_mean: 0.901522, loss_mean_cls: 0.050554, grad_norm: 0.683892
[[34m2025-10-04 12:02:56[0m] Step: 936, Training Logs: loss_final: 0.939041, loss_mean: 0.886465, loss_mean_cls: 0.052576, grad_norm: 0.912771
[[34m2025-10-04 12:02:56[0m] Step: 937, Training Logs: loss_final: 0.953891, loss_mean: 0.901521, loss_mean_cls: 0.052370, grad_norm: 0.730331
[[34m2025-10-04 12:02:57[0m] Step: 938, Training Logs: loss_final: 0.947174, loss_mean: 0.895481, loss_mean_cls: 0.051693, grad_norm: 0.984602
[[34m2025-10-04 12:02:57[0m] Step: 939, Training Logs: loss_final: 0.952661, loss_mean: 0.899779, loss_mean_cls: 0.052881, grad_norm: 0.700506
[[34m2025-10-04 12:02:57[0m] Step: 940, Training Logs: loss_final: 0.975514, loss_mean: 0.923533, loss_mean_cls: 0.051981, grad_norm: 0.798944
[[34m2025-10-04 12:02:57[0m] Step: 941, Training Logs: loss_final: 0.946759, loss_mean: 0.895488, loss_mean_cls: 0.051272, grad_norm: 0.746063
[[34m2025-10-04 12:02:58[0m] Step: 942, Training Logs: loss_final: 0.950195, loss_mean: 0.898871, loss_mean_cls: 0.051325, grad_norm: 0.739282
[[34m2025-10-04 12:02:58[0m] Step: 943, Training Logs: loss_final: 0.966528, loss_mean: 0.915258, loss_mean_cls: 0.051270, grad_norm: 0.704195
[[34m2025-10-04 12:02:58[0m] Step: 944, Training Logs: loss_final: 0.959611, loss_mean: 0.907399, loss_mean_cls: 0.052213, grad_norm: 0.771320
[[34m2025-10-04 12:02:59[0m] Step: 945, Training Logs: loss_final: 0.944706, loss_mean: 0.893309, loss_mean_cls: 0.051397, grad_norm: 0.644618
[[34m2025-10-04 12:02:59[0m] Step: 946, Training Logs: loss_final: 0.946707, loss_mean: 0.894662, loss_mean_cls: 0.052044, grad_norm: 0.544418
[[34m2025-10-04 12:02:59[0m] Step: 947, Training Logs: loss_final: 0.966249, loss_mean: 0.915801, loss_mean_cls: 0.050448, grad_norm: 0.515732
[[34m2025-10-04 12:03:00[0m] Step: 948, Training Logs: loss_final: 0.966474, loss_mean: 0.915908, loss_mean_cls: 0.050566, grad_norm: 0.615374
[[34m2025-10-04 12:03:00[0m] Step: 949, Training Logs: loss_final: 0.972695, loss_mean: 0.923013, loss_mean_cls: 0.049681, grad_norm: 0.489116
[[34m2025-10-04 12:03:00[0m] Step: 950, Training Logs: loss_final: 0.958333, loss_mean: 0.907456, loss_mean_cls: 0.050878, grad_norm: 0.609896
[[34m2025-10-04 12:03:00[0m] Step: 951, Training Logs: loss_final: 0.959882, loss_mean: 0.908553, loss_mean_cls: 0.051329, grad_norm: 0.599050
[[34m2025-10-04 12:03:01[0m] Step: 952, Training Logs: loss_final: 0.948922, loss_mean: 0.897043, loss_mean_cls: 0.051879, grad_norm: 0.796239
[[34m2025-10-04 12:03:01[0m] Step: 953, Training Logs: loss_final: 0.948779, loss_mean: 0.897451, loss_mean_cls: 0.051328, grad_norm: 0.652376
[[34m2025-10-04 12:03:01[0m] Step: 954, Training Logs: loss_final: 0.952755, loss_mean: 0.902519, loss_mean_cls: 0.050236, grad_norm: 0.587048
[[34m2025-10-04 12:03:02[0m] Step: 955, Training Logs: loss_final: 0.950885, loss_mean: 0.900156, loss_mean_cls: 0.050729, grad_norm: 0.805912
[[34m2025-10-04 12:03:02[0m] Step: 956, Training Logs: loss_final: 0.945441, loss_mean: 0.893846, loss_mean_cls: 0.051595, grad_norm: 0.648480
[[34m2025-10-04 12:03:02[0m] Step: 957, Training Logs: loss_final: 0.947342, loss_mean: 0.895130, loss_mean_cls: 0.052212, grad_norm: 0.772545
[[34m2025-10-04 12:03:03[0m] Step: 958, Training Logs: loss_final: 0.952652, loss_mean: 0.903101, loss_mean_cls: 0.049551, grad_norm: 0.605707
[[34m2025-10-04 12:03:03[0m] Step: 959, Training Logs: loss_final: 0.954364, loss_mean: 0.903196, loss_mean_cls: 0.051169, grad_norm: 0.797121
[[34m2025-10-04 12:03:03[0m] Step: 960, Training Logs: loss_final: 0.956310, loss_mean: 0.905500, loss_mean_cls: 0.050810, grad_norm: 0.824382
[[34m2025-10-04 12:03:03[0m] Step: 961, Training Logs: loss_final: 0.967080, loss_mean: 0.916643, loss_mean_cls: 0.050436, grad_norm: 0.690140
[[34m2025-10-04 12:03:04[0m] Step: 962, Training Logs: loss_final: 0.981907, loss_mean: 0.931575, loss_mean_cls: 0.050332, grad_norm: 1.003065
[[34m2025-10-04 12:03:04[0m] Step: 963, Training Logs: loss_final: 0.936139, loss_mean: 0.883424, loss_mean_cls: 0.052715, grad_norm: 0.644653
[[34m2025-10-04 12:03:04[0m] Step: 964, Training Logs: loss_final: 0.941095, loss_mean: 0.888940, loss_mean_cls: 0.052155, grad_norm: 0.723129
[[34m2025-10-04 12:03:05[0m] Step: 965, Training Logs: loss_final: 0.950404, loss_mean: 0.900744, loss_mean_cls: 0.049660, grad_norm: 0.813456
[[34m2025-10-04 12:03:05[0m] Step: 966, Training Logs: loss_final: 0.933031, loss_mean: 0.881195, loss_mean_cls: 0.051836, grad_norm: 0.869195
[[34m2025-10-04 12:03:05[0m] Step: 967, Training Logs: loss_final: 0.946834, loss_mean: 0.895750, loss_mean_cls: 0.051084, grad_norm: 0.656018
[[34m2025-10-04 12:03:06[0m] Step: 968, Training Logs: loss_final: 0.947897, loss_mean: 0.896510, loss_mean_cls: 0.051387, grad_norm: 0.903539
[[34m2025-10-04 12:03:06[0m] Step: 969, Training Logs: loss_final: 0.952845, loss_mean: 0.901490, loss_mean_cls: 0.051355, grad_norm: 0.892634
[[34m2025-10-04 12:03:06[0m] Step: 970, Training Logs: loss_final: 0.951384, loss_mean: 0.900877, loss_mean_cls: 0.050507, grad_norm: 0.777986
[[34m2025-10-04 12:03:07[0m] Step: 971, Training Logs: loss_final: 0.953169, loss_mean: 0.902411, loss_mean_cls: 0.050759, grad_norm: 0.552399
[[34m2025-10-04 12:03:07[0m] Step: 972, Training Logs: loss_final: 0.955125, loss_mean: 0.903016, loss_mean_cls: 0.052109, grad_norm: 0.796928
[[34m2025-10-04 12:03:07[0m] Step: 973, Training Logs: loss_final: 0.944146, loss_mean: 0.892337, loss_mean_cls: 0.051809, grad_norm: 0.876000
[[34m2025-10-04 12:03:07[0m] Step: 974, Training Logs: loss_final: 0.953131, loss_mean: 0.902293, loss_mean_cls: 0.050838, grad_norm: 0.860864
[[34m2025-10-04 12:03:08[0m] Step: 975, Training Logs: loss_final: 0.966277, loss_mean: 0.915110, loss_mean_cls: 0.051167, grad_norm: 0.811094
[[34m2025-10-04 12:03:08[0m] Step: 976, Training Logs: loss_final: 0.959193, loss_mean: 0.908737, loss_mean_cls: 0.050456, grad_norm: 0.782303
[[34m2025-10-04 12:03:08[0m] Step: 977, Training Logs: loss_final: 0.978037, loss_mean: 0.927147, loss_mean_cls: 0.050891, grad_norm: 0.614985
[[34m2025-10-04 12:03:09[0m] Step: 978, Training Logs: loss_final: 0.954855, loss_mean: 0.904187, loss_mean_cls: 0.050668, grad_norm: 0.702839
[[34m2025-10-04 12:03:09[0m] Step: 979, Training Logs: loss_final: 0.951742, loss_mean: 0.900311, loss_mean_cls: 0.051431, grad_norm: 0.730374
[[34m2025-10-04 12:03:09[0m] Step: 980, Training Logs: loss_final: 0.952003, loss_mean: 0.900945, loss_mean_cls: 0.051057, grad_norm: 0.553544
[[34m2025-10-04 12:03:09[0m] Step: 981, Training Logs: loss_final: 0.949784, loss_mean: 0.897815, loss_mean_cls: 0.051969, grad_norm: 0.778555
[[34m2025-10-04 12:03:10[0m] Step: 982, Training Logs: loss_final: 0.971003, loss_mean: 0.921161, loss_mean_cls: 0.049843, grad_norm: 0.582156
[[34m2025-10-04 12:03:10[0m] Step: 983, Training Logs: loss_final: 0.951178, loss_mean: 0.900498, loss_mean_cls: 0.050680, grad_norm: 0.735859
[[34m2025-10-04 12:03:10[0m] Step: 984, Training Logs: loss_final: 0.951819, loss_mean: 0.900760, loss_mean_cls: 0.051060, grad_norm: 0.729028
[[34m2025-10-04 12:03:11[0m] Step: 985, Training Logs: loss_final: 0.954572, loss_mean: 0.902314, loss_mean_cls: 0.052258, grad_norm: 0.718333
[[34m2025-10-04 12:03:11[0m] Step: 986, Training Logs: loss_final: 0.955670, loss_mean: 0.904347, loss_mean_cls: 0.051322, grad_norm: 0.907026
[[34m2025-10-04 12:03:11[0m] Step: 987, Training Logs: loss_final: 0.955213, loss_mean: 0.904755, loss_mean_cls: 0.050458, grad_norm: 0.738342
[[34m2025-10-04 12:03:12[0m] Step: 988, Training Logs: loss_final: 0.959036, loss_mean: 0.908020, loss_mean_cls: 0.051016, grad_norm: 0.571961
[[34m2025-10-04 12:03:12[0m] Step: 989, Training Logs: loss_final: 0.952984, loss_mean: 0.901795, loss_mean_cls: 0.051188, grad_norm: 0.724804
[[34m2025-10-04 12:03:12[0m] Step: 990, Training Logs: loss_final: 0.945989, loss_mean: 0.895021, loss_mean_cls: 0.050967, grad_norm: 0.727614
[[34m2025-10-04 12:03:12[0m] Step: 991, Training Logs: loss_final: 0.967502, loss_mean: 0.915946, loss_mean_cls: 0.051556, grad_norm: 0.688045
[[34m2025-10-04 12:03:13[0m] Step: 992, Training Logs: loss_final: 0.958293, loss_mean: 0.908311, loss_mean_cls: 0.049982, grad_norm: 0.550117
[[34m2025-10-04 12:03:13[0m] Step: 993, Training Logs: loss_final: 0.937174, loss_mean: 0.886402, loss_mean_cls: 0.050772, grad_norm: 0.500539
[[34m2025-10-04 12:03:13[0m] Step: 994, Training Logs: loss_final: 0.959509, loss_mean: 0.907549, loss_mean_cls: 0.051960, grad_norm: 0.695886
[[34m2025-10-04 12:03:14[0m] Step: 995, Training Logs: loss_final: 0.945277, loss_mean: 0.893960, loss_mean_cls: 0.051317, grad_norm: 0.548638
[[34m2025-10-04 12:03:14[0m] Step: 996, Training Logs: loss_final: 0.956657, loss_mean: 0.906065, loss_mean_cls: 0.050592, grad_norm: 0.580647
[[34m2025-10-04 12:03:14[0m] Step: 997, Training Logs: loss_final: 0.974583, loss_mean: 0.924016, loss_mean_cls: 0.050566, grad_norm: 0.606199
[[34m2025-10-04 12:03:14[0m] Step: 998, Training Logs: loss_final: 0.937894, loss_mean: 0.885952, loss_mean_cls: 0.051943, grad_norm: 0.617123
[[34m2025-10-04 12:03:15[0m] Step: 999, Training Logs: loss_final: 0.968904, loss_mean: 0.918565, loss_mean_cls: 0.050339, grad_norm: 0.845769
[[34m2025-10-04 12:03:15[0m] Step: 1000, Training Logs: loss_final: 0.948538, loss_mean: 0.897692, loss_mean_cls: 0.050845, grad_norm: 0.730397
[[34m2025-10-04 12:03:15[0m] Step: 1001, Training Logs: loss_final: 0.959350, loss_mean: 0.908105, loss_mean_cls: 0.051245, grad_norm: 0.822682
[[34m2025-10-04 12:03:16[0m] Step: 1002, Training Logs: loss_final: 0.972551, loss_mean: 0.921281, loss_mean_cls: 0.051269, grad_norm: 0.489752
[[34m2025-10-04 12:03:16[0m] Step: 1003, Training Logs: loss_final: 0.974727, loss_mean: 0.923548, loss_mean_cls: 0.051180, grad_norm: 0.695354
[[34m2025-10-04 12:03:16[0m] Step: 1004, Training Logs: loss_final: 0.948018, loss_mean: 0.896638, loss_mean_cls: 0.051380, grad_norm: 0.545681
[[34m2025-10-04 12:03:17[0m] Step: 1005, Training Logs: loss_final: 0.958920, loss_mean: 0.905517, loss_mean_cls: 0.053403, grad_norm: 0.781602
[[34m2025-10-04 12:03:17[0m] Step: 1006, Training Logs: loss_final: 0.928725, loss_mean: 0.876913, loss_mean_cls: 0.051812, grad_norm: 0.649392
[[34m2025-10-04 12:03:17[0m] Step: 1007, Training Logs: loss_final: 0.936830, loss_mean: 0.885507, loss_mean_cls: 0.051324, grad_norm: 0.880261
[[34m2025-10-04 12:03:17[0m] Step: 1008, Training Logs: loss_final: 0.958285, loss_mean: 0.907489, loss_mean_cls: 0.050796, grad_norm: 1.072271
[[34m2025-10-04 12:03:18[0m] Step: 1009, Training Logs: loss_final: 0.933357, loss_mean: 0.881141, loss_mean_cls: 0.052216, grad_norm: 0.809382
[[34m2025-10-04 12:03:18[0m] Step: 1010, Training Logs: loss_final: 0.958435, loss_mean: 0.907058, loss_mean_cls: 0.051377, grad_norm: 0.797081
[[34m2025-10-04 12:03:18[0m] Step: 1011, Training Logs: loss_final: 0.954424, loss_mean: 0.903795, loss_mean_cls: 0.050630, grad_norm: 0.710178
[[34m2025-10-04 12:03:19[0m] Step: 1012, Training Logs: loss_final: 0.941195, loss_mean: 0.890707, loss_mean_cls: 0.050488, grad_norm: 0.758720
[[34m2025-10-04 12:03:19[0m] Step: 1013, Training Logs: loss_final: 0.970753, loss_mean: 0.919383, loss_mean_cls: 0.051370, grad_norm: 0.689249
[[34m2025-10-04 12:03:19[0m] Step: 1014, Training Logs: loss_final: 0.937706, loss_mean: 0.886459, loss_mean_cls: 0.051247, grad_norm: 0.653300
[[34m2025-10-04 12:03:19[0m] Step: 1015, Training Logs: loss_final: 0.960959, loss_mean: 0.911030, loss_mean_cls: 0.049929, grad_norm: 0.612671
[[34m2025-10-04 12:03:20[0m] Step: 1016, Training Logs: loss_final: 0.937543, loss_mean: 0.886677, loss_mean_cls: 0.050867, grad_norm: 0.683959
[[34m2025-10-04 12:03:20[0m] Step: 1017, Training Logs: loss_final: 0.981372, loss_mean: 0.932410, loss_mean_cls: 0.048962, grad_norm: 0.628584
[[34m2025-10-04 12:03:20[0m] Step: 1018, Training Logs: loss_final: 0.945509, loss_mean: 0.894902, loss_mean_cls: 0.050607, grad_norm: 0.767858
[[34m2025-10-04 12:03:21[0m] Step: 1019, Training Logs: loss_final: 0.950965, loss_mean: 0.899826, loss_mean_cls: 0.051139, grad_norm: 0.681576
[[34m2025-10-04 12:03:21[0m] Step: 1020, Training Logs: loss_final: 0.941640, loss_mean: 0.890495, loss_mean_cls: 0.051145, grad_norm: 0.847471
[[34m2025-10-04 12:03:21[0m] Step: 1021, Training Logs: loss_final: 0.948811, loss_mean: 0.896638, loss_mean_cls: 0.052172, grad_norm: 0.721592
[[34m2025-10-04 12:03:21[0m] Step: 1022, Training Logs: loss_final: 0.955495, loss_mean: 0.904944, loss_mean_cls: 0.050551, grad_norm: 0.778167
[[34m2025-10-04 12:03:22[0m] Step: 1023, Training Logs: loss_final: 0.956280, loss_mean: 0.905014, loss_mean_cls: 0.051266, grad_norm: 0.772684
[[34m2025-10-04 12:03:22[0m] Step: 1024, Training Logs: loss_final: 0.970575, loss_mean: 0.920898, loss_mean_cls: 0.049676, grad_norm: 1.014128
[[34m2025-10-04 12:03:22[0m] Step: 1025, Training Logs: loss_final: 0.943609, loss_mean: 0.892531, loss_mean_cls: 0.051078, grad_norm: 0.601603
[[34m2025-10-04 12:03:23[0m] Step: 1026, Training Logs: loss_final: 0.958966, loss_mean: 0.907818, loss_mean_cls: 0.051148, grad_norm: 1.055282
[[34m2025-10-04 12:03:23[0m] Step: 1027, Training Logs: loss_final: 0.937675, loss_mean: 0.886839, loss_mean_cls: 0.050835, grad_norm: 0.842798
[[34m2025-10-04 12:03:23[0m] Step: 1028, Training Logs: loss_final: 0.956841, loss_mean: 0.904934, loss_mean_cls: 0.051907, grad_norm: 0.994966
[[34m2025-10-04 12:03:24[0m] Step: 1029, Training Logs: loss_final: 0.973781, loss_mean: 0.923835, loss_mean_cls: 0.049945, grad_norm: 0.845089
[[34m2025-10-04 12:03:24[0m] Step: 1030, Training Logs: loss_final: 0.936520, loss_mean: 0.885574, loss_mean_cls: 0.050945, grad_norm: 1.089976
[[34m2025-10-04 12:03:24[0m] Step: 1031, Training Logs: loss_final: 0.944851, loss_mean: 0.894446, loss_mean_cls: 0.050405, grad_norm: 0.887662
[[34m2025-10-04 12:03:24[0m] Step: 1032, Training Logs: loss_final: 0.937380, loss_mean: 0.885993, loss_mean_cls: 0.051387, grad_norm: 0.826897
[[34m2025-10-04 12:03:25[0m] Step: 1033, Training Logs: loss_final: 0.954901, loss_mean: 0.905445, loss_mean_cls: 0.049456, grad_norm: 0.661785
[[34m2025-10-04 12:03:25[0m] Step: 1034, Training Logs: loss_final: 0.945533, loss_mean: 0.894661, loss_mean_cls: 0.050872, grad_norm: 0.796699
[[34m2025-10-04 12:03:25[0m] Step: 1035, Training Logs: loss_final: 0.952436, loss_mean: 0.901978, loss_mean_cls: 0.050459, grad_norm: 0.937178
[[34m2025-10-04 12:03:26[0m] Step: 1036, Training Logs: loss_final: 0.932455, loss_mean: 0.880755, loss_mean_cls: 0.051700, grad_norm: 0.808120
[[34m2025-10-04 12:03:26[0m] Step: 1037, Training Logs: loss_final: 0.971089, loss_mean: 0.919907, loss_mean_cls: 0.051182, grad_norm: 1.040156
[[34m2025-10-04 12:03:26[0m] Step: 1038, Training Logs: loss_final: 0.930452, loss_mean: 0.879061, loss_mean_cls: 0.051391, grad_norm: 0.602857
[[34m2025-10-04 12:03:27[0m] Step: 1039, Training Logs: loss_final: 0.958406, loss_mean: 0.909478, loss_mean_cls: 0.048928, grad_norm: 1.061513
[[34m2025-10-04 12:03:27[0m] Step: 1040, Training Logs: loss_final: 0.961072, loss_mean: 0.909138, loss_mean_cls: 0.051934, grad_norm: 0.650484
[[34m2025-10-04 12:03:27[0m] Step: 1041, Training Logs: loss_final: 0.968840, loss_mean: 0.918303, loss_mean_cls: 0.050538, grad_norm: 0.918375
[[34m2025-10-04 12:03:27[0m] Step: 1042, Training Logs: loss_final: 0.936202, loss_mean: 0.885931, loss_mean_cls: 0.050270, grad_norm: 0.815510
[[34m2025-10-04 12:03:28[0m] Step: 1043, Training Logs: loss_final: 0.945210, loss_mean: 0.894189, loss_mean_cls: 0.051021, grad_norm: 0.925777
[[34m2025-10-04 12:03:28[0m] Step: 1044, Training Logs: loss_final: 0.952747, loss_mean: 0.901433, loss_mean_cls: 0.051314, grad_norm: 0.768023
[[34m2025-10-04 12:03:28[0m] Step: 1045, Training Logs: loss_final: 0.958428, loss_mean: 0.908099, loss_mean_cls: 0.050329, grad_norm: 0.863592
[[34m2025-10-04 12:03:29[0m] Step: 1046, Training Logs: loss_final: 0.930832, loss_mean: 0.880666, loss_mean_cls: 0.050166, grad_norm: 0.707447
[[34m2025-10-04 12:03:29[0m] Step: 1047, Training Logs: loss_final: 0.937660, loss_mean: 0.886760, loss_mean_cls: 0.050899, grad_norm: 0.708995
[[34m2025-10-04 12:03:29[0m] Step: 1048, Training Logs: loss_final: 0.950557, loss_mean: 0.899617, loss_mean_cls: 0.050940, grad_norm: 0.868410
[[34m2025-10-04 12:03:29[0m] Step: 1049, Training Logs: loss_final: 0.955549, loss_mean: 0.905908, loss_mean_cls: 0.049641, grad_norm: 0.908213
[[34m2025-10-04 12:03:30[0m] Step: 1050, Training Logs: loss_final: 0.953051, loss_mean: 0.903199, loss_mean_cls: 0.049852, grad_norm: 0.631777
[[34m2025-10-04 12:03:30[0m] Step: 1051, Training Logs: loss_final: 0.952766, loss_mean: 0.902220, loss_mean_cls: 0.050546, grad_norm: 0.539794
[[34m2025-10-04 12:03:30[0m] Step: 1052, Training Logs: loss_final: 0.950767, loss_mean: 0.899219, loss_mean_cls: 0.051548, grad_norm: 0.617250
[[34m2025-10-04 12:03:31[0m] Step: 1053, Training Logs: loss_final: 0.959308, loss_mean: 0.908977, loss_mean_cls: 0.050331, grad_norm: 0.553243
[[34m2025-10-04 12:03:31[0m] Step: 1054, Training Logs: loss_final: 0.953705, loss_mean: 0.903462, loss_mean_cls: 0.050244, grad_norm: 0.655078
[[34m2025-10-04 12:03:31[0m] Step: 1055, Training Logs: loss_final: 0.959036, loss_mean: 0.909847, loss_mean_cls: 0.049189, grad_norm: 0.579363
[[34m2025-10-04 12:03:32[0m] Step: 1056, Training Logs: loss_final: 0.966869, loss_mean: 0.915924, loss_mean_cls: 0.050945, grad_norm: 0.558382
[[34m2025-10-04 12:03:32[0m] Step: 1057, Training Logs: loss_final: 0.954478, loss_mean: 0.904511, loss_mean_cls: 0.049967, grad_norm: 0.671647
[[34m2025-10-04 12:03:32[0m] Step: 1058, Training Logs: loss_final: 0.934934, loss_mean: 0.884354, loss_mean_cls: 0.050580, grad_norm: 0.588558
[[34m2025-10-04 12:03:32[0m] Step: 1059, Training Logs: loss_final: 0.954998, loss_mean: 0.905816, loss_mean_cls: 0.049182, grad_norm: 0.768030
[[34m2025-10-04 12:03:33[0m] Step: 1060, Training Logs: loss_final: 0.937691, loss_mean: 0.887167, loss_mean_cls: 0.050524, grad_norm: 0.574273
[[34m2025-10-04 12:03:33[0m] Step: 1061, Training Logs: loss_final: 0.962666, loss_mean: 0.912001, loss_mean_cls: 0.050666, grad_norm: 0.810041
[[34m2025-10-04 12:03:33[0m] Step: 1062, Training Logs: loss_final: 0.932005, loss_mean: 0.881197, loss_mean_cls: 0.050809, grad_norm: 0.662353
[[34m2025-10-04 12:03:34[0m] Step: 1063, Training Logs: loss_final: 0.945934, loss_mean: 0.893501, loss_mean_cls: 0.052433, grad_norm: 1.051534
[[34m2025-10-04 12:03:34[0m] Step: 1064, Training Logs: loss_final: 0.943485, loss_mean: 0.893254, loss_mean_cls: 0.050231, grad_norm: 0.778134
[[34m2025-10-04 12:03:34[0m] Step: 1065, Training Logs: loss_final: 0.943245, loss_mean: 0.892482, loss_mean_cls: 0.050763, grad_norm: 0.923980
[[34m2025-10-04 12:03:34[0m] Step: 1066, Training Logs: loss_final: 0.960737, loss_mean: 0.911094, loss_mean_cls: 0.049643, grad_norm: 0.590939
[[34m2025-10-04 12:03:35[0m] Step: 1067, Training Logs: loss_final: 0.939908, loss_mean: 0.888847, loss_mean_cls: 0.051061, grad_norm: 0.911057
[[34m2025-10-04 12:03:35[0m] Step: 1068, Training Logs: loss_final: 0.944353, loss_mean: 0.893332, loss_mean_cls: 0.051021, grad_norm: 0.656477
[[34m2025-10-04 12:03:35[0m] Step: 1069, Training Logs: loss_final: 0.945796, loss_mean: 0.894305, loss_mean_cls: 0.051491, grad_norm: 0.947233
[[34m2025-10-04 12:03:36[0m] Step: 1070, Training Logs: loss_final: 0.958630, loss_mean: 0.908273, loss_mean_cls: 0.050357, grad_norm: 0.681302
[[34m2025-10-04 12:03:36[0m] Step: 1071, Training Logs: loss_final: 0.963998, loss_mean: 0.913496, loss_mean_cls: 0.050502, grad_norm: 0.677791
[[34m2025-10-04 12:03:36[0m] Step: 1072, Training Logs: loss_final: 0.946582, loss_mean: 0.896235, loss_mean_cls: 0.050348, grad_norm: 0.813412
[[34m2025-10-04 12:03:37[0m] Step: 1073, Training Logs: loss_final: 0.936257, loss_mean: 0.885997, loss_mean_cls: 0.050260, grad_norm: 0.647214
[[34m2025-10-04 12:03:37[0m] Step: 1074, Training Logs: loss_final: 0.955473, loss_mean: 0.905156, loss_mean_cls: 0.050318, grad_norm: 0.761432
[[34m2025-10-04 12:03:37[0m] Step: 1075, Training Logs: loss_final: 0.947360, loss_mean: 0.896829, loss_mean_cls: 0.050531, grad_norm: 0.704021
[[34m2025-10-04 12:03:37[0m] Step: 1076, Training Logs: loss_final: 0.930616, loss_mean: 0.879364, loss_mean_cls: 0.051252, grad_norm: 0.923672
[[34m2025-10-04 12:03:38[0m] Step: 1077, Training Logs: loss_final: 0.975273, loss_mean: 0.925529, loss_mean_cls: 0.049744, grad_norm: 0.939991
[[34m2025-10-04 12:03:38[0m] Step: 1078, Training Logs: loss_final: 0.937889, loss_mean: 0.887487, loss_mean_cls: 0.050402, grad_norm: 0.774927
[[34m2025-10-04 12:03:38[0m] Step: 1079, Training Logs: loss_final: 0.965639, loss_mean: 0.915249, loss_mean_cls: 0.050390, grad_norm: 0.939788
[[34m2025-10-04 12:03:39[0m] Step: 1080, Training Logs: loss_final: 0.950898, loss_mean: 0.898914, loss_mean_cls: 0.051984, grad_norm: 0.707301
[[34m2025-10-04 12:03:39[0m] Step: 1081, Training Logs: loss_final: 0.929600, loss_mean: 0.878915, loss_mean_cls: 0.050685, grad_norm: 0.818564
[[34m2025-10-04 12:03:39[0m] Step: 1082, Training Logs: loss_final: 0.956038, loss_mean: 0.904673, loss_mean_cls: 0.051365, grad_norm: 0.645443
[[34m2025-10-04 12:03:40[0m] Step: 1083, Training Logs: loss_final: 0.962347, loss_mean: 0.911476, loss_mean_cls: 0.050871, grad_norm: 0.535758
[[34m2025-10-04 12:03:40[0m] Step: 1084, Training Logs: loss_final: 0.954706, loss_mean: 0.903898, loss_mean_cls: 0.050809, grad_norm: 0.577725
[[34m2025-10-04 12:03:40[0m] Step: 1085, Training Logs: loss_final: 0.951019, loss_mean: 0.900561, loss_mean_cls: 0.050458, grad_norm: 0.721615
[[34m2025-10-04 12:03:40[0m] Step: 1086, Training Logs: loss_final: 0.960086, loss_mean: 0.909941, loss_mean_cls: 0.050145, grad_norm: 0.637449
[[34m2025-10-04 12:03:41[0m] Step: 1087, Training Logs: loss_final: 0.945655, loss_mean: 0.893947, loss_mean_cls: 0.051708, grad_norm: 0.619389
[[34m2025-10-04 12:03:41[0m] Step: 1088, Training Logs: loss_final: 0.936306, loss_mean: 0.885307, loss_mean_cls: 0.050998, grad_norm: 0.719432
[[34m2025-10-04 12:03:41[0m] Step: 1089, Training Logs: loss_final: 0.970384, loss_mean: 0.920544, loss_mean_cls: 0.049839, grad_norm: 0.962030
[[34m2025-10-04 12:03:42[0m] Step: 1090, Training Logs: loss_final: 0.965297, loss_mean: 0.915069, loss_mean_cls: 0.050228, grad_norm: 0.587633
[[34m2025-10-04 12:03:42[0m] Step: 1091, Training Logs: loss_final: 0.951719, loss_mean: 0.901109, loss_mean_cls: 0.050611, grad_norm: 0.833106
[[34m2025-10-04 12:03:42[0m] Step: 1092, Training Logs: loss_final: 0.946479, loss_mean: 0.896073, loss_mean_cls: 0.050406, grad_norm: 0.788188
[[34m2025-10-04 12:03:43[0m] Step: 1093, Training Logs: loss_final: 0.940319, loss_mean: 0.889251, loss_mean_cls: 0.051069, grad_norm: 0.622865
[[34m2025-10-04 12:03:43[0m] Step: 1094, Training Logs: loss_final: 0.986497, loss_mean: 0.936936, loss_mean_cls: 0.049562, grad_norm: 0.611199
[[34m2025-10-04 12:03:43[0m] Step: 1095, Training Logs: loss_final: 0.942947, loss_mean: 0.891194, loss_mean_cls: 0.051753, grad_norm: 1.002906
[[34m2025-10-04 12:03:43[0m] Step: 1096, Training Logs: loss_final: 0.948615, loss_mean: 0.897446, loss_mean_cls: 0.051169, grad_norm: 1.188841
[[34m2025-10-04 12:03:44[0m] Step: 1097, Training Logs: loss_final: 0.957090, loss_mean: 0.906886, loss_mean_cls: 0.050205, grad_norm: 0.697049
[[34m2025-10-04 12:03:44[0m] Step: 1098, Training Logs: loss_final: 0.947143, loss_mean: 0.896399, loss_mean_cls: 0.050744, grad_norm: 1.021264
[[34m2025-10-04 12:03:44[0m] Step: 1099, Training Logs: loss_final: 0.924320, loss_mean: 0.873297, loss_mean_cls: 0.051023, grad_norm: 0.925755
[[34m2025-10-04 12:03:45[0m] Step: 1100, Training Logs: loss_final: 0.944277, loss_mean: 0.894067, loss_mean_cls: 0.050210, grad_norm: 0.921844
[[34m2025-10-04 12:03:45[0m] Step: 1101, Training Logs: loss_final: 0.961436, loss_mean: 0.910047, loss_mean_cls: 0.051389, grad_norm: 0.873941
[[34m2025-10-04 12:03:45[0m] Step: 1102, Training Logs: loss_final: 0.927124, loss_mean: 0.877332, loss_mean_cls: 0.049792, grad_norm: 0.893971
[[34m2025-10-04 12:03:46[0m] Step: 1103, Training Logs: loss_final: 0.932859, loss_mean: 0.882932, loss_mean_cls: 0.049927, grad_norm: 0.815555
[[34m2025-10-04 12:03:46[0m] Step: 1104, Training Logs: loss_final: 0.966933, loss_mean: 0.915885, loss_mean_cls: 0.051049, grad_norm: 0.613666
[[34m2025-10-04 12:03:46[0m] Step: 1105, Training Logs: loss_final: 0.937522, loss_mean: 0.886205, loss_mean_cls: 0.051317, grad_norm: 0.734734
[[34m2025-10-04 12:03:47[0m] Step: 1106, Training Logs: loss_final: 0.962581, loss_mean: 0.912111, loss_mean_cls: 0.050470, grad_norm: 0.585725
[[34m2025-10-04 12:03:47[0m] Step: 1107, Training Logs: loss_final: 0.939671, loss_mean: 0.888765, loss_mean_cls: 0.050905, grad_norm: 0.753046
[[34m2025-10-04 12:03:47[0m] Step: 1108, Training Logs: loss_final: 0.937176, loss_mean: 0.887312, loss_mean_cls: 0.049864, grad_norm: 0.622921
[[34m2025-10-04 12:03:47[0m] Step: 1109, Training Logs: loss_final: 0.964767, loss_mean: 0.913933, loss_mean_cls: 0.050834, grad_norm: 0.739218
[[34m2025-10-04 12:03:48[0m] Step: 1110, Training Logs: loss_final: 0.955905, loss_mean: 0.905743, loss_mean_cls: 0.050163, grad_norm: 0.671162
[[34m2025-10-04 12:03:48[0m] Step: 1111, Training Logs: loss_final: 0.941174, loss_mean: 0.891274, loss_mean_cls: 0.049900, grad_norm: 0.645633
[[34m2025-10-04 12:03:48[0m] Step: 1112, Training Logs: loss_final: 0.937282, loss_mean: 0.884994, loss_mean_cls: 0.052289, grad_norm: 0.747763
[[34m2025-10-04 12:03:49[0m] Step: 1113, Training Logs: loss_final: 0.925695, loss_mean: 0.874826, loss_mean_cls: 0.050869, grad_norm: 0.582172
[[34m2025-10-04 12:03:49[0m] Step: 1114, Training Logs: loss_final: 0.952873, loss_mean: 0.902342, loss_mean_cls: 0.050532, grad_norm: 0.780832
[[34m2025-10-04 12:03:49[0m] Step: 1115, Training Logs: loss_final: 0.934864, loss_mean: 0.884750, loss_mean_cls: 0.050113, grad_norm: 0.677099
[[34m2025-10-04 12:03:50[0m] Step: 1116, Training Logs: loss_final: 0.939405, loss_mean: 0.888918, loss_mean_cls: 0.050487, grad_norm: 0.612719
[[34m2025-10-04 12:03:50[0m] Step: 1117, Training Logs: loss_final: 0.945162, loss_mean: 0.894370, loss_mean_cls: 0.050793, grad_norm: 0.557000
[[34m2025-10-04 12:03:50[0m] Step: 1118, Training Logs: loss_final: 0.949650, loss_mean: 0.897858, loss_mean_cls: 0.051791, grad_norm: 0.568609
[[34m2025-10-04 12:03:50[0m] Step: 1119, Training Logs: loss_final: 0.940304, loss_mean: 0.890670, loss_mean_cls: 0.049635, grad_norm: 0.570643
[[34m2025-10-04 12:03:51[0m] Step: 1120, Training Logs: loss_final: 0.936007, loss_mean: 0.885374, loss_mean_cls: 0.050633, grad_norm: 0.657011
[[34m2025-10-04 12:03:51[0m] Step: 1121, Training Logs: loss_final: 0.949648, loss_mean: 0.898755, loss_mean_cls: 0.050893, grad_norm: 0.803123
[[34m2025-10-04 12:03:51[0m] Step: 1122, Training Logs: loss_final: 0.916945, loss_mean: 0.865475, loss_mean_cls: 0.051470, grad_norm: 0.702925
[[34m2025-10-04 12:03:52[0m] Step: 1123, Training Logs: loss_final: 0.961427, loss_mean: 0.910488, loss_mean_cls: 0.050939, grad_norm: 0.658906
[[34m2025-10-04 12:03:52[0m] Step: 1124, Training Logs: loss_final: 0.937056, loss_mean: 0.885836, loss_mean_cls: 0.051221, grad_norm: 0.750437
[[34m2025-10-04 12:03:52[0m] Step: 1125, Training Logs: loss_final: 0.933496, loss_mean: 0.881865, loss_mean_cls: 0.051631, grad_norm: 0.755549
[[34m2025-10-04 12:03:52[0m] Step: 1126, Training Logs: loss_final: 0.923450, loss_mean: 0.873044, loss_mean_cls: 0.050406, grad_norm: 0.995104
[[34m2025-10-04 12:03:53[0m] Step: 1127, Training Logs: loss_final: 0.932857, loss_mean: 0.882664, loss_mean_cls: 0.050192, grad_norm: 0.834511
[[34m2025-10-04 12:03:53[0m] Step: 1128, Training Logs: loss_final: 0.942807, loss_mean: 0.892574, loss_mean_cls: 0.050232, grad_norm: 0.835620
[[34m2025-10-04 12:03:53[0m] Step: 1129, Training Logs: loss_final: 0.968875, loss_mean: 0.919708, loss_mean_cls: 0.049167, grad_norm: 0.696867
[[34m2025-10-04 12:03:54[0m] Step: 1130, Training Logs: loss_final: 0.957296, loss_mean: 0.905954, loss_mean_cls: 0.051342, grad_norm: 1.073618
[[34m2025-10-04 12:03:54[0m] Step: 1131, Training Logs: loss_final: 0.923000, loss_mean: 0.872827, loss_mean_cls: 0.050173, grad_norm: 0.847517
[[34m2025-10-04 12:03:54[0m] Step: 1132, Training Logs: loss_final: 0.961503, loss_mean: 0.913139, loss_mean_cls: 0.048364, grad_norm: 0.655996
[[34m2025-10-04 12:03:55[0m] Step: 1133, Training Logs: loss_final: 0.918772, loss_mean: 0.867415, loss_mean_cls: 0.051357, grad_norm: 1.010307
[[34m2025-10-04 12:03:55[0m] Step: 1134, Training Logs: loss_final: 0.969169, loss_mean: 0.919089, loss_mean_cls: 0.050080, grad_norm: 0.659439
[[34m2025-10-04 12:03:55[0m] Step: 1135, Training Logs: loss_final: 0.953715, loss_mean: 0.902370, loss_mean_cls: 0.051346, grad_norm: 0.770352
[[34m2025-10-04 12:03:55[0m] Step: 1136, Training Logs: loss_final: 0.969302, loss_mean: 0.919790, loss_mean_cls: 0.049512, grad_norm: 0.593940
[[34m2025-10-04 12:03:56[0m] Step: 1137, Training Logs: loss_final: 0.943382, loss_mean: 0.891189, loss_mean_cls: 0.052192, grad_norm: 0.751155
[[34m2025-10-04 12:03:56[0m] Step: 1138, Training Logs: loss_final: 0.964883, loss_mean: 0.915207, loss_mean_cls: 0.049676, grad_norm: 0.725504
[[34m2025-10-04 12:03:56[0m] Step: 1139, Training Logs: loss_final: 0.946741, loss_mean: 0.896170, loss_mean_cls: 0.050571, grad_norm: 0.927073
[[34m2025-10-04 12:03:57[0m] Step: 1140, Training Logs: loss_final: 0.965086, loss_mean: 0.913816, loss_mean_cls: 0.051269, grad_norm: 0.879885
[[34m2025-10-04 12:03:57[0m] Step: 1141, Training Logs: loss_final: 0.946006, loss_mean: 0.895617, loss_mean_cls: 0.050389, grad_norm: 0.827877
[[34m2025-10-04 12:03:57[0m] Step: 1142, Training Logs: loss_final: 0.932005, loss_mean: 0.880551, loss_mean_cls: 0.051454, grad_norm: 0.797335
[[34m2025-10-04 12:03:57[0m] Step: 1143, Training Logs: loss_final: 0.950294, loss_mean: 0.900922, loss_mean_cls: 0.049372, grad_norm: 0.667473
[[34m2025-10-04 12:03:58[0m] Step: 1144, Training Logs: loss_final: 0.936779, loss_mean: 0.886476, loss_mean_cls: 0.050303, grad_norm: 0.578474
[[34m2025-10-04 12:03:58[0m] Step: 1145, Training Logs: loss_final: 0.923717, loss_mean: 0.872953, loss_mean_cls: 0.050765, grad_norm: 0.575919
[[34m2025-10-04 12:03:58[0m] Step: 1146, Training Logs: loss_final: 0.935604, loss_mean: 0.885423, loss_mean_cls: 0.050182, grad_norm: 0.661888
[[34m2025-10-04 12:03:59[0m] Step: 1147, Training Logs: loss_final: 0.953302, loss_mean: 0.903192, loss_mean_cls: 0.050110, grad_norm: 0.590264
[[34m2025-10-04 12:03:59[0m] Step: 1148, Training Logs: loss_final: 0.966324, loss_mean: 0.915475, loss_mean_cls: 0.050849, grad_norm: 0.847167
[[34m2025-10-04 12:03:59[0m] Step: 1149, Training Logs: loss_final: 0.951539, loss_mean: 0.900882, loss_mean_cls: 0.050656, grad_norm: 0.907225
[[34m2025-10-04 12:04:00[0m] Step: 1150, Training Logs: loss_final: 0.958362, loss_mean: 0.908353, loss_mean_cls: 0.050010, grad_norm: 0.883741
[[34m2025-10-04 12:04:00[0m] Step: 1151, Training Logs: loss_final: 0.937118, loss_mean: 0.886868, loss_mean_cls: 0.050251, grad_norm: 0.655541
[[34m2025-10-04 12:04:00[0m] Step: 1152, Training Logs: loss_final: 0.927547, loss_mean: 0.876517, loss_mean_cls: 0.051029, grad_norm: 0.764309
[[34m2025-10-04 12:04:00[0m] Step: 1153, Training Logs: loss_final: 0.971941, loss_mean: 0.921645, loss_mean_cls: 0.050296, grad_norm: 0.656564
[[34m2025-10-04 12:04:01[0m] Step: 1154, Training Logs: loss_final: 0.941594, loss_mean: 0.891486, loss_mean_cls: 0.050108, grad_norm: 0.505461
[[34m2025-10-04 12:04:01[0m] Step: 1155, Training Logs: loss_final: 0.951810, loss_mean: 0.901152, loss_mean_cls: 0.050658, grad_norm: 0.711565
[[34m2025-10-04 12:04:01[0m] Step: 1156, Training Logs: loss_final: 0.951151, loss_mean: 0.900048, loss_mean_cls: 0.051103, grad_norm: 0.596991
[[34m2025-10-04 12:04:02[0m] Step: 1157, Training Logs: loss_final: 0.921881, loss_mean: 0.871285, loss_mean_cls: 0.050596, grad_norm: 0.976959
[[34m2025-10-04 12:04:02[0m] Step: 1158, Training Logs: loss_final: 0.962224, loss_mean: 0.912845, loss_mean_cls: 0.049380, grad_norm: 1.029960
[[34m2025-10-04 12:04:02[0m] Step: 1159, Training Logs: loss_final: 0.951166, loss_mean: 0.900899, loss_mean_cls: 0.050268, grad_norm: 0.896163
[[34m2025-10-04 12:04:03[0m] Step: 1160, Training Logs: loss_final: 0.959921, loss_mean: 0.910488, loss_mean_cls: 0.049433, grad_norm: 0.725951
[[34m2025-10-04 12:04:03[0m] Step: 1161, Training Logs: loss_final: 0.937808, loss_mean: 0.888082, loss_mean_cls: 0.049725, grad_norm: 0.634936
[[34m2025-10-04 12:04:03[0m] Step: 1162, Training Logs: loss_final: 0.930813, loss_mean: 0.880010, loss_mean_cls: 0.050803, grad_norm: 0.840307
[[34m2025-10-04 12:04:03[0m] Step: 1163, Training Logs: loss_final: 0.953892, loss_mean: 0.903200, loss_mean_cls: 0.050691, grad_norm: 0.648163
[[34m2025-10-04 12:04:04[0m] Step: 1164, Training Logs: loss_final: 0.955525, loss_mean: 0.904259, loss_mean_cls: 0.051266, grad_norm: 0.800640
[[34m2025-10-04 12:04:04[0m] Step: 1165, Training Logs: loss_final: 0.953745, loss_mean: 0.904641, loss_mean_cls: 0.049104, grad_norm: 0.863433
[[34m2025-10-04 12:04:04[0m] Step: 1166, Training Logs: loss_final: 0.949633, loss_mean: 0.898260, loss_mean_cls: 0.051373, grad_norm: 0.699871
[[34m2025-10-04 12:04:05[0m] Step: 1167, Training Logs: loss_final: 0.952684, loss_mean: 0.901913, loss_mean_cls: 0.050771, grad_norm: 0.763800
[[34m2025-10-04 12:04:05[0m] Step: 1168, Training Logs: loss_final: 0.956854, loss_mean: 0.906014, loss_mean_cls: 0.050840, grad_norm: 0.507327
[[34m2025-10-04 12:04:05[0m] Step: 1169, Training Logs: loss_final: 0.923296, loss_mean: 0.873420, loss_mean_cls: 0.049877, grad_norm: 0.699059
[[34m2025-10-04 12:04:05[0m] Step: 1170, Training Logs: loss_final: 0.941730, loss_mean: 0.892550, loss_mean_cls: 0.049180, grad_norm: 0.618654
[[34m2025-10-04 12:04:06[0m] Step: 1171, Training Logs: loss_final: 0.943429, loss_mean: 0.893253, loss_mean_cls: 0.050176, grad_norm: 0.606713
[[34m2025-10-04 12:04:06[0m] Step: 1172, Training Logs: loss_final: 0.938341, loss_mean: 0.887715, loss_mean_cls: 0.050626, grad_norm: 0.829028
[[34m2025-10-04 12:04:06[0m] Step: 1173, Training Logs: loss_final: 0.917371, loss_mean: 0.865851, loss_mean_cls: 0.051520, grad_norm: 0.649203
[[34m2025-10-04 12:04:07[0m] Step: 1174, Training Logs: loss_final: 0.926703, loss_mean: 0.875734, loss_mean_cls: 0.050968, grad_norm: 0.731136
[[34m2025-10-04 12:04:07[0m] Step: 1175, Training Logs: loss_final: 0.959544, loss_mean: 0.909317, loss_mean_cls: 0.050227, grad_norm: 0.978550
[[34m2025-10-04 12:04:07[0m] Step: 1176, Training Logs: loss_final: 0.933362, loss_mean: 0.883579, loss_mean_cls: 0.049782, grad_norm: 0.504276
[[34m2025-10-04 12:04:07[0m] Step: 1177, Training Logs: loss_final: 0.940215, loss_mean: 0.890560, loss_mean_cls: 0.049656, grad_norm: 1.071404
[[34m2025-10-04 12:04:08[0m] Step: 1178, Training Logs: loss_final: 0.944105, loss_mean: 0.893581, loss_mean_cls: 0.050524, grad_norm: 0.627879
[[34m2025-10-04 12:04:08[0m] Step: 1179, Training Logs: loss_final: 0.956614, loss_mean: 0.907182, loss_mean_cls: 0.049433, grad_norm: 0.809458
[[34m2025-10-04 12:04:08[0m] Step: 1180, Training Logs: loss_final: 0.946261, loss_mean: 0.895287, loss_mean_cls: 0.050974, grad_norm: 0.901084
[[34m2025-10-04 12:04:09[0m] Step: 1181, Training Logs: loss_final: 0.930346, loss_mean: 0.879678, loss_mean_cls: 0.050669, grad_norm: 0.914844
[[34m2025-10-04 12:04:09[0m] Step: 1182, Training Logs: loss_final: 0.931741, loss_mean: 0.879460, loss_mean_cls: 0.052281, grad_norm: 0.758011
[[34m2025-10-04 12:04:09[0m] Step: 1183, Training Logs: loss_final: 0.957486, loss_mean: 0.906544, loss_mean_cls: 0.050943, grad_norm: 0.943527
[[34m2025-10-04 12:04:10[0m] Step: 1184, Training Logs: loss_final: 0.939654, loss_mean: 0.888598, loss_mean_cls: 0.051055, grad_norm: 0.576330
[[34m2025-10-04 12:04:10[0m] Step: 1185, Training Logs: loss_final: 0.933534, loss_mean: 0.883848, loss_mean_cls: 0.049685, grad_norm: 0.719522
[[34m2025-10-04 12:04:10[0m] Step: 1186, Training Logs: loss_final: 0.945028, loss_mean: 0.894518, loss_mean_cls: 0.050510, grad_norm: 0.641028
[[34m2025-10-04 12:04:10[0m] Step: 1187, Training Logs: loss_final: 0.967194, loss_mean: 0.917215, loss_mean_cls: 0.049979, grad_norm: 0.544328
[[34m2025-10-04 12:04:11[0m] Step: 1188, Training Logs: loss_final: 0.940900, loss_mean: 0.891274, loss_mean_cls: 0.049626, grad_norm: 0.725612
[[34m2025-10-04 12:04:11[0m] Step: 1189, Training Logs: loss_final: 0.961795, loss_mean: 0.911666, loss_mean_cls: 0.050130, grad_norm: 0.594051
[[34m2025-10-04 12:04:11[0m] Step: 1190, Training Logs: loss_final: 0.941285, loss_mean: 0.892058, loss_mean_cls: 0.049227, grad_norm: 0.543413
[[34m2025-10-04 12:04:12[0m] Step: 1191, Training Logs: loss_final: 0.953555, loss_mean: 0.902822, loss_mean_cls: 0.050733, grad_norm: 0.593268
[[34m2025-10-04 12:04:12[0m] Step: 1192, Training Logs: loss_final: 0.953782, loss_mean: 0.903609, loss_mean_cls: 0.050173, grad_norm: 0.560370
[[34m2025-10-04 12:04:12[0m] Step: 1193, Training Logs: loss_final: 0.981439, loss_mean: 0.932567, loss_mean_cls: 0.048872, grad_norm: 0.754785
[[34m2025-10-04 12:04:12[0m] Step: 1194, Training Logs: loss_final: 0.936750, loss_mean: 0.886657, loss_mean_cls: 0.050093, grad_norm: 0.794007
[[34m2025-10-04 12:04:13[0m] Step: 1195, Training Logs: loss_final: 0.925510, loss_mean: 0.874991, loss_mean_cls: 0.050520, grad_norm: 0.516537
[[34m2025-10-04 12:04:13[0m] Step: 1196, Training Logs: loss_final: 0.944966, loss_mean: 0.894353, loss_mean_cls: 0.050613, grad_norm: 0.713819
[[34m2025-10-04 12:04:13[0m] Step: 1197, Training Logs: loss_final: 0.932394, loss_mean: 0.882463, loss_mean_cls: 0.049931, grad_norm: 0.548295
[[34m2025-10-04 12:04:14[0m] Step: 1198, Training Logs: loss_final: 0.953368, loss_mean: 0.904632, loss_mean_cls: 0.048736, grad_norm: 0.582501
[[34m2025-10-04 12:04:14[0m] Step: 1199, Training Logs: loss_final: 0.959341, loss_mean: 0.910439, loss_mean_cls: 0.048903, grad_norm: 0.634731
[[34m2025-10-04 12:04:14[0m] Step: 1200, Training Logs: loss_final: 0.964534, loss_mean: 0.915182, loss_mean_cls: 0.049352, grad_norm: 0.506842
[[34m2025-10-04 12:04:14[0m] Step: 1201, Training Logs: loss_final: 0.948481, loss_mean: 0.898645, loss_mean_cls: 0.049836, grad_norm: 0.670239
[[34m2025-10-04 12:04:15[0m] Step: 1202, Training Logs: loss_final: 0.943669, loss_mean: 0.891679, loss_mean_cls: 0.051990, grad_norm: 0.732255
[[34m2025-10-04 12:04:15[0m] Step: 1203, Training Logs: loss_final: 0.950242, loss_mean: 0.900076, loss_mean_cls: 0.050166, grad_norm: 0.711579
[[34m2025-10-04 12:04:15[0m] Step: 1204, Training Logs: loss_final: 0.952507, loss_mean: 0.903727, loss_mean_cls: 0.048779, grad_norm: 1.058803
[[34m2025-10-04 12:04:16[0m] Step: 1205, Training Logs: loss_final: 0.956621, loss_mean: 0.906760, loss_mean_cls: 0.049861, grad_norm: 0.688916
[[34m2025-10-04 12:04:16[0m] Step: 1206, Training Logs: loss_final: 0.954116, loss_mean: 0.904397, loss_mean_cls: 0.049719, grad_norm: 0.773914
[[34m2025-10-04 12:04:16[0m] Step: 1207, Training Logs: loss_final: 0.977028, loss_mean: 0.927396, loss_mean_cls: 0.049632, grad_norm: 0.962049
[[34m2025-10-04 12:04:17[0m] Step: 1208, Training Logs: loss_final: 0.932822, loss_mean: 0.882427, loss_mean_cls: 0.050395, grad_norm: 0.961458
[[34m2025-10-04 12:04:17[0m] Step: 1209, Training Logs: loss_final: 0.942397, loss_mean: 0.893269, loss_mean_cls: 0.049128, grad_norm: 0.717561
[[34m2025-10-04 12:04:17[0m] Step: 1210, Training Logs: loss_final: 0.950407, loss_mean: 0.899651, loss_mean_cls: 0.050756, grad_norm: 0.602262
[[34m2025-10-04 12:04:17[0m] Step: 1211, Training Logs: loss_final: 0.914898, loss_mean: 0.864221, loss_mean_cls: 0.050677, grad_norm: 0.814395
[[34m2025-10-04 12:04:18[0m] Step: 1212, Training Logs: loss_final: 0.975816, loss_mean: 0.925566, loss_mean_cls: 0.050250, grad_norm: 0.548772
[[34m2025-10-04 12:04:18[0m] Step: 1213, Training Logs: loss_final: 0.948424, loss_mean: 0.898173, loss_mean_cls: 0.050251, grad_norm: 0.694920
[[34m2025-10-04 12:04:18[0m] Step: 1214, Training Logs: loss_final: 0.906363, loss_mean: 0.856050, loss_mean_cls: 0.050313, grad_norm: 0.756459
[[34m2025-10-04 12:04:19[0m] Step: 1215, Training Logs: loss_final: 0.962112, loss_mean: 0.913236, loss_mean_cls: 0.048877, grad_norm: 0.589022
[[34m2025-10-04 12:04:19[0m] Step: 1216, Training Logs: loss_final: 0.954085, loss_mean: 0.903776, loss_mean_cls: 0.050309, grad_norm: 0.665669
[[34m2025-10-04 12:04:19[0m] Step: 1217, Training Logs: loss_final: 0.932564, loss_mean: 0.882268, loss_mean_cls: 0.050296, grad_norm: 0.702331
[[34m2025-10-04 12:04:20[0m] Step: 1218, Training Logs: loss_final: 0.947314, loss_mean: 0.896949, loss_mean_cls: 0.050364, grad_norm: 0.657534
[[34m2025-10-04 12:04:20[0m] Step: 1219, Training Logs: loss_final: 0.952540, loss_mean: 0.901929, loss_mean_cls: 0.050612, grad_norm: 0.694714
[[34m2025-10-04 12:04:20[0m] Step: 1220, Training Logs: loss_final: 0.950413, loss_mean: 0.900566, loss_mean_cls: 0.049847, grad_norm: 0.806125
[[34m2025-10-04 12:04:20[0m] Step: 1221, Training Logs: loss_final: 0.946602, loss_mean: 0.897279, loss_mean_cls: 0.049323, grad_norm: 0.613477
[[34m2025-10-04 12:04:21[0m] Step: 1222, Training Logs: loss_final: 0.959039, loss_mean: 0.909521, loss_mean_cls: 0.049519, grad_norm: 0.808779
[[34m2025-10-04 12:04:21[0m] Step: 1223, Training Logs: loss_final: 0.950223, loss_mean: 0.900453, loss_mean_cls: 0.049770, grad_norm: 0.794564
[[34m2025-10-04 12:04:21[0m] Step: 1224, Training Logs: loss_final: 0.980529, loss_mean: 0.930763, loss_mean_cls: 0.049766, grad_norm: 0.641183
[[34m2025-10-04 12:04:22[0m] Step: 1225, Training Logs: loss_final: 0.938667, loss_mean: 0.888762, loss_mean_cls: 0.049905, grad_norm: 0.842194
[[34m2025-10-04 12:04:22[0m] Step: 1226, Training Logs: loss_final: 0.945931, loss_mean: 0.895257, loss_mean_cls: 0.050674, grad_norm: 0.668536
[[34m2025-10-04 12:04:22[0m] Step: 1227, Training Logs: loss_final: 0.931463, loss_mean: 0.881237, loss_mean_cls: 0.050227, grad_norm: 0.650539
[[34m2025-10-04 12:04:22[0m] Step: 1228, Training Logs: loss_final: 0.939646, loss_mean: 0.889594, loss_mean_cls: 0.050052, grad_norm: 0.620476
[[34m2025-10-04 12:04:23[0m] Step: 1229, Training Logs: loss_final: 0.954095, loss_mean: 0.902880, loss_mean_cls: 0.051216, grad_norm: 0.503911
[[34m2025-10-04 12:04:23[0m] Step: 1230, Training Logs: loss_final: 0.947064, loss_mean: 0.896404, loss_mean_cls: 0.050660, grad_norm: 0.507027
[[34m2025-10-04 12:04:23[0m] Step: 1231, Training Logs: loss_final: 0.977785, loss_mean: 0.928846, loss_mean_cls: 0.048940, grad_norm: 0.622089
[[34m2025-10-04 12:04:24[0m] Step: 1232, Training Logs: loss_final: 0.970746, loss_mean: 0.920504, loss_mean_cls: 0.050242, grad_norm: 0.724621
[[34m2025-10-04 12:04:24[0m] Step: 1233, Training Logs: loss_final: 0.957865, loss_mean: 0.908063, loss_mean_cls: 0.049803, grad_norm: 0.486712
[[34m2025-10-04 12:04:24[0m] Step: 1234, Training Logs: loss_final: 0.939569, loss_mean: 0.888938, loss_mean_cls: 0.050631, grad_norm: 0.510845
[[34m2025-10-04 12:04:25[0m] Step: 1235, Training Logs: loss_final: 0.927398, loss_mean: 0.877180, loss_mean_cls: 0.050218, grad_norm: 0.544932
[[34m2025-10-04 12:04:25[0m] Step: 1236, Training Logs: loss_final: 0.942657, loss_mean: 0.892017, loss_mean_cls: 0.050640, grad_norm: 0.707190
[[34m2025-10-04 12:04:25[0m] Step: 1237, Training Logs: loss_final: 0.940755, loss_mean: 0.889660, loss_mean_cls: 0.051095, grad_norm: 0.699411
[[34m2025-10-04 12:04:25[0m] Step: 1238, Training Logs: loss_final: 0.943317, loss_mean: 0.893845, loss_mean_cls: 0.049472, grad_norm: 0.629183
[[34m2025-10-04 12:04:26[0m] Step: 1239, Training Logs: loss_final: 0.952450, loss_mean: 0.902409, loss_mean_cls: 0.050041, grad_norm: 0.570351
[[34m2025-10-04 12:04:26[0m] Step: 1240, Training Logs: loss_final: 0.911958, loss_mean: 0.863198, loss_mean_cls: 0.048761, grad_norm: 0.664937
[[34m2025-10-04 12:04:26[0m] Step: 1241, Training Logs: loss_final: 0.951518, loss_mean: 0.901124, loss_mean_cls: 0.050394, grad_norm: 0.681365
[[34m2025-10-04 12:04:27[0m] Step: 1242, Training Logs: loss_final: 0.946837, loss_mean: 0.897040, loss_mean_cls: 0.049797, grad_norm: 0.564757
[[34m2025-10-04 12:04:27[0m] Step: 1243, Training Logs: loss_final: 0.944196, loss_mean: 0.893707, loss_mean_cls: 0.050489, grad_norm: 0.627057
[[34m2025-10-04 12:04:27[0m] Step: 1244, Training Logs: loss_final: 0.941937, loss_mean: 0.891355, loss_mean_cls: 0.050582, grad_norm: 0.736938
[[34m2025-10-04 12:04:27[0m] Step: 1245, Training Logs: loss_final: 0.942374, loss_mean: 0.892090, loss_mean_cls: 0.050284, grad_norm: 0.626638
[[34m2025-10-04 12:04:28[0m] Step: 1246, Training Logs: loss_final: 0.930969, loss_mean: 0.880669, loss_mean_cls: 0.050300, grad_norm: 0.856918
[[34m2025-10-04 12:04:28[0m] Step: 1247, Training Logs: loss_final: 0.933503, loss_mean: 0.883177, loss_mean_cls: 0.050326, grad_norm: 0.481265
[[34m2025-10-04 12:04:28[0m] Step: 1248, Training Logs: loss_final: 0.937002, loss_mean: 0.886653, loss_mean_cls: 0.050349, grad_norm: 0.722150
[[34m2025-10-04 12:04:29[0m] Step: 1249, Training Logs: loss_final: 0.946771, loss_mean: 0.896528, loss_mean_cls: 0.050243, grad_norm: 0.638899
[[34m2025-10-04 12:04:29[0m] Step: 1250, Training Logs: loss_final: 0.950150, loss_mean: 0.899297, loss_mean_cls: 0.050854, grad_norm: 0.803652
[[34m2025-10-04 12:04:29[0m] Step: 1251, Training Logs: loss_final: 0.967678, loss_mean: 0.916366, loss_mean_cls: 0.051312, grad_norm: 0.565943
[[34m2025-10-04 12:04:29[0m] Step: 1252, Training Logs: loss_final: 0.951894, loss_mean: 0.900623, loss_mean_cls: 0.051271, grad_norm: 0.516891
[[34m2025-10-04 12:04:30[0m] Step: 1253, Training Logs: loss_final: 0.918663, loss_mean: 0.867900, loss_mean_cls: 0.050763, grad_norm: 0.673502
[[34m2025-10-04 12:04:30[0m] Step: 1254, Training Logs: loss_final: 0.940880, loss_mean: 0.890491, loss_mean_cls: 0.050389, grad_norm: 0.569124
[[34m2025-10-04 12:04:30[0m] Step: 1255, Training Logs: loss_final: 0.943070, loss_mean: 0.893645, loss_mean_cls: 0.049426, grad_norm: 0.748168
[[34m2025-10-04 12:04:31[0m] Step: 1256, Training Logs: loss_final: 0.930700, loss_mean: 0.880673, loss_mean_cls: 0.050027, grad_norm: 0.681020
[[34m2025-10-04 12:04:31[0m] Step: 1257, Training Logs: loss_final: 0.952551, loss_mean: 0.902650, loss_mean_cls: 0.049901, grad_norm: 0.981595
[[34m2025-10-04 12:04:31[0m] Step: 1258, Training Logs: loss_final: 0.925222, loss_mean: 0.875327, loss_mean_cls: 0.049895, grad_norm: 0.839485
[[34m2025-10-04 12:04:32[0m] Step: 1259, Training Logs: loss_final: 0.942466, loss_mean: 0.892559, loss_mean_cls: 0.049907, grad_norm: 0.891518
[[34m2025-10-04 12:04:32[0m] Step: 1260, Training Logs: loss_final: 0.944602, loss_mean: 0.894747, loss_mean_cls: 0.049854, grad_norm: 0.861944
[[34m2025-10-04 12:04:32[0m] Step: 1261, Training Logs: loss_final: 0.942030, loss_mean: 0.892896, loss_mean_cls: 0.049134, grad_norm: 0.774862
[[34m2025-10-04 12:04:33[0m] Step: 1262, Training Logs: loss_final: 0.954274, loss_mean: 0.904013, loss_mean_cls: 0.050261, grad_norm: 0.755843
[[34m2025-10-04 12:04:33[0m] Step: 1263, Training Logs: loss_final: 0.965469, loss_mean: 0.915386, loss_mean_cls: 0.050083, grad_norm: 0.766419
[[34m2025-10-04 12:04:33[0m] Step: 1264, Training Logs: loss_final: 0.944032, loss_mean: 0.893753, loss_mean_cls: 0.050278, grad_norm: 0.800364
[[34m2025-10-04 12:04:33[0m] Step: 1265, Training Logs: loss_final: 0.944436, loss_mean: 0.894422, loss_mean_cls: 0.050013, grad_norm: 0.740667
[[34m2025-10-04 12:04:34[0m] Step: 1266, Training Logs: loss_final: 0.973172, loss_mean: 0.923120, loss_mean_cls: 0.050052, grad_norm: 0.657207
[[34m2025-10-04 12:04:34[0m] Step: 1267, Training Logs: loss_final: 0.950763, loss_mean: 0.901154, loss_mean_cls: 0.049608, grad_norm: 0.731520
[[34m2025-10-04 12:04:34[0m] Step: 1268, Training Logs: loss_final: 0.923868, loss_mean: 0.874282, loss_mean_cls: 0.049586, grad_norm: 0.641390
[[34m2025-10-04 12:04:35[0m] Step: 1269, Training Logs: loss_final: 0.969118, loss_mean: 0.920007, loss_mean_cls: 0.049111, grad_norm: 0.696603
[[34m2025-10-04 12:04:35[0m] Step: 1270, Training Logs: loss_final: 0.934610, loss_mean: 0.884515, loss_mean_cls: 0.050096, grad_norm: 0.885056
[[34m2025-10-04 12:04:35[0m] Step: 1271, Training Logs: loss_final: 0.923004, loss_mean: 0.871755, loss_mean_cls: 0.051248, grad_norm: 0.693888
[[34m2025-10-04 12:04:35[0m] Step: 1272, Training Logs: loss_final: 0.941095, loss_mean: 0.891840, loss_mean_cls: 0.049256, grad_norm: 0.921147
[[34m2025-10-04 12:04:36[0m] Step: 1273, Training Logs: loss_final: 0.921646, loss_mean: 0.871849, loss_mean_cls: 0.049798, grad_norm: 0.660111
[[34m2025-10-04 12:04:36[0m] Step: 1274, Training Logs: loss_final: 0.957675, loss_mean: 0.907721, loss_mean_cls: 0.049954, grad_norm: 0.702191
[[34m2025-10-04 12:04:36[0m] Step: 1275, Training Logs: loss_final: 0.936254, loss_mean: 0.886250, loss_mean_cls: 0.050004, grad_norm: 0.619686
[[34m2025-10-04 12:04:37[0m] Step: 1276, Training Logs: loss_final: 0.938788, loss_mean: 0.889219, loss_mean_cls: 0.049569, grad_norm: 0.547533
[[34m2025-10-04 12:04:37[0m] Step: 1277, Training Logs: loss_final: 0.941594, loss_mean: 0.891983, loss_mean_cls: 0.049610, grad_norm: 0.958292
[[34m2025-10-04 12:04:37[0m] Step: 1278, Training Logs: loss_final: 0.948991, loss_mean: 0.900916, loss_mean_cls: 0.048075, grad_norm: 0.578398
[[34m2025-10-04 12:04:38[0m] Step: 1279, Training Logs: loss_final: 0.942697, loss_mean: 0.894309, loss_mean_cls: 0.048389, grad_norm: 0.899269
[[34m2025-10-04 12:04:38[0m] Step: 1280, Training Logs: loss_final: 0.954395, loss_mean: 0.905223, loss_mean_cls: 0.049171, grad_norm: 0.610660
[[34m2025-10-04 12:04:38[0m] Step: 1281, Training Logs: loss_final: 0.936071, loss_mean: 0.886660, loss_mean_cls: 0.049410, grad_norm: 0.733950
[[34m2025-10-04 12:04:38[0m] Step: 1282, Training Logs: loss_final: 0.931749, loss_mean: 0.880676, loss_mean_cls: 0.051073, grad_norm: 1.193040
[[34m2025-10-04 12:04:39[0m] Step: 1283, Training Logs: loss_final: 0.954392, loss_mean: 0.905621, loss_mean_cls: 0.048771, grad_norm: 0.737599
[[34m2025-10-04 12:04:39[0m] Step: 1284, Training Logs: loss_final: 0.949244, loss_mean: 0.900375, loss_mean_cls: 0.048869, grad_norm: 0.943918
[[34m2025-10-04 12:04:39[0m] Step: 1285, Training Logs: loss_final: 0.940967, loss_mean: 0.891505, loss_mean_cls: 0.049463, grad_norm: 0.866570
[[34m2025-10-04 12:04:40[0m] Step: 1286, Training Logs: loss_final: 0.956809, loss_mean: 0.907722, loss_mean_cls: 0.049086, grad_norm: 0.993206
[[34m2025-10-04 12:04:40[0m] Step: 1287, Training Logs: loss_final: 0.938253, loss_mean: 0.887828, loss_mean_cls: 0.050425, grad_norm: 0.848462
[[34m2025-10-04 12:04:40[0m] Step: 1288, Training Logs: loss_final: 0.958977, loss_mean: 0.910126, loss_mean_cls: 0.048851, grad_norm: 0.990058
[[34m2025-10-04 12:04:41[0m] Step: 1289, Training Logs: loss_final: 0.931882, loss_mean: 0.882792, loss_mean_cls: 0.049090, grad_norm: 0.465885
[[34m2025-10-04 12:04:41[0m] Step: 1290, Training Logs: loss_final: 0.941135, loss_mean: 0.892697, loss_mean_cls: 0.048438, grad_norm: 0.820447
[[34m2025-10-04 12:04:41[0m] Step: 1291, Training Logs: loss_final: 0.937487, loss_mean: 0.888023, loss_mean_cls: 0.049465, grad_norm: 0.644611
[[34m2025-10-04 12:04:41[0m] Step: 1292, Training Logs: loss_final: 0.952492, loss_mean: 0.902477, loss_mean_cls: 0.050015, grad_norm: 0.494214
[[34m2025-10-04 12:04:42[0m] Step: 1293, Training Logs: loss_final: 0.935084, loss_mean: 0.885296, loss_mean_cls: 0.049788, grad_norm: 0.638421
[[34m2025-10-04 12:04:42[0m] Step: 1294, Training Logs: loss_final: 0.931336, loss_mean: 0.882158, loss_mean_cls: 0.049178, grad_norm: 0.504767
[[34m2025-10-04 12:04:42[0m] Step: 1295, Training Logs: loss_final: 0.949568, loss_mean: 0.899241, loss_mean_cls: 0.050327, grad_norm: 0.630487
[[34m2025-10-04 12:04:43[0m] Step: 1296, Training Logs: loss_final: 0.931559, loss_mean: 0.882719, loss_mean_cls: 0.048840, grad_norm: 0.473136
[[34m2025-10-04 12:04:43[0m] Step: 1297, Training Logs: loss_final: 0.950276, loss_mean: 0.900751, loss_mean_cls: 0.049525, grad_norm: 0.681269
[[34m2025-10-04 12:04:43[0m] Step: 1298, Training Logs: loss_final: 0.915379, loss_mean: 0.864212, loss_mean_cls: 0.051168, grad_norm: 0.660452
[[34m2025-10-04 12:04:43[0m] Step: 1299, Training Logs: loss_final: 0.945666, loss_mean: 0.896445, loss_mean_cls: 0.049221, grad_norm: 0.428834
[[34m2025-10-04 12:04:44[0m] Step: 1300, Training Logs: loss_final: 0.948252, loss_mean: 0.897410, loss_mean_cls: 0.050842, grad_norm: 0.836385
[[34m2025-10-04 12:04:44[0m] Step: 1301, Training Logs: loss_final: 0.951428, loss_mean: 0.900571, loss_mean_cls: 0.050857, grad_norm: 0.561836
[[34m2025-10-04 12:04:44[0m] Step: 1302, Training Logs: loss_final: 0.945972, loss_mean: 0.895924, loss_mean_cls: 0.050048, grad_norm: 0.604374
[[34m2025-10-04 12:04:45[0m] Step: 1303, Training Logs: loss_final: 0.933859, loss_mean: 0.883045, loss_mean_cls: 0.050814, grad_norm: 0.616422
[[34m2025-10-04 12:04:45[0m] Step: 1304, Training Logs: loss_final: 0.932867, loss_mean: 0.882917, loss_mean_cls: 0.049950, grad_norm: 0.685475
[[34m2025-10-04 12:04:45[0m] Step: 1305, Training Logs: loss_final: 0.950858, loss_mean: 0.901819, loss_mean_cls: 0.049039, grad_norm: 0.506230
[[34m2025-10-04 12:04:46[0m] Step: 1306, Training Logs: loss_final: 0.928597, loss_mean: 0.878077, loss_mean_cls: 0.050520, grad_norm: 0.840492
[[34m2025-10-04 12:04:46[0m] Step: 1307, Training Logs: loss_final: 0.942109, loss_mean: 0.893017, loss_mean_cls: 0.049092, grad_norm: 0.725394
[[34m2025-10-04 12:04:46[0m] Step: 1308, Training Logs: loss_final: 0.946986, loss_mean: 0.896943, loss_mean_cls: 0.050043, grad_norm: 0.565874
[[34m2025-10-04 12:04:46[0m] Step: 1309, Training Logs: loss_final: 0.947482, loss_mean: 0.898482, loss_mean_cls: 0.049000, grad_norm: 0.722728
[[34m2025-10-04 12:04:47[0m] Step: 1310, Training Logs: loss_final: 0.927064, loss_mean: 0.875657, loss_mean_cls: 0.051407, grad_norm: 0.763746
[[34m2025-10-04 12:04:47[0m] Step: 1311, Training Logs: loss_final: 0.935544, loss_mean: 0.885270, loss_mean_cls: 0.050274, grad_norm: 0.886894
[[34m2025-10-04 12:04:47[0m] Step: 1312, Training Logs: loss_final: 0.934164, loss_mean: 0.884342, loss_mean_cls: 0.049822, grad_norm: 0.840696
[[34m2025-10-04 12:04:48[0m] Step: 1313, Training Logs: loss_final: 0.952044, loss_mean: 0.900990, loss_mean_cls: 0.051054, grad_norm: 0.525243
[[34m2025-10-04 12:04:48[0m] Step: 1314, Training Logs: loss_final: 0.924644, loss_mean: 0.874734, loss_mean_cls: 0.049910, grad_norm: 0.711088
[[34m2025-10-04 12:04:48[0m] Step: 1315, Training Logs: loss_final: 0.938343, loss_mean: 0.887544, loss_mean_cls: 0.050799, grad_norm: 0.675390
[[34m2025-10-04 12:04:48[0m] Step: 1316, Training Logs: loss_final: 0.925542, loss_mean: 0.874635, loss_mean_cls: 0.050906, grad_norm: 0.682438
[[34m2025-10-04 12:04:49[0m] Step: 1317, Training Logs: loss_final: 0.938489, loss_mean: 0.887795, loss_mean_cls: 0.050694, grad_norm: 0.833262
[[34m2025-10-04 12:04:49[0m] Step: 1318, Training Logs: loss_final: 0.955767, loss_mean: 0.905226, loss_mean_cls: 0.050541, grad_norm: 0.674722
[[34m2025-10-04 12:04:49[0m] Step: 1319, Training Logs: loss_final: 0.948720, loss_mean: 0.898831, loss_mean_cls: 0.049890, grad_norm: 0.572843
[[34m2025-10-04 12:04:50[0m] Step: 1320, Training Logs: loss_final: 0.936842, loss_mean: 0.886029, loss_mean_cls: 0.050814, grad_norm: 0.778675
[[34m2025-10-04 12:04:50[0m] Step: 1321, Training Logs: loss_final: 0.941939, loss_mean: 0.891919, loss_mean_cls: 0.050020, grad_norm: 0.606317
[[34m2025-10-04 12:04:50[0m] Step: 1322, Training Logs: loss_final: 0.927733, loss_mean: 0.878393, loss_mean_cls: 0.049340, grad_norm: 0.572711
[[34m2025-10-04 12:04:50[0m] Step: 1323, Training Logs: loss_final: 0.941068, loss_mean: 0.891670, loss_mean_cls: 0.049399, grad_norm: 0.679004
[[34m2025-10-04 12:04:51[0m] Step: 1324, Training Logs: loss_final: 0.920115, loss_mean: 0.870886, loss_mean_cls: 0.049229, grad_norm: 0.659070
[[34m2025-10-04 12:04:51[0m] Step: 1325, Training Logs: loss_final: 0.933106, loss_mean: 0.883432, loss_mean_cls: 0.049673, grad_norm: 0.737376
[[34m2025-10-04 12:04:51[0m] Step: 1326, Training Logs: loss_final: 0.957966, loss_mean: 0.908708, loss_mean_cls: 0.049258, grad_norm: 0.507972
[[34m2025-10-04 12:04:52[0m] Step: 1327, Training Logs: loss_final: 0.935748, loss_mean: 0.886579, loss_mean_cls: 0.049169, grad_norm: 0.620774
[[34m2025-10-04 12:04:52[0m] Step: 1328, Training Logs: loss_final: 0.946850, loss_mean: 0.896600, loss_mean_cls: 0.050250, grad_norm: 0.800616
[[34m2025-10-04 12:04:52[0m] Step: 1329, Training Logs: loss_final: 0.946563, loss_mean: 0.896026, loss_mean_cls: 0.050538, grad_norm: 0.677193
[[34m2025-10-04 12:04:53[0m] Step: 1330, Training Logs: loss_final: 0.945632, loss_mean: 0.896367, loss_mean_cls: 0.049265, grad_norm: 0.711628
[[34m2025-10-04 12:04:53[0m] Step: 1331, Training Logs: loss_final: 0.937233, loss_mean: 0.886700, loss_mean_cls: 0.050533, grad_norm: 0.515915
[[34m2025-10-04 12:04:53[0m] Step: 1332, Training Logs: loss_final: 0.962921, loss_mean: 0.912861, loss_mean_cls: 0.050060, grad_norm: 0.603721
[[34m2025-10-04 12:04:53[0m] Step: 1333, Training Logs: loss_final: 0.941512, loss_mean: 0.890626, loss_mean_cls: 0.050885, grad_norm: 0.610231
[[34m2025-10-04 12:04:54[0m] Step: 1334, Training Logs: loss_final: 0.951249, loss_mean: 0.900871, loss_mean_cls: 0.050378, grad_norm: 0.656745
[[34m2025-10-04 12:04:54[0m] Step: 1335, Training Logs: loss_final: 0.943850, loss_mean: 0.893140, loss_mean_cls: 0.050709, grad_norm: 0.655879
[[34m2025-10-04 12:04:54[0m] Step: 1336, Training Logs: loss_final: 0.920712, loss_mean: 0.870056, loss_mean_cls: 0.050656, grad_norm: 0.699132
[[34m2025-10-04 12:04:55[0m] Step: 1337, Training Logs: loss_final: 0.946601, loss_mean: 0.897277, loss_mean_cls: 0.049324, grad_norm: 0.832177
[[34m2025-10-04 12:04:55[0m] Step: 1338, Training Logs: loss_final: 0.949525, loss_mean: 0.900330, loss_mean_cls: 0.049196, grad_norm: 0.599532
[[34m2025-10-04 12:04:55[0m] Step: 1339, Training Logs: loss_final: 0.922996, loss_mean: 0.872377, loss_mean_cls: 0.050618, grad_norm: 0.646036
[[34m2025-10-04 12:04:55[0m] Step: 1340, Training Logs: loss_final: 0.942639, loss_mean: 0.892588, loss_mean_cls: 0.050051, grad_norm: 0.822775
[[34m2025-10-04 12:04:56[0m] Step: 1341, Training Logs: loss_final: 0.924571, loss_mean: 0.874806, loss_mean_cls: 0.049765, grad_norm: 0.564463
[[34m2025-10-04 12:04:56[0m] Step: 1342, Training Logs: loss_final: 0.963172, loss_mean: 0.913696, loss_mean_cls: 0.049476, grad_norm: 0.641082
[[34m2025-10-04 12:04:56[0m] Step: 1343, Training Logs: loss_final: 0.946331, loss_mean: 0.897142, loss_mean_cls: 0.049189, grad_norm: 0.648978
[[34m2025-10-04 12:04:57[0m] Step: 1344, Training Logs: loss_final: 0.942901, loss_mean: 0.894094, loss_mean_cls: 0.048807, grad_norm: 0.651189
[[34m2025-10-04 12:04:57[0m] Step: 1345, Training Logs: loss_final: 0.927876, loss_mean: 0.878447, loss_mean_cls: 0.049430, grad_norm: 0.601444
[[34m2025-10-04 12:04:57[0m] Step: 1346, Training Logs: loss_final: 0.932443, loss_mean: 0.881399, loss_mean_cls: 0.051044, grad_norm: 0.604157
[[34m2025-10-04 12:04:57[0m] Step: 1347, Training Logs: loss_final: 0.943556, loss_mean: 0.893181, loss_mean_cls: 0.050375, grad_norm: 0.550631
[[34m2025-10-04 12:04:58[0m] Step: 1348, Training Logs: loss_final: 0.946250, loss_mean: 0.896052, loss_mean_cls: 0.050198, grad_norm: 0.638381
[[34m2025-10-04 12:04:58[0m] Step: 1349, Training Logs: loss_final: 0.944040, loss_mean: 0.893749, loss_mean_cls: 0.050292, grad_norm: 0.669953
[[34m2025-10-04 12:04:58[0m] Step: 1350, Training Logs: loss_final: 0.950158, loss_mean: 0.901949, loss_mean_cls: 0.048209, grad_norm: 0.531612
[[34m2025-10-04 12:04:59[0m] Step: 1351, Training Logs: loss_final: 0.937536, loss_mean: 0.888768, loss_mean_cls: 0.048768, grad_norm: 0.637156
[[34m2025-10-04 12:04:59[0m] Step: 1352, Training Logs: loss_final: 0.927560, loss_mean: 0.878160, loss_mean_cls: 0.049400, grad_norm: 0.597700
[[34m2025-10-04 12:04:59[0m] Step: 1353, Training Logs: loss_final: 0.944965, loss_mean: 0.896047, loss_mean_cls: 0.048918, grad_norm: 0.659782
[[34m2025-10-04 12:05:00[0m] Step: 1354, Training Logs: loss_final: 0.919868, loss_mean: 0.870733, loss_mean_cls: 0.049135, grad_norm: 0.512437
[[34m2025-10-04 12:05:00[0m] Step: 1355, Training Logs: loss_final: 0.950658, loss_mean: 0.900611, loss_mean_cls: 0.050047, grad_norm: 0.625660
[[34m2025-10-04 12:05:00[0m] Step: 1356, Training Logs: loss_final: 0.943501, loss_mean: 0.894513, loss_mean_cls: 0.048989, grad_norm: 0.468565
[[34m2025-10-04 12:05:00[0m] Step: 1357, Training Logs: loss_final: 0.946718, loss_mean: 0.895566, loss_mean_cls: 0.051152, grad_norm: 0.630974
[[34m2025-10-04 12:05:01[0m] Step: 1358, Training Logs: loss_final: 0.922803, loss_mean: 0.872640, loss_mean_cls: 0.050163, grad_norm: 0.644766
[[34m2025-10-04 12:05:01[0m] Step: 1359, Training Logs: loss_final: 0.941980, loss_mean: 0.892530, loss_mean_cls: 0.049451, grad_norm: 0.663227
[[34m2025-10-04 12:05:01[0m] Step: 1360, Training Logs: loss_final: 0.911683, loss_mean: 0.861411, loss_mean_cls: 0.050271, grad_norm: 0.884249
[[34m2025-10-04 12:05:02[0m] Step: 1361, Training Logs: loss_final: 0.935783, loss_mean: 0.886841, loss_mean_cls: 0.048942, grad_norm: 0.918239
[[34m2025-10-04 12:05:02[0m] Step: 1362, Training Logs: loss_final: 0.930028, loss_mean: 0.879220, loss_mean_cls: 0.050808, grad_norm: 0.737432
[[34m2025-10-04 12:05:02[0m] Step: 1363, Training Logs: loss_final: 0.947301, loss_mean: 0.897787, loss_mean_cls: 0.049515, grad_norm: 0.741290
[[34m2025-10-04 12:05:03[0m] Step: 1364, Training Logs: loss_final: 0.945922, loss_mean: 0.896351, loss_mean_cls: 0.049571, grad_norm: 0.784266
[[34m2025-10-04 12:05:03[0m] Step: 1365, Training Logs: loss_final: 0.964715, loss_mean: 0.914665, loss_mean_cls: 0.050049, grad_norm: 0.703994
[[34m2025-10-04 12:05:03[0m] Step: 1366, Training Logs: loss_final: 0.931762, loss_mean: 0.882041, loss_mean_cls: 0.049721, grad_norm: 0.643106
[[34m2025-10-04 12:05:03[0m] Step: 1367, Training Logs: loss_final: 0.939185, loss_mean: 0.890402, loss_mean_cls: 0.048782, grad_norm: 0.507424
[[34m2025-10-04 12:05:04[0m] Step: 1368, Training Logs: loss_final: 0.932247, loss_mean: 0.883403, loss_mean_cls: 0.048844, grad_norm: 0.560657
[[34m2025-10-04 12:05:04[0m] Step: 1369, Training Logs: loss_final: 0.918829, loss_mean: 0.868575, loss_mean_cls: 0.050254, grad_norm: 0.667306
[[34m2025-10-04 12:05:04[0m] Step: 1370, Training Logs: loss_final: 0.930855, loss_mean: 0.880096, loss_mean_cls: 0.050759, grad_norm: 0.712050
[[34m2025-10-04 12:05:05[0m] Step: 1371, Training Logs: loss_final: 0.915456, loss_mean: 0.866389, loss_mean_cls: 0.049067, grad_norm: 0.495573
[[34m2025-10-04 12:05:05[0m] Step: 1372, Training Logs: loss_final: 0.935480, loss_mean: 0.886548, loss_mean_cls: 0.048933, grad_norm: 0.759448
[[34m2025-10-04 12:05:05[0m] Step: 1373, Training Logs: loss_final: 0.945077, loss_mean: 0.895082, loss_mean_cls: 0.049995, grad_norm: 0.670187
[[34m2025-10-04 12:05:05[0m] Step: 1374, Training Logs: loss_final: 0.930553, loss_mean: 0.880536, loss_mean_cls: 0.050017, grad_norm: 0.591071
[[34m2025-10-04 12:05:06[0m] Step: 1375, Training Logs: loss_final: 0.949687, loss_mean: 0.901359, loss_mean_cls: 0.048327, grad_norm: 0.575465
[[34m2025-10-04 12:05:06[0m] Step: 1376, Training Logs: loss_final: 0.930343, loss_mean: 0.880919, loss_mean_cls: 0.049424, grad_norm: 0.473583
[[34m2025-10-04 12:05:06[0m] Step: 1377, Training Logs: loss_final: 0.931741, loss_mean: 0.881358, loss_mean_cls: 0.050383, grad_norm: 0.775902
[[34m2025-10-04 12:05:07[0m] Step: 1378, Training Logs: loss_final: 0.918612, loss_mean: 0.870014, loss_mean_cls: 0.048598, grad_norm: 0.590028
[[34m2025-10-04 12:05:07[0m] Step: 1379, Training Logs: loss_final: 0.938651, loss_mean: 0.889098, loss_mean_cls: 0.049553, grad_norm: 0.662132
[[34m2025-10-04 12:05:07[0m] Step: 1380, Training Logs: loss_final: 0.943865, loss_mean: 0.892983, loss_mean_cls: 0.050883, grad_norm: 0.776585
[[34m2025-10-04 12:05:07[0m] Step: 1381, Training Logs: loss_final: 0.936401, loss_mean: 0.887522, loss_mean_cls: 0.048879, grad_norm: 0.766453
[[34m2025-10-04 12:05:08[0m] Step: 1382, Training Logs: loss_final: 0.929181, loss_mean: 0.879484, loss_mean_cls: 0.049697, grad_norm: 0.780723
[[34m2025-10-04 12:05:08[0m] Step: 1383, Training Logs: loss_final: 0.946636, loss_mean: 0.897476, loss_mean_cls: 0.049160, grad_norm: 0.598543
[[34m2025-10-04 12:05:08[0m] Step: 1384, Training Logs: loss_final: 0.947217, loss_mean: 0.896255, loss_mean_cls: 0.050961, grad_norm: 0.853880
[[34m2025-10-04 12:05:09[0m] Step: 1385, Training Logs: loss_final: 0.954812, loss_mean: 0.904731, loss_mean_cls: 0.050081, grad_norm: 0.630543
[[34m2025-10-04 12:05:09[0m] Step: 1386, Training Logs: loss_final: 0.895728, loss_mean: 0.846125, loss_mean_cls: 0.049604, grad_norm: 0.617236
[[34m2025-10-04 12:05:09[0m] Step: 1387, Training Logs: loss_final: 0.937039, loss_mean: 0.887471, loss_mean_cls: 0.049568, grad_norm: 0.777652
[[34m2025-10-04 12:05:10[0m] Step: 1388, Training Logs: loss_final: 0.930414, loss_mean: 0.880772, loss_mean_cls: 0.049642, grad_norm: 0.575547
[[34m2025-10-04 12:05:10[0m] Step: 1389, Training Logs: loss_final: 0.946161, loss_mean: 0.896525, loss_mean_cls: 0.049636, grad_norm: 0.642988
[[34m2025-10-04 12:05:10[0m] Step: 1390, Training Logs: loss_final: 0.957255, loss_mean: 0.908367, loss_mean_cls: 0.048887, grad_norm: 0.836286
[[34m2025-10-04 12:05:10[0m] Step: 1391, Training Logs: loss_final: 0.913900, loss_mean: 0.862506, loss_mean_cls: 0.051393, grad_norm: 0.604525
[[34m2025-10-04 12:05:11[0m] Step: 1392, Training Logs: loss_final: 0.942780, loss_mean: 0.893398, loss_mean_cls: 0.049382, grad_norm: 0.798204
[[34m2025-10-04 12:05:11[0m] Step: 1393, Training Logs: loss_final: 0.942844, loss_mean: 0.892728, loss_mean_cls: 0.050116, grad_norm: 0.799326
[[34m2025-10-04 12:05:11[0m] Step: 1394, Training Logs: loss_final: 0.927055, loss_mean: 0.876410, loss_mean_cls: 0.050646, grad_norm: 0.646603
[[34m2025-10-04 12:05:12[0m] Step: 1395, Training Logs: loss_final: 0.938744, loss_mean: 0.890163, loss_mean_cls: 0.048581, grad_norm: 0.795740
[[34m2025-10-04 12:05:12[0m] Step: 1396, Training Logs: loss_final: 0.948942, loss_mean: 0.899855, loss_mean_cls: 0.049087, grad_norm: 0.741187
[[34m2025-10-04 12:05:12[0m] Step: 1397, Training Logs: loss_final: 0.959818, loss_mean: 0.911490, loss_mean_cls: 0.048328, grad_norm: 0.897161
[[34m2025-10-04 12:05:13[0m] Step: 1398, Training Logs: loss_final: 0.953185, loss_mean: 0.903233, loss_mean_cls: 0.049952, grad_norm: 0.712273
[[34m2025-10-04 12:05:13[0m] Step: 1399, Training Logs: loss_final: 0.940952, loss_mean: 0.891165, loss_mean_cls: 0.049786, grad_norm: 0.691010
[[34m2025-10-04 12:05:13[0m] Step: 1400, Training Logs: loss_final: 0.931669, loss_mean: 0.882293, loss_mean_cls: 0.049376, grad_norm: 0.718173
[[34m2025-10-04 12:05:13[0m] Step: 1401, Training Logs: loss_final: 0.908255, loss_mean: 0.858240, loss_mean_cls: 0.050015, grad_norm: 0.540245
[[34m2025-10-04 12:05:14[0m] Step: 1402, Training Logs: loss_final: 0.943707, loss_mean: 0.893633, loss_mean_cls: 0.050075, grad_norm: 0.630611
[[34m2025-10-04 12:05:14[0m] Step: 1403, Training Logs: loss_final: 0.930231, loss_mean: 0.880286, loss_mean_cls: 0.049945, grad_norm: 0.584714
[[34m2025-10-04 12:05:14[0m] Step: 1404, Training Logs: loss_final: 0.912706, loss_mean: 0.863549, loss_mean_cls: 0.049156, grad_norm: 0.526707
[[34m2025-10-04 12:05:15[0m] Step: 1405, Training Logs: loss_final: 0.931922, loss_mean: 0.883062, loss_mean_cls: 0.048860, grad_norm: 0.481061
[[34m2025-10-04 12:05:15[0m] Step: 1406, Training Logs: loss_final: 0.926420, loss_mean: 0.875541, loss_mean_cls: 0.050879, grad_norm: 0.639951
[[34m2025-10-04 12:05:15[0m] Step: 1407, Training Logs: loss_final: 0.939470, loss_mean: 0.889435, loss_mean_cls: 0.050034, grad_norm: 0.602947
[[34m2025-10-04 12:05:15[0m] Step: 1408, Training Logs: loss_final: 0.944123, loss_mean: 0.894266, loss_mean_cls: 0.049858, grad_norm: 0.746040
[[34m2025-10-04 12:05:16[0m] Step: 1409, Training Logs: loss_final: 0.916533, loss_mean: 0.866766, loss_mean_cls: 0.049768, grad_norm: 0.628580
[[34m2025-10-04 12:05:16[0m] Step: 1410, Training Logs: loss_final: 0.931739, loss_mean: 0.882351, loss_mean_cls: 0.049388, grad_norm: 0.590608
[[34m2025-10-04 12:05:16[0m] Step: 1411, Training Logs: loss_final: 0.939047, loss_mean: 0.887971, loss_mean_cls: 0.051076, grad_norm: 0.468423
[[34m2025-10-04 12:05:17[0m] Step: 1412, Training Logs: loss_final: 0.941284, loss_mean: 0.892796, loss_mean_cls: 0.048488, grad_norm: 0.457067
[[34m2025-10-04 12:05:17[0m] Step: 1413, Training Logs: loss_final: 0.943998, loss_mean: 0.895171, loss_mean_cls: 0.048828, grad_norm: 0.523857
[[34m2025-10-04 12:05:17[0m] Step: 1414, Training Logs: loss_final: 0.937007, loss_mean: 0.886746, loss_mean_cls: 0.050261, grad_norm: 0.609116
[[34m2025-10-04 12:05:18[0m] Step: 1415, Training Logs: loss_final: 0.948513, loss_mean: 0.899191, loss_mean_cls: 0.049322, grad_norm: 0.662577
[[34m2025-10-04 12:05:18[0m] Step: 1416, Training Logs: loss_final: 0.945041, loss_mean: 0.894768, loss_mean_cls: 0.050274, grad_norm: 0.510112
[[34m2025-10-04 12:05:18[0m] Step: 1417, Training Logs: loss_final: 0.928059, loss_mean: 0.877761, loss_mean_cls: 0.050299, grad_norm: 0.462661
[[34m2025-10-04 12:05:18[0m] Step: 1418, Training Logs: loss_final: 0.945823, loss_mean: 0.895883, loss_mean_cls: 0.049941, grad_norm: 0.404754
[[34m2025-10-04 12:05:19[0m] Step: 1419, Training Logs: loss_final: 0.929808, loss_mean: 0.879413, loss_mean_cls: 0.050395, grad_norm: 0.517607
[[34m2025-10-04 12:05:19[0m] Step: 1420, Training Logs: loss_final: 0.934135, loss_mean: 0.884628, loss_mean_cls: 0.049506, grad_norm: 0.510015
[[34m2025-10-04 12:05:19[0m] Step: 1421, Training Logs: loss_final: 0.929534, loss_mean: 0.880217, loss_mean_cls: 0.049317, grad_norm: 0.466646
[[34m2025-10-04 12:05:20[0m] Step: 1422, Training Logs: loss_final: 0.941865, loss_mean: 0.891877, loss_mean_cls: 0.049988, grad_norm: 0.569418
[[34m2025-10-04 12:05:20[0m] Step: 1423, Training Logs: loss_final: 0.936972, loss_mean: 0.887737, loss_mean_cls: 0.049236, grad_norm: 0.575231
[[34m2025-10-04 12:05:20[0m] Step: 1424, Training Logs: loss_final: 0.937304, loss_mean: 0.888604, loss_mean_cls: 0.048700, grad_norm: 0.484126
[[34m2025-10-04 12:05:20[0m] Step: 1425, Training Logs: loss_final: 0.943965, loss_mean: 0.895282, loss_mean_cls: 0.048683, grad_norm: 0.908096
[[34m2025-10-04 12:05:21[0m] Step: 1426, Training Logs: loss_final: 0.939362, loss_mean: 0.890074, loss_mean_cls: 0.049289, grad_norm: 0.781400
[[34m2025-10-04 12:05:21[0m] Step: 1427, Training Logs: loss_final: 0.936050, loss_mean: 0.886800, loss_mean_cls: 0.049250, grad_norm: 0.714636
[[34m2025-10-04 12:05:21[0m] Step: 1428, Training Logs: loss_final: 0.908198, loss_mean: 0.856838, loss_mean_cls: 0.051360, grad_norm: 0.813025
[[34m2025-10-04 12:05:22[0m] Step: 1429, Training Logs: loss_final: 0.947954, loss_mean: 0.899078, loss_mean_cls: 0.048875, grad_norm: 1.005158
[[34m2025-10-04 12:05:22[0m] Step: 1430, Training Logs: loss_final: 0.941317, loss_mean: 0.892106, loss_mean_cls: 0.049212, grad_norm: 0.485819
[[34m2025-10-04 12:05:22[0m] Step: 1431, Training Logs: loss_final: 0.917826, loss_mean: 0.866225, loss_mean_cls: 0.051600, grad_norm: 0.932277
[[34m2025-10-04 12:05:23[0m] Step: 1432, Training Logs: loss_final: 0.938786, loss_mean: 0.889147, loss_mean_cls: 0.049639, grad_norm: 1.037155
[[34m2025-10-04 12:05:23[0m] Step: 1433, Training Logs: loss_final: 0.948458, loss_mean: 0.899034, loss_mean_cls: 0.049425, grad_norm: 0.774400
[[34m2025-10-04 12:05:23[0m] Step: 1434, Training Logs: loss_final: 0.921528, loss_mean: 0.871198, loss_mean_cls: 0.050330, grad_norm: 0.747723
[[34m2025-10-04 12:05:23[0m] Step: 1435, Training Logs: loss_final: 0.930349, loss_mean: 0.878717, loss_mean_cls: 0.051633, grad_norm: 0.890058
[[34m2025-10-04 12:05:24[0m] Step: 1436, Training Logs: loss_final: 0.917660, loss_mean: 0.867132, loss_mean_cls: 0.050528, grad_norm: 0.622968
[[34m2025-10-04 12:05:24[0m] Step: 1437, Training Logs: loss_final: 0.931018, loss_mean: 0.881859, loss_mean_cls: 0.049159, grad_norm: 0.824170
[[34m2025-10-04 12:05:24[0m] Step: 1438, Training Logs: loss_final: 0.939626, loss_mean: 0.890320, loss_mean_cls: 0.049306, grad_norm: 0.794506
[[34m2025-10-04 12:05:25[0m] Step: 1439, Training Logs: loss_final: 0.932439, loss_mean: 0.882511, loss_mean_cls: 0.049928, grad_norm: 0.777983
[[34m2025-10-04 12:05:25[0m] Step: 1440, Training Logs: loss_final: 0.942158, loss_mean: 0.892358, loss_mean_cls: 0.049800, grad_norm: 1.023564
[[34m2025-10-04 12:05:25[0m] Step: 1441, Training Logs: loss_final: 0.942887, loss_mean: 0.893335, loss_mean_cls: 0.049552, grad_norm: 0.729128
[[34m2025-10-04 12:05:26[0m] Step: 1442, Training Logs: loss_final: 0.930777, loss_mean: 0.881836, loss_mean_cls: 0.048941, grad_norm: 0.896942
[[34m2025-10-04 12:05:26[0m] Step: 1443, Training Logs: loss_final: 0.956887, loss_mean: 0.907218, loss_mean_cls: 0.049669, grad_norm: 0.850096
[[34m2025-10-04 12:05:26[0m] Step: 1444, Training Logs: loss_final: 0.955271, loss_mean: 0.907534, loss_mean_cls: 0.047737, grad_norm: 0.633021
[[34m2025-10-04 12:05:26[0m] Step: 1445, Training Logs: loss_final: 0.926834, loss_mean: 0.877527, loss_mean_cls: 0.049307, grad_norm: 0.731372
[[34m2025-10-04 12:05:27[0m] Step: 1446, Training Logs: loss_final: 0.940459, loss_mean: 0.890361, loss_mean_cls: 0.050098, grad_norm: 0.820461
[[34m2025-10-04 12:05:27[0m] Step: 1447, Training Logs: loss_final: 0.953245, loss_mean: 0.904375, loss_mean_cls: 0.048870, grad_norm: 0.774255
[[34m2025-10-04 12:05:27[0m] Step: 1448, Training Logs: loss_final: 0.933737, loss_mean: 0.884380, loss_mean_cls: 0.049357, grad_norm: 0.765067
[[34m2025-10-04 12:05:28[0m] Step: 1449, Training Logs: loss_final: 0.938208, loss_mean: 0.888211, loss_mean_cls: 0.049998, grad_norm: 0.549748
[[34m2025-10-04 12:05:28[0m] Step: 1450, Training Logs: loss_final: 0.931770, loss_mean: 0.880098, loss_mean_cls: 0.051672, grad_norm: 0.867699
[[34m2025-10-04 12:05:28[0m] Step: 1451, Training Logs: loss_final: 0.932172, loss_mean: 0.881737, loss_mean_cls: 0.050435, grad_norm: 0.807126
[[34m2025-10-04 12:05:28[0m] Step: 1452, Training Logs: loss_final: 0.943432, loss_mean: 0.893062, loss_mean_cls: 0.050370, grad_norm: 0.940386
[[34m2025-10-04 12:05:29[0m] Step: 1453, Training Logs: loss_final: 0.921575, loss_mean: 0.871969, loss_mean_cls: 0.049606, grad_norm: 0.768779
[[34m2025-10-04 12:05:29[0m] Step: 1454, Training Logs: loss_final: 0.929292, loss_mean: 0.879536, loss_mean_cls: 0.049757, grad_norm: 0.828902
[[34m2025-10-04 12:05:29[0m] Step: 1455, Training Logs: loss_final: 0.950085, loss_mean: 0.900963, loss_mean_cls: 0.049121, grad_norm: 0.652102
[[34m2025-10-04 12:05:30[0m] Step: 1456, Training Logs: loss_final: 0.934879, loss_mean: 0.884913, loss_mean_cls: 0.049966, grad_norm: 0.708554
[[34m2025-10-04 12:05:30[0m] Step: 1457, Training Logs: loss_final: 0.928967, loss_mean: 0.879965, loss_mean_cls: 0.049002, grad_norm: 0.543761
[[34m2025-10-04 12:05:30[0m] Step: 1458, Training Logs: loss_final: 0.925972, loss_mean: 0.876141, loss_mean_cls: 0.049831, grad_norm: 0.711204
[[34m2025-10-04 12:05:30[0m] Step: 1459, Training Logs: loss_final: 0.935262, loss_mean: 0.887066, loss_mean_cls: 0.048196, grad_norm: 0.400199
[[34m2025-10-04 12:05:31[0m] Step: 1460, Training Logs: loss_final: 0.928168, loss_mean: 0.879738, loss_mean_cls: 0.048429, grad_norm: 0.631057
[[34m2025-10-04 12:05:31[0m] Step: 1461, Training Logs: loss_final: 0.917164, loss_mean: 0.867367, loss_mean_cls: 0.049796, grad_norm: 0.661828
[[34m2025-10-04 12:05:31[0m] Step: 1462, Training Logs: loss_final: 0.933523, loss_mean: 0.884836, loss_mean_cls: 0.048687, grad_norm: 0.465010
[[34m2025-10-04 12:05:32[0m] Step: 1463, Training Logs: loss_final: 0.934055, loss_mean: 0.884252, loss_mean_cls: 0.049803, grad_norm: 0.787359
[[34m2025-10-04 12:05:32[0m] Step: 1464, Training Logs: loss_final: 0.925433, loss_mean: 0.875829, loss_mean_cls: 0.049604, grad_norm: 0.562890
[[34m2025-10-04 12:05:32[0m] Step: 1465, Training Logs: loss_final: 0.948587, loss_mean: 0.899959, loss_mean_cls: 0.048628, grad_norm: 0.723803
[[34m2025-10-04 12:05:33[0m] Step: 1466, Training Logs: loss_final: 0.956357, loss_mean: 0.906955, loss_mean_cls: 0.049402, grad_norm: 0.633712
[[34m2025-10-04 12:05:33[0m] Step: 1467, Training Logs: loss_final: 0.944809, loss_mean: 0.893870, loss_mean_cls: 0.050939, grad_norm: 0.654887
[[34m2025-10-04 12:05:33[0m] Step: 1468, Training Logs: loss_final: 0.924116, loss_mean: 0.874156, loss_mean_cls: 0.049960, grad_norm: 0.878018
[[34m2025-10-04 12:05:33[0m] Step: 1469, Training Logs: loss_final: 0.939932, loss_mean: 0.889563, loss_mean_cls: 0.050369, grad_norm: 0.695097
[[34m2025-10-04 12:05:34[0m] Step: 1470, Training Logs: loss_final: 0.927509, loss_mean: 0.877617, loss_mean_cls: 0.049891, grad_norm: 0.875809
[[34m2025-10-04 12:05:34[0m] Step: 1471, Training Logs: loss_final: 0.953799, loss_mean: 0.904487, loss_mean_cls: 0.049312, grad_norm: 0.524920
[[34m2025-10-04 12:05:34[0m] Step: 1472, Training Logs: loss_final: 0.922510, loss_mean: 0.873296, loss_mean_cls: 0.049213, grad_norm: 0.804908
[[34m2025-10-04 12:05:35[0m] Step: 1473, Training Logs: loss_final: 0.935096, loss_mean: 0.884533, loss_mean_cls: 0.050563, grad_norm: 0.564049
[[34m2025-10-04 12:05:35[0m] Step: 1474, Training Logs: loss_final: 0.933228, loss_mean: 0.883533, loss_mean_cls: 0.049696, grad_norm: 0.877522
[[34m2025-10-04 12:05:35[0m] Step: 1475, Training Logs: loss_final: 0.912860, loss_mean: 0.863316, loss_mean_cls: 0.049544, grad_norm: 0.718533
[[34m2025-10-04 12:05:36[0m] Step: 1476, Training Logs: loss_final: 0.954223, loss_mean: 0.905863, loss_mean_cls: 0.048360, grad_norm: 0.607130
[[34m2025-10-04 12:05:36[0m] Step: 1477, Training Logs: loss_final: 0.946531, loss_mean: 0.895268, loss_mean_cls: 0.051262, grad_norm: 0.834305
[[34m2025-10-04 12:05:36[0m] Step: 1478, Training Logs: loss_final: 0.915480, loss_mean: 0.864534, loss_mean_cls: 0.050946, grad_norm: 0.539812
[[34m2025-10-04 12:05:36[0m] Step: 1479, Training Logs: loss_final: 0.940730, loss_mean: 0.891731, loss_mean_cls: 0.048999, grad_norm: 0.801809
[[34m2025-10-04 12:05:37[0m] Step: 1480, Training Logs: loss_final: 0.940153, loss_mean: 0.890540, loss_mean_cls: 0.049612, grad_norm: 0.831056
[[34m2025-10-04 12:05:37[0m] Step: 1481, Training Logs: loss_final: 0.952947, loss_mean: 0.904256, loss_mean_cls: 0.048691, grad_norm: 0.565907
[[34m2025-10-04 12:05:37[0m] Step: 1482, Training Logs: loss_final: 0.962433, loss_mean: 0.912356, loss_mean_cls: 0.050076, grad_norm: 1.056570
[[34m2025-10-04 12:05:38[0m] Step: 1483, Training Logs: loss_final: 0.934233, loss_mean: 0.884878, loss_mean_cls: 0.049356, grad_norm: 0.769849
[[34m2025-10-04 12:05:38[0m] Step: 1484, Training Logs: loss_final: 0.938710, loss_mean: 0.890019, loss_mean_cls: 0.048691, grad_norm: 0.887951
[[34m2025-10-04 12:05:38[0m] Step: 1485, Training Logs: loss_final: 0.921860, loss_mean: 0.873425, loss_mean_cls: 0.048434, grad_norm: 0.926569
[[34m2025-10-04 12:05:38[0m] Step: 1486, Training Logs: loss_final: 0.940029, loss_mean: 0.889982, loss_mean_cls: 0.050047, grad_norm: 0.717228
[[34m2025-10-04 12:05:39[0m] Step: 1487, Training Logs: loss_final: 0.949932, loss_mean: 0.901359, loss_mean_cls: 0.048573, grad_norm: 0.818227
[[34m2025-10-04 12:05:39[0m] Step: 1488, Training Logs: loss_final: 0.956820, loss_mean: 0.908046, loss_mean_cls: 0.048774, grad_norm: 0.927730
[[34m2025-10-04 12:05:39[0m] Step: 1489, Training Logs: loss_final: 0.933851, loss_mean: 0.884300, loss_mean_cls: 0.049551, grad_norm: 0.484915
[[34m2025-10-04 12:05:40[0m] Step: 1490, Training Logs: loss_final: 0.936641, loss_mean: 0.887448, loss_mean_cls: 0.049193, grad_norm: 0.853019
[[34m2025-10-04 12:05:40[0m] Step: 1491, Training Logs: loss_final: 0.935523, loss_mean: 0.884580, loss_mean_cls: 0.050943, grad_norm: 0.566770
[[34m2025-10-04 12:05:40[0m] Step: 1492, Training Logs: loss_final: 0.945647, loss_mean: 0.897660, loss_mean_cls: 0.047986, grad_norm: 0.483808
[[34m2025-10-04 12:05:40[0m] Step: 1493, Training Logs: loss_final: 0.952820, loss_mean: 0.903674, loss_mean_cls: 0.049147, grad_norm: 0.574489
[[34m2025-10-04 12:05:41[0m] Step: 1494, Training Logs: loss_final: 0.923108, loss_mean: 0.872541, loss_mean_cls: 0.050567, grad_norm: 0.671945
[[34m2025-10-04 12:05:41[0m] Step: 1495, Training Logs: loss_final: 0.917621, loss_mean: 0.868355, loss_mean_cls: 0.049266, grad_norm: 0.714933
[[34m2025-10-04 12:05:41[0m] Step: 1496, Training Logs: loss_final: 0.936727, loss_mean: 0.886486, loss_mean_cls: 0.050241, grad_norm: 0.695379
[[34m2025-10-04 12:05:42[0m] Step: 1497, Training Logs: loss_final: 0.948445, loss_mean: 0.898936, loss_mean_cls: 0.049509, grad_norm: 0.899202
[[34m2025-10-04 12:05:42[0m] Step: 1498, Training Logs: loss_final: 0.935851, loss_mean: 0.885946, loss_mean_cls: 0.049905, grad_norm: 0.652036
[[34m2025-10-04 12:05:42[0m] Step: 1499, Training Logs: loss_final: 0.967750, loss_mean: 0.919801, loss_mean_cls: 0.047948, grad_norm: 1.042792
[[34m2025-10-04 12:05:43[0m] Step: 1500, Training Logs: loss_final: 0.923985, loss_mean: 0.874298, loss_mean_cls: 0.049687, grad_norm: 0.650879
[[34m2025-10-04 12:05:43[0m] Step: 1501, Training Logs: loss_final: 0.939852, loss_mean: 0.890473, loss_mean_cls: 0.049378, grad_norm: 0.822900
[[34m2025-10-04 12:05:43[0m] Step: 1502, Training Logs: loss_final: 0.948505, loss_mean: 0.898154, loss_mean_cls: 0.050351, grad_norm: 0.649082
[[34m2025-10-04 12:05:43[0m] Step: 1503, Training Logs: loss_final: 0.937983, loss_mean: 0.888767, loss_mean_cls: 0.049217, grad_norm: 0.883776
[[34m2025-10-04 12:05:44[0m] Step: 1504, Training Logs: loss_final: 0.927578, loss_mean: 0.878453, loss_mean_cls: 0.049125, grad_norm: 0.672285
[[34m2025-10-04 12:05:44[0m] Step: 1505, Training Logs: loss_final: 0.922494, loss_mean: 0.873176, loss_mean_cls: 0.049318, grad_norm: 0.590567
[[34m2025-10-04 12:05:44[0m] Step: 1506, Training Logs: loss_final: 0.916803, loss_mean: 0.867958, loss_mean_cls: 0.048845, grad_norm: 0.855587
[[34m2025-10-04 12:05:45[0m] Step: 1507, Training Logs: loss_final: 0.939299, loss_mean: 0.889798, loss_mean_cls: 0.049501, grad_norm: 0.452440
[[34m2025-10-04 12:05:45[0m] Step: 1508, Training Logs: loss_final: 0.937734, loss_mean: 0.889371, loss_mean_cls: 0.048364, grad_norm: 0.796988
[[34m2025-10-04 12:05:45[0m] Step: 1509, Training Logs: loss_final: 0.921280, loss_mean: 0.871403, loss_mean_cls: 0.049877, grad_norm: 0.494257
[[34m2025-10-04 12:05:45[0m] Step: 1510, Training Logs: loss_final: 0.930162, loss_mean: 0.881984, loss_mean_cls: 0.048178, grad_norm: 0.777184
[[34m2025-10-04 12:05:46[0m] Step: 1511, Training Logs: loss_final: 0.923532, loss_mean: 0.874139, loss_mean_cls: 0.049393, grad_norm: 0.692792
[[34m2025-10-04 12:05:46[0m] Step: 1512, Training Logs: loss_final: 0.929358, loss_mean: 0.878534, loss_mean_cls: 0.050825, grad_norm: 0.618746
[[34m2025-10-04 12:05:46[0m] Step: 1513, Training Logs: loss_final: 0.937491, loss_mean: 0.887521, loss_mean_cls: 0.049970, grad_norm: 0.541423
[[34m2025-10-04 12:05:47[0m] Step: 1514, Training Logs: loss_final: 0.941484, loss_mean: 0.891910, loss_mean_cls: 0.049574, grad_norm: 0.539435
[[34m2025-10-04 12:05:47[0m] Step: 1515, Training Logs: loss_final: 0.918256, loss_mean: 0.869015, loss_mean_cls: 0.049241, grad_norm: 0.647416
[[34m2025-10-04 12:05:47[0m] Step: 1516, Training Logs: loss_final: 0.910121, loss_mean: 0.861395, loss_mean_cls: 0.048726, grad_norm: 0.565657
[[34m2025-10-04 12:05:48[0m] Step: 1517, Training Logs: loss_final: 0.940351, loss_mean: 0.890709, loss_mean_cls: 0.049642, grad_norm: 0.693047
[[34m2025-10-04 12:05:48[0m] Step: 1518, Training Logs: loss_final: 0.932434, loss_mean: 0.884169, loss_mean_cls: 0.048265, grad_norm: 0.602376
[[34m2025-10-04 12:05:48[0m] Step: 1519, Training Logs: loss_final: 0.937572, loss_mean: 0.888446, loss_mean_cls: 0.049126, grad_norm: 0.662247
[[34m2025-10-04 12:05:48[0m] Step: 1520, Training Logs: loss_final: 0.936840, loss_mean: 0.888072, loss_mean_cls: 0.048768, grad_norm: 0.577640
[[34m2025-10-04 12:05:49[0m] Step: 1521, Training Logs: loss_final: 0.953734, loss_mean: 0.903818, loss_mean_cls: 0.049916, grad_norm: 0.555303
[[34m2025-10-04 12:05:49[0m] Step: 1522, Training Logs: loss_final: 0.932294, loss_mean: 0.883280, loss_mean_cls: 0.049014, grad_norm: 0.551621
[[34m2025-10-04 12:05:49[0m] Step: 1523, Training Logs: loss_final: 0.958769, loss_mean: 0.910155, loss_mean_cls: 0.048614, grad_norm: 0.629061
[[34m2025-10-04 12:05:50[0m] Step: 1524, Training Logs: loss_final: 0.954232, loss_mean: 0.905465, loss_mean_cls: 0.048767, grad_norm: 0.565576
[[34m2025-10-04 12:05:50[0m] Step: 1525, Training Logs: loss_final: 0.915668, loss_mean: 0.865651, loss_mean_cls: 0.050017, grad_norm: 0.678724
[[34m2025-10-04 12:05:50[0m] Step: 1526, Training Logs: loss_final: 0.962700, loss_mean: 0.914599, loss_mean_cls: 0.048101, grad_norm: 0.571508
[[34m2025-10-04 12:05:50[0m] Step: 1527, Training Logs: loss_final: 0.936803, loss_mean: 0.887108, loss_mean_cls: 0.049694, grad_norm: 0.891626
[[34m2025-10-04 12:05:51[0m] Step: 1528, Training Logs: loss_final: 0.925843, loss_mean: 0.875643, loss_mean_cls: 0.050200, grad_norm: 0.833768
[[34m2025-10-04 12:05:51[0m] Step: 1529, Training Logs: loss_final: 0.920595, loss_mean: 0.870272, loss_mean_cls: 0.050323, grad_norm: 1.048266
[[34m2025-10-04 12:05:51[0m] Step: 1530, Training Logs: loss_final: 0.928025, loss_mean: 0.878293, loss_mean_cls: 0.049732, grad_norm: 0.765826
[[34m2025-10-04 12:05:52[0m] Step: 1531, Training Logs: loss_final: 0.912275, loss_mean: 0.862348, loss_mean_cls: 0.049927, grad_norm: 0.862869
[[34m2025-10-04 12:05:52[0m] Step: 1532, Training Logs: loss_final: 0.926928, loss_mean: 0.876951, loss_mean_cls: 0.049977, grad_norm: 1.024337
[[34m2025-10-04 12:05:52[0m] Step: 1533, Training Logs: loss_final: 0.927836, loss_mean: 0.877479, loss_mean_cls: 0.050357, grad_norm: 0.667867
[[34m2025-10-04 12:05:53[0m] Step: 1534, Training Logs: loss_final: 0.938802, loss_mean: 0.888705, loss_mean_cls: 0.050096, grad_norm: 0.855367
[[34m2025-10-04 12:05:53[0m] Step: 1535, Training Logs: loss_final: 0.930085, loss_mean: 0.880174, loss_mean_cls: 0.049911, grad_norm: 0.819596
[[34m2025-10-04 12:05:53[0m] Step: 1536, Training Logs: loss_final: 0.920248, loss_mean: 0.872367, loss_mean_cls: 0.047881, grad_norm: 0.517221
[[34m2025-10-04 12:05:53[0m] Step: 1537, Training Logs: loss_final: 0.926899, loss_mean: 0.880080, loss_mean_cls: 0.046819, grad_norm: 0.810303
[[34m2025-10-04 12:05:54[0m] Step: 1538, Training Logs: loss_final: 0.930848, loss_mean: 0.882923, loss_mean_cls: 0.047925, grad_norm: 0.620475
[[34m2025-10-04 12:05:54[0m] Step: 1539, Training Logs: loss_final: 0.927764, loss_mean: 0.879003, loss_mean_cls: 0.048761, grad_norm: 0.907636
[[34m2025-10-04 12:05:54[0m] Step: 1540, Training Logs: loss_final: 0.936437, loss_mean: 0.887558, loss_mean_cls: 0.048879, grad_norm: 0.632538
[[34m2025-10-04 12:05:55[0m] Step: 1541, Training Logs: loss_final: 0.942586, loss_mean: 0.894651, loss_mean_cls: 0.047936, grad_norm: 0.755337
[[34m2025-10-04 12:05:55[0m] Step: 1542, Training Logs: loss_final: 0.946644, loss_mean: 0.897339, loss_mean_cls: 0.049306, grad_norm: 0.672827
[[34m2025-10-04 12:05:55[0m] Step: 1543, Training Logs: loss_final: 0.926550, loss_mean: 0.875392, loss_mean_cls: 0.051157, grad_norm: 0.793103
[[34m2025-10-04 12:05:56[0m] Step: 1544, Training Logs: loss_final: 0.948483, loss_mean: 0.899527, loss_mean_cls: 0.048955, grad_norm: 0.824788
[[34m2025-10-04 12:05:56[0m] Step: 1545, Training Logs: loss_final: 0.960465, loss_mean: 0.911344, loss_mean_cls: 0.049122, grad_norm: 0.741037
[[34m2025-10-04 12:05:56[0m] Step: 1546, Training Logs: loss_final: 0.927870, loss_mean: 0.876570, loss_mean_cls: 0.051300, grad_norm: 0.732833
[[34m2025-10-04 12:05:56[0m] Step: 1547, Training Logs: loss_final: 0.922995, loss_mean: 0.873151, loss_mean_cls: 0.049844, grad_norm: 0.830635
[[34m2025-10-04 12:05:57[0m] Step: 1548, Training Logs: loss_final: 0.930744, loss_mean: 0.883623, loss_mean_cls: 0.047121, grad_norm: 0.657173
[[34m2025-10-04 12:05:57[0m] Step: 1549, Training Logs: loss_final: 0.940679, loss_mean: 0.891122, loss_mean_cls: 0.049557, grad_norm: 0.873539
[[34m2025-10-04 12:05:57[0m] Step: 1550, Training Logs: loss_final: 0.922555, loss_mean: 0.871603, loss_mean_cls: 0.050952, grad_norm: 0.668536
[[34m2025-10-04 12:05:58[0m] Step: 1551, Training Logs: loss_final: 0.930258, loss_mean: 0.880498, loss_mean_cls: 0.049760, grad_norm: 0.637800
[[34m2025-10-04 12:05:58[0m] Step: 1552, Training Logs: loss_final: 0.930891, loss_mean: 0.882371, loss_mean_cls: 0.048521, grad_norm: 0.677649
[[34m2025-10-04 12:05:58[0m] Step: 1553, Training Logs: loss_final: 0.929779, loss_mean: 0.880909, loss_mean_cls: 0.048871, grad_norm: 0.502653
[[34m2025-10-04 12:05:58[0m] Step: 1554, Training Logs: loss_final: 0.915386, loss_mean: 0.865552, loss_mean_cls: 0.049834, grad_norm: 0.498809
[[34m2025-10-04 12:05:59[0m] Step: 1555, Training Logs: loss_final: 0.933238, loss_mean: 0.883874, loss_mean_cls: 0.049364, grad_norm: 0.611076
[[34m2025-10-04 12:05:59[0m] Step: 1556, Training Logs: loss_final: 0.902060, loss_mean: 0.851893, loss_mean_cls: 0.050166, grad_norm: 0.691355
[[34m2025-10-04 12:05:59[0m] Step: 1557, Training Logs: loss_final: 0.912796, loss_mean: 0.862647, loss_mean_cls: 0.050149, grad_norm: 0.576914
[[34m2025-10-04 12:06:00[0m] Step: 1558, Training Logs: loss_final: 0.899683, loss_mean: 0.849951, loss_mean_cls: 0.049732, grad_norm: 0.590510
[[34m2025-10-04 12:06:00[0m] Step: 1559, Training Logs: loss_final: 0.936346, loss_mean: 0.887490, loss_mean_cls: 0.048856, grad_norm: 0.365048
[[34m2025-10-04 12:06:00[0m] Step: 1560, Training Logs: loss_final: 0.934572, loss_mean: 0.885507, loss_mean_cls: 0.049065, grad_norm: 0.544220
[[34m2025-10-04 12:06:00[0m] Step: 1561, Training Logs: loss_final: 0.924958, loss_mean: 0.875133, loss_mean_cls: 0.049825, grad_norm: 0.622417
[[34m2025-10-04 12:06:01[0m] Step: 1562, Training Logs: loss_final: 0.925088, loss_mean: 0.875669, loss_mean_cls: 0.049419, grad_norm: 0.554121
[[34m2025-10-04 12:06:01[0m] Step: 1563, Training Logs: loss_final: 0.930394, loss_mean: 0.881878, loss_mean_cls: 0.048517, grad_norm: 0.769462
[[34m2025-10-04 12:06:01[0m] Step: 1564, Training Logs: loss_final: 0.927537, loss_mean: 0.877490, loss_mean_cls: 0.050048, grad_norm: 0.554144
[[34m2025-10-04 12:06:02[0m] Step: 1565, Training Logs: loss_final: 0.955315, loss_mean: 0.906854, loss_mean_cls: 0.048461, grad_norm: 0.808881
[[34m2025-10-04 12:06:02[0m] Step: 1566, Training Logs: loss_final: 0.955724, loss_mean: 0.906901, loss_mean_cls: 0.048822, grad_norm: 0.717145
[[34m2025-10-04 12:06:02[0m] Step: 1567, Training Logs: loss_final: 0.924991, loss_mean: 0.875184, loss_mean_cls: 0.049807, grad_norm: 0.736510
[[34m2025-10-04 12:06:03[0m] Step: 1568, Training Logs: loss_final: 0.930931, loss_mean: 0.881525, loss_mean_cls: 0.049406, grad_norm: 0.735052
[[34m2025-10-04 12:06:03[0m] Step: 1569, Training Logs: loss_final: 0.942381, loss_mean: 0.893637, loss_mean_cls: 0.048745, grad_norm: 0.711232
[[34m2025-10-04 12:06:03[0m] Step: 1570, Training Logs: loss_final: 0.928354, loss_mean: 0.878811, loss_mean_cls: 0.049542, grad_norm: 0.767763
[[34m2025-10-04 12:06:03[0m] Step: 1571, Training Logs: loss_final: 0.920804, loss_mean: 0.871699, loss_mean_cls: 0.049105, grad_norm: 0.721494
[[34m2025-10-04 12:06:04[0m] Step: 1572, Training Logs: loss_final: 0.934279, loss_mean: 0.885045, loss_mean_cls: 0.049234, grad_norm: 0.625460
[[34m2025-10-04 12:06:04[0m] Step: 1573, Training Logs: loss_final: 0.926283, loss_mean: 0.878672, loss_mean_cls: 0.047610, grad_norm: 0.606252
[[34m2025-10-04 12:06:04[0m] Step: 1574, Training Logs: loss_final: 0.923718, loss_mean: 0.875095, loss_mean_cls: 0.048624, grad_norm: 0.720575
[[34m2025-10-04 12:06:05[0m] Step: 1575, Training Logs: loss_final: 0.920139, loss_mean: 0.870684, loss_mean_cls: 0.049455, grad_norm: 0.587391
[[34m2025-10-04 12:06:05[0m] Step: 1576, Training Logs: loss_final: 0.940764, loss_mean: 0.892586, loss_mean_cls: 0.048178, grad_norm: 0.682702
[[34m2025-10-04 12:06:05[0m] Step: 1577, Training Logs: loss_final: 0.940979, loss_mean: 0.892401, loss_mean_cls: 0.048578, grad_norm: 0.596110
[[34m2025-10-04 12:06:06[0m] Step: 1578, Training Logs: loss_final: 0.921602, loss_mean: 0.873229, loss_mean_cls: 0.048373, grad_norm: 0.490861
[[34m2025-10-04 12:06:06[0m] Step: 1579, Training Logs: loss_final: 0.927024, loss_mean: 0.876992, loss_mean_cls: 0.050033, grad_norm: 0.689624
[[34m2025-10-04 12:06:06[0m] Step: 1580, Training Logs: loss_final: 0.928963, loss_mean: 0.879322, loss_mean_cls: 0.049642, grad_norm: 0.608207
[[34m2025-10-04 12:06:06[0m] Step: 1581, Training Logs: loss_final: 0.924934, loss_mean: 0.876223, loss_mean_cls: 0.048712, grad_norm: 0.715372
[[34m2025-10-04 12:06:07[0m] Step: 1582, Training Logs: loss_final: 0.935902, loss_mean: 0.886357, loss_mean_cls: 0.049546, grad_norm: 0.552955
[[34m2025-10-04 12:06:07[0m] Step: 1583, Training Logs: loss_final: 0.933906, loss_mean: 0.884360, loss_mean_cls: 0.049547, grad_norm: 0.663886
[[34m2025-10-04 12:06:07[0m] Step: 1584, Training Logs: loss_final: 0.946588, loss_mean: 0.897867, loss_mean_cls: 0.048722, grad_norm: 0.390517
[[34m2025-10-04 12:06:08[0m] Step: 1585, Training Logs: loss_final: 0.931741, loss_mean: 0.882579, loss_mean_cls: 0.049163, grad_norm: 0.566521
[[34m2025-10-04 12:06:08[0m] Step: 1586, Training Logs: loss_final: 0.922578, loss_mean: 0.872765, loss_mean_cls: 0.049814, grad_norm: 0.493574
[[34m2025-10-04 12:06:08[0m] Step: 1587, Training Logs: loss_final: 0.932773, loss_mean: 0.884071, loss_mean_cls: 0.048703, grad_norm: 0.590415
[[34m2025-10-04 12:06:08[0m] Step: 1588, Training Logs: loss_final: 0.928407, loss_mean: 0.879198, loss_mean_cls: 0.049208, grad_norm: 0.609251
[[34m2025-10-04 12:06:09[0m] Step: 1589, Training Logs: loss_final: 0.945578, loss_mean: 0.896538, loss_mean_cls: 0.049040, grad_norm: 0.636822
[[34m2025-10-04 12:06:09[0m] Step: 1590, Training Logs: loss_final: 0.913780, loss_mean: 0.864296, loss_mean_cls: 0.049484, grad_norm: 0.463971
[[34m2025-10-04 12:06:09[0m] Step: 1591, Training Logs: loss_final: 0.909087, loss_mean: 0.860725, loss_mean_cls: 0.048361, grad_norm: 0.525915
[[34m2025-10-04 12:06:10[0m] Step: 1592, Training Logs: loss_final: 0.920117, loss_mean: 0.870354, loss_mean_cls: 0.049763, grad_norm: 0.562394
[[34m2025-10-04 12:06:10[0m] Step: 1593, Training Logs: loss_final: 0.940031, loss_mean: 0.890957, loss_mean_cls: 0.049075, grad_norm: 0.576685
[[34m2025-10-04 12:06:10[0m] Step: 1594, Training Logs: loss_final: 0.950031, loss_mean: 0.902002, loss_mean_cls: 0.048029, grad_norm: 0.621457
[[34m2025-10-04 12:06:11[0m] Step: 1595, Training Logs: loss_final: 0.940421, loss_mean: 0.890296, loss_mean_cls: 0.050125, grad_norm: 0.532616
[[34m2025-10-04 12:06:11[0m] Step: 1596, Training Logs: loss_final: 0.930695, loss_mean: 0.881184, loss_mean_cls: 0.049511, grad_norm: 0.658836
[[34m2025-10-04 12:06:11[0m] Step: 1597, Training Logs: loss_final: 0.921767, loss_mean: 0.872775, loss_mean_cls: 0.048992, grad_norm: 0.512929
[[34m2025-10-04 12:06:11[0m] Step: 1598, Training Logs: loss_final: 0.931629, loss_mean: 0.882999, loss_mean_cls: 0.048630, grad_norm: 0.513705
[[34m2025-10-04 12:06:12[0m] Step: 1599, Training Logs: loss_final: 0.929108, loss_mean: 0.880319, loss_mean_cls: 0.048789, grad_norm: 0.608061
[[34m2025-10-04 12:06:12[0m] Step: 1600, Training Logs: loss_final: 0.930025, loss_mean: 0.881510, loss_mean_cls: 0.048516, grad_norm: 0.688967
[[34m2025-10-04 12:06:12[0m] Step: 1601, Training Logs: loss_final: 0.923033, loss_mean: 0.872602, loss_mean_cls: 0.050431, grad_norm: 0.534930
[[34m2025-10-04 12:06:13[0m] Step: 1602, Training Logs: loss_final: 0.936179, loss_mean: 0.885668, loss_mean_cls: 0.050511, grad_norm: 0.703541
[[34m2025-10-04 12:06:13[0m] Step: 1603, Training Logs: loss_final: 0.915717, loss_mean: 0.866074, loss_mean_cls: 0.049643, grad_norm: 0.572664
[[34m2025-10-04 12:06:13[0m] Step: 1604, Training Logs: loss_final: 0.935980, loss_mean: 0.886572, loss_mean_cls: 0.049408, grad_norm: 0.743651
[[34m2025-10-04 12:06:13[0m] Step: 1605, Training Logs: loss_final: 0.938572, loss_mean: 0.890635, loss_mean_cls: 0.047937, grad_norm: 0.626302
[[34m2025-10-04 12:06:14[0m] Step: 1606, Training Logs: loss_final: 0.918040, loss_mean: 0.868103, loss_mean_cls: 0.049937, grad_norm: 0.551918
[[34m2025-10-04 12:06:14[0m] Step: 1607, Training Logs: loss_final: 0.958080, loss_mean: 0.909777, loss_mean_cls: 0.048302, grad_norm: 0.519059
[[34m2025-10-04 12:06:14[0m] Step: 1608, Training Logs: loss_final: 0.937107, loss_mean: 0.887111, loss_mean_cls: 0.049996, grad_norm: 0.606226
[[34m2025-10-04 12:06:15[0m] Step: 1609, Training Logs: loss_final: 0.953038, loss_mean: 0.903569, loss_mean_cls: 0.049469, grad_norm: 0.575283
[[34m2025-10-04 12:06:15[0m] Step: 1610, Training Logs: loss_final: 0.899858, loss_mean: 0.850240, loss_mean_cls: 0.049618, grad_norm: 0.721514
[[34m2025-10-04 12:06:15[0m] Step: 1611, Training Logs: loss_final: 0.935937, loss_mean: 0.886154, loss_mean_cls: 0.049783, grad_norm: 0.606374
[[34m2025-10-04 12:06:16[0m] Step: 1612, Training Logs: loss_final: 0.938708, loss_mean: 0.889657, loss_mean_cls: 0.049051, grad_norm: 0.693483
[[34m2025-10-04 12:06:16[0m] Step: 1613, Training Logs: loss_final: 0.944650, loss_mean: 0.895317, loss_mean_cls: 0.049332, grad_norm: 0.443678
[[34m2025-10-04 12:06:16[0m] Step: 1614, Training Logs: loss_final: 0.936180, loss_mean: 0.887403, loss_mean_cls: 0.048777, grad_norm: 0.771917
[[34m2025-10-04 12:06:16[0m] Step: 1615, Training Logs: loss_final: 0.934491, loss_mean: 0.885279, loss_mean_cls: 0.049211, grad_norm: 0.335304
[[34m2025-10-04 12:06:17[0m] Step: 1616, Training Logs: loss_final: 0.933920, loss_mean: 0.886172, loss_mean_cls: 0.047748, grad_norm: 0.600824
[[34m2025-10-04 12:06:17[0m] Step: 1617, Training Logs: loss_final: 0.932979, loss_mean: 0.883697, loss_mean_cls: 0.049282, grad_norm: 0.414795
[[34m2025-10-04 12:06:17[0m] Step: 1618, Training Logs: loss_final: 0.910272, loss_mean: 0.861676, loss_mean_cls: 0.048596, grad_norm: 0.502650
[[34m2025-10-04 12:06:18[0m] Step: 1619, Training Logs: loss_final: 0.935611, loss_mean: 0.886767, loss_mean_cls: 0.048845, grad_norm: 0.502299
[[34m2025-10-04 12:06:18[0m] Step: 1620, Training Logs: loss_final: 0.924049, loss_mean: 0.876202, loss_mean_cls: 0.047847, grad_norm: 0.430128
[[34m2025-10-04 12:06:18[0m] Step: 1621, Training Logs: loss_final: 0.933761, loss_mean: 0.884197, loss_mean_cls: 0.049564, grad_norm: 0.560097
[[34m2025-10-04 12:06:18[0m] Step: 1622, Training Logs: loss_final: 0.920095, loss_mean: 0.871229, loss_mean_cls: 0.048866, grad_norm: 0.403086
[[34m2025-10-04 12:06:19[0m] Step: 1623, Training Logs: loss_final: 0.893613, loss_mean: 0.844321, loss_mean_cls: 0.049292, grad_norm: 0.470922
[[34m2025-10-04 12:06:19[0m] Step: 1624, Training Logs: loss_final: 0.926072, loss_mean: 0.877514, loss_mean_cls: 0.048558, grad_norm: 0.770590
[[34m2025-10-04 12:06:19[0m] Step: 1625, Training Logs: loss_final: 0.935009, loss_mean: 0.886645, loss_mean_cls: 0.048364, grad_norm: 0.840768
[[34m2025-10-04 12:06:20[0m] Step: 1626, Training Logs: loss_final: 0.926058, loss_mean: 0.877926, loss_mean_cls: 0.048132, grad_norm: 0.487894
[[34m2025-10-04 12:06:20[0m] Step: 1627, Training Logs: loss_final: 0.923234, loss_mean: 0.875121, loss_mean_cls: 0.048113, grad_norm: 0.595492
[[34m2025-10-04 12:06:20[0m] Step: 1628, Training Logs: loss_final: 0.940138, loss_mean: 0.891054, loss_mean_cls: 0.049084, grad_norm: 0.501184
[[34m2025-10-04 12:06:21[0m] Step: 1629, Training Logs: loss_final: 0.920257, loss_mean: 0.871333, loss_mean_cls: 0.048924, grad_norm: 0.507722
[[34m2025-10-04 12:06:21[0m] Step: 1630, Training Logs: loss_final: 0.940421, loss_mean: 0.891502, loss_mean_cls: 0.048919, grad_norm: 0.611089
[[34m2025-10-04 12:06:21[0m] Step: 1631, Training Logs: loss_final: 0.925358, loss_mean: 0.876856, loss_mean_cls: 0.048502, grad_norm: 0.524901
[[34m2025-10-04 12:06:21[0m] Step: 1632, Training Logs: loss_final: 0.899881, loss_mean: 0.849910, loss_mean_cls: 0.049971, grad_norm: 0.601645
[[34m2025-10-04 12:06:22[0m] Step: 1633, Training Logs: loss_final: 0.921590, loss_mean: 0.871947, loss_mean_cls: 0.049643, grad_norm: 0.452672
[[34m2025-10-04 12:06:22[0m] Step: 1634, Training Logs: loss_final: 0.939717, loss_mean: 0.889929, loss_mean_cls: 0.049789, grad_norm: 0.872151
[[34m2025-10-04 12:06:22[0m] Step: 1635, Training Logs: loss_final: 0.939212, loss_mean: 0.890568, loss_mean_cls: 0.048643, grad_norm: 0.673445
[[34m2025-10-04 12:06:23[0m] Step: 1636, Training Logs: loss_final: 0.914379, loss_mean: 0.865299, loss_mean_cls: 0.049081, grad_norm: 0.462587
[[34m2025-10-04 12:06:23[0m] Step: 1637, Training Logs: loss_final: 0.937658, loss_mean: 0.889633, loss_mean_cls: 0.048025, grad_norm: 0.574360
[[34m2025-10-04 12:06:23[0m] Step: 1638, Training Logs: loss_final: 0.927327, loss_mean: 0.879136, loss_mean_cls: 0.048191, grad_norm: 0.590372
[[34m2025-10-04 12:06:23[0m] Step: 1639, Training Logs: loss_final: 0.923488, loss_mean: 0.874348, loss_mean_cls: 0.049139, grad_norm: 0.578999
[[34m2025-10-04 12:06:24[0m] Step: 1640, Training Logs: loss_final: 0.926232, loss_mean: 0.877487, loss_mean_cls: 0.048745, grad_norm: 0.710371
[[34m2025-10-04 12:06:24[0m] Step: 1641, Training Logs: loss_final: 0.922057, loss_mean: 0.873415, loss_mean_cls: 0.048643, grad_norm: 0.576384
[[34m2025-10-04 12:06:24[0m] Step: 1642, Training Logs: loss_final: 0.940465, loss_mean: 0.892819, loss_mean_cls: 0.047646, grad_norm: 0.587100
[[34m2025-10-04 12:06:25[0m] Step: 1643, Training Logs: loss_final: 0.912881, loss_mean: 0.863781, loss_mean_cls: 0.049100, grad_norm: 0.747650
[[34m2025-10-04 12:06:25[0m] Step: 1644, Training Logs: loss_final: 0.936682, loss_mean: 0.889236, loss_mean_cls: 0.047446, grad_norm: 0.710382
[[34m2025-10-04 12:06:25[0m] Step: 1645, Training Logs: loss_final: 0.917679, loss_mean: 0.868950, loss_mean_cls: 0.048729, grad_norm: 0.614192
[[34m2025-10-04 12:06:25[0m] Step: 1646, Training Logs: loss_final: 0.950908, loss_mean: 0.902977, loss_mean_cls: 0.047931, grad_norm: 0.692433
[[34m2025-10-04 12:06:26[0m] Step: 1647, Training Logs: loss_final: 0.959074, loss_mean: 0.910759, loss_mean_cls: 0.048315, grad_norm: 0.655858
[[34m2025-10-04 12:06:26[0m] Step: 1648, Training Logs: loss_final: 0.940315, loss_mean: 0.891175, loss_mean_cls: 0.049140, grad_norm: 0.663854
[[34m2025-10-04 12:06:26[0m] Step: 1649, Training Logs: loss_final: 0.948859, loss_mean: 0.900959, loss_mean_cls: 0.047900, grad_norm: 0.822983
[[34m2025-10-04 12:06:27[0m] Step: 1650, Training Logs: loss_final: 0.947134, loss_mean: 0.898544, loss_mean_cls: 0.048590, grad_norm: 0.644371
[[34m2025-10-04 12:06:27[0m] Step: 1651, Training Logs: loss_final: 0.921216, loss_mean: 0.872382, loss_mean_cls: 0.048835, grad_norm: 0.412411
[[34m2025-10-04 12:06:27[0m] Step: 1652, Training Logs: loss_final: 0.935478, loss_mean: 0.886819, loss_mean_cls: 0.048659, grad_norm: 0.554859
[[34m2025-10-04 12:06:28[0m] Step: 1653, Training Logs: loss_final: 0.927611, loss_mean: 0.879671, loss_mean_cls: 0.047940, grad_norm: 0.740171
[[34m2025-10-04 12:06:28[0m] Step: 1654, Training Logs: loss_final: 0.929976, loss_mean: 0.880914, loss_mean_cls: 0.049062, grad_norm: 0.417254
[[34m2025-10-04 12:06:28[0m] Step: 1655, Training Logs: loss_final: 0.919849, loss_mean: 0.871267, loss_mean_cls: 0.048582, grad_norm: 0.741416
[[34m2025-10-04 12:06:28[0m] Step: 1656, Training Logs: loss_final: 0.930750, loss_mean: 0.882140, loss_mean_cls: 0.048610, grad_norm: 0.456031
[[34m2025-10-04 12:06:29[0m] Step: 1657, Training Logs: loss_final: 0.928626, loss_mean: 0.880055, loss_mean_cls: 0.048571, grad_norm: 0.702086
[[34m2025-10-04 12:06:29[0m] Step: 1658, Training Logs: loss_final: 0.930605, loss_mean: 0.881396, loss_mean_cls: 0.049208, grad_norm: 0.525901
[[34m2025-10-04 12:06:29[0m] Step: 1659, Training Logs: loss_final: 0.920522, loss_mean: 0.871796, loss_mean_cls: 0.048727, grad_norm: 0.646531
[[34m2025-10-04 12:06:30[0m] Step: 1660, Training Logs: loss_final: 0.942075, loss_mean: 0.893458, loss_mean_cls: 0.048617, grad_norm: 0.619302
[[34m2025-10-04 12:06:30[0m] Step: 1661, Training Logs: loss_final: 0.927086, loss_mean: 0.879406, loss_mean_cls: 0.047680, grad_norm: 0.600885
[[34m2025-10-04 12:06:30[0m] Step: 1662, Training Logs: loss_final: 0.916888, loss_mean: 0.866708, loss_mean_cls: 0.050180, grad_norm: 0.530808
[[34m2025-10-04 12:06:31[0m] Step: 1663, Training Logs: loss_final: 0.916341, loss_mean: 0.868293, loss_mean_cls: 0.048048, grad_norm: 0.682182
[[34m2025-10-04 12:06:31[0m] Step: 1664, Training Logs: loss_final: 0.910410, loss_mean: 0.860580, loss_mean_cls: 0.049830, grad_norm: 0.563230
[[34m2025-10-04 12:06:31[0m] Step: 1665, Training Logs: loss_final: 0.940586, loss_mean: 0.891091, loss_mean_cls: 0.049495, grad_norm: 0.667033
[[34m2025-10-04 12:06:31[0m] Step: 1666, Training Logs: loss_final: 0.925647, loss_mean: 0.875954, loss_mean_cls: 0.049693, grad_norm: 0.591575
[[34m2025-10-04 12:06:32[0m] Step: 1667, Training Logs: loss_final: 0.916721, loss_mean: 0.866685, loss_mean_cls: 0.050036, grad_norm: 0.470773
[[34m2025-10-04 12:06:32[0m] Step: 1668, Training Logs: loss_final: 0.912766, loss_mean: 0.864027, loss_mean_cls: 0.048739, grad_norm: 0.567845
[[34m2025-10-04 12:06:32[0m] Step: 1669, Training Logs: loss_final: 0.935531, loss_mean: 0.887577, loss_mean_cls: 0.047953, grad_norm: 0.507295
[[34m2025-10-04 12:06:33[0m] Step: 1670, Training Logs: loss_final: 0.953375, loss_mean: 0.904575, loss_mean_cls: 0.048800, grad_norm: 0.633073
[[34m2025-10-04 12:06:33[0m] Step: 1671, Training Logs: loss_final: 0.929009, loss_mean: 0.879088, loss_mean_cls: 0.049921, grad_norm: 0.475090
[[34m2025-10-04 12:06:33[0m] Step: 1672, Training Logs: loss_final: 0.933415, loss_mean: 0.884614, loss_mean_cls: 0.048801, grad_norm: 0.713697
[[34m2025-10-04 12:06:34[0m] Step: 1673, Training Logs: loss_final: 0.925330, loss_mean: 0.876226, loss_mean_cls: 0.049104, grad_norm: 0.771245
[[34m2025-10-04 12:06:34[0m] Step: 1674, Training Logs: loss_final: 0.933249, loss_mean: 0.884629, loss_mean_cls: 0.048621, grad_norm: 0.600958
[[34m2025-10-04 12:06:34[0m] Step: 1675, Training Logs: loss_final: 0.921495, loss_mean: 0.872968, loss_mean_cls: 0.048527, grad_norm: 0.750491
[[34m2025-10-04 12:06:35[0m] Step: 1676, Training Logs: loss_final: 0.920959, loss_mean: 0.871733, loss_mean_cls: 0.049226, grad_norm: 0.532323
[[34m2025-10-04 12:06:35[0m] Step: 1677, Training Logs: loss_final: 0.932521, loss_mean: 0.882569, loss_mean_cls: 0.049952, grad_norm: 0.689553
[[34m2025-10-04 12:06:35[0m] Step: 1678, Training Logs: loss_final: 0.918356, loss_mean: 0.869600, loss_mean_cls: 0.048756, grad_norm: 0.693875
[[34m2025-10-04 12:06:35[0m] Step: 1679, Training Logs: loss_final: 0.927524, loss_mean: 0.879143, loss_mean_cls: 0.048382, grad_norm: 0.795417
[[34m2025-10-04 12:06:36[0m] Step: 1680, Training Logs: loss_final: 0.917690, loss_mean: 0.867874, loss_mean_cls: 0.049816, grad_norm: 0.600673
[[34m2025-10-04 12:06:36[0m] Step: 1681, Training Logs: loss_final: 0.922603, loss_mean: 0.874081, loss_mean_cls: 0.048522, grad_norm: 0.906700
[[34m2025-10-04 12:06:36[0m] Step: 1682, Training Logs: loss_final: 0.912600, loss_mean: 0.862887, loss_mean_cls: 0.049713, grad_norm: 0.616278
[[34m2025-10-04 12:06:37[0m] Step: 1683, Training Logs: loss_final: 0.920927, loss_mean: 0.871330, loss_mean_cls: 0.049597, grad_norm: 0.905143
[[34m2025-10-04 12:06:37[0m] Step: 1684, Training Logs: loss_final: 0.906507, loss_mean: 0.856788, loss_mean_cls: 0.049719, grad_norm: 0.717117
[[34m2025-10-04 12:06:37[0m] Step: 1685, Training Logs: loss_final: 0.945897, loss_mean: 0.897233, loss_mean_cls: 0.048664, grad_norm: 0.786177
[[34m2025-10-04 12:06:37[0m] Step: 1686, Training Logs: loss_final: 0.939799, loss_mean: 0.890664, loss_mean_cls: 0.049134, grad_norm: 0.861323
[[34m2025-10-04 12:06:38[0m] Step: 1687, Training Logs: loss_final: 0.913823, loss_mean: 0.864679, loss_mean_cls: 0.049145, grad_norm: 0.663172
[[34m2025-10-04 12:06:38[0m] Step: 1688, Training Logs: loss_final: 0.948608, loss_mean: 0.900287, loss_mean_cls: 0.048321, grad_norm: 0.689745
[[34m2025-10-04 12:06:38[0m] Step: 1689, Training Logs: loss_final: 0.938304, loss_mean: 0.890327, loss_mean_cls: 0.047978, grad_norm: 0.658175
[[34m2025-10-04 12:06:39[0m] Step: 1690, Training Logs: loss_final: 0.939602, loss_mean: 0.890829, loss_mean_cls: 0.048772, grad_norm: 0.539125
[[34m2025-10-04 12:06:39[0m] Step: 1691, Training Logs: loss_final: 0.927587, loss_mean: 0.878452, loss_mean_cls: 0.049135, grad_norm: 0.577568
[[34m2025-10-04 12:06:39[0m] Step: 1692, Training Logs: loss_final: 0.921796, loss_mean: 0.872411, loss_mean_cls: 0.049385, grad_norm: 0.598033
[[34m2025-10-04 12:06:40[0m] Step: 1693, Training Logs: loss_final: 0.933244, loss_mean: 0.884887, loss_mean_cls: 0.048357, grad_norm: 0.696438
[[34m2025-10-04 12:06:40[0m] Step: 1694, Training Logs: loss_final: 0.938040, loss_mean: 0.888324, loss_mean_cls: 0.049715, grad_norm: 0.463832
[[34m2025-10-04 12:06:40[0m] Step: 1695, Training Logs: loss_final: 0.943227, loss_mean: 0.893812, loss_mean_cls: 0.049415, grad_norm: 0.905591
[[34m2025-10-04 12:06:40[0m] Step: 1696, Training Logs: loss_final: 0.925871, loss_mean: 0.877894, loss_mean_cls: 0.047977, grad_norm: 0.509867
[[34m2025-10-04 12:06:41[0m] Step: 1697, Training Logs: loss_final: 0.938774, loss_mean: 0.890463, loss_mean_cls: 0.048310, grad_norm: 0.651134
[[34m2025-10-04 12:06:41[0m] Step: 1698, Training Logs: loss_final: 0.940420, loss_mean: 0.890625, loss_mean_cls: 0.049795, grad_norm: 0.768301
[[34m2025-10-04 12:06:41[0m] Step: 1699, Training Logs: loss_final: 0.928020, loss_mean: 0.879060, loss_mean_cls: 0.048959, grad_norm: 0.647887
[[34m2025-10-04 12:06:42[0m] Step: 1700, Training Logs: loss_final: 0.910209, loss_mean: 0.860074, loss_mean_cls: 0.050134, grad_norm: 0.619072
[[34m2025-10-04 12:06:42[0m] Step: 1701, Training Logs: loss_final: 0.936904, loss_mean: 0.887843, loss_mean_cls: 0.049062, grad_norm: 0.670923
[[34m2025-10-04 12:06:42[0m] Step: 1702, Training Logs: loss_final: 0.926508, loss_mean: 0.878379, loss_mean_cls: 0.048129, grad_norm: 0.746483
[[34m2025-10-04 12:06:43[0m] Step: 1703, Training Logs: loss_final: 0.907098, loss_mean: 0.858751, loss_mean_cls: 0.048347, grad_norm: 0.617001
[[34m2025-10-04 12:06:43[0m] Step: 1704, Training Logs: loss_final: 0.922624, loss_mean: 0.872703, loss_mean_cls: 0.049920, grad_norm: 0.856751
[[34m2025-10-04 12:06:43[0m] Step: 1705, Training Logs: loss_final: 0.927418, loss_mean: 0.879240, loss_mean_cls: 0.048178, grad_norm: 0.593264
[[34m2025-10-04 12:06:43[0m] Step: 1706, Training Logs: loss_final: 0.924342, loss_mean: 0.875210, loss_mean_cls: 0.049132, grad_norm: 0.768961
[[34m2025-10-04 12:06:44[0m] Step: 1707, Training Logs: loss_final: 0.926235, loss_mean: 0.877584, loss_mean_cls: 0.048651, grad_norm: 0.719953
[[34m2025-10-04 12:06:44[0m] Step: 1708, Training Logs: loss_final: 0.928048, loss_mean: 0.879588, loss_mean_cls: 0.048460, grad_norm: 0.618641
[[34m2025-10-04 12:06:44[0m] Step: 1709, Training Logs: loss_final: 0.932085, loss_mean: 0.883512, loss_mean_cls: 0.048573, grad_norm: 0.512660
[[34m2025-10-04 12:06:45[0m] Step: 1710, Training Logs: loss_final: 0.933579, loss_mean: 0.884524, loss_mean_cls: 0.049055, grad_norm: 0.683517
[[34m2025-10-04 12:06:45[0m] Step: 1711, Training Logs: loss_final: 0.917145, loss_mean: 0.867653, loss_mean_cls: 0.049492, grad_norm: 0.547826
[[34m2025-10-04 12:06:45[0m] Step: 1712, Training Logs: loss_final: 0.919496, loss_mean: 0.870366, loss_mean_cls: 0.049130, grad_norm: 0.680962
[[34m2025-10-04 12:06:45[0m] Step: 1713, Training Logs: loss_final: 0.940709, loss_mean: 0.891906, loss_mean_cls: 0.048803, grad_norm: 0.762617
[[34m2025-10-04 12:06:46[0m] Step: 1714, Training Logs: loss_final: 0.940120, loss_mean: 0.891434, loss_mean_cls: 0.048687, grad_norm: 0.459355
[[34m2025-10-04 12:06:46[0m] Step: 1715, Training Logs: loss_final: 0.928317, loss_mean: 0.879741, loss_mean_cls: 0.048577, grad_norm: 0.644110
[[34m2025-10-04 12:06:46[0m] Step: 1716, Training Logs: loss_final: 0.940362, loss_mean: 0.892046, loss_mean_cls: 0.048316, grad_norm: 0.459719
[[34m2025-10-04 12:06:47[0m] Step: 1717, Training Logs: loss_final: 0.941818, loss_mean: 0.892701, loss_mean_cls: 0.049117, grad_norm: 0.583116
[[34m2025-10-04 12:06:47[0m] Step: 1718, Training Logs: loss_final: 0.934332, loss_mean: 0.885910, loss_mean_cls: 0.048422, grad_norm: 0.502521
[[34m2025-10-04 12:06:47[0m] Step: 1719, Training Logs: loss_final: 0.939627, loss_mean: 0.890907, loss_mean_cls: 0.048720, grad_norm: 0.529919
[[34m2025-10-04 12:06:48[0m] Step: 1720, Training Logs: loss_final: 0.908362, loss_mean: 0.859895, loss_mean_cls: 0.048467, grad_norm: 0.546640
[[34m2025-10-04 12:06:48[0m] Step: 1721, Training Logs: loss_final: 0.914871, loss_mean: 0.864711, loss_mean_cls: 0.050161, grad_norm: 0.510734
[[34m2025-10-04 12:06:48[0m] Step: 1722, Training Logs: loss_final: 0.912823, loss_mean: 0.863115, loss_mean_cls: 0.049708, grad_norm: 0.679770
[[34m2025-10-04 12:06:48[0m] Step: 1723, Training Logs: loss_final: 0.931046, loss_mean: 0.881512, loss_mean_cls: 0.049535, grad_norm: 0.558140
[[34m2025-10-04 12:06:49[0m] Step: 1724, Training Logs: loss_final: 0.901466, loss_mean: 0.852192, loss_mean_cls: 0.049274, grad_norm: 0.611038
[[34m2025-10-04 12:06:49[0m] Step: 1725, Training Logs: loss_final: 0.906515, loss_mean: 0.857679, loss_mean_cls: 0.048836, grad_norm: 0.594131
[[34m2025-10-04 12:06:49[0m] Step: 1726, Training Logs: loss_final: 0.941205, loss_mean: 0.890972, loss_mean_cls: 0.050233, grad_norm: 0.645524
[[34m2025-10-04 12:06:50[0m] Step: 1727, Training Logs: loss_final: 0.931428, loss_mean: 0.882800, loss_mean_cls: 0.048628, grad_norm: 0.632581
[[34m2025-10-04 12:06:50[0m] Step: 1728, Training Logs: loss_final: 0.943325, loss_mean: 0.894569, loss_mean_cls: 0.048756, grad_norm: 0.697703
[[34m2025-10-04 12:06:50[0m] Step: 1729, Training Logs: loss_final: 0.931466, loss_mean: 0.882403, loss_mean_cls: 0.049062, grad_norm: 0.423742
[[34m2025-10-04 12:06:50[0m] Step: 1730, Training Logs: loss_final: 0.913319, loss_mean: 0.863942, loss_mean_cls: 0.049377, grad_norm: 0.709979
[[34m2025-10-04 12:06:51[0m] Step: 1731, Training Logs: loss_final: 0.920988, loss_mean: 0.872800, loss_mean_cls: 0.048188, grad_norm: 0.625760
[[34m2025-10-04 12:06:51[0m] Step: 1732, Training Logs: loss_final: 0.899508, loss_mean: 0.851041, loss_mean_cls: 0.048467, grad_norm: 0.503954
[[34m2025-10-04 12:06:51[0m] Step: 1733, Training Logs: loss_final: 0.931848, loss_mean: 0.882014, loss_mean_cls: 0.049835, grad_norm: 0.632575
[[34m2025-10-04 12:06:52[0m] Step: 1734, Training Logs: loss_final: 0.933124, loss_mean: 0.884363, loss_mean_cls: 0.048761, grad_norm: 0.401482
[[34m2025-10-04 12:06:52[0m] Step: 1735, Training Logs: loss_final: 0.923280, loss_mean: 0.875745, loss_mean_cls: 0.047535, grad_norm: 0.796051
[[34m2025-10-04 12:06:52[0m] Step: 1736, Training Logs: loss_final: 0.924133, loss_mean: 0.876210, loss_mean_cls: 0.047923, grad_norm: 0.668691
[[34m2025-10-04 12:06:52[0m] Step: 1737, Training Logs: loss_final: 0.912822, loss_mean: 0.864356, loss_mean_cls: 0.048466, grad_norm: 0.507132
[[34m2025-10-04 12:06:53[0m] Step: 1738, Training Logs: loss_final: 0.917053, loss_mean: 0.867045, loss_mean_cls: 0.050008, grad_norm: 0.674838
[[34m2025-10-04 12:06:53[0m] Step: 1739, Training Logs: loss_final: 0.928382, loss_mean: 0.879858, loss_mean_cls: 0.048524, grad_norm: 0.755382
[[34m2025-10-04 12:06:53[0m] Step: 1740, Training Logs: loss_final: 0.915811, loss_mean: 0.867068, loss_mean_cls: 0.048744, grad_norm: 0.583163
[[34m2025-10-04 12:06:54[0m] Step: 1741, Training Logs: loss_final: 0.902987, loss_mean: 0.854014, loss_mean_cls: 0.048973, grad_norm: 0.684107
[[34m2025-10-04 12:06:54[0m] Step: 1742, Training Logs: loss_final: 0.920509, loss_mean: 0.871814, loss_mean_cls: 0.048696, grad_norm: 0.592302
[[34m2025-10-04 12:06:54[0m] Step: 1743, Training Logs: loss_final: 0.936133, loss_mean: 0.887688, loss_mean_cls: 0.048445, grad_norm: 0.807000
[[34m2025-10-04 12:06:55[0m] Step: 1744, Training Logs: loss_final: 0.914399, loss_mean: 0.865204, loss_mean_cls: 0.049195, grad_norm: 0.576857
[[34m2025-10-04 12:06:55[0m] Step: 1745, Training Logs: loss_final: 0.967714, loss_mean: 0.919966, loss_mean_cls: 0.047748, grad_norm: 0.720260
[[34m2025-10-04 12:06:55[0m] Step: 1746, Training Logs: loss_final: 0.930962, loss_mean: 0.882420, loss_mean_cls: 0.048542, grad_norm: 0.610231
[[34m2025-10-04 12:06:55[0m] Step: 1747, Training Logs: loss_final: 0.915192, loss_mean: 0.866681, loss_mean_cls: 0.048510, grad_norm: 0.787135
[[34m2025-10-04 12:06:56[0m] Step: 1748, Training Logs: loss_final: 0.916246, loss_mean: 0.867732, loss_mean_cls: 0.048514, grad_norm: 0.666127
[[34m2025-10-04 12:06:56[0m] Step: 1749, Training Logs: loss_final: 0.927541, loss_mean: 0.877904, loss_mean_cls: 0.049636, grad_norm: 0.683074
[[34m2025-10-04 12:06:56[0m] Step: 1750, Training Logs: loss_final: 0.932493, loss_mean: 0.883443, loss_mean_cls: 0.049051, grad_norm: 0.695456
[[34m2025-10-04 12:06:57[0m] Step: 1751, Training Logs: loss_final: 0.912792, loss_mean: 0.863196, loss_mean_cls: 0.049596, grad_norm: 0.520418
[[34m2025-10-04 12:06:57[0m] Step: 1752, Training Logs: loss_final: 0.916490, loss_mean: 0.867412, loss_mean_cls: 0.049078, grad_norm: 0.802745
[[34m2025-10-04 12:06:57[0m] Step: 1753, Training Logs: loss_final: 0.911468, loss_mean: 0.862774, loss_mean_cls: 0.048694, grad_norm: 0.459608
[[34m2025-10-04 12:06:58[0m] Step: 1754, Training Logs: loss_final: 0.932118, loss_mean: 0.885445, loss_mean_cls: 0.046674, grad_norm: 0.607179
[[34m2025-10-04 12:06:58[0m] Step: 1755, Training Logs: loss_final: 0.943048, loss_mean: 0.894715, loss_mean_cls: 0.048332, grad_norm: 0.666141
[[34m2025-10-04 12:06:58[0m] Step: 1756, Training Logs: loss_final: 0.922723, loss_mean: 0.874132, loss_mean_cls: 0.048591, grad_norm: 0.562602
[[34m2025-10-04 12:06:58[0m] Step: 1757, Training Logs: loss_final: 0.934793, loss_mean: 0.885467, loss_mean_cls: 0.049326, grad_norm: 0.776753
[[34m2025-10-04 12:06:59[0m] Step: 1758, Training Logs: loss_final: 0.919136, loss_mean: 0.870860, loss_mean_cls: 0.048276, grad_norm: 0.538134
[[34m2025-10-04 12:06:59[0m] Step: 1759, Training Logs: loss_final: 0.923584, loss_mean: 0.873891, loss_mean_cls: 0.049693, grad_norm: 0.684602
[[34m2025-10-04 12:06:59[0m] Step: 1760, Training Logs: loss_final: 0.940587, loss_mean: 0.891990, loss_mean_cls: 0.048597, grad_norm: 0.847927
[[34m2025-10-04 12:07:00[0m] Step: 1761, Training Logs: loss_final: 0.936225, loss_mean: 0.887783, loss_mean_cls: 0.048442, grad_norm: 0.481594
[[34m2025-10-04 12:07:00[0m] Step: 1762, Training Logs: loss_final: 0.931972, loss_mean: 0.883495, loss_mean_cls: 0.048477, grad_norm: 0.886116
[[34m2025-10-04 12:07:00[0m] Step: 1763, Training Logs: loss_final: 0.922416, loss_mean: 0.874064, loss_mean_cls: 0.048352, grad_norm: 0.604737
[[34m2025-10-04 12:07:00[0m] Step: 1764, Training Logs: loss_final: 0.913611, loss_mean: 0.864786, loss_mean_cls: 0.048826, grad_norm: 0.885994
[[34m2025-10-04 12:07:01[0m] Step: 1765, Training Logs: loss_final: 0.924544, loss_mean: 0.875074, loss_mean_cls: 0.049470, grad_norm: 0.741940
[[34m2025-10-04 12:07:01[0m] Step: 1766, Training Logs: loss_final: 0.924839, loss_mean: 0.876186, loss_mean_cls: 0.048653, grad_norm: 0.742093
[[34m2025-10-04 12:07:01[0m] Step: 1767, Training Logs: loss_final: 0.943960, loss_mean: 0.896199, loss_mean_cls: 0.047762, grad_norm: 0.769773
[[34m2025-10-04 12:07:02[0m] Step: 1768, Training Logs: loss_final: 0.941185, loss_mean: 0.893766, loss_mean_cls: 0.047419, grad_norm: 0.648375
[[34m2025-10-04 12:07:02[0m] Step: 1769, Training Logs: loss_final: 0.931761, loss_mean: 0.884606, loss_mean_cls: 0.047155, grad_norm: 0.779037
[[34m2025-10-04 12:07:02[0m] Step: 1770, Training Logs: loss_final: 0.929218, loss_mean: 0.880774, loss_mean_cls: 0.048444, grad_norm: 0.623544
[[34m2025-10-04 12:07:03[0m] Step: 1771, Training Logs: loss_final: 0.923856, loss_mean: 0.874644, loss_mean_cls: 0.049212, grad_norm: 0.607068
[[34m2025-10-04 12:07:03[0m] Step: 1772, Training Logs: loss_final: 0.944836, loss_mean: 0.896379, loss_mean_cls: 0.048458, grad_norm: 0.679972
[[34m2025-10-04 12:07:03[0m] Step: 1773, Training Logs: loss_final: 0.931325, loss_mean: 0.883163, loss_mean_cls: 0.048163, grad_norm: 0.619377
[[34m2025-10-04 12:07:03[0m] Step: 1774, Training Logs: loss_final: 0.931560, loss_mean: 0.883134, loss_mean_cls: 0.048427, grad_norm: 0.652078
[[34m2025-10-04 12:07:04[0m] Step: 1775, Training Logs: loss_final: 0.937316, loss_mean: 0.890773, loss_mean_cls: 0.046543, grad_norm: 0.708427
[[34m2025-10-04 12:07:04[0m] Step: 1776, Training Logs: loss_final: 0.923530, loss_mean: 0.874636, loss_mean_cls: 0.048895, grad_norm: 0.662503
[[34m2025-10-04 12:07:04[0m] Step: 1777, Training Logs: loss_final: 0.944655, loss_mean: 0.895700, loss_mean_cls: 0.048954, grad_norm: 1.006864
[[34m2025-10-04 12:07:05[0m] Step: 1778, Training Logs: loss_final: 0.917779, loss_mean: 0.869616, loss_mean_cls: 0.048163, grad_norm: 0.537624
[[34m2025-10-04 12:07:05[0m] Step: 1779, Training Logs: loss_final: 0.916067, loss_mean: 0.867219, loss_mean_cls: 0.048847, grad_norm: 0.752777
[[34m2025-10-04 12:07:05[0m] Step: 1780, Training Logs: loss_final: 0.940517, loss_mean: 0.891950, loss_mean_cls: 0.048568, grad_norm: 0.656629
[[34m2025-10-04 12:07:06[0m] Step: 1781, Training Logs: loss_final: 0.923509, loss_mean: 0.876016, loss_mean_cls: 0.047493, grad_norm: 0.680513
[[34m2025-10-04 12:07:06[0m] Step: 1782, Training Logs: loss_final: 0.923797, loss_mean: 0.876508, loss_mean_cls: 0.047289, grad_norm: 0.629591
[[34m2025-10-04 12:07:06[0m] Step: 1783, Training Logs: loss_final: 0.927507, loss_mean: 0.878964, loss_mean_cls: 0.048543, grad_norm: 0.621597
[[34m2025-10-04 12:07:06[0m] Step: 1784, Training Logs: loss_final: 0.937038, loss_mean: 0.889335, loss_mean_cls: 0.047703, grad_norm: 0.758666
[[34m2025-10-04 12:07:07[0m] Step: 1785, Training Logs: loss_final: 0.903214, loss_mean: 0.853361, loss_mean_cls: 0.049853, grad_norm: 0.762636
[[34m2025-10-04 12:07:07[0m] Step: 1786, Training Logs: loss_final: 0.924956, loss_mean: 0.876657, loss_mean_cls: 0.048299, grad_norm: 0.834922
[[34m2025-10-04 12:07:07[0m] Step: 1787, Training Logs: loss_final: 0.906445, loss_mean: 0.856577, loss_mean_cls: 0.049868, grad_norm: 0.870349
[[34m2025-10-04 12:07:08[0m] Step: 1788, Training Logs: loss_final: 0.934407, loss_mean: 0.885598, loss_mean_cls: 0.048809, grad_norm: 0.793057
[[34m2025-10-04 12:07:08[0m] Step: 1789, Training Logs: loss_final: 0.917286, loss_mean: 0.867052, loss_mean_cls: 0.050234, grad_norm: 0.687964
[[34m2025-10-04 12:07:08[0m] Step: 1790, Training Logs: loss_final: 0.932013, loss_mean: 0.883683, loss_mean_cls: 0.048330, grad_norm: 0.713051
[[34m2025-10-04 12:07:08[0m] Step: 1791, Training Logs: loss_final: 0.948954, loss_mean: 0.901637, loss_mean_cls: 0.047316, grad_norm: 0.662217
[[34m2025-10-04 12:07:09[0m] Step: 1792, Training Logs: loss_final: 0.925550, loss_mean: 0.874468, loss_mean_cls: 0.051082, grad_norm: 0.614585
[[34m2025-10-04 12:07:09[0m] Step: 1793, Training Logs: loss_final: 0.909724, loss_mean: 0.859056, loss_mean_cls: 0.050669, grad_norm: 0.640302
[[34m2025-10-04 12:07:09[0m] Step: 1794, Training Logs: loss_final: 0.914624, loss_mean: 0.865493, loss_mean_cls: 0.049131, grad_norm: 0.509170
[[34m2025-10-04 12:07:10[0m] Step: 1795, Training Logs: loss_final: 0.908644, loss_mean: 0.859946, loss_mean_cls: 0.048698, grad_norm: 0.618501
[[34m2025-10-04 12:07:10[0m] Step: 1796, Training Logs: loss_final: 0.908754, loss_mean: 0.860667, loss_mean_cls: 0.048087, grad_norm: 0.498747
[[34m2025-10-04 12:07:10[0m] Step: 1797, Training Logs: loss_final: 0.939549, loss_mean: 0.890086, loss_mean_cls: 0.049463, grad_norm: 0.537825
[[34m2025-10-04 12:07:10[0m] Step: 1798, Training Logs: loss_final: 0.907102, loss_mean: 0.859408, loss_mean_cls: 0.047694, grad_norm: 0.497579
[[34m2025-10-04 12:07:11[0m] Step: 1799, Training Logs: loss_final: 0.930887, loss_mean: 0.882523, loss_mean_cls: 0.048364, grad_norm: 0.648053
[[34m2025-10-04 12:07:11[0m] Step: 1800, Training Logs: loss_final: 0.927955, loss_mean: 0.879618, loss_mean_cls: 0.048337, grad_norm: 0.528295
[[34m2025-10-04 12:07:11[0m] Step: 1801, Training Logs: loss_final: 0.903664, loss_mean: 0.855153, loss_mean_cls: 0.048511, grad_norm: 0.467789
[[34m2025-10-04 12:07:12[0m] Step: 1802, Training Logs: loss_final: 0.918852, loss_mean: 0.869347, loss_mean_cls: 0.049505, grad_norm: 0.541844
[[34m2025-10-04 12:07:12[0m] Step: 1803, Training Logs: loss_final: 0.928393, loss_mean: 0.880348, loss_mean_cls: 0.048045, grad_norm: 0.564958
[[34m2025-10-04 12:07:12[0m] Step: 1804, Training Logs: loss_final: 0.928436, loss_mean: 0.879579, loss_mean_cls: 0.048857, grad_norm: 0.451513
[[34m2025-10-04 12:07:13[0m] Step: 1805, Training Logs: loss_final: 0.937350, loss_mean: 0.889923, loss_mean_cls: 0.047427, grad_norm: 0.534966
[[34m2025-10-04 12:07:13[0m] Step: 1806, Training Logs: loss_final: 0.947296, loss_mean: 0.900149, loss_mean_cls: 0.047146, grad_norm: 0.456632
[[34m2025-10-04 12:07:13[0m] Step: 1807, Training Logs: loss_final: 0.916920, loss_mean: 0.867770, loss_mean_cls: 0.049150, grad_norm: 0.672919
[[34m2025-10-04 12:07:14[0m] Step: 1808, Training Logs: loss_final: 0.920869, loss_mean: 0.872050, loss_mean_cls: 0.048819, grad_norm: 0.648561
[[34m2025-10-04 12:07:14[0m] Step: 1809, Training Logs: loss_final: 0.908527, loss_mean: 0.860716, loss_mean_cls: 0.047811, grad_norm: 0.857780
[[34m2025-10-04 12:07:14[0m] Step: 1810, Training Logs: loss_final: 0.934116, loss_mean: 0.886227, loss_mean_cls: 0.047889, grad_norm: 0.868352
[[34m2025-10-04 12:07:14[0m] Step: 1811, Training Logs: loss_final: 0.914391, loss_mean: 0.864612, loss_mean_cls: 0.049779, grad_norm: 0.741314
[[34m2025-10-04 12:07:15[0m] Step: 1812, Training Logs: loss_final: 0.926579, loss_mean: 0.878618, loss_mean_cls: 0.047961, grad_norm: 0.601118
[[34m2025-10-04 12:07:15[0m] Step: 1813, Training Logs: loss_final: 0.914101, loss_mean: 0.864693, loss_mean_cls: 0.049408, grad_norm: 0.843714
[[34m2025-10-04 12:07:15[0m] Step: 1814, Training Logs: loss_final: 0.908065, loss_mean: 0.859251, loss_mean_cls: 0.048814, grad_norm: 0.716481
[[34m2025-10-04 12:07:16[0m] Step: 1815, Training Logs: loss_final: 0.925567, loss_mean: 0.878212, loss_mean_cls: 0.047354, grad_norm: 0.826720
[[34m2025-10-04 12:07:16[0m] Step: 1816, Training Logs: loss_final: 0.896134, loss_mean: 0.847826, loss_mean_cls: 0.048308, grad_norm: 0.632040
[[34m2025-10-04 12:07:16[0m] Step: 1817, Training Logs: loss_final: 0.941407, loss_mean: 0.894339, loss_mean_cls: 0.047068, grad_norm: 0.879926
[[34m2025-10-04 12:07:16[0m] Step: 1818, Training Logs: loss_final: 0.878983, loss_mean: 0.829418, loss_mean_cls: 0.049565, grad_norm: 0.624194
[[34m2025-10-04 12:07:17[0m] Step: 1819, Training Logs: loss_final: 0.946167, loss_mean: 0.897936, loss_mean_cls: 0.048231, grad_norm: 0.617928
[[34m2025-10-04 12:07:17[0m] Step: 1820, Training Logs: loss_final: 0.911778, loss_mean: 0.862730, loss_mean_cls: 0.049048, grad_norm: 0.738568
[[34m2025-10-04 12:07:17[0m] Step: 1821, Training Logs: loss_final: 0.915164, loss_mean: 0.865888, loss_mean_cls: 0.049276, grad_norm: 0.630748
[[34m2025-10-04 12:07:18[0m] Step: 1822, Training Logs: loss_final: 0.926869, loss_mean: 0.877479, loss_mean_cls: 0.049390, grad_norm: 0.569030
[[34m2025-10-04 12:07:18[0m] Step: 1823, Training Logs: loss_final: 0.955017, loss_mean: 0.907896, loss_mean_cls: 0.047121, grad_norm: 0.474309
[[34m2025-10-04 12:07:18[0m] Step: 1824, Training Logs: loss_final: 0.911660, loss_mean: 0.863756, loss_mean_cls: 0.047904, grad_norm: 0.547649
[[34m2025-10-04 12:07:18[0m] Step: 1825, Training Logs: loss_final: 0.946113, loss_mean: 0.897022, loss_mean_cls: 0.049091, grad_norm: 0.488821
[[34m2025-10-04 12:07:19[0m] Step: 1826, Training Logs: loss_final: 0.913054, loss_mean: 0.866050, loss_mean_cls: 0.047005, grad_norm: 0.670923
[[34m2025-10-04 12:07:19[0m] Step: 1827, Training Logs: loss_final: 0.921060, loss_mean: 0.873200, loss_mean_cls: 0.047860, grad_norm: 0.479127
[[34m2025-10-04 12:07:19[0m] Step: 1828, Training Logs: loss_final: 0.916362, loss_mean: 0.867401, loss_mean_cls: 0.048962, grad_norm: 0.648072
[[34m2025-10-04 12:07:20[0m] Step: 1829, Training Logs: loss_final: 0.934200, loss_mean: 0.885906, loss_mean_cls: 0.048294, grad_norm: 0.575318
[[34m2025-10-04 12:07:20[0m] Step: 1830, Training Logs: loss_final: 0.923235, loss_mean: 0.875444, loss_mean_cls: 0.047791, grad_norm: 0.501440
[[34m2025-10-04 12:07:20[0m] Step: 1831, Training Logs: loss_final: 0.930767, loss_mean: 0.882565, loss_mean_cls: 0.048203, grad_norm: 0.689045
[[34m2025-10-04 12:07:21[0m] Step: 1832, Training Logs: loss_final: 0.924957, loss_mean: 0.874907, loss_mean_cls: 0.050050, grad_norm: 0.792593
[[34m2025-10-04 12:07:21[0m] Step: 1833, Training Logs: loss_final: 0.937907, loss_mean: 0.889803, loss_mean_cls: 0.048104, grad_norm: 0.599530
[[34m2025-10-04 12:07:21[0m] Step: 1834, Training Logs: loss_final: 0.931499, loss_mean: 0.884213, loss_mean_cls: 0.047286, grad_norm: 0.440679
[[34m2025-10-04 12:07:21[0m] Step: 1835, Training Logs: loss_final: 0.913546, loss_mean: 0.864507, loss_mean_cls: 0.049039, grad_norm: 0.574994
[[34m2025-10-04 12:07:22[0m] Step: 1836, Training Logs: loss_final: 0.915317, loss_mean: 0.867646, loss_mean_cls: 0.047670, grad_norm: 0.448906
[[34m2025-10-04 12:07:22[0m] Step: 1837, Training Logs: loss_final: 0.924187, loss_mean: 0.875466, loss_mean_cls: 0.048721, grad_norm: 0.578402
[[34m2025-10-04 12:07:22[0m] Step: 1838, Training Logs: loss_final: 0.919639, loss_mean: 0.869638, loss_mean_cls: 0.050001, grad_norm: 0.501511
[[34m2025-10-04 12:07:23[0m] Step: 1839, Training Logs: loss_final: 0.930405, loss_mean: 0.882633, loss_mean_cls: 0.047772, grad_norm: 0.551450
[[34m2025-10-04 12:07:23[0m] Step: 1840, Training Logs: loss_final: 0.930240, loss_mean: 0.880969, loss_mean_cls: 0.049271, grad_norm: 0.806272
[[34m2025-10-04 12:07:23[0m] Step: 1841, Training Logs: loss_final: 0.901203, loss_mean: 0.853292, loss_mean_cls: 0.047911, grad_norm: 0.506153
[[34m2025-10-04 12:07:23[0m] Step: 1842, Training Logs: loss_final: 0.908183, loss_mean: 0.858440, loss_mean_cls: 0.049743, grad_norm: 0.767874
[[34m2025-10-04 12:07:24[0m] Step: 1843, Training Logs: loss_final: 0.942818, loss_mean: 0.894675, loss_mean_cls: 0.048143, grad_norm: 0.539701
[[34m2025-10-04 12:07:24[0m] Step: 1844, Training Logs: loss_final: 0.910301, loss_mean: 0.862042, loss_mean_cls: 0.048259, grad_norm: 0.586702
[[34m2025-10-04 12:07:24[0m] Step: 1845, Training Logs: loss_final: 0.933510, loss_mean: 0.884069, loss_mean_cls: 0.049442, grad_norm: 0.584282
[[34m2025-10-04 12:07:25[0m] Step: 1846, Training Logs: loss_final: 0.926797, loss_mean: 0.878133, loss_mean_cls: 0.048664, grad_norm: 0.647320
[[34m2025-10-04 12:07:25[0m] Step: 1847, Training Logs: loss_final: 0.894431, loss_mean: 0.845047, loss_mean_cls: 0.049384, grad_norm: 0.592361
[[34m2025-10-04 12:07:25[0m] Step: 1848, Training Logs: loss_final: 0.940403, loss_mean: 0.892252, loss_mean_cls: 0.048151, grad_norm: 0.853311
[[34m2025-10-04 12:07:25[0m] Step: 1849, Training Logs: loss_final: 0.933544, loss_mean: 0.885578, loss_mean_cls: 0.047966, grad_norm: 0.654630
[[34m2025-10-04 12:07:26[0m] Step: 1850, Training Logs: loss_final: 0.928110, loss_mean: 0.880283, loss_mean_cls: 0.047826, grad_norm: 0.742539
[[34m2025-10-04 12:07:26[0m] Step: 1851, Training Logs: loss_final: 0.930861, loss_mean: 0.883326, loss_mean_cls: 0.047535, grad_norm: 0.979274
[[34m2025-10-04 12:07:26[0m] Step: 1852, Training Logs: loss_final: 0.922714, loss_mean: 0.874067, loss_mean_cls: 0.048647, grad_norm: 0.680881
[[34m2025-10-04 12:07:27[0m] Step: 1853, Training Logs: loss_final: 0.941434, loss_mean: 0.892809, loss_mean_cls: 0.048624, grad_norm: 0.799559
[[34m2025-10-04 12:07:27[0m] Step: 1854, Training Logs: loss_final: 0.922345, loss_mean: 0.874672, loss_mean_cls: 0.047673, grad_norm: 0.648291
[[34m2025-10-04 12:07:27[0m] Step: 1855, Training Logs: loss_final: 0.930101, loss_mean: 0.882609, loss_mean_cls: 0.047491, grad_norm: 0.757831
[[34m2025-10-04 12:07:28[0m] Step: 1856, Training Logs: loss_final: 0.936504, loss_mean: 0.888779, loss_mean_cls: 0.047725, grad_norm: 0.644022
[[34m2025-10-04 12:07:28[0m] Step: 1857, Training Logs: loss_final: 0.939635, loss_mean: 0.891049, loss_mean_cls: 0.048585, grad_norm: 0.558581
[[34m2025-10-04 12:07:28[0m] Step: 1858, Training Logs: loss_final: 0.918706, loss_mean: 0.870016, loss_mean_cls: 0.048690, grad_norm: 0.793825
[[34m2025-10-04 12:07:28[0m] Step: 1859, Training Logs: loss_final: 0.913066, loss_mean: 0.863870, loss_mean_cls: 0.049196, grad_norm: 0.608388
[[34m2025-10-04 12:07:29[0m] Step: 1860, Training Logs: loss_final: 0.931697, loss_mean: 0.883727, loss_mean_cls: 0.047970, grad_norm: 0.689120
[[34m2025-10-04 12:07:29[0m] Step: 1861, Training Logs: loss_final: 0.943699, loss_mean: 0.894718, loss_mean_cls: 0.048982, grad_norm: 0.760105
[[34m2025-10-04 12:07:29[0m] Step: 1862, Training Logs: loss_final: 0.925954, loss_mean: 0.876210, loss_mean_cls: 0.049743, grad_norm: 0.550434
[[34m2025-10-04 12:07:30[0m] Step: 1863, Training Logs: loss_final: 0.918843, loss_mean: 0.870399, loss_mean_cls: 0.048444, grad_norm: 0.610415
[[34m2025-10-04 12:07:30[0m] Step: 1864, Training Logs: loss_final: 0.936826, loss_mean: 0.888500, loss_mean_cls: 0.048326, grad_norm: 0.706288
[[34m2025-10-04 12:07:30[0m] Step: 1865, Training Logs: loss_final: 0.927271, loss_mean: 0.879569, loss_mean_cls: 0.047702, grad_norm: 0.511028
[[34m2025-10-04 12:07:31[0m] Step: 1866, Training Logs: loss_final: 0.899787, loss_mean: 0.851725, loss_mean_cls: 0.048061, grad_norm: 0.696816
[[34m2025-10-04 12:07:31[0m] Step: 1867, Training Logs: loss_final: 0.898663, loss_mean: 0.849916, loss_mean_cls: 0.048747, grad_norm: 0.525382
[[34m2025-10-04 12:07:31[0m] Step: 1868, Training Logs: loss_final: 0.897666, loss_mean: 0.848373, loss_mean_cls: 0.049293, grad_norm: 0.736167
[[34m2025-10-04 12:07:31[0m] Step: 1869, Training Logs: loss_final: 0.904886, loss_mean: 0.855526, loss_mean_cls: 0.049359, grad_norm: 0.583999
[[34m2025-10-04 12:07:32[0m] Step: 1870, Training Logs: loss_final: 0.921094, loss_mean: 0.873802, loss_mean_cls: 0.047292, grad_norm: 0.494825
[[34m2025-10-04 12:07:32[0m] Step: 1871, Training Logs: loss_final: 0.914428, loss_mean: 0.865476, loss_mean_cls: 0.048952, grad_norm: 0.631423
[[34m2025-10-04 12:07:32[0m] Step: 1872, Training Logs: loss_final: 0.929804, loss_mean: 0.881413, loss_mean_cls: 0.048392, grad_norm: 0.550571
[[34m2025-10-04 12:07:33[0m] Step: 1873, Training Logs: loss_final: 0.916767, loss_mean: 0.868275, loss_mean_cls: 0.048492, grad_norm: 0.519760
[[34m2025-10-04 12:07:33[0m] Step: 1874, Training Logs: loss_final: 0.923773, loss_mean: 0.875867, loss_mean_cls: 0.047906, grad_norm: 0.475135
[[34m2025-10-04 12:07:33[0m] Step: 1875, Training Logs: loss_final: 0.933418, loss_mean: 0.886864, loss_mean_cls: 0.046554, grad_norm: 0.651759
[[34m2025-10-04 12:07:33[0m] Step: 1876, Training Logs: loss_final: 0.915213, loss_mean: 0.866116, loss_mean_cls: 0.049097, grad_norm: 0.411047
[[34m2025-10-04 12:07:34[0m] Step: 1877, Training Logs: loss_final: 0.928686, loss_mean: 0.880217, loss_mean_cls: 0.048469, grad_norm: 0.536347
[[34m2025-10-04 12:07:34[0m] Step: 1878, Training Logs: loss_final: 0.912486, loss_mean: 0.864854, loss_mean_cls: 0.047631, grad_norm: 0.428328
[[34m2025-10-04 12:07:34[0m] Step: 1879, Training Logs: loss_final: 0.918192, loss_mean: 0.869094, loss_mean_cls: 0.049098, grad_norm: 0.418856
[[34m2025-10-04 12:07:35[0m] Step: 1880, Training Logs: loss_final: 0.919636, loss_mean: 0.871099, loss_mean_cls: 0.048538, grad_norm: 0.568565
[[34m2025-10-04 12:07:35[0m] Step: 1881, Training Logs: loss_final: 0.925776, loss_mean: 0.877321, loss_mean_cls: 0.048456, grad_norm: 0.398002
[[34m2025-10-04 12:07:35[0m] Step: 1882, Training Logs: loss_final: 0.917940, loss_mean: 0.869094, loss_mean_cls: 0.048846, grad_norm: 0.556993
[[34m2025-10-04 12:07:35[0m] Step: 1883, Training Logs: loss_final: 0.920163, loss_mean: 0.872741, loss_mean_cls: 0.047421, grad_norm: 0.498143
[[34m2025-10-04 12:07:36[0m] Step: 1884, Training Logs: loss_final: 0.914969, loss_mean: 0.865676, loss_mean_cls: 0.049294, grad_norm: 0.545167
[[34m2025-10-04 12:07:36[0m] Step: 1885, Training Logs: loss_final: 0.940014, loss_mean: 0.891398, loss_mean_cls: 0.048616, grad_norm: 0.634957
[[34m2025-10-04 12:07:36[0m] Step: 1886, Training Logs: loss_final: 0.913807, loss_mean: 0.865650, loss_mean_cls: 0.048157, grad_norm: 0.494001
[[34m2025-10-04 12:07:37[0m] Step: 1887, Training Logs: loss_final: 0.929263, loss_mean: 0.881277, loss_mean_cls: 0.047986, grad_norm: 0.704559
[[34m2025-10-04 12:07:37[0m] Step: 1888, Training Logs: loss_final: 0.946290, loss_mean: 0.900092, loss_mean_cls: 0.046197, grad_norm: 0.518085
[[34m2025-10-04 12:07:37[0m] Step: 1889, Training Logs: loss_final: 0.925621, loss_mean: 0.876456, loss_mean_cls: 0.049165, grad_norm: 0.620263
[[34m2025-10-04 12:07:38[0m] Step: 1890, Training Logs: loss_final: 0.922056, loss_mean: 0.872938, loss_mean_cls: 0.049118, grad_norm: 0.782977
[[34m2025-10-04 12:07:38[0m] Step: 1891, Training Logs: loss_final: 0.916816, loss_mean: 0.868619, loss_mean_cls: 0.048196, grad_norm: 0.699222
[[34m2025-10-04 12:07:38[0m] Step: 1892, Training Logs: loss_final: 0.926095, loss_mean: 0.878559, loss_mean_cls: 0.047536, grad_norm: 0.788595
[[34m2025-10-04 12:07:38[0m] Step: 1893, Training Logs: loss_final: 0.922923, loss_mean: 0.874196, loss_mean_cls: 0.048727, grad_norm: 0.374595
[[34m2025-10-04 12:07:39[0m] Step: 1894, Training Logs: loss_final: 0.912006, loss_mean: 0.863330, loss_mean_cls: 0.048676, grad_norm: 0.683045
[[34m2025-10-04 12:07:39[0m] Step: 1895, Training Logs: loss_final: 0.931508, loss_mean: 0.884078, loss_mean_cls: 0.047430, grad_norm: 0.526031
[[34m2025-10-04 12:07:39[0m] Step: 1896, Training Logs: loss_final: 0.920970, loss_mean: 0.873014, loss_mean_cls: 0.047956, grad_norm: 0.705269
[[34m2025-10-04 12:07:40[0m] Step: 1897, Training Logs: loss_final: 0.925343, loss_mean: 0.876319, loss_mean_cls: 0.049023, grad_norm: 0.577086
[[34m2025-10-04 12:07:40[0m] Step: 1898, Training Logs: loss_final: 0.935716, loss_mean: 0.888103, loss_mean_cls: 0.047613, grad_norm: 0.476721
[[34m2025-10-04 12:07:40[0m] Step: 1899, Training Logs: loss_final: 0.925084, loss_mean: 0.877757, loss_mean_cls: 0.047327, grad_norm: 0.711573
[[34m2025-10-04 12:07:40[0m] Step: 1900, Training Logs: loss_final: 0.929958, loss_mean: 0.880514, loss_mean_cls: 0.049444, grad_norm: 0.576969
[[34m2025-10-04 12:07:41[0m] Step: 1901, Training Logs: loss_final: 0.917771, loss_mean: 0.868214, loss_mean_cls: 0.049557, grad_norm: 0.426455
[[34m2025-10-04 12:07:41[0m] Step: 1902, Training Logs: loss_final: 0.929970, loss_mean: 0.880458, loss_mean_cls: 0.049511, grad_norm: 0.509125
[[34m2025-10-04 12:07:41[0m] Step: 1903, Training Logs: loss_final: 0.917251, loss_mean: 0.868240, loss_mean_cls: 0.049011, grad_norm: 0.575172
[[34m2025-10-04 12:07:42[0m] Step: 1904, Training Logs: loss_final: 0.931006, loss_mean: 0.883532, loss_mean_cls: 0.047474, grad_norm: 0.450981
[[34m2025-10-04 12:07:42[0m] Step: 1905, Training Logs: loss_final: 0.917044, loss_mean: 0.869122, loss_mean_cls: 0.047922, grad_norm: 0.488062
[[34m2025-10-04 12:07:42[0m] Step: 1906, Training Logs: loss_final: 0.930167, loss_mean: 0.882354, loss_mean_cls: 0.047813, grad_norm: 0.578291
[[34m2025-10-04 12:07:43[0m] Step: 1907, Training Logs: loss_final: 0.931178, loss_mean: 0.883249, loss_mean_cls: 0.047929, grad_norm: 0.464061
[[34m2025-10-04 12:07:43[0m] Step: 1908, Training Logs: loss_final: 0.916619, loss_mean: 0.868564, loss_mean_cls: 0.048054, grad_norm: 0.601692
[[34m2025-10-04 12:07:43[0m] Step: 1909, Training Logs: loss_final: 0.896922, loss_mean: 0.848147, loss_mean_cls: 0.048774, grad_norm: 0.891011
[[34m2025-10-04 12:07:43[0m] Step: 1910, Training Logs: loss_final: 0.921947, loss_mean: 0.873927, loss_mean_cls: 0.048021, grad_norm: 0.511996
[[34m2025-10-04 12:07:44[0m] Step: 1911, Training Logs: loss_final: 0.922629, loss_mean: 0.874467, loss_mean_cls: 0.048163, grad_norm: 0.710193
[[34m2025-10-04 12:07:44[0m] Step: 1912, Training Logs: loss_final: 0.900040, loss_mean: 0.849857, loss_mean_cls: 0.050183, grad_norm: 0.759879
[[34m2025-10-04 12:07:44[0m] Step: 1913, Training Logs: loss_final: 0.901914, loss_mean: 0.852147, loss_mean_cls: 0.049766, grad_norm: 0.461910
[[34m2025-10-04 12:07:45[0m] Step: 1914, Training Logs: loss_final: 0.904711, loss_mean: 0.856971, loss_mean_cls: 0.047740, grad_norm: 0.917408
[[34m2025-10-04 12:07:45[0m] Step: 1915, Training Logs: loss_final: 0.903704, loss_mean: 0.856023, loss_mean_cls: 0.047681, grad_norm: 0.376161
[[34m2025-10-04 12:07:45[0m] Step: 1916, Training Logs: loss_final: 0.935847, loss_mean: 0.887219, loss_mean_cls: 0.048628, grad_norm: 0.762430
[[34m2025-10-04 12:07:46[0m] Step: 1917, Training Logs: loss_final: 0.905354, loss_mean: 0.856365, loss_mean_cls: 0.048989, grad_norm: 0.601376
[[34m2025-10-04 12:07:46[0m] Step: 1918, Training Logs: loss_final: 0.903860, loss_mean: 0.854406, loss_mean_cls: 0.049454, grad_norm: 0.626351
[[34m2025-10-04 12:07:46[0m] Step: 1919, Training Logs: loss_final: 0.941324, loss_mean: 0.892541, loss_mean_cls: 0.048782, grad_norm: 0.760803
[[34m2025-10-04 12:07:46[0m] Step: 1920, Training Logs: loss_final: 0.918239, loss_mean: 0.870176, loss_mean_cls: 0.048063, grad_norm: 0.634731
[[34m2025-10-04 12:07:47[0m] Step: 1921, Training Logs: loss_final: 0.931838, loss_mean: 0.883524, loss_mean_cls: 0.048314, grad_norm: 0.515896
[[34m2025-10-04 12:07:47[0m] Step: 1922, Training Logs: loss_final: 0.924960, loss_mean: 0.876541, loss_mean_cls: 0.048419, grad_norm: 0.783012
[[34m2025-10-04 12:07:47[0m] Step: 1923, Training Logs: loss_final: 0.918664, loss_mean: 0.870300, loss_mean_cls: 0.048364, grad_norm: 0.573584
[[34m2025-10-04 12:07:48[0m] Step: 1924, Training Logs: loss_final: 0.894717, loss_mean: 0.845621, loss_mean_cls: 0.049096, grad_norm: 0.817715
[[34m2025-10-04 12:07:48[0m] Step: 1925, Training Logs: loss_final: 0.908544, loss_mean: 0.859414, loss_mean_cls: 0.049130, grad_norm: 0.482280
[[34m2025-10-04 12:07:48[0m] Step: 1926, Training Logs: loss_final: 0.928911, loss_mean: 0.880407, loss_mean_cls: 0.048504, grad_norm: 0.701544
[[34m2025-10-04 12:07:49[0m] Step: 1927, Training Logs: loss_final: 0.948689, loss_mean: 0.901496, loss_mean_cls: 0.047193, grad_norm: 0.475425
[[34m2025-10-04 12:07:49[0m] Step: 1928, Training Logs: loss_final: 0.908602, loss_mean: 0.860250, loss_mean_cls: 0.048352, grad_norm: 0.686450
[[34m2025-10-04 12:07:49[0m] Step: 1929, Training Logs: loss_final: 0.944749, loss_mean: 0.897145, loss_mean_cls: 0.047605, grad_norm: 0.499645
[[34m2025-10-04 12:07:49[0m] Step: 1930, Training Logs: loss_final: 0.914012, loss_mean: 0.866495, loss_mean_cls: 0.047517, grad_norm: 0.565629
[[34m2025-10-04 12:07:50[0m] Step: 1931, Training Logs: loss_final: 0.931814, loss_mean: 0.884050, loss_mean_cls: 0.047764, grad_norm: 0.541865
[[34m2025-10-04 12:07:50[0m] Step: 1932, Training Logs: loss_final: 0.927092, loss_mean: 0.878450, loss_mean_cls: 0.048643, grad_norm: 0.502437
[[34m2025-10-04 12:07:50[0m] Step: 1933, Training Logs: loss_final: 0.935976, loss_mean: 0.887981, loss_mean_cls: 0.047995, grad_norm: 0.463373
[[34m2025-10-04 12:07:51[0m] Step: 1934, Training Logs: loss_final: 0.925243, loss_mean: 0.876900, loss_mean_cls: 0.048343, grad_norm: 0.522299
[[34m2025-10-04 12:07:51[0m] Step: 1935, Training Logs: loss_final: 0.920773, loss_mean: 0.873009, loss_mean_cls: 0.047764, grad_norm: 0.624217
[[34m2025-10-04 12:07:51[0m] Step: 1936, Training Logs: loss_final: 0.920856, loss_mean: 0.872919, loss_mean_cls: 0.047936, grad_norm: 0.517690
[[34m2025-10-04 12:07:51[0m] Step: 1937, Training Logs: loss_final: 0.935572, loss_mean: 0.887634, loss_mean_cls: 0.047939, grad_norm: 0.687640
[[34m2025-10-04 12:07:52[0m] Step: 1938, Training Logs: loss_final: 0.925182, loss_mean: 0.874882, loss_mean_cls: 0.050300, grad_norm: 0.465368
[[34m2025-10-04 12:07:52[0m] Step: 1939, Training Logs: loss_final: 0.912658, loss_mean: 0.864470, loss_mean_cls: 0.048188, grad_norm: 0.553464
[[34m2025-10-04 12:07:52[0m] Step: 1940, Training Logs: loss_final: 0.932358, loss_mean: 0.883984, loss_mean_cls: 0.048374, grad_norm: 0.526816
[[34m2025-10-04 12:07:53[0m] Step: 1941, Training Logs: loss_final: 0.922711, loss_mean: 0.875968, loss_mean_cls: 0.046743, grad_norm: 0.383985
[[34m2025-10-04 12:07:53[0m] Step: 1942, Training Logs: loss_final: 0.912981, loss_mean: 0.865072, loss_mean_cls: 0.047908, grad_norm: 0.534765
[[34m2025-10-04 12:07:53[0m] Step: 1943, Training Logs: loss_final: 0.922630, loss_mean: 0.873643, loss_mean_cls: 0.048987, grad_norm: 0.584270
[[34m2025-10-04 12:07:53[0m] Step: 1944, Training Logs: loss_final: 0.915761, loss_mean: 0.867646, loss_mean_cls: 0.048115, grad_norm: 0.479881
[[34m2025-10-04 12:07:54[0m] Step: 1945, Training Logs: loss_final: 0.924837, loss_mean: 0.877396, loss_mean_cls: 0.047441, grad_norm: 0.579852
[[34m2025-10-04 12:07:54[0m] Step: 1946, Training Logs: loss_final: 0.937479, loss_mean: 0.890391, loss_mean_cls: 0.047088, grad_norm: 0.433163
[[34m2025-10-04 12:07:54[0m] Step: 1947, Training Logs: loss_final: 0.911134, loss_mean: 0.863330, loss_mean_cls: 0.047804, grad_norm: 0.550735
[[34m2025-10-04 12:07:55[0m] Step: 1948, Training Logs: loss_final: 0.911895, loss_mean: 0.863390, loss_mean_cls: 0.048504, grad_norm: 0.518361
[[34m2025-10-04 12:07:55[0m] Step: 1949, Training Logs: loss_final: 0.936283, loss_mean: 0.887033, loss_mean_cls: 0.049250, grad_norm: 0.501840
[[34m2025-10-04 12:07:55[0m] Step: 1950, Training Logs: loss_final: 0.947249, loss_mean: 0.899210, loss_mean_cls: 0.048039, grad_norm: 0.564667
[[34m2025-10-04 12:07:56[0m] Step: 1951, Training Logs: loss_final: 0.920692, loss_mean: 0.874035, loss_mean_cls: 0.046657, grad_norm: 0.517772
[[34m2025-10-04 12:07:56[0m] Step: 1952, Training Logs: loss_final: 0.910602, loss_mean: 0.862031, loss_mean_cls: 0.048572, grad_norm: 0.776775
[[34m2025-10-04 12:07:56[0m] Step: 1953, Training Logs: loss_final: 0.942580, loss_mean: 0.895134, loss_mean_cls: 0.047446, grad_norm: 0.538466
[[34m2025-10-04 12:07:56[0m] Step: 1954, Training Logs: loss_final: 0.920839, loss_mean: 0.872097, loss_mean_cls: 0.048742, grad_norm: 0.775027
[[34m2025-10-04 12:07:57[0m] Step: 1955, Training Logs: loss_final: 0.937173, loss_mean: 0.889164, loss_mean_cls: 0.048009, grad_norm: 0.538185
[[34m2025-10-04 12:07:57[0m] Step: 1956, Training Logs: loss_final: 0.912945, loss_mean: 0.864662, loss_mean_cls: 0.048283, grad_norm: 0.595176
[[34m2025-10-04 12:07:57[0m] Step: 1957, Training Logs: loss_final: 0.930944, loss_mean: 0.882071, loss_mean_cls: 0.048873, grad_norm: 0.542776
[[34m2025-10-04 12:07:58[0m] Step: 1958, Training Logs: loss_final: 0.903001, loss_mean: 0.853796, loss_mean_cls: 0.049206, grad_norm: 0.477133
[[34m2025-10-04 12:07:58[0m] Step: 1959, Training Logs: loss_final: 0.899240, loss_mean: 0.850441, loss_mean_cls: 0.048799, grad_norm: 0.562920
[[34m2025-10-04 12:07:58[0m] Step: 1960, Training Logs: loss_final: 0.903091, loss_mean: 0.855438, loss_mean_cls: 0.047652, grad_norm: 0.444387
[[34m2025-10-04 12:07:58[0m] Step: 1961, Training Logs: loss_final: 0.910611, loss_mean: 0.861472, loss_mean_cls: 0.049138, grad_norm: 0.815958
[[34m2025-10-04 12:07:59[0m] Step: 1962, Training Logs: loss_final: 0.954606, loss_mean: 0.906654, loss_mean_cls: 0.047952, grad_norm: 0.456784
[[34m2025-10-04 12:07:59[0m] Step: 1963, Training Logs: loss_final: 0.945957, loss_mean: 0.897585, loss_mean_cls: 0.048372, grad_norm: 0.766687
[[34m2025-10-04 12:07:59[0m] Step: 1964, Training Logs: loss_final: 0.923374, loss_mean: 0.873884, loss_mean_cls: 0.049490, grad_norm: 0.624876
[[34m2025-10-04 12:08:00[0m] Step: 1965, Training Logs: loss_final: 0.932850, loss_mean: 0.885056, loss_mean_cls: 0.047794, grad_norm: 0.622416
[[34m2025-10-04 12:08:00[0m] Step: 1966, Training Logs: loss_final: 0.919003, loss_mean: 0.872001, loss_mean_cls: 0.047002, grad_norm: 0.844742
[[34m2025-10-04 12:08:00[0m] Step: 1967, Training Logs: loss_final: 0.930194, loss_mean: 0.883043, loss_mean_cls: 0.047151, grad_norm: 0.376107
[[34m2025-10-04 12:08:01[0m] Step: 1968, Training Logs: loss_final: 0.951086, loss_mean: 0.902914, loss_mean_cls: 0.048173, grad_norm: 0.786472
[[34m2025-10-04 12:08:01[0m] Step: 1969, Training Logs: loss_final: 0.902217, loss_mean: 0.852614, loss_mean_cls: 0.049603, grad_norm: 0.499659
[[34m2025-10-04 12:08:01[0m] Step: 1970, Training Logs: loss_final: 0.899430, loss_mean: 0.851179, loss_mean_cls: 0.048250, grad_norm: 0.624826
[[34m2025-10-04 12:08:01[0m] Step: 1971, Training Logs: loss_final: 0.907714, loss_mean: 0.859638, loss_mean_cls: 0.048075, grad_norm: 0.487471
[[34m2025-10-04 12:08:02[0m] Step: 1972, Training Logs: loss_final: 0.944461, loss_mean: 0.897321, loss_mean_cls: 0.047140, grad_norm: 0.423798
[[34m2025-10-04 12:08:02[0m] Step: 1973, Training Logs: loss_final: 0.936722, loss_mean: 0.888292, loss_mean_cls: 0.048429, grad_norm: 0.659032
[[34m2025-10-04 12:08:02[0m] Step: 1974, Training Logs: loss_final: 0.936037, loss_mean: 0.888559, loss_mean_cls: 0.047478, grad_norm: 0.573966
[[34m2025-10-04 12:08:03[0m] Step: 1975, Training Logs: loss_final: 0.915623, loss_mean: 0.867433, loss_mean_cls: 0.048189, grad_norm: 0.509926
[[34m2025-10-04 12:08:03[0m] Step: 1976, Training Logs: loss_final: 0.925884, loss_mean: 0.877327, loss_mean_cls: 0.048556, grad_norm: 0.623448
[[34m2025-10-04 12:08:03[0m] Step: 1977, Training Logs: loss_final: 0.922413, loss_mean: 0.874574, loss_mean_cls: 0.047839, grad_norm: 0.725281
[[34m2025-10-04 12:08:03[0m] Step: 1978, Training Logs: loss_final: 0.906429, loss_mean: 0.857618, loss_mean_cls: 0.048811, grad_norm: 0.405948
[[34m2025-10-04 12:08:04[0m] Step: 1979, Training Logs: loss_final: 0.922273, loss_mean: 0.874676, loss_mean_cls: 0.047597, grad_norm: 0.439364
[[34m2025-10-04 12:08:04[0m] Step: 1980, Training Logs: loss_final: 0.940058, loss_mean: 0.892213, loss_mean_cls: 0.047846, grad_norm: 0.517645
[[34m2025-10-04 12:08:04[0m] Step: 1981, Training Logs: loss_final: 0.898730, loss_mean: 0.849800, loss_mean_cls: 0.048930, grad_norm: 0.455505
[[34m2025-10-04 12:08:05[0m] Step: 1982, Training Logs: loss_final: 0.923823, loss_mean: 0.876406, loss_mean_cls: 0.047418, grad_norm: 0.539737
[[34m2025-10-04 12:08:05[0m] Step: 1983, Training Logs: loss_final: 0.906237, loss_mean: 0.857170, loss_mean_cls: 0.049067, grad_norm: 0.529959
[[34m2025-10-04 12:08:05[0m] Step: 1984, Training Logs: loss_final: 0.935969, loss_mean: 0.887192, loss_mean_cls: 0.048777, grad_norm: 0.488194
[[34m2025-10-04 12:08:06[0m] Step: 1985, Training Logs: loss_final: 0.924752, loss_mean: 0.876836, loss_mean_cls: 0.047917, grad_norm: 0.451647
[[34m2025-10-04 12:08:06[0m] Step: 1986, Training Logs: loss_final: 0.916364, loss_mean: 0.868199, loss_mean_cls: 0.048164, grad_norm: 0.498810
[[34m2025-10-04 12:08:06[0m] Step: 1987, Training Logs: loss_final: 0.914636, loss_mean: 0.866081, loss_mean_cls: 0.048555, grad_norm: 0.387543
[[34m2025-10-04 12:08:06[0m] Step: 1988, Training Logs: loss_final: 0.897749, loss_mean: 0.850037, loss_mean_cls: 0.047712, grad_norm: 0.630617
[[34m2025-10-04 12:08:07[0m] Step: 1989, Training Logs: loss_final: 0.935176, loss_mean: 0.886732, loss_mean_cls: 0.048444, grad_norm: 0.463509
[[34m2025-10-04 12:08:07[0m] Step: 1990, Training Logs: loss_final: 0.914600, loss_mean: 0.867722, loss_mean_cls: 0.046878, grad_norm: 0.587508
[[34m2025-10-04 12:08:07[0m] Step: 1991, Training Logs: loss_final: 0.919941, loss_mean: 0.871690, loss_mean_cls: 0.048251, grad_norm: 0.671424
[[34m2025-10-04 12:08:08[0m] Step: 1992, Training Logs: loss_final: 0.900180, loss_mean: 0.852873, loss_mean_cls: 0.047307, grad_norm: 0.672010
[[34m2025-10-04 12:08:08[0m] Step: 1993, Training Logs: loss_final: 0.923408, loss_mean: 0.875025, loss_mean_cls: 0.048384, grad_norm: 0.543231
[[34m2025-10-04 12:08:08[0m] Step: 1994, Training Logs: loss_final: 0.903244, loss_mean: 0.854512, loss_mean_cls: 0.048732, grad_norm: 0.459667
[[34m2025-10-04 12:08:08[0m] Step: 1995, Training Logs: loss_final: 0.930847, loss_mean: 0.881554, loss_mean_cls: 0.049292, grad_norm: 0.517941
[[34m2025-10-04 12:08:09[0m] Step: 1996, Training Logs: loss_final: 0.927279, loss_mean: 0.879564, loss_mean_cls: 0.047715, grad_norm: 0.485323
[[34m2025-10-04 12:08:09[0m] Step: 1997, Training Logs: loss_final: 0.922534, loss_mean: 0.875768, loss_mean_cls: 0.046766, grad_norm: 0.434701
[[34m2025-10-04 12:08:09[0m] Step: 1998, Training Logs: loss_final: 0.910386, loss_mean: 0.862460, loss_mean_cls: 0.047926, grad_norm: 0.559528
[[34m2025-10-04 12:08:10[0m] Step: 1999, Training Logs: loss_final: 0.935479, loss_mean: 0.887769, loss_mean_cls: 0.047710, grad_norm: 0.608176
[[34m2025-10-04 12:08:10[0m] Step: 2000, Training Logs: loss_final: 0.914653, loss_mean: 0.865673, loss_mean_cls: 0.048980, grad_norm: 0.555757
[[34m2025-10-04 12:08:10[0m] Step: 2001, Training Logs: loss_final: 0.927891, loss_mean: 0.881483, loss_mean_cls: 0.046409, grad_norm: 0.893081
[[34m2025-10-04 12:08:10[0m] Step: 2002, Training Logs: loss_final: 0.916695, loss_mean: 0.868675, loss_mean_cls: 0.048020, grad_norm: 0.475470
[[34m2025-10-04 12:08:11[0m] Step: 2003, Training Logs: loss_final: 0.917132, loss_mean: 0.867831, loss_mean_cls: 0.049301, grad_norm: 0.861466
[[34m2025-10-04 12:08:11[0m] Step: 2004, Training Logs: loss_final: 0.900248, loss_mean: 0.851403, loss_mean_cls: 0.048845, grad_norm: 0.518439
[[34m2025-10-04 12:08:11[0m] Step: 2005, Training Logs: loss_final: 0.914494, loss_mean: 0.866889, loss_mean_cls: 0.047605, grad_norm: 0.563594
[[34m2025-10-04 12:08:12[0m] Step: 2006, Training Logs: loss_final: 0.914681, loss_mean: 0.865871, loss_mean_cls: 0.048810, grad_norm: 0.733292
[[34m2025-10-04 12:08:12[0m] Step: 2007, Training Logs: loss_final: 0.889390, loss_mean: 0.839584, loss_mean_cls: 0.049806, grad_norm: 0.578934
[[34m2025-10-04 12:08:12[0m] Step: 2008, Training Logs: loss_final: 0.920371, loss_mean: 0.872716, loss_mean_cls: 0.047654, grad_norm: 0.760017
[[34m2025-10-04 12:08:13[0m] Step: 2009, Training Logs: loss_final: 0.929859, loss_mean: 0.882102, loss_mean_cls: 0.047757, grad_norm: 0.789934
[[34m2025-10-04 12:08:13[0m] Step: 2010, Training Logs: loss_final: 0.926790, loss_mean: 0.878680, loss_mean_cls: 0.048111, grad_norm: 0.547801
[[34m2025-10-04 12:08:13[0m] Step: 2011, Training Logs: loss_final: 0.911355, loss_mean: 0.862981, loss_mean_cls: 0.048374, grad_norm: 0.970159
[[34m2025-10-04 12:08:13[0m] Step: 2012, Training Logs: loss_final: 0.902356, loss_mean: 0.854494, loss_mean_cls: 0.047862, grad_norm: 0.841631
[[34m2025-10-04 12:08:14[0m] Step: 2013, Training Logs: loss_final: 0.913368, loss_mean: 0.864991, loss_mean_cls: 0.048377, grad_norm: 0.959324
[[34m2025-10-04 12:08:14[0m] Step: 2014, Training Logs: loss_final: 0.947797, loss_mean: 0.899675, loss_mean_cls: 0.048122, grad_norm: 0.794436
[[34m2025-10-04 12:08:14[0m] Step: 2015, Training Logs: loss_final: 0.906822, loss_mean: 0.859057, loss_mean_cls: 0.047765, grad_norm: 0.663319
[[34m2025-10-04 12:08:15[0m] Step: 2016, Training Logs: loss_final: 0.924421, loss_mean: 0.877239, loss_mean_cls: 0.047182, grad_norm: 0.780253
[[34m2025-10-04 12:08:15[0m] Step: 2017, Training Logs: loss_final: 0.922346, loss_mean: 0.875846, loss_mean_cls: 0.046499, grad_norm: 0.916537
[[34m2025-10-04 12:08:15[0m] Step: 2018, Training Logs: loss_final: 0.926681, loss_mean: 0.878424, loss_mean_cls: 0.048257, grad_norm: 0.515672
[[34m2025-10-04 12:08:15[0m] Step: 2019, Training Logs: loss_final: 0.925612, loss_mean: 0.875633, loss_mean_cls: 0.049979, grad_norm: 0.674826
[[34m2025-10-04 12:08:16[0m] Step: 2020, Training Logs: loss_final: 0.933078, loss_mean: 0.884844, loss_mean_cls: 0.048234, grad_norm: 0.589924
[[34m2025-10-04 12:08:16[0m] Step: 2021, Training Logs: loss_final: 0.929497, loss_mean: 0.880916, loss_mean_cls: 0.048581, grad_norm: 0.552665
[[34m2025-10-04 12:08:16[0m] Step: 2022, Training Logs: loss_final: 0.915512, loss_mean: 0.866748, loss_mean_cls: 0.048764, grad_norm: 0.399108
[[34m2025-10-04 12:08:17[0m] Step: 2023, Training Logs: loss_final: 0.918200, loss_mean: 0.869309, loss_mean_cls: 0.048891, grad_norm: 0.866525
[[34m2025-10-04 12:08:17[0m] Step: 2024, Training Logs: loss_final: 0.906257, loss_mean: 0.859075, loss_mean_cls: 0.047182, grad_norm: 0.372694
[[34m2025-10-04 12:08:17[0m] Step: 2025, Training Logs: loss_final: 0.925594, loss_mean: 0.877617, loss_mean_cls: 0.047977, grad_norm: 0.723344
[[34m2025-10-04 12:08:18[0m] Step: 2026, Training Logs: loss_final: 0.905625, loss_mean: 0.857219, loss_mean_cls: 0.048405, grad_norm: 0.751093
[[34m2025-10-04 12:08:18[0m] Step: 2027, Training Logs: loss_final: 0.914617, loss_mean: 0.866334, loss_mean_cls: 0.048282, grad_norm: 0.626526
[[34m2025-10-04 12:08:18[0m] Step: 2028, Training Logs: loss_final: 0.922113, loss_mean: 0.873980, loss_mean_cls: 0.048134, grad_norm: 0.758920
[[34m2025-10-04 12:08:18[0m] Step: 2029, Training Logs: loss_final: 0.928657, loss_mean: 0.879651, loss_mean_cls: 0.049006, grad_norm: 0.751782
[[34m2025-10-04 12:08:19[0m] Step: 2030, Training Logs: loss_final: 0.920172, loss_mean: 0.872256, loss_mean_cls: 0.047917, grad_norm: 0.592718
[[34m2025-10-04 12:08:19[0m] Step: 2031, Training Logs: loss_final: 0.930615, loss_mean: 0.882128, loss_mean_cls: 0.048487, grad_norm: 0.615951
[[34m2025-10-04 12:08:19[0m] Step: 2032, Training Logs: loss_final: 0.920071, loss_mean: 0.870905, loss_mean_cls: 0.049166, grad_norm: 0.557851
[[34m2025-10-04 12:08:20[0m] Step: 2033, Training Logs: loss_final: 0.924034, loss_mean: 0.876263, loss_mean_cls: 0.047772, grad_norm: 0.486958
[[34m2025-10-04 12:08:20[0m] Step: 2034, Training Logs: loss_final: 0.937805, loss_mean: 0.889686, loss_mean_cls: 0.048119, grad_norm: 0.738284
[[34m2025-10-04 12:08:20[0m] Step: 2035, Training Logs: loss_final: 0.922952, loss_mean: 0.875281, loss_mean_cls: 0.047671, grad_norm: 0.516956
[[34m2025-10-04 12:08:20[0m] Step: 2036, Training Logs: loss_final: 0.917558, loss_mean: 0.870416, loss_mean_cls: 0.047142, grad_norm: 0.612178
[[34m2025-10-04 12:08:21[0m] Step: 2037, Training Logs: loss_final: 0.920117, loss_mean: 0.871972, loss_mean_cls: 0.048145, grad_norm: 0.640628
[[34m2025-10-04 12:08:21[0m] Step: 2038, Training Logs: loss_final: 0.940117, loss_mean: 0.891847, loss_mean_cls: 0.048270, grad_norm: 0.610637
[[34m2025-10-04 12:08:21[0m] Step: 2039, Training Logs: loss_final: 0.930997, loss_mean: 0.883773, loss_mean_cls: 0.047224, grad_norm: 0.538569
[[34m2025-10-04 12:08:22[0m] Step: 2040, Training Logs: loss_final: 0.928594, loss_mean: 0.879664, loss_mean_cls: 0.048931, grad_norm: 0.565009
[[34m2025-10-04 12:08:22[0m] Step: 2041, Training Logs: loss_final: 0.910947, loss_mean: 0.863208, loss_mean_cls: 0.047739, grad_norm: 0.604604
[[34m2025-10-04 12:08:22[0m] Step: 2042, Training Logs: loss_final: 0.927663, loss_mean: 0.878913, loss_mean_cls: 0.048750, grad_norm: 0.492659
[[34m2025-10-04 12:08:23[0m] Step: 2043, Training Logs: loss_final: 0.920079, loss_mean: 0.872437, loss_mean_cls: 0.047642, grad_norm: 0.401923
[[34m2025-10-04 12:08:23[0m] Step: 2044, Training Logs: loss_final: 0.911989, loss_mean: 0.862937, loss_mean_cls: 0.049052, grad_norm: 0.601164
[[34m2025-10-04 12:08:23[0m] Step: 2045, Training Logs: loss_final: 0.913123, loss_mean: 0.864897, loss_mean_cls: 0.048226, grad_norm: 0.483752
[[34m2025-10-04 12:08:23[0m] Step: 2046, Training Logs: loss_final: 0.928655, loss_mean: 0.879643, loss_mean_cls: 0.049012, grad_norm: 0.558907
[[34m2025-10-04 12:08:24[0m] Step: 2047, Training Logs: loss_final: 0.919250, loss_mean: 0.871175, loss_mean_cls: 0.048075, grad_norm: 0.433230
[[34m2025-10-04 12:08:24[0m] Step: 2048, Training Logs: loss_final: 0.940841, loss_mean: 0.892970, loss_mean_cls: 0.047870, grad_norm: 0.644018
[[34m2025-10-04 12:08:24[0m] Step: 2049, Training Logs: loss_final: 0.926539, loss_mean: 0.878136, loss_mean_cls: 0.048403, grad_norm: 0.676299
[[34m2025-10-04 12:08:25[0m] Step: 2050, Training Logs: loss_final: 0.934238, loss_mean: 0.887113, loss_mean_cls: 0.047126, grad_norm: 0.710459
[[34m2025-10-04 12:08:25[0m] Step: 2051, Training Logs: loss_final: 0.922018, loss_mean: 0.874419, loss_mean_cls: 0.047599, grad_norm: 0.824616
[[34m2025-10-04 12:08:25[0m] Step: 2052, Training Logs: loss_final: 0.904133, loss_mean: 0.855887, loss_mean_cls: 0.048246, grad_norm: 0.689716
[[34m2025-10-04 12:08:26[0m] Step: 2053, Training Logs: loss_final: 0.932684, loss_mean: 0.884796, loss_mean_cls: 0.047888, grad_norm: 0.871220
[[34m2025-10-04 12:08:26[0m] Step: 2054, Training Logs: loss_final: 0.913889, loss_mean: 0.865059, loss_mean_cls: 0.048830, grad_norm: 0.915614
[[34m2025-10-04 12:08:26[0m] Step: 2055, Training Logs: loss_final: 0.919888, loss_mean: 0.872417, loss_mean_cls: 0.047471, grad_norm: 0.482243
[[34m2025-10-04 12:08:26[0m] Step: 2056, Training Logs: loss_final: 0.922712, loss_mean: 0.873126, loss_mean_cls: 0.049586, grad_norm: 0.900112
[[34m2025-10-04 12:08:27[0m] Step: 2057, Training Logs: loss_final: 0.918103, loss_mean: 0.870391, loss_mean_cls: 0.047712, grad_norm: 0.458292
[[34m2025-10-04 12:08:27[0m] Step: 2058, Training Logs: loss_final: 0.920454, loss_mean: 0.872482, loss_mean_cls: 0.047972, grad_norm: 0.638080
[[34m2025-10-04 12:08:27[0m] Step: 2059, Training Logs: loss_final: 0.919355, loss_mean: 0.871462, loss_mean_cls: 0.047893, grad_norm: 0.726309
[[34m2025-10-04 12:08:28[0m] Step: 2060, Training Logs: loss_final: 0.915410, loss_mean: 0.866704, loss_mean_cls: 0.048706, grad_norm: 0.581479
[[34m2025-10-04 12:08:28[0m] Step: 2061, Training Logs: loss_final: 0.922595, loss_mean: 0.875037, loss_mean_cls: 0.047558, grad_norm: 0.679010
[[34m2025-10-04 12:08:28[0m] Step: 2062, Training Logs: loss_final: 0.918268, loss_mean: 0.870810, loss_mean_cls: 0.047458, grad_norm: 0.671779
[[34m2025-10-04 12:08:28[0m] Step: 2063, Training Logs: loss_final: 0.921031, loss_mean: 0.873479, loss_mean_cls: 0.047552, grad_norm: 0.550173
[[34m2025-10-04 12:08:29[0m] Step: 2064, Training Logs: loss_final: 0.911686, loss_mean: 0.863895, loss_mean_cls: 0.047790, grad_norm: 0.608186
[[34m2025-10-04 12:08:29[0m] Step: 2065, Training Logs: loss_final: 0.909975, loss_mean: 0.862335, loss_mean_cls: 0.047640, grad_norm: 0.639389
[[34m2025-10-04 12:08:29[0m] Step: 2066, Training Logs: loss_final: 0.916275, loss_mean: 0.868139, loss_mean_cls: 0.048136, grad_norm: 0.596320
[[34m2025-10-04 12:08:30[0m] Step: 2067, Training Logs: loss_final: 0.901847, loss_mean: 0.852327, loss_mean_cls: 0.049519, grad_norm: 0.590867
[[34m2025-10-04 12:08:30[0m] Step: 2068, Training Logs: loss_final: 0.926725, loss_mean: 0.878963, loss_mean_cls: 0.047762, grad_norm: 0.584032
[[34m2025-10-04 12:08:30[0m] Step: 2069, Training Logs: loss_final: 0.920728, loss_mean: 0.871873, loss_mean_cls: 0.048856, grad_norm: 0.453701
[[34m2025-10-04 12:08:31[0m] Step: 2070, Training Logs: loss_final: 0.916543, loss_mean: 0.869668, loss_mean_cls: 0.046875, grad_norm: 0.688819
[[34m2025-10-04 12:08:31[0m] Step: 2071, Training Logs: loss_final: 0.914779, loss_mean: 0.867929, loss_mean_cls: 0.046850, grad_norm: 0.466216
[[34m2025-10-04 12:08:31[0m] Step: 2072, Training Logs: loss_final: 0.939832, loss_mean: 0.892378, loss_mean_cls: 0.047454, grad_norm: 0.615348
[[34m2025-10-04 12:08:31[0m] Step: 2073, Training Logs: loss_final: 0.915392, loss_mean: 0.867269, loss_mean_cls: 0.048123, grad_norm: 0.511544
[[34m2025-10-04 12:08:32[0m] Step: 2074, Training Logs: loss_final: 0.932167, loss_mean: 0.884383, loss_mean_cls: 0.047784, grad_norm: 0.384129
[[34m2025-10-04 12:08:32[0m] Step: 2075, Training Logs: loss_final: 0.900425, loss_mean: 0.852830, loss_mean_cls: 0.047594, grad_norm: 0.767505
[[34m2025-10-04 12:08:32[0m] Step: 2076, Training Logs: loss_final: 0.931419, loss_mean: 0.883209, loss_mean_cls: 0.048210, grad_norm: 0.401269
[[34m2025-10-04 12:08:33[0m] Step: 2077, Training Logs: loss_final: 0.912081, loss_mean: 0.863961, loss_mean_cls: 0.048120, grad_norm: 0.544594
[[34m2025-10-04 12:08:33[0m] Step: 2078, Training Logs: loss_final: 0.933414, loss_mean: 0.884647, loss_mean_cls: 0.048767, grad_norm: 0.857995
[[34m2025-10-04 12:08:33[0m] Step: 2079, Training Logs: loss_final: 0.937036, loss_mean: 0.887358, loss_mean_cls: 0.049678, grad_norm: 0.552474
[[34m2025-10-04 12:08:34[0m] Step: 2080, Training Logs: loss_final: 0.950991, loss_mean: 0.903562, loss_mean_cls: 0.047428, grad_norm: 0.751696
[[34m2025-10-04 12:08:34[0m] Step: 2081, Training Logs: loss_final: 0.896865, loss_mean: 0.849419, loss_mean_cls: 0.047446, grad_norm: 0.536668
[[34m2025-10-04 12:08:34[0m] Step: 2082, Training Logs: loss_final: 0.912775, loss_mean: 0.864468, loss_mean_cls: 0.048306, grad_norm: 0.724135
[[34m2025-10-04 12:08:34[0m] Step: 2083, Training Logs: loss_final: 0.907558, loss_mean: 0.858020, loss_mean_cls: 0.049538, grad_norm: 0.692373
[[34m2025-10-04 12:08:35[0m] Step: 2084, Training Logs: loss_final: 0.918449, loss_mean: 0.870737, loss_mean_cls: 0.047712, grad_norm: 0.513033
[[34m2025-10-04 12:08:35[0m] Step: 2085, Training Logs: loss_final: 0.913257, loss_mean: 0.865106, loss_mean_cls: 0.048151, grad_norm: 0.635129
[[34m2025-10-04 12:08:35[0m] Step: 2086, Training Logs: loss_final: 0.923844, loss_mean: 0.876419, loss_mean_cls: 0.047425, grad_norm: 0.685581
[[34m2025-10-04 12:08:36[0m] Step: 2087, Training Logs: loss_final: 0.903238, loss_mean: 0.855174, loss_mean_cls: 0.048064, grad_norm: 0.551879
[[34m2025-10-04 12:08:36[0m] Step: 2088, Training Logs: loss_final: 0.895202, loss_mean: 0.847222, loss_mean_cls: 0.047979, grad_norm: 0.668978
[[34m2025-10-04 12:08:36[0m] Step: 2089, Training Logs: loss_final: 0.911790, loss_mean: 0.862361, loss_mean_cls: 0.049428, grad_norm: 0.596208
[[34m2025-10-04 12:08:36[0m] Step: 2090, Training Logs: loss_final: 0.930052, loss_mean: 0.883099, loss_mean_cls: 0.046952, grad_norm: 0.663303
[[34m2025-10-04 12:08:37[0m] Step: 2091, Training Logs: loss_final: 0.928018, loss_mean: 0.879939, loss_mean_cls: 0.048078, grad_norm: 0.442855
[[34m2025-10-04 12:08:37[0m] Step: 2092, Training Logs: loss_final: 0.930506, loss_mean: 0.883725, loss_mean_cls: 0.046781, grad_norm: 0.581612
[[34m2025-10-04 12:08:37[0m] Step: 2093, Training Logs: loss_final: 0.905775, loss_mean: 0.858244, loss_mean_cls: 0.047531, grad_norm: 0.436988
[[34m2025-10-04 12:08:38[0m] Step: 2094, Training Logs: loss_final: 0.942774, loss_mean: 0.895897, loss_mean_cls: 0.046877, grad_norm: 0.572015
[[34m2025-10-04 12:08:38[0m] Step: 2095, Training Logs: loss_final: 0.930794, loss_mean: 0.882998, loss_mean_cls: 0.047797, grad_norm: 0.419804
[[34m2025-10-04 12:08:38[0m] Step: 2096, Training Logs: loss_final: 0.925853, loss_mean: 0.877242, loss_mean_cls: 0.048611, grad_norm: 0.537615
[[34m2025-10-04 12:08:39[0m] Step: 2097, Training Logs: loss_final: 0.915301, loss_mean: 0.866294, loss_mean_cls: 0.049007, grad_norm: 0.491570
[[34m2025-10-04 12:08:39[0m] Step: 2098, Training Logs: loss_final: 0.918551, loss_mean: 0.870638, loss_mean_cls: 0.047913, grad_norm: 0.500431
[[34m2025-10-04 12:08:39[0m] Step: 2099, Training Logs: loss_final: 0.930933, loss_mean: 0.884056, loss_mean_cls: 0.046877, grad_norm: 0.555137
[[34m2025-10-04 12:08:39[0m] Step: 2100, Training Logs: loss_final: 0.954481, loss_mean: 0.907758, loss_mean_cls: 0.046723, grad_norm: 0.533986
[[34m2025-10-04 12:08:40[0m] Step: 2101, Training Logs: loss_final: 0.932467, loss_mean: 0.886064, loss_mean_cls: 0.046404, grad_norm: 0.483978
[[34m2025-10-04 12:08:40[0m] Step: 2102, Training Logs: loss_final: 0.939945, loss_mean: 0.892390, loss_mean_cls: 0.047555, grad_norm: 0.449443
[[34m2025-10-04 12:08:40[0m] Step: 2103, Training Logs: loss_final: 0.911993, loss_mean: 0.863672, loss_mean_cls: 0.048321, grad_norm: 0.677364
[[34m2025-10-04 12:08:41[0m] Step: 2104, Training Logs: loss_final: 0.923498, loss_mean: 0.875373, loss_mean_cls: 0.048124, grad_norm: 0.488692
[[34m2025-10-04 12:08:41[0m] Step: 2105, Training Logs: loss_final: 0.906577, loss_mean: 0.859216, loss_mean_cls: 0.047361, grad_norm: 0.536853
[[34m2025-10-04 12:08:41[0m] Step: 2106, Training Logs: loss_final: 0.929043, loss_mean: 0.881763, loss_mean_cls: 0.047280, grad_norm: 0.447767
[[34m2025-10-04 12:08:42[0m] Step: 2107, Training Logs: loss_final: 0.917623, loss_mean: 0.869861, loss_mean_cls: 0.047762, grad_norm: 0.398911
[[34m2025-10-04 12:08:42[0m] Step: 2108, Training Logs: loss_final: 0.913174, loss_mean: 0.864396, loss_mean_cls: 0.048778, grad_norm: 0.492846
[[34m2025-10-04 12:08:42[0m] Step: 2109, Training Logs: loss_final: 0.939175, loss_mean: 0.892470, loss_mean_cls: 0.046705, grad_norm: 0.386666
[[34m2025-10-04 12:08:42[0m] Step: 2110, Training Logs: loss_final: 0.901037, loss_mean: 0.853582, loss_mean_cls: 0.047455, grad_norm: 0.488605
[[34m2025-10-04 12:08:43[0m] Step: 2111, Training Logs: loss_final: 0.927116, loss_mean: 0.879379, loss_mean_cls: 0.047737, grad_norm: 0.421362
[[34m2025-10-04 12:08:43[0m] Step: 2112, Training Logs: loss_final: 0.916646, loss_mean: 0.869109, loss_mean_cls: 0.047536, grad_norm: 0.535425
[[34m2025-10-04 12:08:43[0m] Step: 2113, Training Logs: loss_final: 0.911881, loss_mean: 0.865685, loss_mean_cls: 0.046196, grad_norm: 0.493700
[[34m2025-10-04 12:08:44[0m] Step: 2114, Training Logs: loss_final: 0.925763, loss_mean: 0.877959, loss_mean_cls: 0.047804, grad_norm: 0.417331
[[34m2025-10-04 12:08:44[0m] Step: 2115, Training Logs: loss_final: 0.899737, loss_mean: 0.851345, loss_mean_cls: 0.048392, grad_norm: 0.490290
[[34m2025-10-04 12:08:44[0m] Step: 2116, Training Logs: loss_final: 0.940450, loss_mean: 0.892813, loss_mean_cls: 0.047637, grad_norm: 0.383223
[[34m2025-10-04 12:08:44[0m] Step: 2117, Training Logs: loss_final: 0.916176, loss_mean: 0.868623, loss_mean_cls: 0.047554, grad_norm: 0.581514
[[34m2025-10-04 12:08:45[0m] Step: 2118, Training Logs: loss_final: 0.904177, loss_mean: 0.855154, loss_mean_cls: 0.049022, grad_norm: 0.549311
[[34m2025-10-04 12:08:45[0m] Step: 2119, Training Logs: loss_final: 0.918480, loss_mean: 0.869596, loss_mean_cls: 0.048884, grad_norm: 0.637524
[[34m2025-10-04 12:08:45[0m] Step: 2120, Training Logs: loss_final: 0.900389, loss_mean: 0.851960, loss_mean_cls: 0.048429, grad_norm: 0.684350
[[34m2025-10-04 12:08:46[0m] Step: 2121, Training Logs: loss_final: 0.922288, loss_mean: 0.872843, loss_mean_cls: 0.049445, grad_norm: 0.683892
[[34m2025-10-04 12:08:46[0m] Step: 2122, Training Logs: loss_final: 0.931155, loss_mean: 0.884236, loss_mean_cls: 0.046919, grad_norm: 0.970688
[[34m2025-10-04 12:08:46[0m] Step: 2123, Training Logs: loss_final: 0.914727, loss_mean: 0.866759, loss_mean_cls: 0.047968, grad_norm: 0.843016
[[34m2025-10-04 12:08:47[0m] Step: 2124, Training Logs: loss_final: 0.912230, loss_mean: 0.864752, loss_mean_cls: 0.047477, grad_norm: 0.961917
[[34m2025-10-04 12:08:47[0m] Step: 2125, Training Logs: loss_final: 0.918887, loss_mean: 0.871366, loss_mean_cls: 0.047521, grad_norm: 0.943326
[[34m2025-10-04 12:08:47[0m] Step: 2126, Training Logs: loss_final: 0.919431, loss_mean: 0.871389, loss_mean_cls: 0.048042, grad_norm: 0.500176
[[34m2025-10-04 12:08:47[0m] Step: 2127, Training Logs: loss_final: 0.901764, loss_mean: 0.852233, loss_mean_cls: 0.049530, grad_norm: 0.815371
[[34m2025-10-04 12:08:48[0m] Step: 2128, Training Logs: loss_final: 0.930526, loss_mean: 0.882703, loss_mean_cls: 0.047822, grad_norm: 0.568600
[[34m2025-10-04 12:08:48[0m] Step: 2129, Training Logs: loss_final: 0.905353, loss_mean: 0.856878, loss_mean_cls: 0.048474, grad_norm: 0.644163
[[34m2025-10-04 12:08:48[0m] Step: 2130, Training Logs: loss_final: 0.932156, loss_mean: 0.883693, loss_mean_cls: 0.048462, grad_norm: 0.627530
[[34m2025-10-04 12:08:49[0m] Step: 2131, Training Logs: loss_final: 0.924832, loss_mean: 0.876621, loss_mean_cls: 0.048211, grad_norm: 0.437564
[[34m2025-10-04 12:08:49[0m] Step: 2132, Training Logs: loss_final: 0.931802, loss_mean: 0.884890, loss_mean_cls: 0.046912, grad_norm: 0.672678
[[34m2025-10-04 12:08:49[0m] Step: 2133, Training Logs: loss_final: 0.902178, loss_mean: 0.853731, loss_mean_cls: 0.048447, grad_norm: 0.561882
[[34m2025-10-04 12:08:49[0m] Step: 2134, Training Logs: loss_final: 0.910486, loss_mean: 0.862793, loss_mean_cls: 0.047692, grad_norm: 0.537130
[[34m2025-10-04 12:08:50[0m] Step: 2135, Training Logs: loss_final: 0.911660, loss_mean: 0.862778, loss_mean_cls: 0.048882, grad_norm: 0.465007
[[34m2025-10-04 12:08:50[0m] Step: 2136, Training Logs: loss_final: 0.930676, loss_mean: 0.882390, loss_mean_cls: 0.048286, grad_norm: 0.491119
[[34m2025-10-04 12:08:50[0m] Step: 2137, Training Logs: loss_final: 0.920709, loss_mean: 0.873721, loss_mean_cls: 0.046988, grad_norm: 0.556416
[[34m2025-10-04 12:08:51[0m] Step: 2138, Training Logs: loss_final: 0.912275, loss_mean: 0.863982, loss_mean_cls: 0.048293, grad_norm: 0.506516
[[34m2025-10-04 12:08:51[0m] Step: 2139, Training Logs: loss_final: 0.909018, loss_mean: 0.861438, loss_mean_cls: 0.047580, grad_norm: 0.506465
[[34m2025-10-04 12:08:51[0m] Step: 2140, Training Logs: loss_final: 0.938114, loss_mean: 0.891709, loss_mean_cls: 0.046405, grad_norm: 0.538876
[[34m2025-10-04 12:08:52[0m] Step: 2141, Training Logs: loss_final: 0.934506, loss_mean: 0.887457, loss_mean_cls: 0.047049, grad_norm: 0.670591
[[34m2025-10-04 12:08:52[0m] Step: 2142, Training Logs: loss_final: 0.916760, loss_mean: 0.868722, loss_mean_cls: 0.048038, grad_norm: 0.383654
[[34m2025-10-04 12:08:52[0m] Step: 2143, Training Logs: loss_final: 0.928696, loss_mean: 0.881454, loss_mean_cls: 0.047241, grad_norm: 0.588914
[[34m2025-10-04 12:08:52[0m] Step: 2144, Training Logs: loss_final: 0.930866, loss_mean: 0.883090, loss_mean_cls: 0.047777, grad_norm: 0.666529
[[34m2025-10-04 12:08:53[0m] Step: 2145, Training Logs: loss_final: 0.939818, loss_mean: 0.890931, loss_mean_cls: 0.048887, grad_norm: 0.672056
[[34m2025-10-04 12:08:53[0m] Step: 2146, Training Logs: loss_final: 0.924816, loss_mean: 0.876134, loss_mean_cls: 0.048682, grad_norm: 0.429948
[[34m2025-10-04 12:08:53[0m] Step: 2147, Training Logs: loss_final: 0.915096, loss_mean: 0.866373, loss_mean_cls: 0.048722, grad_norm: 0.868972
[[34m2025-10-04 12:08:54[0m] Step: 2148, Training Logs: loss_final: 0.926098, loss_mean: 0.877613, loss_mean_cls: 0.048485, grad_norm: 0.646067
[[34m2025-10-04 12:08:54[0m] Step: 2149, Training Logs: loss_final: 0.920124, loss_mean: 0.871666, loss_mean_cls: 0.048458, grad_norm: 0.652042
[[34m2025-10-04 12:08:54[0m] Step: 2150, Training Logs: loss_final: 0.941323, loss_mean: 0.893412, loss_mean_cls: 0.047911, grad_norm: 0.717685
[[34m2025-10-04 12:08:54[0m] Step: 2151, Training Logs: loss_final: 0.920994, loss_mean: 0.873136, loss_mean_cls: 0.047858, grad_norm: 0.619629
[[34m2025-10-04 12:08:55[0m] Step: 2152, Training Logs: loss_final: 0.910285, loss_mean: 0.864020, loss_mean_cls: 0.046265, grad_norm: 0.756481
[[34m2025-10-04 12:08:55[0m] Step: 2153, Training Logs: loss_final: 0.928747, loss_mean: 0.880612, loss_mean_cls: 0.048135, grad_norm: 0.877514
[[34m2025-10-04 12:08:55[0m] Step: 2154, Training Logs: loss_final: 0.906321, loss_mean: 0.857195, loss_mean_cls: 0.049126, grad_norm: 0.523700
[[34m2025-10-04 12:08:56[0m] Step: 2155, Training Logs: loss_final: 0.912400, loss_mean: 0.863786, loss_mean_cls: 0.048613, grad_norm: 0.966038
[[34m2025-10-04 12:08:56[0m] Step: 2156, Training Logs: loss_final: 0.923760, loss_mean: 0.876891, loss_mean_cls: 0.046869, grad_norm: 0.824908
[[34m2025-10-04 12:08:56[0m] Step: 2157, Training Logs: loss_final: 0.906528, loss_mean: 0.857841, loss_mean_cls: 0.048687, grad_norm: 0.430622
[[34m2025-10-04 12:08:57[0m] Step: 2158, Training Logs: loss_final: 0.928470, loss_mean: 0.882020, loss_mean_cls: 0.046451, grad_norm: 0.892120
[[34m2025-10-04 12:08:57[0m] Step: 2159, Training Logs: loss_final: 0.909870, loss_mean: 0.863254, loss_mean_cls: 0.046616, grad_norm: 0.755876
[[34m2025-10-04 12:08:57[0m] Step: 2160, Training Logs: loss_final: 0.893326, loss_mean: 0.845475, loss_mean_cls: 0.047851, grad_norm: 0.652506
[[34m2025-10-04 12:08:57[0m] Step: 2161, Training Logs: loss_final: 0.931968, loss_mean: 0.884028, loss_mean_cls: 0.047940, grad_norm: 0.827633
[[34m2025-10-04 12:08:58[0m] Step: 2162, Training Logs: loss_final: 0.916891, loss_mean: 0.869697, loss_mean_cls: 0.047194, grad_norm: 0.555067
[[34m2025-10-04 12:08:58[0m] Step: 2163, Training Logs: loss_final: 0.916549, loss_mean: 0.868331, loss_mean_cls: 0.048218, grad_norm: 0.546934
[[34m2025-10-04 12:08:58[0m] Step: 2164, Training Logs: loss_final: 0.930848, loss_mean: 0.883023, loss_mean_cls: 0.047824, grad_norm: 0.862242
[[34m2025-10-04 12:08:59[0m] Step: 2165, Training Logs: loss_final: 0.924347, loss_mean: 0.877421, loss_mean_cls: 0.046925, grad_norm: 0.464690
[[34m2025-10-04 12:08:59[0m] Step: 2166, Training Logs: loss_final: 0.921189, loss_mean: 0.873220, loss_mean_cls: 0.047970, grad_norm: 0.572587
[[34m2025-10-04 12:08:59[0m] Step: 2167, Training Logs: loss_final: 0.942773, loss_mean: 0.894480, loss_mean_cls: 0.048293, grad_norm: 0.500619
[[34m2025-10-04 12:09:00[0m] Step: 2168, Training Logs: loss_final: 0.933279, loss_mean: 0.885427, loss_mean_cls: 0.047852, grad_norm: 0.509092
[[34m2025-10-04 12:09:00[0m] Step: 2169, Training Logs: loss_final: 0.928216, loss_mean: 0.880919, loss_mean_cls: 0.047297, grad_norm: 0.632742
[[34m2025-10-04 12:09:00[0m] Step: 2170, Training Logs: loss_final: 0.927114, loss_mean: 0.879611, loss_mean_cls: 0.047503, grad_norm: 0.460994
[[34m2025-10-04 12:09:00[0m] Step: 2171, Training Logs: loss_final: 0.928570, loss_mean: 0.880666, loss_mean_cls: 0.047903, grad_norm: 0.442255
[[34m2025-10-04 12:09:01[0m] Step: 2172, Training Logs: loss_final: 0.919839, loss_mean: 0.872372, loss_mean_cls: 0.047467, grad_norm: 0.543263
[[34m2025-10-04 12:09:01[0m] Step: 2173, Training Logs: loss_final: 0.924886, loss_mean: 0.877013, loss_mean_cls: 0.047873, grad_norm: 0.522831
[[34m2025-10-04 12:09:01[0m] Step: 2174, Training Logs: loss_final: 0.923524, loss_mean: 0.876265, loss_mean_cls: 0.047259, grad_norm: 0.425909
[[34m2025-10-04 12:09:02[0m] Step: 2175, Training Logs: loss_final: 0.913169, loss_mean: 0.864958, loss_mean_cls: 0.048211, grad_norm: 0.754055
[[34m2025-10-04 12:09:02[0m] Step: 2176, Training Logs: loss_final: 0.913086, loss_mean: 0.865491, loss_mean_cls: 0.047595, grad_norm: 0.446526
[[34m2025-10-04 12:09:02[0m] Step: 2177, Training Logs: loss_final: 0.916032, loss_mean: 0.868468, loss_mean_cls: 0.047565, grad_norm: 0.405727
[[34m2025-10-04 12:09:02[0m] Step: 2178, Training Logs: loss_final: 0.904989, loss_mean: 0.855692, loss_mean_cls: 0.049296, grad_norm: 0.469298
[[34m2025-10-04 12:09:03[0m] Step: 2179, Training Logs: loss_final: 0.907007, loss_mean: 0.858903, loss_mean_cls: 0.048104, grad_norm: 0.549835
[[34m2025-10-04 12:09:03[0m] Step: 2180, Training Logs: loss_final: 0.915221, loss_mean: 0.867381, loss_mean_cls: 0.047840, grad_norm: 0.382269
[[34m2025-10-04 12:09:03[0m] Step: 2181, Training Logs: loss_final: 0.915256, loss_mean: 0.866876, loss_mean_cls: 0.048380, grad_norm: 0.539655
[[34m2025-10-04 12:09:04[0m] Step: 2182, Training Logs: loss_final: 0.897431, loss_mean: 0.849754, loss_mean_cls: 0.047677, grad_norm: 0.525881
[[34m2025-10-04 12:09:04[0m] Step: 2183, Training Logs: loss_final: 0.928164, loss_mean: 0.880929, loss_mean_cls: 0.047235, grad_norm: 0.586781
[[34m2025-10-04 12:09:04[0m] Step: 2184, Training Logs: loss_final: 0.934983, loss_mean: 0.886614, loss_mean_cls: 0.048368, grad_norm: 0.553240
[[34m2025-10-04 12:09:04[0m] Step: 2185, Training Logs: loss_final: 0.915892, loss_mean: 0.868861, loss_mean_cls: 0.047031, grad_norm: 0.516523
[[34m2025-10-04 12:09:05[0m] Step: 2186, Training Logs: loss_final: 0.910051, loss_mean: 0.862396, loss_mean_cls: 0.047655, grad_norm: 0.442548
[[34m2025-10-04 12:09:05[0m] Step: 2187, Training Logs: loss_final: 0.924112, loss_mean: 0.876951, loss_mean_cls: 0.047161, grad_norm: 0.533584
[[34m2025-10-04 12:09:05[0m] Step: 2188, Training Logs: loss_final: 0.909072, loss_mean: 0.861481, loss_mean_cls: 0.047592, grad_norm: 0.405082
[[34m2025-10-04 12:09:06[0m] Step: 2189, Training Logs: loss_final: 0.924521, loss_mean: 0.876809, loss_mean_cls: 0.047712, grad_norm: 0.599237
[[34m2025-10-04 12:09:06[0m] Step: 2190, Training Logs: loss_final: 0.883790, loss_mean: 0.835545, loss_mean_cls: 0.048245, grad_norm: 0.705024
[[34m2025-10-04 12:09:06[0m] Step: 2191, Training Logs: loss_final: 0.905986, loss_mean: 0.857733, loss_mean_cls: 0.048254, grad_norm: 0.589003
[[34m2025-10-04 12:09:06[0m] Step: 2192, Training Logs: loss_final: 0.921940, loss_mean: 0.875204, loss_mean_cls: 0.046736, grad_norm: 0.659623
[[34m2025-10-04 12:09:07[0m] Step: 2193, Training Logs: loss_final: 0.936279, loss_mean: 0.888166, loss_mean_cls: 0.048113, grad_norm: 0.526481
[[34m2025-10-04 12:09:07[0m] Step: 2194, Training Logs: loss_final: 0.903064, loss_mean: 0.853976, loss_mean_cls: 0.049088, grad_norm: 0.632133
[[34m2025-10-04 12:09:07[0m] Step: 2195, Training Logs: loss_final: 0.941663, loss_mean: 0.894622, loss_mean_cls: 0.047041, grad_norm: 0.806925
[[34m2025-10-04 12:09:08[0m] Step: 2196, Training Logs: loss_final: 0.916704, loss_mean: 0.870200, loss_mean_cls: 0.046503, grad_norm: 0.622154
[[34m2025-10-04 12:09:08[0m] Step: 2197, Training Logs: loss_final: 0.920691, loss_mean: 0.872275, loss_mean_cls: 0.048416, grad_norm: 0.618454
[[34m2025-10-04 12:09:08[0m] Step: 2198, Training Logs: loss_final: 0.898103, loss_mean: 0.850136, loss_mean_cls: 0.047967, grad_norm: 0.828840
[[34m2025-10-04 12:09:09[0m] Step: 2199, Training Logs: loss_final: 0.931504, loss_mean: 0.883465, loss_mean_cls: 0.048039, grad_norm: 0.562124
[[34m2025-10-04 12:09:09[0m] Step: 2200, Training Logs: loss_final: 0.919048, loss_mean: 0.872266, loss_mean_cls: 0.046782, grad_norm: 0.730284
[[34m2025-10-04 12:09:09[0m] Step: 2201, Training Logs: loss_final: 0.915937, loss_mean: 0.867913, loss_mean_cls: 0.048024, grad_norm: 0.418753
[[34m2025-10-04 12:09:09[0m] Step: 2202, Training Logs: loss_final: 0.910446, loss_mean: 0.863464, loss_mean_cls: 0.046981, grad_norm: 0.415155
[[34m2025-10-04 12:09:10[0m] Step: 2203, Training Logs: loss_final: 0.911353, loss_mean: 0.864672, loss_mean_cls: 0.046681, grad_norm: 0.538095
[[34m2025-10-04 12:09:10[0m] Step: 2204, Training Logs: loss_final: 0.917786, loss_mean: 0.870295, loss_mean_cls: 0.047491, grad_norm: 0.603923
[[34m2025-10-04 12:09:10[0m] Step: 2205, Training Logs: loss_final: 0.933692, loss_mean: 0.886244, loss_mean_cls: 0.047448, grad_norm: 0.655434
[[34m2025-10-04 12:09:11[0m] Step: 2206, Training Logs: loss_final: 0.904772, loss_mean: 0.856194, loss_mean_cls: 0.048578, grad_norm: 0.595498
[[34m2025-10-04 12:09:11[0m] Step: 2207, Training Logs: loss_final: 0.912366, loss_mean: 0.864651, loss_mean_cls: 0.047715, grad_norm: 0.599858
[[34m2025-10-04 12:09:11[0m] Step: 2208, Training Logs: loss_final: 0.937644, loss_mean: 0.891025, loss_mean_cls: 0.046618, grad_norm: 0.468941
[[34m2025-10-04 12:09:12[0m] Step: 2209, Training Logs: loss_final: 0.934750, loss_mean: 0.887861, loss_mean_cls: 0.046889, grad_norm: 0.595300
[[34m2025-10-04 12:09:12[0m] Step: 2210, Training Logs: loss_final: 0.932719, loss_mean: 0.886417, loss_mean_cls: 0.046302, grad_norm: 0.513216
[[34m2025-10-04 12:09:12[0m] Step: 2211, Training Logs: loss_final: 0.925434, loss_mean: 0.878022, loss_mean_cls: 0.047412, grad_norm: 0.556120
[[34m2025-10-04 12:09:12[0m] Step: 2212, Training Logs: loss_final: 0.924445, loss_mean: 0.875088, loss_mean_cls: 0.049357, grad_norm: 0.583788
[[34m2025-10-04 12:09:13[0m] Step: 2213, Training Logs: loss_final: 0.919408, loss_mean: 0.872233, loss_mean_cls: 0.047175, grad_norm: 0.662573
[[34m2025-10-04 12:09:13[0m] Step: 2214, Training Logs: loss_final: 0.900987, loss_mean: 0.852988, loss_mean_cls: 0.047998, grad_norm: 0.635788
[[34m2025-10-04 12:09:13[0m] Step: 2215, Training Logs: loss_final: 0.918055, loss_mean: 0.870255, loss_mean_cls: 0.047800, grad_norm: 0.542909
[[34m2025-10-04 12:09:14[0m] Step: 2216, Training Logs: loss_final: 0.919823, loss_mean: 0.871961, loss_mean_cls: 0.047862, grad_norm: 0.790761
[[34m2025-10-04 12:09:14[0m] Step: 2217, Training Logs: loss_final: 0.921610, loss_mean: 0.874255, loss_mean_cls: 0.047355, grad_norm: 0.409387
[[34m2025-10-04 12:09:14[0m] Step: 2218, Training Logs: loss_final: 0.931437, loss_mean: 0.884143, loss_mean_cls: 0.047294, grad_norm: 0.681693
[[34m2025-10-04 12:09:14[0m] Step: 2219, Training Logs: loss_final: 0.921932, loss_mean: 0.873836, loss_mean_cls: 0.048096, grad_norm: 0.868471
[[34m2025-10-04 12:09:15[0m] Step: 2220, Training Logs: loss_final: 0.928791, loss_mean: 0.882521, loss_mean_cls: 0.046269, grad_norm: 0.436000
[[34m2025-10-04 12:09:15[0m] Step: 2221, Training Logs: loss_final: 0.913267, loss_mean: 0.864747, loss_mean_cls: 0.048520, grad_norm: 0.874886
[[34m2025-10-04 12:09:15[0m] Step: 2222, Training Logs: loss_final: 0.899312, loss_mean: 0.850904, loss_mean_cls: 0.048408, grad_norm: 0.538950
[[34m2025-10-04 12:09:16[0m] Step: 2223, Training Logs: loss_final: 0.905974, loss_mean: 0.858307, loss_mean_cls: 0.047667, grad_norm: 0.566559
[[34m2025-10-04 12:09:16[0m] Step: 2224, Training Logs: loss_final: 0.910490, loss_mean: 0.862611, loss_mean_cls: 0.047879, grad_norm: 0.671020
[[34m2025-10-04 12:09:16[0m] Step: 2225, Training Logs: loss_final: 0.924255, loss_mean: 0.876685, loss_mean_cls: 0.047570, grad_norm: 0.398527
[[34m2025-10-04 12:09:16[0m] Step: 2226, Training Logs: loss_final: 0.923241, loss_mean: 0.875955, loss_mean_cls: 0.047285, grad_norm: 0.616817
[[34m2025-10-04 12:09:17[0m] Step: 2227, Training Logs: loss_final: 0.906671, loss_mean: 0.859044, loss_mean_cls: 0.047627, grad_norm: 0.496214
[[34m2025-10-04 12:09:17[0m] Step: 2228, Training Logs: loss_final: 0.936231, loss_mean: 0.889469, loss_mean_cls: 0.046762, grad_norm: 0.515601
[[34m2025-10-04 12:09:17[0m] Step: 2229, Training Logs: loss_final: 0.918391, loss_mean: 0.871162, loss_mean_cls: 0.047229, grad_norm: 0.501240
[[34m2025-10-04 12:09:18[0m] Step: 2230, Training Logs: loss_final: 0.935648, loss_mean: 0.887960, loss_mean_cls: 0.047688, grad_norm: 0.546849
[[34m2025-10-04 12:09:18[0m] Step: 2231, Training Logs: loss_final: 0.891156, loss_mean: 0.841681, loss_mean_cls: 0.049475, grad_norm: 0.582876
[[34m2025-10-04 12:09:18[0m] Step: 2232, Training Logs: loss_final: 0.911344, loss_mean: 0.863838, loss_mean_cls: 0.047506, grad_norm: 0.507013
[[34m2025-10-04 12:09:19[0m] Step: 2233, Training Logs: loss_final: 0.896979, loss_mean: 0.849042, loss_mean_cls: 0.047937, grad_norm: 0.866156
[[34m2025-10-04 12:09:19[0m] Step: 2234, Training Logs: loss_final: 0.930440, loss_mean: 0.883302, loss_mean_cls: 0.047137, grad_norm: 0.606384
[[34m2025-10-04 12:09:19[0m] Step: 2235, Training Logs: loss_final: 0.905272, loss_mean: 0.856700, loss_mean_cls: 0.048572, grad_norm: 0.662140
[[34m2025-10-04 12:09:19[0m] Step: 2236, Training Logs: loss_final: 0.914406, loss_mean: 0.865969, loss_mean_cls: 0.048437, grad_norm: 0.560392
[[34m2025-10-04 12:09:20[0m] Step: 2237, Training Logs: loss_final: 0.919339, loss_mean: 0.871127, loss_mean_cls: 0.048212, grad_norm: 0.461251
[[34m2025-10-04 12:09:20[0m] Step: 2238, Training Logs: loss_final: 0.941230, loss_mean: 0.893738, loss_mean_cls: 0.047492, grad_norm: 0.624450
[[34m2025-10-04 12:09:20[0m] Step: 2239, Training Logs: loss_final: 0.937156, loss_mean: 0.888410, loss_mean_cls: 0.048746, grad_norm: 0.894791
[[34m2025-10-04 12:09:21[0m] Step: 2240, Training Logs: loss_final: 0.921732, loss_mean: 0.875020, loss_mean_cls: 0.046712, grad_norm: 0.431692
[[34m2025-10-04 12:09:21[0m] Step: 2241, Training Logs: loss_final: 0.921130, loss_mean: 0.873363, loss_mean_cls: 0.047767, grad_norm: 1.041181
[[34m2025-10-04 12:09:21[0m] Step: 2242, Training Logs: loss_final: 0.935221, loss_mean: 0.888695, loss_mean_cls: 0.046527, grad_norm: 0.587009
[[34m2025-10-04 12:09:22[0m] Step: 2243, Training Logs: loss_final: 0.937660, loss_mean: 0.889358, loss_mean_cls: 0.048302, grad_norm: 0.652052
[[34m2025-10-04 12:09:22[0m] Step: 2244, Training Logs: loss_final: 0.945012, loss_mean: 0.897784, loss_mean_cls: 0.047227, grad_norm: 1.038935
[[34m2025-10-04 12:09:22[0m] Step: 2245, Training Logs: loss_final: 0.930207, loss_mean: 0.882752, loss_mean_cls: 0.047456, grad_norm: 0.538058
[[34m2025-10-04 12:09:22[0m] Step: 2246, Training Logs: loss_final: 0.924796, loss_mean: 0.877388, loss_mean_cls: 0.047408, grad_norm: 0.718094
[[34m2025-10-04 12:09:23[0m] Step: 2247, Training Logs: loss_final: 0.909358, loss_mean: 0.860663, loss_mean_cls: 0.048695, grad_norm: 0.724343
[[34m2025-10-04 12:09:23[0m] Step: 2248, Training Logs: loss_final: 0.927039, loss_mean: 0.879580, loss_mean_cls: 0.047459, grad_norm: 0.471074
[[34m2025-10-04 12:09:23[0m] Step: 2249, Training Logs: loss_final: 0.929962, loss_mean: 0.882749, loss_mean_cls: 0.047213, grad_norm: 0.669764
[[34m2025-10-04 12:09:24[0m] Step: 2250, Training Logs: loss_final: 0.924217, loss_mean: 0.876355, loss_mean_cls: 0.047862, grad_norm: 0.636337
[[34m2025-10-04 12:09:24[0m] Step: 2251, Training Logs: loss_final: 0.896409, loss_mean: 0.848622, loss_mean_cls: 0.047786, grad_norm: 0.545722
[[34m2025-10-04 12:09:24[0m] Step: 2252, Training Logs: loss_final: 0.905893, loss_mean: 0.857040, loss_mean_cls: 0.048853, grad_norm: 0.823999
[[34m2025-10-04 12:09:24[0m] Step: 2253, Training Logs: loss_final: 0.928060, loss_mean: 0.880876, loss_mean_cls: 0.047184, grad_norm: 0.472218
[[34m2025-10-04 12:09:25[0m] Step: 2254, Training Logs: loss_final: 0.924866, loss_mean: 0.877688, loss_mean_cls: 0.047178, grad_norm: 0.645725
[[34m2025-10-04 12:09:25[0m] Step: 2255, Training Logs: loss_final: 0.915888, loss_mean: 0.868538, loss_mean_cls: 0.047350, grad_norm: 0.636819
[[34m2025-10-04 12:09:25[0m] Step: 2256, Training Logs: loss_final: 0.919851, loss_mean: 0.873583, loss_mean_cls: 0.046268, grad_norm: 0.501091
[[34m2025-10-04 12:09:26[0m] Step: 2257, Training Logs: loss_final: 0.886288, loss_mean: 0.837967, loss_mean_cls: 0.048321, grad_norm: 0.500491
[[34m2025-10-04 12:09:26[0m] Step: 2258, Training Logs: loss_final: 0.895663, loss_mean: 0.847376, loss_mean_cls: 0.048287, grad_norm: 0.445883
[[34m2025-10-04 12:09:26[0m] Step: 2259, Training Logs: loss_final: 0.911347, loss_mean: 0.862454, loss_mean_cls: 0.048893, grad_norm: 0.567844
[[34m2025-10-04 12:09:27[0m] Step: 2260, Training Logs: loss_final: 0.907828, loss_mean: 0.860921, loss_mean_cls: 0.046907, grad_norm: 0.434194
[[34m2025-10-04 12:09:27[0m] Step: 2261, Training Logs: loss_final: 0.908268, loss_mean: 0.860970, loss_mean_cls: 0.047298, grad_norm: 0.694215
[[34m2025-10-04 12:09:27[0m] Step: 2262, Training Logs: loss_final: 0.914537, loss_mean: 0.868096, loss_mean_cls: 0.046441, grad_norm: 0.597077
[[34m2025-10-04 12:09:27[0m] Step: 2263, Training Logs: loss_final: 0.903978, loss_mean: 0.857014, loss_mean_cls: 0.046964, grad_norm: 0.649804
[[34m2025-10-04 12:09:28[0m] Step: 2264, Training Logs: loss_final: 0.910305, loss_mean: 0.862668, loss_mean_cls: 0.047636, grad_norm: 0.644110
[[34m2025-10-04 12:09:28[0m] Step: 2265, Training Logs: loss_final: 0.918130, loss_mean: 0.870473, loss_mean_cls: 0.047658, grad_norm: 0.610890
[[34m2025-10-04 12:09:28[0m] Step: 2266, Training Logs: loss_final: 0.896991, loss_mean: 0.849086, loss_mean_cls: 0.047905, grad_norm: 0.864600
[[34m2025-10-04 12:09:29[0m] Step: 2267, Training Logs: loss_final: 0.919496, loss_mean: 0.871635, loss_mean_cls: 0.047861, grad_norm: 0.703904
[[34m2025-10-04 12:09:29[0m] Step: 2268, Training Logs: loss_final: 0.905008, loss_mean: 0.856346, loss_mean_cls: 0.048662, grad_norm: 0.716444
[[34m2025-10-04 12:09:29[0m] Step: 2269, Training Logs: loss_final: 0.917557, loss_mean: 0.869480, loss_mean_cls: 0.048077, grad_norm: 0.561986
[[34m2025-10-04 12:09:30[0m] Step: 2270, Training Logs: loss_final: 0.932797, loss_mean: 0.884842, loss_mean_cls: 0.047955, grad_norm: 0.597644
[[34m2025-10-04 12:09:30[0m] Step: 2271, Training Logs: loss_final: 0.911520, loss_mean: 0.865253, loss_mean_cls: 0.046267, grad_norm: 0.709752
[[34m2025-10-04 12:09:30[0m] Step: 2272, Training Logs: loss_final: 0.920728, loss_mean: 0.873739, loss_mean_cls: 0.046989, grad_norm: 0.467613
[[34m2025-10-04 12:09:30[0m] Step: 2273, Training Logs: loss_final: 0.918464, loss_mean: 0.870328, loss_mean_cls: 0.048136, grad_norm: 0.722109
[[34m2025-10-04 12:09:31[0m] Step: 2274, Training Logs: loss_final: 0.922957, loss_mean: 0.875077, loss_mean_cls: 0.047880, grad_norm: 0.599932
[[34m2025-10-04 12:09:31[0m] Step: 2275, Training Logs: loss_final: 0.918271, loss_mean: 0.870444, loss_mean_cls: 0.047827, grad_norm: 0.744520
[[34m2025-10-04 12:09:31[0m] Step: 2276, Training Logs: loss_final: 0.917135, loss_mean: 0.869276, loss_mean_cls: 0.047859, grad_norm: 0.763749
[[34m2025-10-04 12:09:32[0m] Step: 2277, Training Logs: loss_final: 0.925555, loss_mean: 0.878449, loss_mean_cls: 0.047106, grad_norm: 0.752626
[[34m2025-10-04 12:09:32[0m] Step: 2278, Training Logs: loss_final: 0.917683, loss_mean: 0.870191, loss_mean_cls: 0.047492, grad_norm: 0.719523
[[34m2025-10-04 12:09:32[0m] Step: 2279, Training Logs: loss_final: 0.925663, loss_mean: 0.878496, loss_mean_cls: 0.047167, grad_norm: 0.494261
[[34m2025-10-04 12:09:32[0m] Step: 2280, Training Logs: loss_final: 0.934495, loss_mean: 0.886682, loss_mean_cls: 0.047813, grad_norm: 0.732694
[[34m2025-10-04 12:09:33[0m] Step: 2281, Training Logs: loss_final: 0.901312, loss_mean: 0.852353, loss_mean_cls: 0.048959, grad_norm: 0.599205
[[34m2025-10-04 12:09:33[0m] Step: 2282, Training Logs: loss_final: 0.919060, loss_mean: 0.871961, loss_mean_cls: 0.047098, grad_norm: 0.619445
[[34m2025-10-04 12:09:33[0m] Step: 2283, Training Logs: loss_final: 0.922441, loss_mean: 0.875034, loss_mean_cls: 0.047407, grad_norm: 0.736916
[[34m2025-10-04 12:09:34[0m] Step: 2284, Training Logs: loss_final: 0.931376, loss_mean: 0.883867, loss_mean_cls: 0.047510, grad_norm: 0.518524
[[34m2025-10-04 12:09:34[0m] Step: 2285, Training Logs: loss_final: 0.904760, loss_mean: 0.856789, loss_mean_cls: 0.047971, grad_norm: 0.692588
[[34m2025-10-04 12:09:34[0m] Step: 2286, Training Logs: loss_final: 0.924378, loss_mean: 0.876939, loss_mean_cls: 0.047438, grad_norm: 0.588157
[[34m2025-10-04 12:09:35[0m] Step: 2287, Training Logs: loss_final: 0.931571, loss_mean: 0.884702, loss_mean_cls: 0.046869, grad_norm: 0.629412
[[34m2025-10-04 12:09:35[0m] Step: 2288, Training Logs: loss_final: 0.939295, loss_mean: 0.891602, loss_mean_cls: 0.047693, grad_norm: 0.584042
[[34m2025-10-04 12:09:35[0m] Step: 2289, Training Logs: loss_final: 0.903743, loss_mean: 0.856578, loss_mean_cls: 0.047165, grad_norm: 0.463662
[[34m2025-10-04 12:09:35[0m] Step: 2290, Training Logs: loss_final: 0.911094, loss_mean: 0.863577, loss_mean_cls: 0.047517, grad_norm: 0.520793
[[34m2025-10-04 12:09:36[0m] Step: 2291, Training Logs: loss_final: 0.916007, loss_mean: 0.868287, loss_mean_cls: 0.047720, grad_norm: 0.473616
[[34m2025-10-04 12:09:36[0m] Step: 2292, Training Logs: loss_final: 0.901346, loss_mean: 0.854010, loss_mean_cls: 0.047337, grad_norm: 0.518582
[[34m2025-10-04 12:09:36[0m] Step: 2293, Training Logs: loss_final: 0.906571, loss_mean: 0.858261, loss_mean_cls: 0.048310, grad_norm: 0.425049
[[34m2025-10-04 12:09:37[0m] Step: 2294, Training Logs: loss_final: 0.920454, loss_mean: 0.872220, loss_mean_cls: 0.048235, grad_norm: 0.521640
[[34m2025-10-04 12:09:37[0m] Step: 2295, Training Logs: loss_final: 0.903274, loss_mean: 0.856919, loss_mean_cls: 0.046356, grad_norm: 0.591149
[[34m2025-10-04 12:09:37[0m] Step: 2296, Training Logs: loss_final: 0.917593, loss_mean: 0.869351, loss_mean_cls: 0.048242, grad_norm: 0.505022
[[34m2025-10-04 12:09:37[0m] Step: 2297, Training Logs: loss_final: 0.909469, loss_mean: 0.862153, loss_mean_cls: 0.047316, grad_norm: 0.461768
[[34m2025-10-04 12:09:38[0m] Step: 2298, Training Logs: loss_final: 0.909222, loss_mean: 0.861534, loss_mean_cls: 0.047688, grad_norm: 0.680112
[[34m2025-10-04 12:09:38[0m] Step: 2299, Training Logs: loss_final: 0.923536, loss_mean: 0.876530, loss_mean_cls: 0.047006, grad_norm: 0.495554
[[34m2025-10-04 12:09:38[0m] Step: 2300, Training Logs: loss_final: 0.920910, loss_mean: 0.872454, loss_mean_cls: 0.048456, grad_norm: 0.521787
[[34m2025-10-04 12:09:39[0m] Step: 2301, Training Logs: loss_final: 0.887105, loss_mean: 0.838600, loss_mean_cls: 0.048505, grad_norm: 0.637261
[[34m2025-10-04 12:09:39[0m] Step: 2302, Training Logs: loss_final: 0.912624, loss_mean: 0.864622, loss_mean_cls: 0.048002, grad_norm: 0.481207
[[34m2025-10-04 12:09:39[0m] Step: 2303, Training Logs: loss_final: 0.929971, loss_mean: 0.882020, loss_mean_cls: 0.047951, grad_norm: 0.528243
[[34m2025-10-04 12:09:39[0m] Step: 2304, Training Logs: loss_final: 0.918570, loss_mean: 0.871548, loss_mean_cls: 0.047022, grad_norm: 0.526741
[[34m2025-10-04 12:09:40[0m] Step: 2305, Training Logs: loss_final: 0.912186, loss_mean: 0.864975, loss_mean_cls: 0.047211, grad_norm: 0.610371
[[34m2025-10-04 12:09:40[0m] Step: 2306, Training Logs: loss_final: 0.927348, loss_mean: 0.880018, loss_mean_cls: 0.047330, grad_norm: 0.560075
[[34m2025-10-04 12:09:40[0m] Step: 2307, Training Logs: loss_final: 0.925236, loss_mean: 0.877611, loss_mean_cls: 0.047625, grad_norm: 0.650403
[[34m2025-10-04 12:09:41[0m] Step: 2308, Training Logs: loss_final: 0.935584, loss_mean: 0.889534, loss_mean_cls: 0.046050, grad_norm: 0.531330
[[34m2025-10-04 12:09:41[0m] Step: 2309, Training Logs: loss_final: 0.887045, loss_mean: 0.838349, loss_mean_cls: 0.048696, grad_norm: 0.589083
[[34m2025-10-04 12:09:41[0m] Step: 2310, Training Logs: loss_final: 0.918318, loss_mean: 0.870874, loss_mean_cls: 0.047444, grad_norm: 0.582437
[[34m2025-10-04 12:09:42[0m] Step: 2311, Training Logs: loss_final: 0.940409, loss_mean: 0.894487, loss_mean_cls: 0.045922, grad_norm: 0.566367
[[34m2025-10-04 12:09:42[0m] Step: 2312, Training Logs: loss_final: 0.914997, loss_mean: 0.867637, loss_mean_cls: 0.047360, grad_norm: 0.658710
[[34m2025-10-04 12:09:42[0m] Step: 2313, Training Logs: loss_final: 0.919181, loss_mean: 0.871571, loss_mean_cls: 0.047610, grad_norm: 0.527316
[[34m2025-10-04 12:09:42[0m] Step: 2314, Training Logs: loss_final: 0.911833, loss_mean: 0.863890, loss_mean_cls: 0.047944, grad_norm: 0.558827
[[34m2025-10-04 12:09:43[0m] Step: 2315, Training Logs: loss_final: 0.925256, loss_mean: 0.877990, loss_mean_cls: 0.047266, grad_norm: 0.514551
[[34m2025-10-04 12:09:43[0m] Step: 2316, Training Logs: loss_final: 0.913294, loss_mean: 0.866968, loss_mean_cls: 0.046326, grad_norm: 0.489363
[[34m2025-10-04 12:09:43[0m] Step: 2317, Training Logs: loss_final: 0.926690, loss_mean: 0.878173, loss_mean_cls: 0.048517, grad_norm: 0.563678
[[34m2025-10-04 12:09:44[0m] Step: 2318, Training Logs: loss_final: 0.903001, loss_mean: 0.854626, loss_mean_cls: 0.048375, grad_norm: 0.492618
[[34m2025-10-04 12:09:44[0m] Step: 2319, Training Logs: loss_final: 0.896004, loss_mean: 0.848849, loss_mean_cls: 0.047155, grad_norm: 0.459355
[[34m2025-10-04 12:09:44[0m] Step: 2320, Training Logs: loss_final: 0.919789, loss_mean: 0.872709, loss_mean_cls: 0.047080, grad_norm: 0.602158
[[34m2025-10-04 12:09:44[0m] Step: 2321, Training Logs: loss_final: 0.908228, loss_mean: 0.860615, loss_mean_cls: 0.047613, grad_norm: 0.321881
[[34m2025-10-04 12:09:45[0m] Step: 2322, Training Logs: loss_final: 0.923095, loss_mean: 0.874673, loss_mean_cls: 0.048421, grad_norm: 0.541344
[[34m2025-10-04 12:09:45[0m] Step: 2323, Training Logs: loss_final: 0.901579, loss_mean: 0.853820, loss_mean_cls: 0.047759, grad_norm: 0.386867
[[34m2025-10-04 12:09:45[0m] Step: 2324, Training Logs: loss_final: 0.904304, loss_mean: 0.856697, loss_mean_cls: 0.047607, grad_norm: 0.476434
[[34m2025-10-04 12:09:46[0m] Step: 2325, Training Logs: loss_final: 0.921397, loss_mean: 0.874357, loss_mean_cls: 0.047039, grad_norm: 0.572612
[[34m2025-10-04 12:09:46[0m] Step: 2326, Training Logs: loss_final: 0.903352, loss_mean: 0.855650, loss_mean_cls: 0.047702, grad_norm: 0.431115
[[34m2025-10-04 12:09:46[0m] Step: 2327, Training Logs: loss_final: 0.916929, loss_mean: 0.868273, loss_mean_cls: 0.048655, grad_norm: 0.671172
[[34m2025-10-04 12:09:47[0m] Step: 2328, Training Logs: loss_final: 0.927931, loss_mean: 0.880393, loss_mean_cls: 0.047538, grad_norm: 0.406380
[[34m2025-10-04 12:09:47[0m] Step: 2329, Training Logs: loss_final: 0.937875, loss_mean: 0.890021, loss_mean_cls: 0.047854, grad_norm: 0.681370
[[34m2025-10-04 12:09:47[0m] Step: 2330, Training Logs: loss_final: 0.911475, loss_mean: 0.864273, loss_mean_cls: 0.047202, grad_norm: 0.655781
[[34m2025-10-04 12:09:47[0m] Step: 2331, Training Logs: loss_final: 0.899804, loss_mean: 0.853528, loss_mean_cls: 0.046276, grad_norm: 0.760388
[[34m2025-10-04 12:09:48[0m] Step: 2332, Training Logs: loss_final: 0.930696, loss_mean: 0.883786, loss_mean_cls: 0.046909, grad_norm: 0.548853
[[34m2025-10-04 12:09:48[0m] Step: 2333, Training Logs: loss_final: 0.902588, loss_mean: 0.854835, loss_mean_cls: 0.047753, grad_norm: 0.693367
[[34m2025-10-04 12:09:48[0m] Step: 2334, Training Logs: loss_final: 0.931995, loss_mean: 0.885782, loss_mean_cls: 0.046213, grad_norm: 0.578780
[[34m2025-10-04 12:09:49[0m] Step: 2335, Training Logs: loss_final: 0.916437, loss_mean: 0.868025, loss_mean_cls: 0.048412, grad_norm: 0.664301
[[34m2025-10-04 12:09:49[0m] Step: 2336, Training Logs: loss_final: 0.882614, loss_mean: 0.834380, loss_mean_cls: 0.048234, grad_norm: 0.788787
[[34m2025-10-04 12:09:49[0m] Step: 2337, Training Logs: loss_final: 0.897489, loss_mean: 0.849774, loss_mean_cls: 0.047715, grad_norm: 0.551772
[[34m2025-10-04 12:09:49[0m] Step: 2338, Training Logs: loss_final: 0.911750, loss_mean: 0.864054, loss_mean_cls: 0.047696, grad_norm: 0.662066
[[34m2025-10-04 12:09:50[0m] Step: 2339, Training Logs: loss_final: 0.907866, loss_mean: 0.860060, loss_mean_cls: 0.047806, grad_norm: 0.432297
[[34m2025-10-04 12:09:50[0m] Step: 2340, Training Logs: loss_final: 0.897291, loss_mean: 0.849042, loss_mean_cls: 0.048249, grad_norm: 0.807821
[[34m2025-10-04 12:09:50[0m] Step: 2341, Training Logs: loss_final: 0.920571, loss_mean: 0.873221, loss_mean_cls: 0.047350, grad_norm: 0.588683
[[34m2025-10-04 12:09:51[0m] Step: 2342, Training Logs: loss_final: 0.917770, loss_mean: 0.870891, loss_mean_cls: 0.046879, grad_norm: 0.559636
[[34m2025-10-04 12:09:51[0m] Step: 2343, Training Logs: loss_final: 0.936096, loss_mean: 0.888898, loss_mean_cls: 0.047198, grad_norm: 0.631891
[[34m2025-10-04 12:09:51[0m] Step: 2344, Training Logs: loss_final: 0.912916, loss_mean: 0.865350, loss_mean_cls: 0.047566, grad_norm: 0.495246
[[34m2025-10-04 12:09:51[0m] Step: 2345, Training Logs: loss_final: 0.915794, loss_mean: 0.868071, loss_mean_cls: 0.047723, grad_norm: 0.742656
[[34m2025-10-04 12:09:52[0m] Step: 2346, Training Logs: loss_final: 0.928613, loss_mean: 0.881021, loss_mean_cls: 0.047591, grad_norm: 0.603106
[[34m2025-10-04 12:09:52[0m] Step: 2347, Training Logs: loss_final: 0.906325, loss_mean: 0.858497, loss_mean_cls: 0.047828, grad_norm: 0.581205
[[34m2025-10-04 12:09:52[0m] Step: 2348, Training Logs: loss_final: 0.909579, loss_mean: 0.861684, loss_mean_cls: 0.047895, grad_norm: 0.476161
[[34m2025-10-04 12:09:53[0m] Step: 2349, Training Logs: loss_final: 0.914626, loss_mean: 0.867549, loss_mean_cls: 0.047077, grad_norm: 0.586151
[[34m2025-10-04 12:09:53[0m] Step: 2350, Training Logs: loss_final: 0.915135, loss_mean: 0.866272, loss_mean_cls: 0.048863, grad_norm: 0.475084
[[34m2025-10-04 12:09:53[0m] Step: 2351, Training Logs: loss_final: 0.918219, loss_mean: 0.870531, loss_mean_cls: 0.047688, grad_norm: 0.487879
[[34m2025-10-04 12:09:53[0m] Step: 2352, Training Logs: loss_final: 0.934330, loss_mean: 0.886889, loss_mean_cls: 0.047441, grad_norm: 0.587308
[[34m2025-10-04 12:09:54[0m] Step: 2353, Training Logs: loss_final: 0.909914, loss_mean: 0.863165, loss_mean_cls: 0.046750, grad_norm: 0.353581
[[34m2025-10-04 12:09:54[0m] Step: 2354, Training Logs: loss_final: 0.925424, loss_mean: 0.878536, loss_mean_cls: 0.046888, grad_norm: 0.510733
[[34m2025-10-04 12:09:54[0m] Step: 2355, Training Logs: loss_final: 0.920124, loss_mean: 0.872624, loss_mean_cls: 0.047499, grad_norm: 0.620335
[[34m2025-10-04 12:09:55[0m] Step: 2356, Training Logs: loss_final: 0.895102, loss_mean: 0.847242, loss_mean_cls: 0.047860, grad_norm: 0.472557
[[34m2025-10-04 12:09:55[0m] Step: 2357, Training Logs: loss_final: 0.928879, loss_mean: 0.881183, loss_mean_cls: 0.047695, grad_norm: 0.734945
[[34m2025-10-04 12:09:55[0m] Step: 2358, Training Logs: loss_final: 0.901829, loss_mean: 0.854620, loss_mean_cls: 0.047209, grad_norm: 0.474979
[[34m2025-10-04 12:09:56[0m] Step: 2359, Training Logs: loss_final: 0.904959, loss_mean: 0.856722, loss_mean_cls: 0.048237, grad_norm: 0.614858
[[34m2025-10-04 12:09:56[0m] Step: 2360, Training Logs: loss_final: 0.882704, loss_mean: 0.834719, loss_mean_cls: 0.047985, grad_norm: 0.386832
[[34m2025-10-04 12:09:56[0m] Step: 2361, Training Logs: loss_final: 0.898125, loss_mean: 0.849879, loss_mean_cls: 0.048245, grad_norm: 0.499225
[[34m2025-10-04 12:09:56[0m] Step: 2362, Training Logs: loss_final: 0.917675, loss_mean: 0.870600, loss_mean_cls: 0.047076, grad_norm: 0.702252
[[34m2025-10-04 12:09:57[0m] Step: 2363, Training Logs: loss_final: 0.910804, loss_mean: 0.863806, loss_mean_cls: 0.046998, grad_norm: 0.458252
[[34m2025-10-04 12:09:57[0m] Step: 2364, Training Logs: loss_final: 0.899409, loss_mean: 0.852334, loss_mean_cls: 0.047076, grad_norm: 0.632396
[[34m2025-10-04 12:09:57[0m] Step: 2365, Training Logs: loss_final: 0.893750, loss_mean: 0.845228, loss_mean_cls: 0.048523, grad_norm: 0.612541
[[34m2025-10-04 12:09:58[0m] Step: 2366, Training Logs: loss_final: 0.921502, loss_mean: 0.875267, loss_mean_cls: 0.046236, grad_norm: 0.379357
[[34m2025-10-04 12:09:58[0m] Step: 2367, Training Logs: loss_final: 0.912664, loss_mean: 0.864464, loss_mean_cls: 0.048200, grad_norm: 0.658447
[[34m2025-10-04 12:09:58[0m] Step: 2368, Training Logs: loss_final: 0.914773, loss_mean: 0.868115, loss_mean_cls: 0.046658, grad_norm: 0.524421
[[34m2025-10-04 12:09:59[0m] Step: 2369, Training Logs: loss_final: 0.937546, loss_mean: 0.890898, loss_mean_cls: 0.046648, grad_norm: 0.371319
[[34m2025-10-04 12:09:59[0m] Step: 2370, Training Logs: loss_final: 0.901818, loss_mean: 0.854353, loss_mean_cls: 0.047466, grad_norm: 0.534939
[[34m2025-10-04 12:09:59[0m] Step: 2371, Training Logs: loss_final: 0.914921, loss_mean: 0.867720, loss_mean_cls: 0.047201, grad_norm: 0.542919
[[34m2025-10-04 12:09:59[0m] Step: 2372, Training Logs: loss_final: 0.933571, loss_mean: 0.885827, loss_mean_cls: 0.047744, grad_norm: 0.522071
[[34m2025-10-04 12:10:00[0m] Step: 2373, Training Logs: loss_final: 0.935311, loss_mean: 0.888405, loss_mean_cls: 0.046906, grad_norm: 0.732304
[[34m2025-10-04 12:10:00[0m] Step: 2374, Training Logs: loss_final: 0.886252, loss_mean: 0.838555, loss_mean_cls: 0.047697, grad_norm: 0.772807
[[34m2025-10-04 12:10:00[0m] Step: 2375, Training Logs: loss_final: 0.894126, loss_mean: 0.845115, loss_mean_cls: 0.049011, grad_norm: 0.784118
[[34m2025-10-04 12:10:01[0m] Step: 2376, Training Logs: loss_final: 0.900644, loss_mean: 0.853117, loss_mean_cls: 0.047528, grad_norm: 0.529850
[[34m2025-10-04 12:10:01[0m] Step: 2377, Training Logs: loss_final: 0.915306, loss_mean: 0.868408, loss_mean_cls: 0.046898, grad_norm: 0.944549
[[34m2025-10-04 12:10:01[0m] Step: 2378, Training Logs: loss_final: 0.931960, loss_mean: 0.884955, loss_mean_cls: 0.047004, grad_norm: 0.680518
[[34m2025-10-04 12:10:01[0m] Step: 2379, Training Logs: loss_final: 0.909004, loss_mean: 0.860989, loss_mean_cls: 0.048016, grad_norm: 0.480003
[[34m2025-10-04 12:10:02[0m] Step: 2380, Training Logs: loss_final: 0.888918, loss_mean: 0.841328, loss_mean_cls: 0.047590, grad_norm: 0.871395
[[34m2025-10-04 12:10:02[0m] Step: 2381, Training Logs: loss_final: 0.900688, loss_mean: 0.853283, loss_mean_cls: 0.047404, grad_norm: 0.466745
[[34m2025-10-04 12:10:02[0m] Step: 2382, Training Logs: loss_final: 0.898204, loss_mean: 0.850051, loss_mean_cls: 0.048153, grad_norm: 0.738020
[[34m2025-10-04 12:10:03[0m] Step: 2383, Training Logs: loss_final: 0.917281, loss_mean: 0.869385, loss_mean_cls: 0.047895, grad_norm: 0.540341
[[34m2025-10-04 12:10:03[0m] Step: 2384, Training Logs: loss_final: 0.909882, loss_mean: 0.863000, loss_mean_cls: 0.046883, grad_norm: 0.508248
[[34m2025-10-04 12:10:03[0m] Step: 2385, Training Logs: loss_final: 0.908908, loss_mean: 0.860862, loss_mean_cls: 0.048046, grad_norm: 0.628534
[[34m2025-10-04 12:10:04[0m] Step: 2386, Training Logs: loss_final: 0.922787, loss_mean: 0.875411, loss_mean_cls: 0.047376, grad_norm: 0.596280
[[34m2025-10-04 12:10:04[0m] Step: 2387, Training Logs: loss_final: 0.928670, loss_mean: 0.881899, loss_mean_cls: 0.046772, grad_norm: 0.515481
[[34m2025-10-04 12:10:04[0m] Step: 2388, Training Logs: loss_final: 0.911028, loss_mean: 0.863946, loss_mean_cls: 0.047081, grad_norm: 0.656406
[[34m2025-10-04 12:10:04[0m] Step: 2389, Training Logs: loss_final: 0.914569, loss_mean: 0.866151, loss_mean_cls: 0.048419, grad_norm: 0.633480
[[34m2025-10-04 12:10:05[0m] Step: 2390, Training Logs: loss_final: 0.919843, loss_mean: 0.873119, loss_mean_cls: 0.046724, grad_norm: 0.436727
[[34m2025-10-04 12:10:05[0m] Step: 2391, Training Logs: loss_final: 0.921198, loss_mean: 0.874379, loss_mean_cls: 0.046820, grad_norm: 0.704414
[[34m2025-10-04 12:10:05[0m] Step: 2392, Training Logs: loss_final: 0.921169, loss_mean: 0.874006, loss_mean_cls: 0.047162, grad_norm: 0.586272
[[34m2025-10-04 12:10:06[0m] Step: 2393, Training Logs: loss_final: 0.928477, loss_mean: 0.883440, loss_mean_cls: 0.045038, grad_norm: 0.487583
[[34m2025-10-04 12:10:06[0m] Step: 2394, Training Logs: loss_final: 0.928568, loss_mean: 0.880671, loss_mean_cls: 0.047897, grad_norm: 0.458519
[[34m2025-10-04 12:10:06[0m] Step: 2395, Training Logs: loss_final: 0.929614, loss_mean: 0.882723, loss_mean_cls: 0.046891, grad_norm: 0.454455
[[34m2025-10-04 12:10:07[0m] Step: 2396, Training Logs: loss_final: 0.895569, loss_mean: 0.847315, loss_mean_cls: 0.048253, grad_norm: 0.372630
[[34m2025-10-04 12:10:07[0m] Step: 2397, Training Logs: loss_final: 0.899997, loss_mean: 0.853013, loss_mean_cls: 0.046984, grad_norm: 0.524117
[[34m2025-10-04 12:10:07[0m] Step: 2398, Training Logs: loss_final: 0.923808, loss_mean: 0.876935, loss_mean_cls: 0.046873, grad_norm: 0.380123
[[34m2025-10-04 12:10:07[0m] Step: 2399, Training Logs: loss_final: 0.915746, loss_mean: 0.868841, loss_mean_cls: 0.046905, grad_norm: 0.461059
[[34m2025-10-04 12:10:08[0m] Step: 2400, Training Logs: loss_final: 0.903914, loss_mean: 0.854821, loss_mean_cls: 0.049093, grad_norm: 0.519036
[[34m2025-10-04 12:10:08[0m] Step: 2401, Training Logs: loss_final: 0.915460, loss_mean: 0.867635, loss_mean_cls: 0.047825, grad_norm: 0.485761
[[34m2025-10-04 12:10:08[0m] Step: 2402, Training Logs: loss_final: 0.911047, loss_mean: 0.863095, loss_mean_cls: 0.047953, grad_norm: 0.731531
[[34m2025-10-04 12:10:09[0m] Step: 2403, Training Logs: loss_final: 0.902695, loss_mean: 0.854352, loss_mean_cls: 0.048343, grad_norm: 0.793794
[[34m2025-10-04 12:10:09[0m] Step: 2404, Training Logs: loss_final: 0.895725, loss_mean: 0.848221, loss_mean_cls: 0.047504, grad_norm: 0.835889
[[34m2025-10-04 12:10:09[0m] Step: 2405, Training Logs: loss_final: 0.917973, loss_mean: 0.870125, loss_mean_cls: 0.047848, grad_norm: 0.760151
[[34m2025-10-04 12:10:09[0m] Step: 2406, Training Logs: loss_final: 0.924600, loss_mean: 0.876962, loss_mean_cls: 0.047639, grad_norm: 0.556703
[[34m2025-10-04 12:10:10[0m] Step: 2407, Training Logs: loss_final: 0.921666, loss_mean: 0.874397, loss_mean_cls: 0.047269, grad_norm: 0.496896
[[34m2025-10-04 12:10:10[0m] Step: 2408, Training Logs: loss_final: 0.928871, loss_mean: 0.881617, loss_mean_cls: 0.047254, grad_norm: 0.664105
[[34m2025-10-04 12:10:10[0m] Step: 2409, Training Logs: loss_final: 0.918175, loss_mean: 0.870312, loss_mean_cls: 0.047863, grad_norm: 0.565444
[[34m2025-10-04 12:10:11[0m] Step: 2410, Training Logs: loss_final: 0.908467, loss_mean: 0.860546, loss_mean_cls: 0.047921, grad_norm: 0.499554
[[34m2025-10-04 12:10:11[0m] Step: 2411, Training Logs: loss_final: 0.900652, loss_mean: 0.852846, loss_mean_cls: 0.047807, grad_norm: 0.662360
[[34m2025-10-04 12:10:11[0m] Step: 2412, Training Logs: loss_final: 0.922553, loss_mean: 0.875074, loss_mean_cls: 0.047479, grad_norm: 0.510505
[[34m2025-10-04 12:10:11[0m] Step: 2413, Training Logs: loss_final: 0.905252, loss_mean: 0.857112, loss_mean_cls: 0.048141, grad_norm: 0.449277
[[34m2025-10-04 12:10:12[0m] Step: 2414, Training Logs: loss_final: 0.913973, loss_mean: 0.866644, loss_mean_cls: 0.047329, grad_norm: 0.551751
[[34m2025-10-04 12:10:12[0m] Step: 2415, Training Logs: loss_final: 0.921159, loss_mean: 0.875451, loss_mean_cls: 0.045708, grad_norm: 0.608369
[[34m2025-10-04 12:10:12[0m] Step: 2416, Training Logs: loss_final: 0.918005, loss_mean: 0.869963, loss_mean_cls: 0.048042, grad_norm: 0.520432
[[34m2025-10-04 12:10:13[0m] Step: 2417, Training Logs: loss_final: 0.926308, loss_mean: 0.879028, loss_mean_cls: 0.047279, grad_norm: 0.599058
[[34m2025-10-04 12:10:13[0m] Step: 2418, Training Logs: loss_final: 0.904563, loss_mean: 0.856709, loss_mean_cls: 0.047854, grad_norm: 0.643577
[[34m2025-10-04 12:10:13[0m] Step: 2419, Training Logs: loss_final: 0.904854, loss_mean: 0.857501, loss_mean_cls: 0.047354, grad_norm: 0.489796
[[34m2025-10-04 12:10:14[0m] Step: 2420, Training Logs: loss_final: 0.926144, loss_mean: 0.879192, loss_mean_cls: 0.046951, grad_norm: 0.410310
[[34m2025-10-04 12:10:14[0m] Step: 2421, Training Logs: loss_final: 0.925419, loss_mean: 0.876808, loss_mean_cls: 0.048611, grad_norm: 0.573609
[[34m2025-10-04 12:10:14[0m] Step: 2422, Training Logs: loss_final: 0.910825, loss_mean: 0.862344, loss_mean_cls: 0.048481, grad_norm: 0.496903
[[34m2025-10-04 12:10:14[0m] Step: 2423, Training Logs: loss_final: 0.920890, loss_mean: 0.873820, loss_mean_cls: 0.047070, grad_norm: 0.530885
[[34m2025-10-04 12:10:15[0m] Step: 2424, Training Logs: loss_final: 0.918481, loss_mean: 0.871001, loss_mean_cls: 0.047480, grad_norm: 0.467982
[[34m2025-10-04 12:10:15[0m] Step: 2425, Training Logs: loss_final: 0.895502, loss_mean: 0.847210, loss_mean_cls: 0.048292, grad_norm: 0.570209
[[34m2025-10-04 12:10:15[0m] Step: 2426, Training Logs: loss_final: 0.928239, loss_mean: 0.880514, loss_mean_cls: 0.047726, grad_norm: 0.448870
[[34m2025-10-04 12:10:16[0m] Step: 2427, Training Logs: loss_final: 0.908422, loss_mean: 0.859750, loss_mean_cls: 0.048671, grad_norm: 0.428597
[[34m2025-10-04 12:10:16[0m] Step: 2428, Training Logs: loss_final: 0.912418, loss_mean: 0.864135, loss_mean_cls: 0.048282, grad_norm: 0.552110
[[34m2025-10-04 12:10:16[0m] Step: 2429, Training Logs: loss_final: 0.930957, loss_mean: 0.884737, loss_mean_cls: 0.046220, grad_norm: 0.399827
[[34m2025-10-04 12:10:17[0m] Step: 2430, Training Logs: loss_final: 0.923368, loss_mean: 0.877251, loss_mean_cls: 0.046116, grad_norm: 0.624949
[[34m2025-10-04 12:10:17[0m] Step: 2431, Training Logs: loss_final: 0.943578, loss_mean: 0.896073, loss_mean_cls: 0.047505, grad_norm: 0.577380
[[34m2025-10-04 12:10:17[0m] Step: 2432, Training Logs: loss_final: 0.920989, loss_mean: 0.873284, loss_mean_cls: 0.047705, grad_norm: 0.361703
[[34m2025-10-04 12:10:17[0m] Step: 2433, Training Logs: loss_final: 0.902841, loss_mean: 0.855270, loss_mean_cls: 0.047572, grad_norm: 0.480683
[[34m2025-10-04 12:10:18[0m] Step: 2434, Training Logs: loss_final: 0.919629, loss_mean: 0.871130, loss_mean_cls: 0.048499, grad_norm: 0.390267
[[34m2025-10-04 12:10:18[0m] Step: 2435, Training Logs: loss_final: 0.927230, loss_mean: 0.880070, loss_mean_cls: 0.047160, grad_norm: 0.510607
[[34m2025-10-04 12:10:18[0m] Step: 2436, Training Logs: loss_final: 0.911285, loss_mean: 0.863695, loss_mean_cls: 0.047590, grad_norm: 0.467916
[[34m2025-10-04 12:10:19[0m] Step: 2437, Training Logs: loss_final: 0.920670, loss_mean: 0.874219, loss_mean_cls: 0.046451, grad_norm: 0.382961
[[34m2025-10-04 12:10:19[0m] Step: 2438, Training Logs: loss_final: 0.910412, loss_mean: 0.863003, loss_mean_cls: 0.047408, grad_norm: 0.557349
[[34m2025-10-04 12:10:19[0m] Step: 2439, Training Logs: loss_final: 0.925191, loss_mean: 0.877846, loss_mean_cls: 0.047345, grad_norm: 0.379741
[[34m2025-10-04 12:10:19[0m] Step: 2440, Training Logs: loss_final: 0.932314, loss_mean: 0.884652, loss_mean_cls: 0.047662, grad_norm: 0.591750
[[34m2025-10-04 12:10:20[0m] Step: 2441, Training Logs: loss_final: 0.884571, loss_mean: 0.837015, loss_mean_cls: 0.047556, grad_norm: 0.495523
[[34m2025-10-04 12:10:20[0m] Step: 2442, Training Logs: loss_final: 0.918301, loss_mean: 0.870466, loss_mean_cls: 0.047835, grad_norm: 0.609150
[[34m2025-10-04 12:10:20[0m] Step: 2443, Training Logs: loss_final: 0.904114, loss_mean: 0.855887, loss_mean_cls: 0.048228, grad_norm: 0.641075
[[34m2025-10-04 12:10:21[0m] Step: 2444, Training Logs: loss_final: 0.936672, loss_mean: 0.889901, loss_mean_cls: 0.046771, grad_norm: 0.475691
[[34m2025-10-04 12:10:21[0m] Step: 2445, Training Logs: loss_final: 0.912043, loss_mean: 0.865018, loss_mean_cls: 0.047025, grad_norm: 0.813201
[[34m2025-10-04 12:10:21[0m] Step: 2446, Training Logs: loss_final: 0.931350, loss_mean: 0.883521, loss_mean_cls: 0.047830, grad_norm: 0.478941
[[34m2025-10-04 12:10:22[0m] Step: 2447, Training Logs: loss_final: 0.912438, loss_mean: 0.864211, loss_mean_cls: 0.048227, grad_norm: 0.523931
[[34m2025-10-04 12:10:22[0m] Step: 2448, Training Logs: loss_final: 0.922978, loss_mean: 0.874757, loss_mean_cls: 0.048221, grad_norm: 0.578105
[[34m2025-10-04 12:10:22[0m] Step: 2449, Training Logs: loss_final: 0.918031, loss_mean: 0.870745, loss_mean_cls: 0.047286, grad_norm: 0.608060
[[34m2025-10-04 12:10:22[0m] Step: 2450, Training Logs: loss_final: 0.912991, loss_mean: 0.865399, loss_mean_cls: 0.047592, grad_norm: 0.507757
[[34m2025-10-04 12:10:23[0m] Step: 2451, Training Logs: loss_final: 0.913489, loss_mean: 0.865314, loss_mean_cls: 0.048175, grad_norm: 0.558748
[[34m2025-10-04 12:10:23[0m] Step: 2452, Training Logs: loss_final: 0.926538, loss_mean: 0.878969, loss_mean_cls: 0.047569, grad_norm: 0.524422
[[34m2025-10-04 12:10:23[0m] Step: 2453, Training Logs: loss_final: 0.925509, loss_mean: 0.877306, loss_mean_cls: 0.048203, grad_norm: 0.448907
[[34m2025-10-04 12:10:24[0m] Step: 2454, Training Logs: loss_final: 0.906933, loss_mean: 0.859541, loss_mean_cls: 0.047391, grad_norm: 0.433523
[[34m2025-10-04 12:10:24[0m] Step: 2455, Training Logs: loss_final: 0.918070, loss_mean: 0.871542, loss_mean_cls: 0.046528, grad_norm: 0.552734
[[34m2025-10-04 12:10:24[0m] Step: 2456, Training Logs: loss_final: 0.903825, loss_mean: 0.856581, loss_mean_cls: 0.047244, grad_norm: 0.484664
[[34m2025-10-04 12:10:24[0m] Step: 2457, Training Logs: loss_final: 0.916525, loss_mean: 0.868544, loss_mean_cls: 0.047981, grad_norm: 0.454483
[[34m2025-10-04 12:10:25[0m] Step: 2458, Training Logs: loss_final: 0.905613, loss_mean: 0.857967, loss_mean_cls: 0.047646, grad_norm: 0.493588
[[34m2025-10-04 12:10:25[0m] Step: 2459, Training Logs: loss_final: 0.929616, loss_mean: 0.883271, loss_mean_cls: 0.046345, grad_norm: 0.517317
[[34m2025-10-04 12:10:25[0m] Step: 2460, Training Logs: loss_final: 0.930424, loss_mean: 0.883012, loss_mean_cls: 0.047412, grad_norm: 0.462055
[[34m2025-10-04 12:10:26[0m] Step: 2461, Training Logs: loss_final: 0.942444, loss_mean: 0.896162, loss_mean_cls: 0.046283, grad_norm: 0.626413
[[34m2025-10-04 12:10:26[0m] Step: 2462, Training Logs: loss_final: 0.893746, loss_mean: 0.845110, loss_mean_cls: 0.048637, grad_norm: 0.476347
[[34m2025-10-04 12:10:26[0m] Step: 2463, Training Logs: loss_final: 0.920496, loss_mean: 0.872014, loss_mean_cls: 0.048482, grad_norm: 0.477460
[[34m2025-10-04 12:10:27[0m] Step: 2464, Training Logs: loss_final: 0.914419, loss_mean: 0.867615, loss_mean_cls: 0.046805, grad_norm: 0.429563
[[34m2025-10-04 12:10:27[0m] Step: 2465, Training Logs: loss_final: 0.917217, loss_mean: 0.870197, loss_mean_cls: 0.047020, grad_norm: 0.518735
[[34m2025-10-04 12:10:27[0m] Step: 2466, Training Logs: loss_final: 0.902077, loss_mean: 0.854445, loss_mean_cls: 0.047633, grad_norm: 0.406275
[[34m2025-10-04 12:10:27[0m] Step: 2467, Training Logs: loss_final: 0.911101, loss_mean: 0.863845, loss_mean_cls: 0.047256, grad_norm: 0.583114
[[34m2025-10-04 12:10:28[0m] Step: 2468, Training Logs: loss_final: 0.891725, loss_mean: 0.843359, loss_mean_cls: 0.048366, grad_norm: 0.400768
[[34m2025-10-04 12:10:28[0m] Step: 2469, Training Logs: loss_final: 0.908049, loss_mean: 0.860564, loss_mean_cls: 0.047485, grad_norm: 0.548475
[[34m2025-10-04 12:10:28[0m] Step: 2470, Training Logs: loss_final: 0.887559, loss_mean: 0.839270, loss_mean_cls: 0.048289, grad_norm: 0.492841
[[34m2025-10-04 12:10:29[0m] Step: 2471, Training Logs: loss_final: 0.918380, loss_mean: 0.870399, loss_mean_cls: 0.047980, grad_norm: 0.427349
[[34m2025-10-04 12:10:29[0m] Step: 2472, Training Logs: loss_final: 0.915517, loss_mean: 0.868726, loss_mean_cls: 0.046791, grad_norm: 0.622404
[[34m2025-10-04 12:10:29[0m] Step: 2473, Training Logs: loss_final: 0.932438, loss_mean: 0.884897, loss_mean_cls: 0.047541, grad_norm: 0.476383
[[34m2025-10-04 12:10:29[0m] Step: 2474, Training Logs: loss_final: 0.909926, loss_mean: 0.862350, loss_mean_cls: 0.047576, grad_norm: 0.498898
[[34m2025-10-04 12:10:30[0m] Step: 2475, Training Logs: loss_final: 0.907536, loss_mean: 0.860370, loss_mean_cls: 0.047166, grad_norm: 0.462074
[[34m2025-10-04 12:10:30[0m] Step: 2476, Training Logs: loss_final: 0.919370, loss_mean: 0.872720, loss_mean_cls: 0.046649, grad_norm: 0.455509
[[34m2025-10-04 12:10:30[0m] Step: 2477, Training Logs: loss_final: 0.902450, loss_mean: 0.856221, loss_mean_cls: 0.046230, grad_norm: 0.431316
[[34m2025-10-04 12:10:31[0m] Step: 2478, Training Logs: loss_final: 0.895665, loss_mean: 0.848251, loss_mean_cls: 0.047414, grad_norm: 0.518415
[[34m2025-10-04 12:10:31[0m] Step: 2479, Training Logs: loss_final: 0.924951, loss_mean: 0.878096, loss_mean_cls: 0.046855, grad_norm: 0.334387
[[34m2025-10-04 12:10:31[0m] Step: 2480, Training Logs: loss_final: 0.901837, loss_mean: 0.855433, loss_mean_cls: 0.046404, grad_norm: 0.582115
[[34m2025-10-04 12:10:32[0m] Step: 2481, Training Logs: loss_final: 0.910627, loss_mean: 0.861907, loss_mean_cls: 0.048719, grad_norm: 0.454917
[[34m2025-10-04 12:10:32[0m] Step: 2482, Training Logs: loss_final: 0.914366, loss_mean: 0.866301, loss_mean_cls: 0.048064, grad_norm: 0.542044
[[34m2025-10-04 12:10:32[0m] Step: 2483, Training Logs: loss_final: 0.913817, loss_mean: 0.866084, loss_mean_cls: 0.047732, grad_norm: 0.396289
[[34m2025-10-04 12:10:32[0m] Step: 2484, Training Logs: loss_final: 0.919064, loss_mean: 0.872442, loss_mean_cls: 0.046622, grad_norm: 0.481625
[[34m2025-10-04 12:10:33[0m] Step: 2485, Training Logs: loss_final: 0.915853, loss_mean: 0.868006, loss_mean_cls: 0.047847, grad_norm: 0.436879
[[34m2025-10-04 12:10:33[0m] Step: 2486, Training Logs: loss_final: 0.912727, loss_mean: 0.865349, loss_mean_cls: 0.047378, grad_norm: 0.411762
[[34m2025-10-04 12:10:33[0m] Step: 2487, Training Logs: loss_final: 0.907689, loss_mean: 0.860397, loss_mean_cls: 0.047292, grad_norm: 0.367232
[[34m2025-10-04 12:10:34[0m] Step: 2488, Training Logs: loss_final: 0.919737, loss_mean: 0.872319, loss_mean_cls: 0.047418, grad_norm: 0.646620
[[34m2025-10-04 12:10:34[0m] Step: 2489, Training Logs: loss_final: 0.919023, loss_mean: 0.871921, loss_mean_cls: 0.047102, grad_norm: 0.388328
[[34m2025-10-04 12:10:34[0m] Step: 2490, Training Logs: loss_final: 0.917583, loss_mean: 0.870727, loss_mean_cls: 0.046856, grad_norm: 0.472908
[[34m2025-10-04 12:10:34[0m] Step: 2491, Training Logs: loss_final: 0.902004, loss_mean: 0.854274, loss_mean_cls: 0.047730, grad_norm: 0.415403
[[34m2025-10-04 12:10:35[0m] Step: 2492, Training Logs: loss_final: 0.920677, loss_mean: 0.873695, loss_mean_cls: 0.046982, grad_norm: 0.420015
[[34m2025-10-04 12:10:35[0m] Step: 2493, Training Logs: loss_final: 0.883667, loss_mean: 0.835588, loss_mean_cls: 0.048079, grad_norm: 0.460072
[[34m2025-10-04 12:10:35[0m] Step: 2494, Training Logs: loss_final: 0.927060, loss_mean: 0.880025, loss_mean_cls: 0.047035, grad_norm: 0.687401
[[34m2025-10-04 12:10:36[0m] Step: 2495, Training Logs: loss_final: 0.903872, loss_mean: 0.856151, loss_mean_cls: 0.047721, grad_norm: 0.654689
[[34m2025-10-04 12:10:36[0m] Step: 2496, Training Logs: loss_final: 0.909808, loss_mean: 0.862462, loss_mean_cls: 0.047346, grad_norm: 0.773027
[[34m2025-10-04 12:10:36[0m] Step: 2497, Training Logs: loss_final: 0.910078, loss_mean: 0.862354, loss_mean_cls: 0.047723, grad_norm: 0.390564
[[34m2025-10-04 12:10:37[0m] Step: 2498, Training Logs: loss_final: 0.885635, loss_mean: 0.838055, loss_mean_cls: 0.047581, grad_norm: 0.557466
[[34m2025-10-04 12:10:37[0m] Step: 2499, Training Logs: loss_final: 0.904441, loss_mean: 0.858154, loss_mean_cls: 0.046286, grad_norm: 0.424665
[[34m2025-10-04 12:10:37[0m] Step: 2500, Training Logs: loss_final: 0.899421, loss_mean: 0.852107, loss_mean_cls: 0.047314, grad_norm: 0.443403
[[34m2025-10-04 12:10:37[0m] Step: 2501, Training Logs: loss_final: 0.907392, loss_mean: 0.860770, loss_mean_cls: 0.046622, grad_norm: 0.451832
[[34m2025-10-04 12:10:38[0m] Step: 2502, Training Logs: loss_final: 0.894752, loss_mean: 0.847631, loss_mean_cls: 0.047121, grad_norm: 0.474903
[[34m2025-10-04 12:10:38[0m] Step: 2503, Training Logs: loss_final: 0.906945, loss_mean: 0.859184, loss_mean_cls: 0.047761, grad_norm: 0.491793
[[34m2025-10-04 12:10:38[0m] Step: 2504, Training Logs: loss_final: 0.883627, loss_mean: 0.836458, loss_mean_cls: 0.047169, grad_norm: 0.326039
[[34m2025-10-04 12:10:39[0m] Step: 2505, Training Logs: loss_final: 0.907830, loss_mean: 0.861029, loss_mean_cls: 0.046802, grad_norm: 0.585764
[[34m2025-10-04 12:10:39[0m] Step: 2506, Training Logs: loss_final: 0.898123, loss_mean: 0.850969, loss_mean_cls: 0.047154, grad_norm: 0.471936
[[34m2025-10-04 12:10:39[0m] Step: 2507, Training Logs: loss_final: 0.890579, loss_mean: 0.843598, loss_mean_cls: 0.046981, grad_norm: 0.640055
[[34m2025-10-04 12:10:39[0m] Step: 2508, Training Logs: loss_final: 0.905409, loss_mean: 0.857993, loss_mean_cls: 0.047417, grad_norm: 0.376881
[[34m2025-10-04 12:10:40[0m] Step: 2509, Training Logs: loss_final: 0.899372, loss_mean: 0.851183, loss_mean_cls: 0.048189, grad_norm: 0.332701
[[34m2025-10-04 12:10:40[0m] Step: 2510, Training Logs: loss_final: 0.903309, loss_mean: 0.855503, loss_mean_cls: 0.047806, grad_norm: 0.404584
[[34m2025-10-04 12:10:40[0m] Step: 2511, Training Logs: loss_final: 0.912588, loss_mean: 0.865719, loss_mean_cls: 0.046868, grad_norm: 0.342671
[[34m2025-10-04 12:10:41[0m] Step: 2512, Training Logs: loss_final: 0.893913, loss_mean: 0.845257, loss_mean_cls: 0.048656, grad_norm: 0.590532
[[34m2025-10-04 12:10:41[0m] Step: 2513, Training Logs: loss_final: 0.902048, loss_mean: 0.854076, loss_mean_cls: 0.047972, grad_norm: 0.478986
[[34m2025-10-04 12:10:41[0m] Step: 2514, Training Logs: loss_final: 0.925228, loss_mean: 0.877014, loss_mean_cls: 0.048214, grad_norm: 0.752613
[[34m2025-10-04 12:10:42[0m] Step: 2515, Training Logs: loss_final: 0.916108, loss_mean: 0.869158, loss_mean_cls: 0.046949, grad_norm: 0.484389
[[34m2025-10-04 12:10:42[0m] Step: 2516, Training Logs: loss_final: 0.924968, loss_mean: 0.877818, loss_mean_cls: 0.047150, grad_norm: 0.624519
[[34m2025-10-04 12:10:42[0m] Step: 2517, Training Logs: loss_final: 0.919141, loss_mean: 0.871837, loss_mean_cls: 0.047304, grad_norm: 0.552330
[[34m2025-10-04 12:10:42[0m] Step: 2518, Training Logs: loss_final: 0.937583, loss_mean: 0.891042, loss_mean_cls: 0.046541, grad_norm: 0.561866
[[34m2025-10-04 12:10:43[0m] Step: 2519, Training Logs: loss_final: 0.916287, loss_mean: 0.869645, loss_mean_cls: 0.046642, grad_norm: 0.461774
[[34m2025-10-04 12:10:43[0m] Step: 2520, Training Logs: loss_final: 0.914370, loss_mean: 0.866656, loss_mean_cls: 0.047714, grad_norm: 0.413038
[[34m2025-10-04 12:10:43[0m] Step: 2521, Training Logs: loss_final: 0.897228, loss_mean: 0.849193, loss_mean_cls: 0.048035, grad_norm: 0.470509
[[34m2025-10-04 12:10:44[0m] Step: 2522, Training Logs: loss_final: 0.908727, loss_mean: 0.861520, loss_mean_cls: 0.047207, grad_norm: 0.502137
[[34m2025-10-04 12:10:44[0m] Step: 2523, Training Logs: loss_final: 0.918606, loss_mean: 0.872443, loss_mean_cls: 0.046162, grad_norm: 0.515042
[[34m2025-10-04 12:10:44[0m] Step: 2524, Training Logs: loss_final: 0.914578, loss_mean: 0.868888, loss_mean_cls: 0.045690, grad_norm: 0.537357
[[34m2025-10-04 12:10:44[0m] Step: 2525, Training Logs: loss_final: 0.889165, loss_mean: 0.841472, loss_mean_cls: 0.047693, grad_norm: 0.748117
[[34m2025-10-04 12:10:45[0m] Step: 2526, Training Logs: loss_final: 0.909021, loss_mean: 0.861907, loss_mean_cls: 0.047114, grad_norm: 0.546620
[[34m2025-10-04 12:10:45[0m] Step: 2527, Training Logs: loss_final: 0.921503, loss_mean: 0.874454, loss_mean_cls: 0.047050, grad_norm: 0.811567
[[34m2025-10-04 12:10:45[0m] Step: 2528, Training Logs: loss_final: 0.903669, loss_mean: 0.855180, loss_mean_cls: 0.048488, grad_norm: 0.567718
[[34m2025-10-04 12:10:46[0m] Step: 2529, Training Logs: loss_final: 0.912635, loss_mean: 0.865890, loss_mean_cls: 0.046746, grad_norm: 0.645022
[[34m2025-10-04 12:10:46[0m] Step: 2530, Training Logs: loss_final: 0.906166, loss_mean: 0.859648, loss_mean_cls: 0.046518, grad_norm: 0.691409
[[34m2025-10-04 12:10:46[0m] Step: 2531, Training Logs: loss_final: 0.915686, loss_mean: 0.868138, loss_mean_cls: 0.047549, grad_norm: 0.383189
[[34m2025-10-04 12:10:47[0m] Step: 2532, Training Logs: loss_final: 0.916414, loss_mean: 0.869075, loss_mean_cls: 0.047339, grad_norm: 0.493782
[[34m2025-10-04 12:10:47[0m] Step: 2533, Training Logs: loss_final: 0.918879, loss_mean: 0.870427, loss_mean_cls: 0.048452, grad_norm: 0.768418
[[34m2025-10-04 12:10:47[0m] Step: 2534, Training Logs: loss_final: 0.913740, loss_mean: 0.866357, loss_mean_cls: 0.047383, grad_norm: 0.395421
[[34m2025-10-04 12:10:47[0m] Step: 2535, Training Logs: loss_final: 0.929422, loss_mean: 0.883041, loss_mean_cls: 0.046381, grad_norm: 0.607368
[[34m2025-10-04 12:10:48[0m] Step: 2536, Training Logs: loss_final: 0.902758, loss_mean: 0.854640, loss_mean_cls: 0.048118, grad_norm: 0.607874
[[34m2025-10-04 12:10:48[0m] Step: 2537, Training Logs: loss_final: 0.906084, loss_mean: 0.857629, loss_mean_cls: 0.048455, grad_norm: 0.558529
[[34m2025-10-04 12:10:48[0m] Step: 2538, Training Logs: loss_final: 0.930613, loss_mean: 0.883569, loss_mean_cls: 0.047044, grad_norm: 0.551599
[[34m2025-10-04 12:10:49[0m] Step: 2539, Training Logs: loss_final: 0.927732, loss_mean: 0.880033, loss_mean_cls: 0.047699, grad_norm: 0.748088
[[34m2025-10-04 12:10:49[0m] Step: 2540, Training Logs: loss_final: 0.912015, loss_mean: 0.864409, loss_mean_cls: 0.047606, grad_norm: 0.503171
[[34m2025-10-04 12:10:49[0m] Step: 2541, Training Logs: loss_final: 0.912198, loss_mean: 0.865460, loss_mean_cls: 0.046737, grad_norm: 0.389569
[[34m2025-10-04 12:10:50[0m] Step: 2542, Training Logs: loss_final: 0.927021, loss_mean: 0.880489, loss_mean_cls: 0.046532, grad_norm: 0.580267
[[34m2025-10-04 12:10:50[0m] Step: 2543, Training Logs: loss_final: 0.910357, loss_mean: 0.863331, loss_mean_cls: 0.047026, grad_norm: 0.499332
[[34m2025-10-04 12:10:50[0m] Step: 2544, Training Logs: loss_final: 0.904554, loss_mean: 0.856661, loss_mean_cls: 0.047893, grad_norm: 0.476407
[[34m2025-10-04 12:10:50[0m] Step: 2545, Training Logs: loss_final: 0.920618, loss_mean: 0.873831, loss_mean_cls: 0.046787, grad_norm: 0.611830
[[34m2025-10-04 12:10:51[0m] Step: 2546, Training Logs: loss_final: 0.911150, loss_mean: 0.862494, loss_mean_cls: 0.048657, grad_norm: 0.412602
[[34m2025-10-04 12:10:51[0m] Step: 2547, Training Logs: loss_final: 0.901802, loss_mean: 0.853828, loss_mean_cls: 0.047975, grad_norm: 0.481451
[[34m2025-10-04 12:10:51[0m] Step: 2548, Training Logs: loss_final: 0.874952, loss_mean: 0.826583, loss_mean_cls: 0.048369, grad_norm: 0.583758
[[34m2025-10-04 12:10:52[0m] Step: 2549, Training Logs: loss_final: 0.931039, loss_mean: 0.884550, loss_mean_cls: 0.046489, grad_norm: 0.450024
[[34m2025-10-04 12:10:52[0m] Step: 2550, Training Logs: loss_final: 0.901204, loss_mean: 0.854023, loss_mean_cls: 0.047181, grad_norm: 0.648731
[[34m2025-10-04 12:10:52[0m] Step: 2551, Training Logs: loss_final: 0.908082, loss_mean: 0.860671, loss_mean_cls: 0.047410, grad_norm: 0.290591
[[34m2025-10-04 12:10:53[0m] Step: 2552, Training Logs: loss_final: 0.896861, loss_mean: 0.849619, loss_mean_cls: 0.047242, grad_norm: 0.416391
[[34m2025-10-04 12:10:53[0m] Step: 2553, Training Logs: loss_final: 0.901587, loss_mean: 0.854424, loss_mean_cls: 0.047164, grad_norm: 0.445107
[[34m2025-10-04 12:10:53[0m] Step: 2554, Training Logs: loss_final: 0.896069, loss_mean: 0.848796, loss_mean_cls: 0.047273, grad_norm: 0.454888
[[34m2025-10-04 12:10:53[0m] Step: 2555, Training Logs: loss_final: 0.908857, loss_mean: 0.861830, loss_mean_cls: 0.047027, grad_norm: 0.435365
[[34m2025-10-04 12:10:54[0m] Step: 2556, Training Logs: loss_final: 0.901696, loss_mean: 0.854796, loss_mean_cls: 0.046901, grad_norm: 0.448184
[[34m2025-10-04 12:10:54[0m] Step: 2557, Training Logs: loss_final: 0.925208, loss_mean: 0.878528, loss_mean_cls: 0.046680, grad_norm: 0.430752
[[34m2025-10-04 12:10:54[0m] Step: 2558, Training Logs: loss_final: 0.931218, loss_mean: 0.884324, loss_mean_cls: 0.046894, grad_norm: 0.541831
[[34m2025-10-04 12:10:55[0m] Step: 2559, Training Logs: loss_final: 0.896524, loss_mean: 0.847877, loss_mean_cls: 0.048647, grad_norm: 0.543525
[[34m2025-10-04 12:10:55[0m] Step: 2560, Training Logs: loss_final: 0.900148, loss_mean: 0.852147, loss_mean_cls: 0.048001, grad_norm: 0.487571
[[34m2025-10-04 12:10:55[0m] Step: 2561, Training Logs: loss_final: 0.932345, loss_mean: 0.886044, loss_mean_cls: 0.046300, grad_norm: 0.570512
[[34m2025-10-04 12:10:56[0m] Step: 2562, Training Logs: loss_final: 0.910589, loss_mean: 0.862703, loss_mean_cls: 0.047886, grad_norm: 0.599324
[[34m2025-10-04 12:10:56[0m] Step: 2563, Training Logs: loss_final: 0.902755, loss_mean: 0.854427, loss_mean_cls: 0.048328, grad_norm: 0.734057
[[34m2025-10-04 12:10:56[0m] Step: 2564, Training Logs: loss_final: 0.912655, loss_mean: 0.865592, loss_mean_cls: 0.047063, grad_norm: 0.702418
[[34m2025-10-04 12:10:56[0m] Step: 2565, Training Logs: loss_final: 0.900563, loss_mean: 0.853278, loss_mean_cls: 0.047285, grad_norm: 0.488026
[[34m2025-10-04 12:10:57[0m] Step: 2566, Training Logs: loss_final: 0.918253, loss_mean: 0.871232, loss_mean_cls: 0.047021, grad_norm: 0.676466
[[34m2025-10-04 12:10:57[0m] Step: 2567, Training Logs: loss_final: 0.908867, loss_mean: 0.860655, loss_mean_cls: 0.048212, grad_norm: 0.549678
[[34m2025-10-04 12:10:57[0m] Step: 2568, Training Logs: loss_final: 0.923361, loss_mean: 0.875315, loss_mean_cls: 0.048046, grad_norm: 0.449973
[[34m2025-10-04 12:10:58[0m] Step: 2569, Training Logs: loss_final: 0.911316, loss_mean: 0.863830, loss_mean_cls: 0.047486, grad_norm: 0.684362
[[34m2025-10-04 12:10:58[0m] Step: 2570, Training Logs: loss_final: 0.908882, loss_mean: 0.860106, loss_mean_cls: 0.048776, grad_norm: 0.513153
[[34m2025-10-04 12:10:58[0m] Step: 2571, Training Logs: loss_final: 0.916714, loss_mean: 0.869605, loss_mean_cls: 0.047109, grad_norm: 0.702428
[[34m2025-10-04 12:10:58[0m] Step: 2572, Training Logs: loss_final: 0.894947, loss_mean: 0.845598, loss_mean_cls: 0.049349, grad_norm: 0.731495
[[34m2025-10-04 12:10:59[0m] Step: 2573, Training Logs: loss_final: 0.921860, loss_mean: 0.875188, loss_mean_cls: 0.046671, grad_norm: 0.427573
[[34m2025-10-04 12:10:59[0m] Step: 2574, Training Logs: loss_final: 0.898647, loss_mean: 0.850118, loss_mean_cls: 0.048530, grad_norm: 0.713770
[[34m2025-10-04 12:10:59[0m] Step: 2575, Training Logs: loss_final: 0.932559, loss_mean: 0.885215, loss_mean_cls: 0.047344, grad_norm: 0.384705
[[34m2025-10-04 12:11:00[0m] Step: 2576, Training Logs: loss_final: 0.923547, loss_mean: 0.876062, loss_mean_cls: 0.047485, grad_norm: 0.710863
[[34m2025-10-04 12:11:00[0m] Step: 2577, Training Logs: loss_final: 0.905062, loss_mean: 0.859329, loss_mean_cls: 0.045733, grad_norm: 0.449658
[[34m2025-10-04 12:11:00[0m] Step: 2578, Training Logs: loss_final: 0.918747, loss_mean: 0.872312, loss_mean_cls: 0.046435, grad_norm: 0.586184
[[34m2025-10-04 12:11:01[0m] Step: 2579, Training Logs: loss_final: 0.916463, loss_mean: 0.868274, loss_mean_cls: 0.048189, grad_norm: 0.515845
[[34m2025-10-04 12:11:01[0m] Step: 2580, Training Logs: loss_final: 0.929696, loss_mean: 0.884327, loss_mean_cls: 0.045369, grad_norm: 0.445004
[[34m2025-10-04 12:11:01[0m] Step: 2581, Training Logs: loss_final: 0.906690, loss_mean: 0.858738, loss_mean_cls: 0.047952, grad_norm: 0.714751
[[34m2025-10-04 12:11:01[0m] Step: 2582, Training Logs: loss_final: 0.911876, loss_mean: 0.864752, loss_mean_cls: 0.047123, grad_norm: 0.493230
[[34m2025-10-04 12:11:02[0m] Step: 2583, Training Logs: loss_final: 0.910431, loss_mean: 0.862389, loss_mean_cls: 0.048042, grad_norm: 0.582601
[[34m2025-10-04 12:11:02[0m] Step: 2584, Training Logs: loss_final: 0.913992, loss_mean: 0.866489, loss_mean_cls: 0.047503, grad_norm: 0.624196
[[34m2025-10-04 12:11:02[0m] Step: 2585, Training Logs: loss_final: 0.921886, loss_mean: 0.875154, loss_mean_cls: 0.046732, grad_norm: 0.523325
[[34m2025-10-04 12:11:03[0m] Step: 2586, Training Logs: loss_final: 0.913639, loss_mean: 0.866005, loss_mean_cls: 0.047633, grad_norm: 0.584794
[[34m2025-10-04 12:11:03[0m] Step: 2587, Training Logs: loss_final: 0.894745, loss_mean: 0.847084, loss_mean_cls: 0.047661, grad_norm: 0.763711
[[34m2025-10-04 12:11:03[0m] Step: 2588, Training Logs: loss_final: 0.916376, loss_mean: 0.868846, loss_mean_cls: 0.047530, grad_norm: 0.535773
[[34m2025-10-04 12:11:03[0m] Step: 2589, Training Logs: loss_final: 0.907878, loss_mean: 0.860436, loss_mean_cls: 0.047442, grad_norm: 0.558130
[[34m2025-10-04 12:11:04[0m] Step: 2590, Training Logs: loss_final: 0.923942, loss_mean: 0.877396, loss_mean_cls: 0.046546, grad_norm: 0.555139
[[34m2025-10-04 12:11:04[0m] Step: 2591, Training Logs: loss_final: 0.908974, loss_mean: 0.860768, loss_mean_cls: 0.048206, grad_norm: 0.690721
[[34m2025-10-04 12:11:04[0m] Step: 2592, Training Logs: loss_final: 0.914956, loss_mean: 0.867956, loss_mean_cls: 0.047000, grad_norm: 0.592182
[[34m2025-10-04 12:11:05[0m] Step: 2593, Training Logs: loss_final: 0.920811, loss_mean: 0.874272, loss_mean_cls: 0.046538, grad_norm: 0.638440
[[34m2025-10-04 12:11:05[0m] Step: 2594, Training Logs: loss_final: 0.890153, loss_mean: 0.842119, loss_mean_cls: 0.048034, grad_norm: 0.818231
[[34m2025-10-04 12:11:05[0m] Step: 2595, Training Logs: loss_final: 0.912828, loss_mean: 0.864627, loss_mean_cls: 0.048201, grad_norm: 0.499677
[[34m2025-10-04 12:11:05[0m] Step: 2596, Training Logs: loss_final: 0.921303, loss_mean: 0.874350, loss_mean_cls: 0.046953, grad_norm: 0.746891
[[34m2025-10-04 12:11:06[0m] Step: 2597, Training Logs: loss_final: 0.902481, loss_mean: 0.855595, loss_mean_cls: 0.046887, grad_norm: 0.642813
[[34m2025-10-04 12:11:06[0m] Step: 2598, Training Logs: loss_final: 0.923230, loss_mean: 0.876293, loss_mean_cls: 0.046937, grad_norm: 0.617753
[[34m2025-10-04 12:11:06[0m] Step: 2599, Training Logs: loss_final: 0.926021, loss_mean: 0.879127, loss_mean_cls: 0.046894, grad_norm: 0.898088
[[34m2025-10-04 12:11:07[0m] Step: 2600, Training Logs: loss_final: 0.913316, loss_mean: 0.865779, loss_mean_cls: 0.047537, grad_norm: 0.560466
[[34m2025-10-04 12:11:07[0m] Step: 2601, Training Logs: loss_final: 0.915764, loss_mean: 0.868616, loss_mean_cls: 0.047148, grad_norm: 0.755234
[[34m2025-10-04 12:11:07[0m] Step: 2602, Training Logs: loss_final: 0.925246, loss_mean: 0.877856, loss_mean_cls: 0.047390, grad_norm: 0.779854
[[34m2025-10-04 12:11:08[0m] Step: 2603, Training Logs: loss_final: 0.925164, loss_mean: 0.878421, loss_mean_cls: 0.046743, grad_norm: 0.417341
[[34m2025-10-04 12:11:08[0m] Step: 2604, Training Logs: loss_final: 0.897310, loss_mean: 0.849101, loss_mean_cls: 0.048209, grad_norm: 0.642956
[[34m2025-10-04 12:11:08[0m] Step: 2605, Training Logs: loss_final: 0.902710, loss_mean: 0.855655, loss_mean_cls: 0.047055, grad_norm: 0.672641
[[34m2025-10-04 12:11:08[0m] Step: 2606, Training Logs: loss_final: 0.932670, loss_mean: 0.885370, loss_mean_cls: 0.047300, grad_norm: 0.373134
[[34m2025-10-04 12:11:09[0m] Step: 2607, Training Logs: loss_final: 0.926486, loss_mean: 0.879644, loss_mean_cls: 0.046842, grad_norm: 0.558556
[[34m2025-10-04 12:11:09[0m] Step: 2608, Training Logs: loss_final: 0.898860, loss_mean: 0.852912, loss_mean_cls: 0.045947, grad_norm: 0.420405
[[34m2025-10-04 12:11:09[0m] Step: 2609, Training Logs: loss_final: 0.927002, loss_mean: 0.880058, loss_mean_cls: 0.046944, grad_norm: 0.636258
[[34m2025-10-04 12:11:10[0m] Step: 2610, Training Logs: loss_final: 0.914132, loss_mean: 0.867905, loss_mean_cls: 0.046227, grad_norm: 0.681476
[[34m2025-10-04 12:11:10[0m] Step: 2611, Training Logs: loss_final: 0.916100, loss_mean: 0.868443, loss_mean_cls: 0.047657, grad_norm: 0.487499
[[34m2025-10-04 12:11:10[0m] Step: 2612, Training Logs: loss_final: 0.926455, loss_mean: 0.879208, loss_mean_cls: 0.047247, grad_norm: 0.764229
[[34m2025-10-04 12:11:11[0m] Step: 2613, Training Logs: loss_final: 0.910931, loss_mean: 0.863720, loss_mean_cls: 0.047211, grad_norm: 0.509021
[[34m2025-10-04 12:11:11[0m] Step: 2614, Training Logs: loss_final: 0.913298, loss_mean: 0.865472, loss_mean_cls: 0.047826, grad_norm: 0.775728
[[34m2025-10-04 12:11:11[0m] Step: 2615, Training Logs: loss_final: 0.904908, loss_mean: 0.858142, loss_mean_cls: 0.046766, grad_norm: 0.561447
[[34m2025-10-04 12:11:11[0m] Step: 2616, Training Logs: loss_final: 0.917079, loss_mean: 0.870589, loss_mean_cls: 0.046490, grad_norm: 0.787880
[[34m2025-10-04 12:11:12[0m] Step: 2617, Training Logs: loss_final: 0.911644, loss_mean: 0.863506, loss_mean_cls: 0.048138, grad_norm: 0.529993
[[34m2025-10-04 12:11:12[0m] Step: 2618, Training Logs: loss_final: 0.885785, loss_mean: 0.838255, loss_mean_cls: 0.047530, grad_norm: 0.614666
[[34m2025-10-04 12:11:12[0m] Step: 2619, Training Logs: loss_final: 0.923496, loss_mean: 0.876548, loss_mean_cls: 0.046948, grad_norm: 0.583331
[[34m2025-10-04 12:11:13[0m] Step: 2620, Training Logs: loss_final: 0.899238, loss_mean: 0.851715, loss_mean_cls: 0.047523, grad_norm: 0.401691
[[34m2025-10-04 12:11:13[0m] Step: 2621, Training Logs: loss_final: 0.914561, loss_mean: 0.868002, loss_mean_cls: 0.046559, grad_norm: 0.636758
[[34m2025-10-04 12:11:13[0m] Step: 2622, Training Logs: loss_final: 0.905724, loss_mean: 0.857675, loss_mean_cls: 0.048049, grad_norm: 0.513599
[[34m2025-10-04 12:11:14[0m] Step: 2623, Training Logs: loss_final: 0.908318, loss_mean: 0.861069, loss_mean_cls: 0.047249, grad_norm: 0.388889
[[34m2025-10-04 12:11:14[0m] Step: 2624, Training Logs: loss_final: 0.928320, loss_mean: 0.882434, loss_mean_cls: 0.045886, grad_norm: 0.579233
[[34m2025-10-04 12:11:14[0m] Step: 2625, Training Logs: loss_final: 0.909273, loss_mean: 0.861067, loss_mean_cls: 0.048206, grad_norm: 0.407711
[[34m2025-10-04 12:11:14[0m] Step: 2626, Training Logs: loss_final: 0.896688, loss_mean: 0.850326, loss_mean_cls: 0.046362, grad_norm: 0.513922
[[34m2025-10-04 12:11:15[0m] Step: 2627, Training Logs: loss_final: 0.917507, loss_mean: 0.870578, loss_mean_cls: 0.046928, grad_norm: 0.482353
[[34m2025-10-04 12:11:15[0m] Step: 2628, Training Logs: loss_final: 0.932257, loss_mean: 0.885514, loss_mean_cls: 0.046743, grad_norm: 0.474225
[[34m2025-10-04 12:11:15[0m] Step: 2629, Training Logs: loss_final: 0.886431, loss_mean: 0.839319, loss_mean_cls: 0.047113, grad_norm: 0.431501
[[34m2025-10-04 12:11:16[0m] Step: 2630, Training Logs: loss_final: 0.933015, loss_mean: 0.885480, loss_mean_cls: 0.047535, grad_norm: 0.579731
[[34m2025-10-04 12:11:16[0m] Step: 2631, Training Logs: loss_final: 0.909154, loss_mean: 0.862502, loss_mean_cls: 0.046652, grad_norm: 0.576477
[[34m2025-10-04 12:11:16[0m] Step: 2632, Training Logs: loss_final: 0.910923, loss_mean: 0.863533, loss_mean_cls: 0.047391, grad_norm: 0.411465
[[34m2025-10-04 12:11:16[0m] Step: 2633, Training Logs: loss_final: 0.924510, loss_mean: 0.876737, loss_mean_cls: 0.047773, grad_norm: 0.590082
[[34m2025-10-04 12:11:17[0m] Step: 2634, Training Logs: loss_final: 0.912505, loss_mean: 0.865430, loss_mean_cls: 0.047075, grad_norm: 0.493639
[[34m2025-10-04 12:11:17[0m] Step: 2635, Training Logs: loss_final: 0.911568, loss_mean: 0.863426, loss_mean_cls: 0.048142, grad_norm: 0.450103
[[34m2025-10-04 12:11:17[0m] Step: 2636, Training Logs: loss_final: 0.921975, loss_mean: 0.874105, loss_mean_cls: 0.047870, grad_norm: 0.641590
[[34m2025-10-04 12:11:18[0m] Step: 2637, Training Logs: loss_final: 0.890609, loss_mean: 0.842860, loss_mean_cls: 0.047749, grad_norm: 0.707979
[[34m2025-10-04 12:11:18[0m] Step: 2638, Training Logs: loss_final: 0.904210, loss_mean: 0.857758, loss_mean_cls: 0.046452, grad_norm: 0.555079
[[34m2025-10-04 12:11:18[0m] Step: 2639, Training Logs: loss_final: 0.915585, loss_mean: 0.869700, loss_mean_cls: 0.045885, grad_norm: 0.659947
[[34m2025-10-04 12:11:19[0m] Step: 2640, Training Logs: loss_final: 0.916531, loss_mean: 0.869299, loss_mean_cls: 0.047232, grad_norm: 0.580700
[[34m2025-10-04 12:11:19[0m] Step: 2641, Training Logs: loss_final: 0.913852, loss_mean: 0.867961, loss_mean_cls: 0.045892, grad_norm: 0.429663
[[34m2025-10-04 12:11:19[0m] Step: 2642, Training Logs: loss_final: 0.889292, loss_mean: 0.841041, loss_mean_cls: 0.048252, grad_norm: 0.434852
[[34m2025-10-04 12:11:19[0m] Step: 2643, Training Logs: loss_final: 0.920159, loss_mean: 0.873408, loss_mean_cls: 0.046751, grad_norm: 0.469862
[[34m2025-10-04 12:11:20[0m] Step: 2644, Training Logs: loss_final: 0.917418, loss_mean: 0.870027, loss_mean_cls: 0.047392, grad_norm: 0.424734
[[34m2025-10-04 12:11:20[0m] Step: 2645, Training Logs: loss_final: 0.903379, loss_mean: 0.855532, loss_mean_cls: 0.047847, grad_norm: 0.449387
[[34m2025-10-04 12:11:20[0m] Step: 2646, Training Logs: loss_final: 0.899648, loss_mean: 0.852301, loss_mean_cls: 0.047347, grad_norm: 0.542854
[[34m2025-10-04 12:11:21[0m] Step: 2647, Training Logs: loss_final: 0.928384, loss_mean: 0.880455, loss_mean_cls: 0.047929, grad_norm: 0.494720
[[34m2025-10-04 12:11:21[0m] Step: 2648, Training Logs: loss_final: 0.909517, loss_mean: 0.862807, loss_mean_cls: 0.046709, grad_norm: 0.429220
[[34m2025-10-04 12:11:21[0m] Step: 2649, Training Logs: loss_final: 0.916891, loss_mean: 0.870021, loss_mean_cls: 0.046870, grad_norm: 0.623546
[[34m2025-10-04 12:11:21[0m] Step: 2650, Training Logs: loss_final: 0.903967, loss_mean: 0.856558, loss_mean_cls: 0.047408, grad_norm: 0.400883
[[34m2025-10-04 12:11:22[0m] Step: 2651, Training Logs: loss_final: 0.908020, loss_mean: 0.860604, loss_mean_cls: 0.047416, grad_norm: 0.426548
[[34m2025-10-04 12:11:22[0m] Step: 2652, Training Logs: loss_final: 0.905439, loss_mean: 0.859488, loss_mean_cls: 0.045951, grad_norm: 0.535497
[[34m2025-10-04 12:11:22[0m] Step: 2653, Training Logs: loss_final: 0.904129, loss_mean: 0.856724, loss_mean_cls: 0.047405, grad_norm: 0.541184
[[34m2025-10-04 12:11:23[0m] Step: 2654, Training Logs: loss_final: 0.928459, loss_mean: 0.881663, loss_mean_cls: 0.046796, grad_norm: 0.428976
[[34m2025-10-04 12:11:23[0m] Step: 2655, Training Logs: loss_final: 0.902130, loss_mean: 0.855045, loss_mean_cls: 0.047084, grad_norm: 0.561329
[[34m2025-10-04 12:11:23[0m] Step: 2656, Training Logs: loss_final: 0.908926, loss_mean: 0.862704, loss_mean_cls: 0.046222, grad_norm: 0.607445
[[34m2025-10-04 12:11:23[0m] Step: 2657, Training Logs: loss_final: 0.908289, loss_mean: 0.861616, loss_mean_cls: 0.046674, grad_norm: 0.570029
[[34m2025-10-04 12:11:24[0m] Step: 2658, Training Logs: loss_final: 0.914137, loss_mean: 0.867180, loss_mean_cls: 0.046957, grad_norm: 0.806450
[[34m2025-10-04 12:11:24[0m] Step: 2659, Training Logs: loss_final: 0.906674, loss_mean: 0.858497, loss_mean_cls: 0.048176, grad_norm: 0.694884
[[34m2025-10-04 12:11:24[0m] Step: 2660, Training Logs: loss_final: 0.908003, loss_mean: 0.859419, loss_mean_cls: 0.048584, grad_norm: 0.767969
[[34m2025-10-04 12:11:25[0m] Step: 2661, Training Logs: loss_final: 0.888267, loss_mean: 0.840870, loss_mean_cls: 0.047397, grad_norm: 0.513431
[[34m2025-10-04 12:11:25[0m] Step: 2662, Training Logs: loss_final: 0.909162, loss_mean: 0.861903, loss_mean_cls: 0.047259, grad_norm: 0.653426
[[34m2025-10-04 12:11:25[0m] Step: 2663, Training Logs: loss_final: 0.922675, loss_mean: 0.876082, loss_mean_cls: 0.046593, grad_norm: 0.616727
[[34m2025-10-04 12:11:26[0m] Step: 2664, Training Logs: loss_final: 0.901838, loss_mean: 0.854989, loss_mean_cls: 0.046849, grad_norm: 0.465898
[[34m2025-10-04 12:11:26[0m] Step: 2665, Training Logs: loss_final: 0.913459, loss_mean: 0.867074, loss_mean_cls: 0.046385, grad_norm: 0.620505
[[34m2025-10-04 12:11:26[0m] Step: 2666, Training Logs: loss_final: 0.895932, loss_mean: 0.847402, loss_mean_cls: 0.048530, grad_norm: 0.477731
[[34m2025-10-04 12:11:26[0m] Step: 2667, Training Logs: loss_final: 0.910272, loss_mean: 0.863322, loss_mean_cls: 0.046950, grad_norm: 0.631578
[[34m2025-10-04 12:11:27[0m] Step: 2668, Training Logs: loss_final: 0.917098, loss_mean: 0.870735, loss_mean_cls: 0.046363, grad_norm: 0.437616
[[34m2025-10-04 12:11:27[0m] Step: 2669, Training Logs: loss_final: 0.901661, loss_mean: 0.853412, loss_mean_cls: 0.048249, grad_norm: 0.642681
[[34m2025-10-04 12:11:27[0m] Step: 2670, Training Logs: loss_final: 0.903584, loss_mean: 0.855975, loss_mean_cls: 0.047608, grad_norm: 0.559349
[[34m2025-10-04 12:11:28[0m] Step: 2671, Training Logs: loss_final: 0.899345, loss_mean: 0.852531, loss_mean_cls: 0.046814, grad_norm: 0.448445
[[34m2025-10-04 12:11:28[0m] Step: 2672, Training Logs: loss_final: 0.937432, loss_mean: 0.890856, loss_mean_cls: 0.046576, grad_norm: 0.565467
[[34m2025-10-04 12:11:28[0m] Step: 2673, Training Logs: loss_final: 0.904520, loss_mean: 0.856797, loss_mean_cls: 0.047723, grad_norm: 0.598298
[[34m2025-10-04 12:11:29[0m] Step: 2674, Training Logs: loss_final: 0.911819, loss_mean: 0.863867, loss_mean_cls: 0.047953, grad_norm: 0.572182
[[34m2025-10-04 12:11:29[0m] Step: 2675, Training Logs: loss_final: 0.921074, loss_mean: 0.874494, loss_mean_cls: 0.046580, grad_norm: 0.646629
[[34m2025-10-04 12:11:29[0m] Step: 2676, Training Logs: loss_final: 0.924502, loss_mean: 0.877499, loss_mean_cls: 0.047003, grad_norm: 0.616621
[[34m2025-10-04 12:11:29[0m] Step: 2677, Training Logs: loss_final: 0.884266, loss_mean: 0.836659, loss_mean_cls: 0.047607, grad_norm: 0.569300
[[34m2025-10-04 12:11:30[0m] Step: 2678, Training Logs: loss_final: 0.913921, loss_mean: 0.867000, loss_mean_cls: 0.046921, grad_norm: 0.570040
[[34m2025-10-04 12:11:30[0m] Step: 2679, Training Logs: loss_final: 0.911779, loss_mean: 0.864478, loss_mean_cls: 0.047301, grad_norm: 0.465225
[[34m2025-10-04 12:11:30[0m] Step: 2680, Training Logs: loss_final: 0.908675, loss_mean: 0.862214, loss_mean_cls: 0.046461, grad_norm: 0.506396
[[34m2025-10-04 12:11:31[0m] Step: 2681, Training Logs: loss_final: 0.903101, loss_mean: 0.856119, loss_mean_cls: 0.046982, grad_norm: 0.371252
[[34m2025-10-04 12:11:31[0m] Step: 2682, Training Logs: loss_final: 0.939418, loss_mean: 0.893633, loss_mean_cls: 0.045786, grad_norm: 0.533491
[[34m2025-10-04 12:11:31[0m] Step: 2683, Training Logs: loss_final: 0.906093, loss_mean: 0.859124, loss_mean_cls: 0.046969, grad_norm: 0.747928
[[34m2025-10-04 12:11:31[0m] Step: 2684, Training Logs: loss_final: 0.917875, loss_mean: 0.870693, loss_mean_cls: 0.047182, grad_norm: 0.411695
[[34m2025-10-04 12:11:32[0m] Step: 2685, Training Logs: loss_final: 0.907978, loss_mean: 0.860345, loss_mean_cls: 0.047633, grad_norm: 0.797541
[[34m2025-10-04 12:11:32[0m] Step: 2686, Training Logs: loss_final: 0.912486, loss_mean: 0.866224, loss_mean_cls: 0.046262, grad_norm: 0.455689
[[34m2025-10-04 12:11:32[0m] Step: 2687, Training Logs: loss_final: 0.896560, loss_mean: 0.848610, loss_mean_cls: 0.047951, grad_norm: 0.645315
[[34m2025-10-04 12:11:33[0m] Step: 2688, Training Logs: loss_final: 0.922595, loss_mean: 0.876416, loss_mean_cls: 0.046179, grad_norm: 0.526744
[[34m2025-10-04 12:11:33[0m] Step: 2689, Training Logs: loss_final: 0.906774, loss_mean: 0.860153, loss_mean_cls: 0.046622, grad_norm: 0.597191
[[34m2025-10-04 12:11:33[0m] Step: 2690, Training Logs: loss_final: 0.934264, loss_mean: 0.887648, loss_mean_cls: 0.046616, grad_norm: 0.551434
[[34m2025-10-04 12:11:34[0m] Step: 2691, Training Logs: loss_final: 0.908612, loss_mean: 0.861666, loss_mean_cls: 0.046946, grad_norm: 0.515632
[[34m2025-10-04 12:11:34[0m] Step: 2692, Training Logs: loss_final: 0.901837, loss_mean: 0.855807, loss_mean_cls: 0.046030, grad_norm: 0.742450
[[34m2025-10-04 12:11:34[0m] Step: 2693, Training Logs: loss_final: 0.915736, loss_mean: 0.869794, loss_mean_cls: 0.045943, grad_norm: 0.406962
[[34m2025-10-04 12:11:34[0m] Step: 2694, Training Logs: loss_final: 0.907669, loss_mean: 0.860077, loss_mean_cls: 0.047592, grad_norm: 0.668224
[[34m2025-10-04 12:11:35[0m] Step: 2695, Training Logs: loss_final: 0.932897, loss_mean: 0.885572, loss_mean_cls: 0.047324, grad_norm: 0.470787
[[34m2025-10-04 12:11:35[0m] Step: 2696, Training Logs: loss_final: 0.941978, loss_mean: 0.894566, loss_mean_cls: 0.047413, grad_norm: 0.539332
[[34m2025-10-04 12:11:35[0m] Step: 2697, Training Logs: loss_final: 0.912705, loss_mean: 0.865786, loss_mean_cls: 0.046919, grad_norm: 0.618900
[[34m2025-10-04 12:11:36[0m] Step: 2698, Training Logs: loss_final: 0.905440, loss_mean: 0.856728, loss_mean_cls: 0.048713, grad_norm: 0.459363
[[34m2025-10-04 12:11:36[0m] Step: 2699, Training Logs: loss_final: 0.904756, loss_mean: 0.856752, loss_mean_cls: 0.048004, grad_norm: 0.594244
[[34m2025-10-04 12:11:36[0m] Step: 2700, Training Logs: loss_final: 0.897486, loss_mean: 0.852077, loss_mean_cls: 0.045409, grad_norm: 0.428049
[[34m2025-10-04 12:11:36[0m] Step: 2701, Training Logs: loss_final: 0.920022, loss_mean: 0.872511, loss_mean_cls: 0.047510, grad_norm: 0.447141
[[34m2025-10-04 12:11:37[0m] Step: 2702, Training Logs: loss_final: 0.910121, loss_mean: 0.863000, loss_mean_cls: 0.047121, grad_norm: 0.452564
[[34m2025-10-04 12:11:37[0m] Step: 2703, Training Logs: loss_final: 0.915244, loss_mean: 0.869737, loss_mean_cls: 0.045507, grad_norm: 0.364905
[[34m2025-10-04 12:11:37[0m] Step: 2704, Training Logs: loss_final: 0.903384, loss_mean: 0.856080, loss_mean_cls: 0.047305, grad_norm: 0.362517
[[34m2025-10-04 12:11:38[0m] Step: 2705, Training Logs: loss_final: 0.910917, loss_mean: 0.862512, loss_mean_cls: 0.048405, grad_norm: 0.347857
[[34m2025-10-04 12:11:38[0m] Step: 2706, Training Logs: loss_final: 0.899447, loss_mean: 0.852206, loss_mean_cls: 0.047241, grad_norm: 0.429551
[[34m2025-10-04 12:11:38[0m] Step: 2707, Training Logs: loss_final: 0.880062, loss_mean: 0.831823, loss_mean_cls: 0.048239, grad_norm: 0.404193
[[34m2025-10-04 12:11:39[0m] Step: 2708, Training Logs: loss_final: 0.897427, loss_mean: 0.850406, loss_mean_cls: 0.047021, grad_norm: 0.669966
[[34m2025-10-04 12:11:39[0m] Step: 2709, Training Logs: loss_final: 0.923394, loss_mean: 0.877298, loss_mean_cls: 0.046096, grad_norm: 0.351441
[[34m2025-10-04 12:11:39[0m] Step: 2710, Training Logs: loss_final: 0.905003, loss_mean: 0.858603, loss_mean_cls: 0.046400, grad_norm: 0.568106
[[34m2025-10-04 12:11:39[0m] Step: 2711, Training Logs: loss_final: 0.891500, loss_mean: 0.844090, loss_mean_cls: 0.047411, grad_norm: 0.355531
[[34m2025-10-04 12:11:40[0m] Step: 2712, Training Logs: loss_final: 0.906627, loss_mean: 0.859395, loss_mean_cls: 0.047233, grad_norm: 0.492985
[[34m2025-10-04 12:11:40[0m] Step: 2713, Training Logs: loss_final: 0.916048, loss_mean: 0.870093, loss_mean_cls: 0.045955, grad_norm: 0.482244
[[34m2025-10-04 12:11:40[0m] Step: 2714, Training Logs: loss_final: 0.904204, loss_mean: 0.855645, loss_mean_cls: 0.048558, grad_norm: 0.372069
[[34m2025-10-04 12:11:41[0m] Step: 2715, Training Logs: loss_final: 0.894077, loss_mean: 0.846997, loss_mean_cls: 0.047080, grad_norm: 0.482798
[[34m2025-10-04 12:11:41[0m] Step: 2716, Training Logs: loss_final: 0.913159, loss_mean: 0.866773, loss_mean_cls: 0.046386, grad_norm: 0.490936
[[34m2025-10-04 12:11:41[0m] Step: 2717, Training Logs: loss_final: 0.921791, loss_mean: 0.875678, loss_mean_cls: 0.046113, grad_norm: 0.458869
[[34m2025-10-04 12:11:41[0m] Step: 2718, Training Logs: loss_final: 0.898222, loss_mean: 0.851803, loss_mean_cls: 0.046419, grad_norm: 0.460003
[[34m2025-10-04 12:11:42[0m] Step: 2719, Training Logs: loss_final: 0.892899, loss_mean: 0.844566, loss_mean_cls: 0.048333, grad_norm: 0.478632
[[34m2025-10-04 12:11:42[0m] Step: 2720, Training Logs: loss_final: 0.912269, loss_mean: 0.866546, loss_mean_cls: 0.045723, grad_norm: 0.449750
[[34m2025-10-04 12:11:42[0m] Step: 2721, Training Logs: loss_final: 0.927385, loss_mean: 0.881029, loss_mean_cls: 0.046356, grad_norm: 0.520193
[[34m2025-10-04 12:11:43[0m] Step: 2722, Training Logs: loss_final: 0.905473, loss_mean: 0.859506, loss_mean_cls: 0.045967, grad_norm: 0.564873
[[34m2025-10-04 12:11:43[0m] Step: 2723, Training Logs: loss_final: 0.920501, loss_mean: 0.873820, loss_mean_cls: 0.046680, grad_norm: 0.491679
[[34m2025-10-04 12:11:43[0m] Step: 2724, Training Logs: loss_final: 0.899183, loss_mean: 0.852904, loss_mean_cls: 0.046279, grad_norm: 0.435933
[[34m2025-10-04 12:11:43[0m] Step: 2725, Training Logs: loss_final: 0.900383, loss_mean: 0.853460, loss_mean_cls: 0.046923, grad_norm: 0.567097
[[34m2025-10-04 12:11:44[0m] Step: 2726, Training Logs: loss_final: 0.909328, loss_mean: 0.862877, loss_mean_cls: 0.046451, grad_norm: 0.547281
[[34m2025-10-04 12:11:44[0m] Step: 2727, Training Logs: loss_final: 0.923082, loss_mean: 0.876057, loss_mean_cls: 0.047026, grad_norm: 0.504117
[[34m2025-10-04 12:11:44[0m] Step: 2728, Training Logs: loss_final: 0.937588, loss_mean: 0.891176, loss_mean_cls: 0.046412, grad_norm: 0.426889
[[34m2025-10-04 12:11:45[0m] Step: 2729, Training Logs: loss_final: 0.930924, loss_mean: 0.883226, loss_mean_cls: 0.047698, grad_norm: 0.535398
[[34m2025-10-04 12:11:45[0m] Step: 2730, Training Logs: loss_final: 0.912622, loss_mean: 0.865072, loss_mean_cls: 0.047550, grad_norm: 0.564065
[[34m2025-10-04 12:11:45[0m] Step: 2731, Training Logs: loss_final: 0.916025, loss_mean: 0.869316, loss_mean_cls: 0.046709, grad_norm: 0.404014
[[34m2025-10-04 12:11:46[0m] Step: 2732, Training Logs: loss_final: 0.912913, loss_mean: 0.865512, loss_mean_cls: 0.047401, grad_norm: 0.521987
[[34m2025-10-04 12:11:46[0m] Step: 2733, Training Logs: loss_final: 0.940700, loss_mean: 0.895077, loss_mean_cls: 0.045622, grad_norm: 0.351453
[[34m2025-10-04 12:11:46[0m] Step: 2734, Training Logs: loss_final: 0.895340, loss_mean: 0.847990, loss_mean_cls: 0.047351, grad_norm: 0.551454
[[34m2025-10-04 12:11:46[0m] Step: 2735, Training Logs: loss_final: 0.900171, loss_mean: 0.853342, loss_mean_cls: 0.046829, grad_norm: 0.520464
[[34m2025-10-04 12:11:47[0m] Step: 2736, Training Logs: loss_final: 0.915892, loss_mean: 0.868446, loss_mean_cls: 0.047446, grad_norm: 0.478586
[[34m2025-10-04 12:11:47[0m] Step: 2737, Training Logs: loss_final: 0.888403, loss_mean: 0.839807, loss_mean_cls: 0.048596, grad_norm: 0.614550
[[34m2025-10-04 12:11:47[0m] Step: 2738, Training Logs: loss_final: 0.897510, loss_mean: 0.849658, loss_mean_cls: 0.047852, grad_norm: 0.421041
[[34m2025-10-04 12:11:48[0m] Step: 2739, Training Logs: loss_final: 0.888479, loss_mean: 0.841987, loss_mean_cls: 0.046493, grad_norm: 0.579907
[[34m2025-10-04 12:11:48[0m] Step: 2740, Training Logs: loss_final: 0.935652, loss_mean: 0.889174, loss_mean_cls: 0.046478, grad_norm: 0.640976
[[34m2025-10-04 12:11:48[0m] Step: 2741, Training Logs: loss_final: 0.920699, loss_mean: 0.875140, loss_mean_cls: 0.045559, grad_norm: 0.407756
[[34m2025-10-04 12:11:48[0m] Step: 2742, Training Logs: loss_final: 0.904795, loss_mean: 0.857287, loss_mean_cls: 0.047508, grad_norm: 0.646380
[[34m2025-10-04 12:11:49[0m] Step: 2743, Training Logs: loss_final: 0.917179, loss_mean: 0.870088, loss_mean_cls: 0.047091, grad_norm: 0.504051
[[34m2025-10-04 12:11:49[0m] Step: 2744, Training Logs: loss_final: 0.919016, loss_mean: 0.871837, loss_mean_cls: 0.047179, grad_norm: 0.407702
[[34m2025-10-04 12:11:49[0m] Step: 2745, Training Logs: loss_final: 0.938227, loss_mean: 0.891178, loss_mean_cls: 0.047049, grad_norm: 0.515391
[[34m2025-10-04 12:11:50[0m] Step: 2746, Training Logs: loss_final: 0.918546, loss_mean: 0.872312, loss_mean_cls: 0.046233, grad_norm: 0.455143
[[34m2025-10-04 12:11:50[0m] Step: 2747, Training Logs: loss_final: 0.916806, loss_mean: 0.869712, loss_mean_cls: 0.047094, grad_norm: 0.419358
[[34m2025-10-04 12:11:50[0m] Step: 2748, Training Logs: loss_final: 0.913202, loss_mean: 0.866573, loss_mean_cls: 0.046628, grad_norm: 0.582343
[[34m2025-10-04 12:11:51[0m] Step: 2749, Training Logs: loss_final: 0.900285, loss_mean: 0.853777, loss_mean_cls: 0.046508, grad_norm: 0.602382
[[34m2025-10-04 12:11:51[0m] Step: 2750, Training Logs: loss_final: 0.900974, loss_mean: 0.853488, loss_mean_cls: 0.047486, grad_norm: 0.734234
[[34m2025-10-04 12:11:51[0m] Step: 2751, Training Logs: loss_final: 0.931130, loss_mean: 0.885649, loss_mean_cls: 0.045481, grad_norm: 0.667937
[[34m2025-10-04 12:11:51[0m] Step: 2752, Training Logs: loss_final: 0.916737, loss_mean: 0.870595, loss_mean_cls: 0.046142, grad_norm: 0.751012
[[34m2025-10-04 12:11:52[0m] Step: 2753, Training Logs: loss_final: 0.894745, loss_mean: 0.848890, loss_mean_cls: 0.045855, grad_norm: 0.676361
[[34m2025-10-04 12:11:52[0m] Step: 2754, Training Logs: loss_final: 0.895080, loss_mean: 0.847605, loss_mean_cls: 0.047475, grad_norm: 0.646386
[[34m2025-10-04 12:11:52[0m] Step: 2755, Training Logs: loss_final: 0.894492, loss_mean: 0.846448, loss_mean_cls: 0.048044, grad_norm: 0.453246
[[34m2025-10-04 12:11:53[0m] Step: 2756, Training Logs: loss_final: 0.898762, loss_mean: 0.852415, loss_mean_cls: 0.046348, grad_norm: 0.712473
[[34m2025-10-04 12:11:53[0m] Step: 2757, Training Logs: loss_final: 0.931024, loss_mean: 0.884963, loss_mean_cls: 0.046061, grad_norm: 0.737237
[[34m2025-10-04 12:11:53[0m] Step: 2758, Training Logs: loss_final: 0.907018, loss_mean: 0.860903, loss_mean_cls: 0.046115, grad_norm: 0.418697
[[34m2025-10-04 12:11:53[0m] Step: 2759, Training Logs: loss_final: 0.909918, loss_mean: 0.862605, loss_mean_cls: 0.047313, grad_norm: 0.843101
[[34m2025-10-04 12:11:54[0m] Step: 2760, Training Logs: loss_final: 0.880825, loss_mean: 0.832746, loss_mean_cls: 0.048079, grad_norm: 0.579502
[[34m2025-10-04 12:11:54[0m] Step: 2761, Training Logs: loss_final: 0.908185, loss_mean: 0.862136, loss_mean_cls: 0.046049, grad_norm: 0.614264
[[34m2025-10-04 12:11:54[0m] Step: 2762, Training Logs: loss_final: 0.883448, loss_mean: 0.835109, loss_mean_cls: 0.048339, grad_norm: 0.667403
[[34m2025-10-04 12:11:55[0m] Step: 2763, Training Logs: loss_final: 0.900320, loss_mean: 0.854423, loss_mean_cls: 0.045897, grad_norm: 0.487700
[[34m2025-10-04 12:11:55[0m] Step: 2764, Training Logs: loss_final: 0.913316, loss_mean: 0.865094, loss_mean_cls: 0.048223, grad_norm: 0.744606
[[34m2025-10-04 12:11:55[0m] Step: 2765, Training Logs: loss_final: 0.899646, loss_mean: 0.852571, loss_mean_cls: 0.047075, grad_norm: 0.671449
[[34m2025-10-04 12:11:56[0m] Step: 2766, Training Logs: loss_final: 0.918367, loss_mean: 0.871258, loss_mean_cls: 0.047110, grad_norm: 0.659535
[[34m2025-10-04 12:11:56[0m] Step: 2767, Training Logs: loss_final: 0.934006, loss_mean: 0.886377, loss_mean_cls: 0.047629, grad_norm: 0.713709
[[34m2025-10-04 12:11:56[0m] Step: 2768, Training Logs: loss_final: 0.912307, loss_mean: 0.864769, loss_mean_cls: 0.047538, grad_norm: 0.525443
[[34m2025-10-04 12:11:56[0m] Step: 2769, Training Logs: loss_final: 0.913258, loss_mean: 0.868127, loss_mean_cls: 0.045131, grad_norm: 0.560723
[[34m2025-10-04 12:11:57[0m] Step: 2770, Training Logs: loss_final: 0.913477, loss_mean: 0.866824, loss_mean_cls: 0.046653, grad_norm: 0.532339
[[34m2025-10-04 12:11:57[0m] Step: 2771, Training Logs: loss_final: 0.886885, loss_mean: 0.839725, loss_mean_cls: 0.047161, grad_norm: 0.518286
[[34m2025-10-04 12:11:57[0m] Step: 2772, Training Logs: loss_final: 0.902762, loss_mean: 0.855678, loss_mean_cls: 0.047083, grad_norm: 0.581322
[[34m2025-10-04 12:11:58[0m] Step: 2773, Training Logs: loss_final: 0.916192, loss_mean: 0.870027, loss_mean_cls: 0.046166, grad_norm: 0.395841
[[34m2025-10-04 12:11:58[0m] Step: 2774, Training Logs: loss_final: 0.908217, loss_mean: 0.860852, loss_mean_cls: 0.047364, grad_norm: 0.567320
[[34m2025-10-04 12:11:58[0m] Step: 2775, Training Logs: loss_final: 0.914120, loss_mean: 0.866415, loss_mean_cls: 0.047704, grad_norm: 0.431504
[[34m2025-10-04 12:11:58[0m] Step: 2776, Training Logs: loss_final: 0.915053, loss_mean: 0.868646, loss_mean_cls: 0.046406, grad_norm: 0.471425
[[34m2025-10-04 12:11:59[0m] Step: 2777, Training Logs: loss_final: 0.906831, loss_mean: 0.859107, loss_mean_cls: 0.047725, grad_norm: 0.437247
[[34m2025-10-04 12:11:59[0m] Step: 2778, Training Logs: loss_final: 0.911306, loss_mean: 0.863312, loss_mean_cls: 0.047995, grad_norm: 0.570265
[[34m2025-10-04 12:11:59[0m] Step: 2779, Training Logs: loss_final: 0.913709, loss_mean: 0.867148, loss_mean_cls: 0.046561, grad_norm: 0.478444
[[34m2025-10-04 12:12:00[0m] Step: 2780, Training Logs: loss_final: 0.905802, loss_mean: 0.858073, loss_mean_cls: 0.047729, grad_norm: 0.595476
[[34m2025-10-04 12:12:00[0m] Step: 2781, Training Logs: loss_final: 0.899013, loss_mean: 0.850587, loss_mean_cls: 0.048426, grad_norm: 0.489425
[[34m2025-10-04 12:12:00[0m] Step: 2782, Training Logs: loss_final: 0.909303, loss_mean: 0.862108, loss_mean_cls: 0.047195, grad_norm: 0.358485
[[34m2025-10-04 12:12:01[0m] Step: 2783, Training Logs: loss_final: 0.925008, loss_mean: 0.878157, loss_mean_cls: 0.046851, grad_norm: 0.546545
[[34m2025-10-04 12:12:01[0m] Step: 2784, Training Logs: loss_final: 0.907095, loss_mean: 0.860673, loss_mean_cls: 0.046422, grad_norm: 0.485823
[[34m2025-10-04 12:12:01[0m] Step: 2785, Training Logs: loss_final: 0.886710, loss_mean: 0.840023, loss_mean_cls: 0.046687, grad_norm: 0.384791
[[34m2025-10-04 12:12:01[0m] Step: 2786, Training Logs: loss_final: 0.896051, loss_mean: 0.849539, loss_mean_cls: 0.046512, grad_norm: 0.365336
[[34m2025-10-04 12:12:02[0m] Step: 2787, Training Logs: loss_final: 0.907294, loss_mean: 0.860634, loss_mean_cls: 0.046660, grad_norm: 0.473030
[[34m2025-10-04 12:12:02[0m] Step: 2788, Training Logs: loss_final: 0.911809, loss_mean: 0.864130, loss_mean_cls: 0.047679, grad_norm: 0.423216
[[34m2025-10-04 12:12:02[0m] Step: 2789, Training Logs: loss_final: 0.915687, loss_mean: 0.868647, loss_mean_cls: 0.047040, grad_norm: 0.473437
[[34m2025-10-04 12:12:03[0m] Step: 2790, Training Logs: loss_final: 0.915114, loss_mean: 0.866230, loss_mean_cls: 0.048884, grad_norm: 0.525741
[[34m2025-10-04 12:12:03[0m] Step: 2791, Training Logs: loss_final: 0.895160, loss_mean: 0.848653, loss_mean_cls: 0.046507, grad_norm: 0.456491
[[34m2025-10-04 12:12:03[0m] Step: 2792, Training Logs: loss_final: 0.916134, loss_mean: 0.870102, loss_mean_cls: 0.046032, grad_norm: 0.638023
[[34m2025-10-04 12:12:03[0m] Step: 2793, Training Logs: loss_final: 0.887593, loss_mean: 0.841466, loss_mean_cls: 0.046127, grad_norm: 0.440401
[[34m2025-10-04 12:12:04[0m] Step: 2794, Training Logs: loss_final: 0.914749, loss_mean: 0.868857, loss_mean_cls: 0.045892, grad_norm: 0.555193
[[34m2025-10-04 12:12:04[0m] Step: 2795, Training Logs: loss_final: 0.906669, loss_mean: 0.858831, loss_mean_cls: 0.047838, grad_norm: 0.545384
[[34m2025-10-04 12:12:04[0m] Step: 2796, Training Logs: loss_final: 0.906653, loss_mean: 0.860247, loss_mean_cls: 0.046407, grad_norm: 0.379077
[[34m2025-10-04 12:12:05[0m] Step: 2797, Training Logs: loss_final: 0.915686, loss_mean: 0.869412, loss_mean_cls: 0.046274, grad_norm: 0.535097
[[34m2025-10-04 12:12:05[0m] Step: 2798, Training Logs: loss_final: 0.905438, loss_mean: 0.858885, loss_mean_cls: 0.046553, grad_norm: 0.566699
[[34m2025-10-04 12:12:05[0m] Step: 2799, Training Logs: loss_final: 0.924791, loss_mean: 0.876838, loss_mean_cls: 0.047953, grad_norm: 0.618554
[[34m2025-10-04 12:12:06[0m] Step: 2800, Training Logs: loss_final: 0.924185, loss_mean: 0.878466, loss_mean_cls: 0.045719, grad_norm: 0.449065
[[34m2025-10-04 12:12:06[0m] Step: 2801, Training Logs: loss_final: 0.893590, loss_mean: 0.845861, loss_mean_cls: 0.047730, grad_norm: 0.709357
[[34m2025-10-04 12:12:06[0m] Step: 2802, Training Logs: loss_final: 0.902356, loss_mean: 0.855925, loss_mean_cls: 0.046432, grad_norm: 0.424944
[[34m2025-10-04 12:12:06[0m] Step: 2803, Training Logs: loss_final: 0.912817, loss_mean: 0.865845, loss_mean_cls: 0.046972, grad_norm: 0.469665
[[34m2025-10-04 12:12:07[0m] Step: 2804, Training Logs: loss_final: 0.903682, loss_mean: 0.856627, loss_mean_cls: 0.047056, grad_norm: 0.667826
[[34m2025-10-04 12:12:07[0m] Step: 2805, Training Logs: loss_final: 0.919088, loss_mean: 0.872192, loss_mean_cls: 0.046896, grad_norm: 0.400968
[[34m2025-10-04 12:12:07[0m] Step: 2806, Training Logs: loss_final: 0.913362, loss_mean: 0.866159, loss_mean_cls: 0.047203, grad_norm: 0.750070
[[34m2025-10-04 12:12:08[0m] Step: 2807, Training Logs: loss_final: 0.911098, loss_mean: 0.863208, loss_mean_cls: 0.047890, grad_norm: 0.569794
[[34m2025-10-04 12:12:08[0m] Step: 2808, Training Logs: loss_final: 0.894272, loss_mean: 0.847888, loss_mean_cls: 0.046384, grad_norm: 0.537657
[[34m2025-10-04 12:12:08[0m] Step: 2809, Training Logs: loss_final: 0.889500, loss_mean: 0.841744, loss_mean_cls: 0.047756, grad_norm: 0.660969
[[34m2025-10-04 12:12:09[0m] Step: 2810, Training Logs: loss_final: 0.929079, loss_mean: 0.882883, loss_mean_cls: 0.046196, grad_norm: 0.474551
[[34m2025-10-04 12:12:09[0m] Step: 2811, Training Logs: loss_final: 0.918172, loss_mean: 0.870915, loss_mean_cls: 0.047257, grad_norm: 0.496958
[[34m2025-10-04 12:12:09[0m] Step: 2812, Training Logs: loss_final: 0.902713, loss_mean: 0.855588, loss_mean_cls: 0.047125, grad_norm: 0.580295
[[34m2025-10-04 12:12:09[0m] Step: 2813, Training Logs: loss_final: 0.901086, loss_mean: 0.854289, loss_mean_cls: 0.046797, grad_norm: 0.611512
[[34m2025-10-04 12:12:10[0m] Step: 2814, Training Logs: loss_final: 0.893876, loss_mean: 0.847488, loss_mean_cls: 0.046388, grad_norm: 0.539993
[[34m2025-10-04 12:12:10[0m] Step: 2815, Training Logs: loss_final: 0.915035, loss_mean: 0.870418, loss_mean_cls: 0.044617, grad_norm: 0.710961
[[34m2025-10-04 12:12:10[0m] Step: 2816, Training Logs: loss_final: 0.932524, loss_mean: 0.886528, loss_mean_cls: 0.045997, grad_norm: 0.392862
[[34m2025-10-04 12:12:11[0m] Step: 2817, Training Logs: loss_final: 0.914946, loss_mean: 0.867398, loss_mean_cls: 0.047548, grad_norm: 0.523069
[[34m2025-10-04 12:12:11[0m] Step: 2818, Training Logs: loss_final: 0.913607, loss_mean: 0.867113, loss_mean_cls: 0.046493, grad_norm: 0.523462
[[34m2025-10-04 12:12:11[0m] Step: 2819, Training Logs: loss_final: 0.903670, loss_mean: 0.856996, loss_mean_cls: 0.046674, grad_norm: 0.625012
[[34m2025-10-04 12:12:11[0m] Step: 2820, Training Logs: loss_final: 0.899677, loss_mean: 0.851181, loss_mean_cls: 0.048496, grad_norm: 0.487585
[[34m2025-10-04 12:12:12[0m] Step: 2821, Training Logs: loss_final: 0.907672, loss_mean: 0.860032, loss_mean_cls: 0.047640, grad_norm: 0.543038
[[34m2025-10-04 12:12:12[0m] Step: 2822, Training Logs: loss_final: 0.892974, loss_mean: 0.845316, loss_mean_cls: 0.047658, grad_norm: 0.489074
[[34m2025-10-04 12:12:12[0m] Step: 2823, Training Logs: loss_final: 0.888693, loss_mean: 0.842727, loss_mean_cls: 0.045966, grad_norm: 0.421138
[[34m2025-10-04 12:12:13[0m] Step: 2824, Training Logs: loss_final: 0.899681, loss_mean: 0.851468, loss_mean_cls: 0.048213, grad_norm: 0.425274
[[34m2025-10-04 12:12:13[0m] Step: 2825, Training Logs: loss_final: 0.920923, loss_mean: 0.873958, loss_mean_cls: 0.046965, grad_norm: 0.485684
[[34m2025-10-04 12:12:13[0m] Step: 2826, Training Logs: loss_final: 0.907315, loss_mean: 0.860864, loss_mean_cls: 0.046452, grad_norm: 0.398840
[[34m2025-10-04 12:12:14[0m] Step: 2827, Training Logs: loss_final: 0.913528, loss_mean: 0.868444, loss_mean_cls: 0.045084, grad_norm: 0.478599
[[34m2025-10-04 12:12:14[0m] Step: 2828, Training Logs: loss_final: 0.912072, loss_mean: 0.865828, loss_mean_cls: 0.046245, grad_norm: 0.317030
[[34m2025-10-04 12:12:14[0m] Step: 2829, Training Logs: loss_final: 0.890525, loss_mean: 0.843978, loss_mean_cls: 0.046547, grad_norm: 0.374268
[[34m2025-10-04 12:12:14[0m] Step: 2830, Training Logs: loss_final: 0.917132, loss_mean: 0.869016, loss_mean_cls: 0.048116, grad_norm: 0.436405
[[34m2025-10-04 12:12:15[0m] Step: 2831, Training Logs: loss_final: 0.927261, loss_mean: 0.880959, loss_mean_cls: 0.046302, grad_norm: 0.342027
[[34m2025-10-04 12:12:15[0m] Step: 2832, Training Logs: loss_final: 0.882884, loss_mean: 0.833681, loss_mean_cls: 0.049203, grad_norm: 0.500790
[[34m2025-10-04 12:12:15[0m] Step: 2833, Training Logs: loss_final: 0.909252, loss_mean: 0.862469, loss_mean_cls: 0.046783, grad_norm: 0.419103
[[34m2025-10-04 12:12:16[0m] Step: 2834, Training Logs: loss_final: 0.906442, loss_mean: 0.858711, loss_mean_cls: 0.047731, grad_norm: 0.568357
[[34m2025-10-04 12:12:16[0m] Step: 2835, Training Logs: loss_final: 0.911669, loss_mean: 0.864376, loss_mean_cls: 0.047293, grad_norm: 0.531147
[[34m2025-10-04 12:12:16[0m] Step: 2836, Training Logs: loss_final: 0.895500, loss_mean: 0.847590, loss_mean_cls: 0.047909, grad_norm: 0.477937
[[34m2025-10-04 12:12:17[0m] Step: 2837, Training Logs: loss_final: 0.898360, loss_mean: 0.851284, loss_mean_cls: 0.047075, grad_norm: 0.482054
[[34m2025-10-04 12:12:17[0m] Step: 2838, Training Logs: loss_final: 0.899946, loss_mean: 0.853125, loss_mean_cls: 0.046821, grad_norm: 0.511097
[[34m2025-10-04 12:12:17[0m] Step: 2839, Training Logs: loss_final: 0.907627, loss_mean: 0.860868, loss_mean_cls: 0.046759, grad_norm: 0.549658
[[34m2025-10-04 12:12:17[0m] Step: 2840, Training Logs: loss_final: 0.915843, loss_mean: 0.869537, loss_mean_cls: 0.046306, grad_norm: 0.487702
[[34m2025-10-04 12:12:18[0m] Step: 2841, Training Logs: loss_final: 0.913233, loss_mean: 0.867344, loss_mean_cls: 0.045889, grad_norm: 0.379438
[[34m2025-10-04 12:12:18[0m] Step: 2842, Training Logs: loss_final: 0.921112, loss_mean: 0.874989, loss_mean_cls: 0.046122, grad_norm: 0.651945
[[34m2025-10-04 12:12:18[0m] Step: 2843, Training Logs: loss_final: 0.903858, loss_mean: 0.856043, loss_mean_cls: 0.047815, grad_norm: 0.569691
[[34m2025-10-04 12:12:19[0m] Step: 2844, Training Logs: loss_final: 0.906202, loss_mean: 0.859997, loss_mean_cls: 0.046205, grad_norm: 0.406747
[[34m2025-10-04 12:12:19[0m] Step: 2845, Training Logs: loss_final: 0.902105, loss_mean: 0.855080, loss_mean_cls: 0.047024, grad_norm: 0.493548
[[34m2025-10-04 12:12:19[0m] Step: 2846, Training Logs: loss_final: 0.913491, loss_mean: 0.867388, loss_mean_cls: 0.046103, grad_norm: 0.523862
[[34m2025-10-04 12:12:19[0m] Step: 2847, Training Logs: loss_final: 0.913703, loss_mean: 0.865538, loss_mean_cls: 0.048165, grad_norm: 0.625853
[[34m2025-10-04 12:12:20[0m] Step: 2848, Training Logs: loss_final: 0.899369, loss_mean: 0.851258, loss_mean_cls: 0.048111, grad_norm: 0.424491
[[34m2025-10-04 12:12:20[0m] Step: 2849, Training Logs: loss_final: 0.898290, loss_mean: 0.850446, loss_mean_cls: 0.047844, grad_norm: 0.517735
[[34m2025-10-04 12:12:20[0m] Step: 2850, Training Logs: loss_final: 0.892174, loss_mean: 0.844115, loss_mean_cls: 0.048059, grad_norm: 0.539085
[[34m2025-10-04 12:12:21[0m] Step: 2851, Training Logs: loss_final: 0.905046, loss_mean: 0.858137, loss_mean_cls: 0.046908, grad_norm: 0.499682
[[34m2025-10-04 12:12:21[0m] Step: 2852, Training Logs: loss_final: 0.897852, loss_mean: 0.849737, loss_mean_cls: 0.048115, grad_norm: 0.537066
[[34m2025-10-04 12:12:21[0m] Step: 2853, Training Logs: loss_final: 0.911716, loss_mean: 0.865974, loss_mean_cls: 0.045742, grad_norm: 0.509417
[[34m2025-10-04 12:12:22[0m] Step: 2854, Training Logs: loss_final: 0.915802, loss_mean: 0.869454, loss_mean_cls: 0.046348, grad_norm: 0.512723
[[34m2025-10-04 12:12:22[0m] Step: 2855, Training Logs: loss_final: 0.905018, loss_mean: 0.858082, loss_mean_cls: 0.046937, grad_norm: 0.501915
[[34m2025-10-04 12:12:22[0m] Step: 2856, Training Logs: loss_final: 0.905324, loss_mean: 0.857096, loss_mean_cls: 0.048228, grad_norm: 0.608719
[[34m2025-10-04 12:12:22[0m] Step: 2857, Training Logs: loss_final: 0.912470, loss_mean: 0.864487, loss_mean_cls: 0.047983, grad_norm: 0.542785
[[34m2025-10-04 12:12:23[0m] Step: 2858, Training Logs: loss_final: 0.887162, loss_mean: 0.838576, loss_mean_cls: 0.048586, grad_norm: 0.453574
[[34m2025-10-04 12:12:23[0m] Step: 2859, Training Logs: loss_final: 0.923142, loss_mean: 0.875502, loss_mean_cls: 0.047640, grad_norm: 0.668315
[[34m2025-10-04 12:12:23[0m] Step: 2860, Training Logs: loss_final: 0.912250, loss_mean: 0.866068, loss_mean_cls: 0.046181, grad_norm: 0.483726
[[34m2025-10-04 12:12:24[0m] Step: 2861, Training Logs: loss_final: 0.924164, loss_mean: 0.877602, loss_mean_cls: 0.046562, grad_norm: 0.563297
[[34m2025-10-04 12:12:24[0m] Step: 2862, Training Logs: loss_final: 0.898749, loss_mean: 0.852412, loss_mean_cls: 0.046337, grad_norm: 0.513455
[[34m2025-10-04 12:12:24[0m] Step: 2863, Training Logs: loss_final: 0.909822, loss_mean: 0.862702, loss_mean_cls: 0.047120, grad_norm: 0.542063
[[34m2025-10-04 12:12:24[0m] Step: 2864, Training Logs: loss_final: 0.907101, loss_mean: 0.859598, loss_mean_cls: 0.047503, grad_norm: 0.427373
[[34m2025-10-04 12:12:25[0m] Step: 2865, Training Logs: loss_final: 0.902538, loss_mean: 0.857610, loss_mean_cls: 0.044928, grad_norm: 0.599940
[[34m2025-10-04 12:12:25[0m] Step: 2866, Training Logs: loss_final: 0.902962, loss_mean: 0.856386, loss_mean_cls: 0.046575, grad_norm: 0.387915
[[34m2025-10-04 12:12:25[0m] Step: 2867, Training Logs: loss_final: 0.922220, loss_mean: 0.877264, loss_mean_cls: 0.044957, grad_norm: 0.511193
[[34m2025-10-04 12:12:26[0m] Step: 2868, Training Logs: loss_final: 0.921091, loss_mean: 0.875107, loss_mean_cls: 0.045985, grad_norm: 0.511974
[[34m2025-10-04 12:12:26[0m] Step: 2869, Training Logs: loss_final: 0.887522, loss_mean: 0.839026, loss_mean_cls: 0.048495, grad_norm: 0.426033
[[34m2025-10-04 12:12:26[0m] Step: 2870, Training Logs: loss_final: 0.891738, loss_mean: 0.844964, loss_mean_cls: 0.046774, grad_norm: 0.405469
[[34m2025-10-04 12:12:27[0m] Step: 2871, Training Logs: loss_final: 0.909048, loss_mean: 0.862138, loss_mean_cls: 0.046910, grad_norm: 0.589317
[[34m2025-10-04 12:12:27[0m] Step: 2872, Training Logs: loss_final: 0.906584, loss_mean: 0.859878, loss_mean_cls: 0.046706, grad_norm: 0.462073
[[34m2025-10-04 12:12:27[0m] Step: 2873, Training Logs: loss_final: 0.899888, loss_mean: 0.853179, loss_mean_cls: 0.046708, grad_norm: 0.446340
[[34m2025-10-04 12:12:27[0m] Step: 2874, Training Logs: loss_final: 0.919292, loss_mean: 0.872482, loss_mean_cls: 0.046811, grad_norm: 0.511477
[[34m2025-10-04 12:12:28[0m] Step: 2875, Training Logs: loss_final: 0.918259, loss_mean: 0.871878, loss_mean_cls: 0.046381, grad_norm: 0.379349
[[34m2025-10-04 12:12:28[0m] Step: 2876, Training Logs: loss_final: 0.913567, loss_mean: 0.866848, loss_mean_cls: 0.046719, grad_norm: 0.555709
[[34m2025-10-04 12:12:28[0m] Step: 2877, Training Logs: loss_final: 0.891909, loss_mean: 0.843384, loss_mean_cls: 0.048526, grad_norm: 0.473126
[[34m2025-10-04 12:12:29[0m] Step: 2878, Training Logs: loss_final: 0.908914, loss_mean: 0.862850, loss_mean_cls: 0.046063, grad_norm: 0.390492
[[34m2025-10-04 12:12:29[0m] Step: 2879, Training Logs: loss_final: 0.912604, loss_mean: 0.866627, loss_mean_cls: 0.045977, grad_norm: 0.560868
[[34m2025-10-04 12:12:29[0m] Step: 2880, Training Logs: loss_final: 0.891219, loss_mean: 0.844924, loss_mean_cls: 0.046295, grad_norm: 0.427255
[[34m2025-10-04 12:12:29[0m] Step: 2881, Training Logs: loss_final: 0.908172, loss_mean: 0.860925, loss_mean_cls: 0.047247, grad_norm: 0.505827
[[34m2025-10-04 12:12:30[0m] Step: 2882, Training Logs: loss_final: 0.896618, loss_mean: 0.849234, loss_mean_cls: 0.047385, grad_norm: 0.373846
[[34m2025-10-04 12:12:30[0m] Step: 2883, Training Logs: loss_final: 0.908045, loss_mean: 0.861390, loss_mean_cls: 0.046655, grad_norm: 0.596553
[[34m2025-10-04 12:12:30[0m] Step: 2884, Training Logs: loss_final: 0.893588, loss_mean: 0.847240, loss_mean_cls: 0.046348, grad_norm: 0.673018
[[34m2025-10-04 12:12:31[0m] Step: 2885, Training Logs: loss_final: 0.914848, loss_mean: 0.868181, loss_mean_cls: 0.046667, grad_norm: 0.446720
[[34m2025-10-04 12:12:31[0m] Step: 2886, Training Logs: loss_final: 0.921797, loss_mean: 0.874752, loss_mean_cls: 0.047045, grad_norm: 0.597158
[[34m2025-10-04 12:12:31[0m] Step: 2887, Training Logs: loss_final: 0.888796, loss_mean: 0.840451, loss_mean_cls: 0.048345, grad_norm: 0.397471
[[34m2025-10-04 12:12:31[0m] Step: 2888, Training Logs: loss_final: 0.907497, loss_mean: 0.860826, loss_mean_cls: 0.046671, grad_norm: 0.524183
[[34m2025-10-04 12:12:32[0m] Step: 2889, Training Logs: loss_final: 0.898407, loss_mean: 0.851704, loss_mean_cls: 0.046702, grad_norm: 0.833520
[[34m2025-10-04 12:12:32[0m] Step: 2890, Training Logs: loss_final: 0.893618, loss_mean: 0.845896, loss_mean_cls: 0.047722, grad_norm: 0.435123
[[34m2025-10-04 12:12:32[0m] Step: 2891, Training Logs: loss_final: 0.907224, loss_mean: 0.860471, loss_mean_cls: 0.046753, grad_norm: 0.600587
[[34m2025-10-04 12:12:33[0m] Step: 2892, Training Logs: loss_final: 0.913033, loss_mean: 0.866102, loss_mean_cls: 0.046931, grad_norm: 0.555357
[[34m2025-10-04 12:12:33[0m] Step: 2893, Training Logs: loss_final: 0.911147, loss_mean: 0.864806, loss_mean_cls: 0.046341, grad_norm: 0.395846
[[34m2025-10-04 12:12:33[0m] Step: 2894, Training Logs: loss_final: 0.925234, loss_mean: 0.876992, loss_mean_cls: 0.048242, grad_norm: 0.561373
[[34m2025-10-04 12:12:34[0m] Step: 2895, Training Logs: loss_final: 0.895130, loss_mean: 0.847605, loss_mean_cls: 0.047525, grad_norm: 0.439029
[[34m2025-10-04 12:12:34[0m] Step: 2896, Training Logs: loss_final: 0.921495, loss_mean: 0.873727, loss_mean_cls: 0.047768, grad_norm: 0.537106
[[34m2025-10-04 12:12:34[0m] Step: 2897, Training Logs: loss_final: 0.884501, loss_mean: 0.837783, loss_mean_cls: 0.046718, grad_norm: 0.465776
[[34m2025-10-04 12:12:34[0m] Step: 2898, Training Logs: loss_final: 0.894077, loss_mean: 0.847166, loss_mean_cls: 0.046911, grad_norm: 0.413051
[[34m2025-10-04 12:12:35[0m] Step: 2899, Training Logs: loss_final: 0.893812, loss_mean: 0.847047, loss_mean_cls: 0.046765, grad_norm: 0.518335
[[34m2025-10-04 12:12:35[0m] Step: 2900, Training Logs: loss_final: 0.913126, loss_mean: 0.865970, loss_mean_cls: 0.047156, grad_norm: 0.496921
[[34m2025-10-04 12:12:35[0m] Step: 2901, Training Logs: loss_final: 0.920083, loss_mean: 0.873794, loss_mean_cls: 0.046289, grad_norm: 0.520199
[[34m2025-10-04 12:12:36[0m] Step: 2902, Training Logs: loss_final: 0.927690, loss_mean: 0.881470, loss_mean_cls: 0.046220, grad_norm: 0.414455
[[34m2025-10-04 12:12:36[0m] Step: 2903, Training Logs: loss_final: 0.922018, loss_mean: 0.874363, loss_mean_cls: 0.047654, grad_norm: 0.523389
[[34m2025-10-04 12:12:36[0m] Step: 2904, Training Logs: loss_final: 0.904274, loss_mean: 0.856108, loss_mean_cls: 0.048167, grad_norm: 0.509733
[[34m2025-10-04 12:12:36[0m] Step: 2905, Training Logs: loss_final: 0.908471, loss_mean: 0.861598, loss_mean_cls: 0.046874, grad_norm: 0.450411
[[34m2025-10-04 12:12:37[0m] Step: 2906, Training Logs: loss_final: 0.896315, loss_mean: 0.848192, loss_mean_cls: 0.048124, grad_norm: 0.611869
[[34m2025-10-04 12:12:37[0m] Step: 2907, Training Logs: loss_final: 0.914213, loss_mean: 0.867072, loss_mean_cls: 0.047141, grad_norm: 0.523485
[[34m2025-10-04 12:12:37[0m] Step: 2908, Training Logs: loss_final: 0.889990, loss_mean: 0.842490, loss_mean_cls: 0.047500, grad_norm: 0.563997
[[34m2025-10-04 12:12:38[0m] Step: 2909, Training Logs: loss_final: 0.891690, loss_mean: 0.845327, loss_mean_cls: 0.046363, grad_norm: 0.564187
[[34m2025-10-04 12:12:38[0m] Step: 2910, Training Logs: loss_final: 0.909318, loss_mean: 0.863030, loss_mean_cls: 0.046288, grad_norm: 0.417774
[[34m2025-10-04 12:12:38[0m] Step: 2911, Training Logs: loss_final: 0.900692, loss_mean: 0.854212, loss_mean_cls: 0.046480, grad_norm: 0.596941
[[34m2025-10-04 12:12:39[0m] Step: 2912, Training Logs: loss_final: 0.910013, loss_mean: 0.863453, loss_mean_cls: 0.046560, grad_norm: 0.444355
[[34m2025-10-04 12:12:39[0m] Step: 2913, Training Logs: loss_final: 0.920056, loss_mean: 0.873983, loss_mean_cls: 0.046073, grad_norm: 0.327352
[[34m2025-10-04 12:12:39[0m] Step: 2914, Training Logs: loss_final: 0.904015, loss_mean: 0.857159, loss_mean_cls: 0.046856, grad_norm: 0.291333
[[34m2025-10-04 12:12:39[0m] Step: 2915, Training Logs: loss_final: 0.915989, loss_mean: 0.869251, loss_mean_cls: 0.046737, grad_norm: 0.367662
[[34m2025-10-04 12:12:40[0m] Step: 2916, Training Logs: loss_final: 0.925643, loss_mean: 0.880114, loss_mean_cls: 0.045530, grad_norm: 0.324309
[[34m2025-10-04 12:12:40[0m] Step: 2917, Training Logs: loss_final: 0.913633, loss_mean: 0.867061, loss_mean_cls: 0.046572, grad_norm: 0.489689
[[34m2025-10-04 12:12:40[0m] Step: 2918, Training Logs: loss_final: 0.921362, loss_mean: 0.873249, loss_mean_cls: 0.048112, grad_norm: 0.408004
[[34m2025-10-04 12:12:41[0m] Step: 2919, Training Logs: loss_final: 0.908003, loss_mean: 0.861973, loss_mean_cls: 0.046030, grad_norm: 0.437318
[[34m2025-10-04 12:12:41[0m] Step: 2920, Training Logs: loss_final: 0.914425, loss_mean: 0.867092, loss_mean_cls: 0.047334, grad_norm: 0.400056
[[34m2025-10-04 12:12:41[0m] Step: 2921, Training Logs: loss_final: 0.926196, loss_mean: 0.880607, loss_mean_cls: 0.045589, grad_norm: 0.334679
[[34m2025-10-04 12:12:41[0m] Step: 2922, Training Logs: loss_final: 0.915935, loss_mean: 0.869665, loss_mean_cls: 0.046271, grad_norm: 0.383976
[[34m2025-10-04 12:12:42[0m] Step: 2923, Training Logs: loss_final: 0.907989, loss_mean: 0.860686, loss_mean_cls: 0.047302, grad_norm: 0.349918
[[34m2025-10-04 12:12:42[0m] Step: 2924, Training Logs: loss_final: 0.896915, loss_mean: 0.849204, loss_mean_cls: 0.047711, grad_norm: 0.520287
[[34m2025-10-04 12:12:42[0m] Step: 2925, Training Logs: loss_final: 0.900597, loss_mean: 0.853383, loss_mean_cls: 0.047214, grad_norm: 0.374014
[[34m2025-10-04 12:12:43[0m] Step: 2926, Training Logs: loss_final: 0.911026, loss_mean: 0.864468, loss_mean_cls: 0.046558, grad_norm: 0.554446
[[34m2025-10-04 12:12:43[0m] Step: 2927, Training Logs: loss_final: 0.883109, loss_mean: 0.835152, loss_mean_cls: 0.047957, grad_norm: 0.397955
[[34m2025-10-04 12:12:43[0m] Step: 2928, Training Logs: loss_final: 0.902181, loss_mean: 0.856333, loss_mean_cls: 0.045848, grad_norm: 0.332207
[[34m2025-10-04 12:12:44[0m] Step: 2929, Training Logs: loss_final: 0.925784, loss_mean: 0.879351, loss_mean_cls: 0.046433, grad_norm: 0.526483
[[34m2025-10-04 12:12:44[0m] Step: 2930, Training Logs: loss_final: 0.890937, loss_mean: 0.843562, loss_mean_cls: 0.047375, grad_norm: 0.405950
[[34m2025-10-04 12:12:44[0m] Step: 2931, Training Logs: loss_final: 0.897083, loss_mean: 0.849997, loss_mean_cls: 0.047086, grad_norm: 0.509668
[[34m2025-10-04 12:12:44[0m] Step: 2932, Training Logs: loss_final: 0.900544, loss_mean: 0.853660, loss_mean_cls: 0.046885, grad_norm: 0.449366
[[34m2025-10-04 12:12:45[0m] Step: 2933, Training Logs: loss_final: 0.895772, loss_mean: 0.848493, loss_mean_cls: 0.047279, grad_norm: 0.423368
[[34m2025-10-04 12:12:45[0m] Step: 2934, Training Logs: loss_final: 0.900384, loss_mean: 0.854384, loss_mean_cls: 0.045999, grad_norm: 0.396328
[[34m2025-10-04 12:12:45[0m] Step: 2935, Training Logs: loss_final: 0.896971, loss_mean: 0.850787, loss_mean_cls: 0.046184, grad_norm: 0.327789
[[34m2025-10-04 12:12:46[0m] Step: 2936, Training Logs: loss_final: 0.913969, loss_mean: 0.867328, loss_mean_cls: 0.046641, grad_norm: 0.521005
[[34m2025-10-04 12:12:46[0m] Step: 2937, Training Logs: loss_final: 0.924801, loss_mean: 0.879833, loss_mean_cls: 0.044968, grad_norm: 0.385589
[[34m2025-10-04 12:12:46[0m] Step: 2938, Training Logs: loss_final: 0.896816, loss_mean: 0.850239, loss_mean_cls: 0.046577, grad_norm: 0.445141
[[34m2025-10-04 12:12:46[0m] Step: 2939, Training Logs: loss_final: 0.880433, loss_mean: 0.832553, loss_mean_cls: 0.047880, grad_norm: 0.620927
[[34m2025-10-04 12:12:47[0m] Step: 2940, Training Logs: loss_final: 0.908623, loss_mean: 0.861896, loss_mean_cls: 0.046727, grad_norm: 0.460934
[[34m2025-10-04 12:12:47[0m] Step: 2941, Training Logs: loss_final: 0.929806, loss_mean: 0.883607, loss_mean_cls: 0.046199, grad_norm: 0.575849
[[34m2025-10-04 12:12:47[0m] Step: 2942, Training Logs: loss_final: 0.915341, loss_mean: 0.867729, loss_mean_cls: 0.047612, grad_norm: 0.526959
[[34m2025-10-04 12:12:48[0m] Step: 2943, Training Logs: loss_final: 0.924216, loss_mean: 0.877387, loss_mean_cls: 0.046828, grad_norm: 0.614361
[[34m2025-10-04 12:12:48[0m] Step: 2944, Training Logs: loss_final: 0.914179, loss_mean: 0.867051, loss_mean_cls: 0.047128, grad_norm: 0.596115
[[34m2025-10-04 12:12:48[0m] Step: 2945, Training Logs: loss_final: 0.903242, loss_mean: 0.856435, loss_mean_cls: 0.046808, grad_norm: 0.543000
[[34m2025-10-04 12:12:49[0m] Step: 2946, Training Logs: loss_final: 0.917748, loss_mean: 0.871621, loss_mean_cls: 0.046127, grad_norm: 0.539752
[[34m2025-10-04 12:12:49[0m] Step: 2947, Training Logs: loss_final: 0.898607, loss_mean: 0.851612, loss_mean_cls: 0.046995, grad_norm: 0.483974
[[34m2025-10-04 12:12:49[0m] Step: 2948, Training Logs: loss_final: 0.918738, loss_mean: 0.872962, loss_mean_cls: 0.045776, grad_norm: 0.440359
[[34m2025-10-04 12:12:49[0m] Step: 2949, Training Logs: loss_final: 0.915088, loss_mean: 0.868891, loss_mean_cls: 0.046198, grad_norm: 0.626907
[[34m2025-10-04 12:12:50[0m] Step: 2950, Training Logs: loss_final: 0.893929, loss_mean: 0.846906, loss_mean_cls: 0.047023, grad_norm: 0.354398
[[34m2025-10-04 12:12:50[0m] Step: 2951, Training Logs: loss_final: 0.897931, loss_mean: 0.852397, loss_mean_cls: 0.045534, grad_norm: 0.489056
[[34m2025-10-04 12:12:50[0m] Step: 2952, Training Logs: loss_final: 0.906268, loss_mean: 0.860026, loss_mean_cls: 0.046241, grad_norm: 0.418534
[[34m2025-10-04 12:12:51[0m] Step: 2953, Training Logs: loss_final: 0.897926, loss_mean: 0.850719, loss_mean_cls: 0.047207, grad_norm: 0.431755
[[34m2025-10-04 12:12:51[0m] Step: 2954, Training Logs: loss_final: 0.895159, loss_mean: 0.848534, loss_mean_cls: 0.046625, grad_norm: 0.571027
[[34m2025-10-04 12:12:51[0m] Step: 2955, Training Logs: loss_final: 0.915429, loss_mean: 0.869259, loss_mean_cls: 0.046170, grad_norm: 0.351421
[[34m2025-10-04 12:12:51[0m] Step: 2956, Training Logs: loss_final: 0.910953, loss_mean: 0.863772, loss_mean_cls: 0.047181, grad_norm: 0.388767
[[34m2025-10-04 12:12:52[0m] Step: 2957, Training Logs: loss_final: 0.910724, loss_mean: 0.863992, loss_mean_cls: 0.046732, grad_norm: 0.387552
[[34m2025-10-04 12:12:52[0m] Step: 2958, Training Logs: loss_final: 0.911933, loss_mean: 0.864713, loss_mean_cls: 0.047220, grad_norm: 0.552938
[[34m2025-10-04 12:12:52[0m] Step: 2959, Training Logs: loss_final: 0.884299, loss_mean: 0.837910, loss_mean_cls: 0.046389, grad_norm: 0.486505
[[34m2025-10-04 12:12:53[0m] Step: 2960, Training Logs: loss_final: 0.920517, loss_mean: 0.873877, loss_mean_cls: 0.046640, grad_norm: 0.452774
[[34m2025-10-04 12:12:53[0m] Step: 2961, Training Logs: loss_final: 0.926565, loss_mean: 0.879597, loss_mean_cls: 0.046968, grad_norm: 0.550300
[[34m2025-10-04 12:12:53[0m] Step: 2962, Training Logs: loss_final: 0.896665, loss_mean: 0.849010, loss_mean_cls: 0.047655, grad_norm: 0.553233
[[34m2025-10-04 12:12:54[0m] Step: 2963, Training Logs: loss_final: 0.885185, loss_mean: 0.838213, loss_mean_cls: 0.046972, grad_norm: 0.361995
[[34m2025-10-04 12:12:54[0m] Step: 2964, Training Logs: loss_final: 0.903036, loss_mean: 0.856778, loss_mean_cls: 0.046258, grad_norm: 0.604713
[[34m2025-10-04 12:12:54[0m] Step: 2965, Training Logs: loss_final: 0.915344, loss_mean: 0.869254, loss_mean_cls: 0.046090, grad_norm: 0.474005
[[34m2025-10-04 12:12:54[0m] Step: 2966, Training Logs: loss_final: 0.910137, loss_mean: 0.863902, loss_mean_cls: 0.046235, grad_norm: 0.334681
[[34m2025-10-04 12:12:55[0m] Step: 2967, Training Logs: loss_final: 0.897593, loss_mean: 0.850370, loss_mean_cls: 0.047223, grad_norm: 0.425316
[[34m2025-10-04 12:12:55[0m] Step: 2968, Training Logs: loss_final: 0.930507, loss_mean: 0.884643, loss_mean_cls: 0.045864, grad_norm: 0.518688
[[34m2025-10-04 12:12:55[0m] Step: 2969, Training Logs: loss_final: 0.912095, loss_mean: 0.866637, loss_mean_cls: 0.045458, grad_norm: 0.474894
[[34m2025-10-04 12:12:56[0m] Step: 2970, Training Logs: loss_final: 0.896331, loss_mean: 0.848845, loss_mean_cls: 0.047486, grad_norm: 0.482087
[[34m2025-10-04 12:12:56[0m] Step: 2971, Training Logs: loss_final: 0.888744, loss_mean: 0.842916, loss_mean_cls: 0.045829, grad_norm: 0.533959
[[34m2025-10-04 12:12:56[0m] Step: 2972, Training Logs: loss_final: 0.906013, loss_mean: 0.860191, loss_mean_cls: 0.045822, grad_norm: 0.358435
[[34m2025-10-04 12:12:56[0m] Step: 2973, Training Logs: loss_final: 0.895360, loss_mean: 0.848799, loss_mean_cls: 0.046562, grad_norm: 0.470242
[[34m2025-10-04 12:12:57[0m] Step: 2974, Training Logs: loss_final: 0.914101, loss_mean: 0.867978, loss_mean_cls: 0.046123, grad_norm: 0.403794
[[34m2025-10-04 12:12:57[0m] Step: 2975, Training Logs: loss_final: 0.903187, loss_mean: 0.855889, loss_mean_cls: 0.047298, grad_norm: 0.354701
[[34m2025-10-04 12:12:57[0m] Step: 2976, Training Logs: loss_final: 0.891602, loss_mean: 0.845014, loss_mean_cls: 0.046588, grad_norm: 0.542651
[[34m2025-10-04 12:12:58[0m] Step: 2977, Training Logs: loss_final: 0.902827, loss_mean: 0.856412, loss_mean_cls: 0.046415, grad_norm: 0.413066
[[34m2025-10-04 12:12:58[0m] Step: 2978, Training Logs: loss_final: 0.921330, loss_mean: 0.873754, loss_mean_cls: 0.047576, grad_norm: 0.475322
[[34m2025-10-04 12:12:58[0m] Step: 2979, Training Logs: loss_final: 0.911503, loss_mean: 0.864661, loss_mean_cls: 0.046842, grad_norm: 0.427114
[[34m2025-10-04 12:12:58[0m] Step: 2980, Training Logs: loss_final: 0.919560, loss_mean: 0.873612, loss_mean_cls: 0.045949, grad_norm: 0.481444
[[34m2025-10-04 12:12:59[0m] Step: 2981, Training Logs: loss_final: 0.887162, loss_mean: 0.840164, loss_mean_cls: 0.046998, grad_norm: 0.517248
[[34m2025-10-04 12:12:59[0m] Step: 2982, Training Logs: loss_final: 0.893915, loss_mean: 0.846455, loss_mean_cls: 0.047460, grad_norm: 0.387280
[[34m2025-10-04 12:12:59[0m] Step: 2983, Training Logs: loss_final: 0.909242, loss_mean: 0.862737, loss_mean_cls: 0.046505, grad_norm: 0.647096
[[34m2025-10-04 12:13:00[0m] Step: 2984, Training Logs: loss_final: 0.922021, loss_mean: 0.874084, loss_mean_cls: 0.047937, grad_norm: 0.491530
[[34m2025-10-04 12:13:00[0m] Step: 2985, Training Logs: loss_final: 0.924064, loss_mean: 0.877035, loss_mean_cls: 0.047030, grad_norm: 0.514259
[[34m2025-10-04 12:13:00[0m] Step: 2986, Training Logs: loss_final: 0.898542, loss_mean: 0.851948, loss_mean_cls: 0.046594, grad_norm: 0.398117
[[34m2025-10-04 12:13:01[0m] Step: 2987, Training Logs: loss_final: 0.896959, loss_mean: 0.848947, loss_mean_cls: 0.048012, grad_norm: 0.487034
[[34m2025-10-04 12:13:01[0m] Step: 2988, Training Logs: loss_final: 0.911688, loss_mean: 0.865968, loss_mean_cls: 0.045720, grad_norm: 0.375649
[[34m2025-10-04 12:13:01[0m] Step: 2989, Training Logs: loss_final: 0.908115, loss_mean: 0.861446, loss_mean_cls: 0.046669, grad_norm: 0.586236
[[34m2025-10-04 12:13:01[0m] Step: 2990, Training Logs: loss_final: 0.931305, loss_mean: 0.885561, loss_mean_cls: 0.045744, grad_norm: 0.472137
[[34m2025-10-04 12:13:02[0m] Step: 2991, Training Logs: loss_final: 0.888697, loss_mean: 0.840301, loss_mean_cls: 0.048396, grad_norm: 0.435099
[[34m2025-10-04 12:13:02[0m] Step: 2992, Training Logs: loss_final: 0.874280, loss_mean: 0.827400, loss_mean_cls: 0.046880, grad_norm: 0.451460
[[34m2025-10-04 12:13:02[0m] Step: 2993, Training Logs: loss_final: 0.915598, loss_mean: 0.868761, loss_mean_cls: 0.046837, grad_norm: 0.402460
[[34m2025-10-04 12:13:03[0m] Step: 2994, Training Logs: loss_final: 0.908446, loss_mean: 0.862073, loss_mean_cls: 0.046373, grad_norm: 0.478833
[[34m2025-10-04 12:13:03[0m] Step: 2995, Training Logs: loss_final: 0.926597, loss_mean: 0.879788, loss_mean_cls: 0.046809, grad_norm: 0.457233
[[34m2025-10-04 12:13:03[0m] Step: 2996, Training Logs: loss_final: 0.898598, loss_mean: 0.852105, loss_mean_cls: 0.046494, grad_norm: 0.384337
[[34m2025-10-04 12:13:03[0m] Step: 2997, Training Logs: loss_final: 0.893642, loss_mean: 0.846170, loss_mean_cls: 0.047472, grad_norm: 0.357559
[[34m2025-10-04 12:13:04[0m] Step: 2998, Training Logs: loss_final: 0.914213, loss_mean: 0.867636, loss_mean_cls: 0.046577, grad_norm: 0.392212
[[34m2025-10-04 12:13:04[0m] Step: 2999, Training Logs: loss_final: 0.914967, loss_mean: 0.868601, loss_mean_cls: 0.046366, grad_norm: 0.395322
[[34m2025-10-04 12:13:04[0m] Step: 3000, Training Logs: loss_final: 0.877924, loss_mean: 0.831278, loss_mean_cls: 0.046646, grad_norm: 0.488246
[[34m2025-10-04 12:13:05[0m] Step: 3001, Training Logs: loss_final: 0.888225, loss_mean: 0.841892, loss_mean_cls: 0.046333, grad_norm: 0.347119
[[34m2025-10-04 12:13:05[0m] Step: 3002, Training Logs: loss_final: 0.916403, loss_mean: 0.870174, loss_mean_cls: 0.046229, grad_norm: 0.447757
[[34m2025-10-04 12:13:05[0m] Step: 3003, Training Logs: loss_final: 0.930183, loss_mean: 0.884322, loss_mean_cls: 0.045861, grad_norm: 0.574876
[[34m2025-10-04 12:13:06[0m] Step: 3004, Training Logs: loss_final: 0.921357, loss_mean: 0.875483, loss_mean_cls: 0.045875, grad_norm: 0.361592
[[34m2025-10-04 12:13:06[0m] Step: 3005, Training Logs: loss_final: 0.893741, loss_mean: 0.848181, loss_mean_cls: 0.045559, grad_norm: 0.679879
[[34m2025-10-04 12:13:06[0m] Step: 3006, Training Logs: loss_final: 0.899820, loss_mean: 0.853703, loss_mean_cls: 0.046117, grad_norm: 0.371935
[[34m2025-10-04 12:13:06[0m] Step: 3007, Training Logs: loss_final: 0.908349, loss_mean: 0.862430, loss_mean_cls: 0.045920, grad_norm: 0.554423
[[34m2025-10-04 12:13:07[0m] Step: 3008, Training Logs: loss_final: 0.894223, loss_mean: 0.847403, loss_mean_cls: 0.046820, grad_norm: 0.437857
[[34m2025-10-04 12:13:07[0m] Step: 3009, Training Logs: loss_final: 0.916918, loss_mean: 0.870765, loss_mean_cls: 0.046153, grad_norm: 0.481786
[[34m2025-10-04 12:13:07[0m] Step: 3010, Training Logs: loss_final: 0.922813, loss_mean: 0.876664, loss_mean_cls: 0.046149, grad_norm: 0.417140
[[34m2025-10-04 12:13:08[0m] Step: 3011, Training Logs: loss_final: 0.905497, loss_mean: 0.858655, loss_mean_cls: 0.046842, grad_norm: 0.339403
[[34m2025-10-04 12:13:08[0m] Step: 3012, Training Logs: loss_final: 0.892997, loss_mean: 0.846273, loss_mean_cls: 0.046724, grad_norm: 0.416047
[[34m2025-10-04 12:13:08[0m] Step: 3013, Training Logs: loss_final: 0.920189, loss_mean: 0.872221, loss_mean_cls: 0.047967, grad_norm: 0.428767
[[34m2025-10-04 12:13:08[0m] Step: 3014, Training Logs: loss_final: 0.919767, loss_mean: 0.874200, loss_mean_cls: 0.045567, grad_norm: 0.331736
[[34m2025-10-04 12:13:09[0m] Step: 3015, Training Logs: loss_final: 0.902758, loss_mean: 0.855293, loss_mean_cls: 0.047465, grad_norm: 0.354150
[[34m2025-10-04 12:13:09[0m] Step: 3016, Training Logs: loss_final: 0.895829, loss_mean: 0.849142, loss_mean_cls: 0.046687, grad_norm: 0.429325
[[34m2025-10-04 12:13:09[0m] Step: 3017, Training Logs: loss_final: 0.908909, loss_mean: 0.862296, loss_mean_cls: 0.046613, grad_norm: 0.324333
[[34m2025-10-04 12:13:10[0m] Step: 3018, Training Logs: loss_final: 0.923329, loss_mean: 0.876690, loss_mean_cls: 0.046639, grad_norm: 0.438693
[[34m2025-10-04 12:13:10[0m] Step: 3019, Training Logs: loss_final: 0.902984, loss_mean: 0.856382, loss_mean_cls: 0.046601, grad_norm: 0.402661
[[34m2025-10-04 12:13:10[0m] Step: 3020, Training Logs: loss_final: 0.904497, loss_mean: 0.858781, loss_mean_cls: 0.045717, grad_norm: 0.466908
[[34m2025-10-04 12:13:11[0m] Step: 3021, Training Logs: loss_final: 0.892217, loss_mean: 0.845364, loss_mean_cls: 0.046853, grad_norm: 0.528696
[[34m2025-10-04 12:13:11[0m] Step: 3022, Training Logs: loss_final: 0.918452, loss_mean: 0.872159, loss_mean_cls: 0.046294, grad_norm: 0.426289
[[34m2025-10-04 12:13:11[0m] Step: 3023, Training Logs: loss_final: 0.897746, loss_mean: 0.850740, loss_mean_cls: 0.047006, grad_norm: 0.287046
[[34m2025-10-04 12:13:11[0m] Step: 3024, Training Logs: loss_final: 0.911096, loss_mean: 0.863881, loss_mean_cls: 0.047215, grad_norm: 0.394426
[[34m2025-10-04 12:13:12[0m] Step: 3025, Training Logs: loss_final: 0.900968, loss_mean: 0.853615, loss_mean_cls: 0.047352, grad_norm: 0.403799
[[34m2025-10-04 12:13:12[0m] Step: 3026, Training Logs: loss_final: 0.896903, loss_mean: 0.850248, loss_mean_cls: 0.046656, grad_norm: 0.386057
[[34m2025-10-04 12:13:12[0m] Step: 3027, Training Logs: loss_final: 0.893236, loss_mean: 0.845479, loss_mean_cls: 0.047757, grad_norm: 0.363910
[[34m2025-10-04 12:13:13[0m] Step: 3028, Training Logs: loss_final: 0.903512, loss_mean: 0.856631, loss_mean_cls: 0.046881, grad_norm: 0.396041
[[34m2025-10-04 12:13:13[0m] Step: 3029, Training Logs: loss_final: 0.900988, loss_mean: 0.852628, loss_mean_cls: 0.048360, grad_norm: 0.365706
[[34m2025-10-04 12:13:13[0m] Step: 3030, Training Logs: loss_final: 0.904368, loss_mean: 0.857929, loss_mean_cls: 0.046439, grad_norm: 0.339574
[[34m2025-10-04 12:13:13[0m] Step: 3031, Training Logs: loss_final: 0.905331, loss_mean: 0.859100, loss_mean_cls: 0.046230, grad_norm: 0.363380
[[34m2025-10-04 12:13:14[0m] Step: 3032, Training Logs: loss_final: 0.918266, loss_mean: 0.872018, loss_mean_cls: 0.046248, grad_norm: 0.385499
[[34m2025-10-04 12:13:14[0m] Step: 3033, Training Logs: loss_final: 0.863862, loss_mean: 0.816842, loss_mean_cls: 0.047020, grad_norm: 0.335520
[[34m2025-10-04 12:13:14[0m] Step: 3034, Training Logs: loss_final: 0.906801, loss_mean: 0.859521, loss_mean_cls: 0.047280, grad_norm: 0.462818
[[34m2025-10-04 12:13:15[0m] Step: 3035, Training Logs: loss_final: 0.916897, loss_mean: 0.870743, loss_mean_cls: 0.046154, grad_norm: 0.417791
[[34m2025-10-04 12:13:15[0m] Step: 3036, Training Logs: loss_final: 0.905382, loss_mean: 0.857742, loss_mean_cls: 0.047639, grad_norm: 0.369825
[[34m2025-10-04 12:13:15[0m] Step: 3037, Training Logs: loss_final: 0.905020, loss_mean: 0.857668, loss_mean_cls: 0.047352, grad_norm: 0.362867
[[34m2025-10-04 12:13:16[0m] Step: 3038, Training Logs: loss_final: 0.905937, loss_mean: 0.860016, loss_mean_cls: 0.045921, grad_norm: 0.311393
[[34m2025-10-04 12:13:16[0m] Step: 3039, Training Logs: loss_final: 0.903465, loss_mean: 0.857333, loss_mean_cls: 0.046132, grad_norm: 0.363646
[[34m2025-10-04 12:13:16[0m] Step: 3040, Training Logs: loss_final: 0.906262, loss_mean: 0.859748, loss_mean_cls: 0.046514, grad_norm: 0.375200
[[34m2025-10-04 12:13:16[0m] Step: 3041, Training Logs: loss_final: 0.914840, loss_mean: 0.868173, loss_mean_cls: 0.046667, grad_norm: 0.350880
[[34m2025-10-04 12:13:17[0m] Step: 3042, Training Logs: loss_final: 0.902706, loss_mean: 0.855690, loss_mean_cls: 0.047015, grad_norm: 0.506884
[[34m2025-10-04 12:13:17[0m] Step: 3043, Training Logs: loss_final: 0.917735, loss_mean: 0.871499, loss_mean_cls: 0.046236, grad_norm: 0.453228
[[34m2025-10-04 12:13:17[0m] Step: 3044, Training Logs: loss_final: 0.886019, loss_mean: 0.838489, loss_mean_cls: 0.047531, grad_norm: 0.566735
[[34m2025-10-04 12:13:18[0m] Step: 3045, Training Logs: loss_final: 0.906753, loss_mean: 0.860846, loss_mean_cls: 0.045907, grad_norm: 0.551201
[[34m2025-10-04 12:13:18[0m] Step: 3046, Training Logs: loss_final: 0.923340, loss_mean: 0.877977, loss_mean_cls: 0.045363, grad_norm: 0.453039
[[34m2025-10-04 12:13:18[0m] Step: 3047, Training Logs: loss_final: 0.902929, loss_mean: 0.855664, loss_mean_cls: 0.047264, grad_norm: 0.528111
[[34m2025-10-04 12:13:18[0m] Step: 3048, Training Logs: loss_final: 0.919170, loss_mean: 0.871729, loss_mean_cls: 0.047441, grad_norm: 0.366288
[[34m2025-10-04 12:13:19[0m] Step: 3049, Training Logs: loss_final: 0.912589, loss_mean: 0.866145, loss_mean_cls: 0.046443, grad_norm: 0.457388
[[34m2025-10-04 12:13:19[0m] Step: 3050, Training Logs: loss_final: 0.914858, loss_mean: 0.867941, loss_mean_cls: 0.046918, grad_norm: 0.599933
[[34m2025-10-04 12:13:19[0m] Step: 3051, Training Logs: loss_final: 0.915201, loss_mean: 0.868102, loss_mean_cls: 0.047098, grad_norm: 0.466145
[[34m2025-10-04 12:13:20[0m] Step: 3052, Training Logs: loss_final: 0.912280, loss_mean: 0.865927, loss_mean_cls: 0.046352, grad_norm: 0.470464
[[34m2025-10-04 12:13:20[0m] Step: 3053, Training Logs: loss_final: 0.920281, loss_mean: 0.874176, loss_mean_cls: 0.046105, grad_norm: 0.451324
[[34m2025-10-04 12:13:20[0m] Step: 3054, Training Logs: loss_final: 0.900209, loss_mean: 0.853986, loss_mean_cls: 0.046223, grad_norm: 0.567645
[[34m2025-10-04 12:13:20[0m] Step: 3055, Training Logs: loss_final: 0.910644, loss_mean: 0.864485, loss_mean_cls: 0.046159, grad_norm: 0.394957
[[34m2025-10-04 12:13:21[0m] Step: 3056, Training Logs: loss_final: 0.901164, loss_mean: 0.854866, loss_mean_cls: 0.046298, grad_norm: 0.542091
[[34m2025-10-04 12:13:21[0m] Step: 3057, Training Logs: loss_final: 0.888839, loss_mean: 0.841049, loss_mean_cls: 0.047790, grad_norm: 0.516266
[[34m2025-10-04 12:13:21[0m] Step: 3058, Training Logs: loss_final: 0.910567, loss_mean: 0.864254, loss_mean_cls: 0.046313, grad_norm: 0.376939
[[34m2025-10-04 12:13:22[0m] Step: 3059, Training Logs: loss_final: 0.902833, loss_mean: 0.856792, loss_mean_cls: 0.046041, grad_norm: 0.657521
[[34m2025-10-04 12:13:22[0m] Step: 3060, Training Logs: loss_final: 0.901665, loss_mean: 0.855166, loss_mean_cls: 0.046499, grad_norm: 0.320898
[[34m2025-10-04 12:13:22[0m] Step: 3061, Training Logs: loss_final: 0.928105, loss_mean: 0.881497, loss_mean_cls: 0.046608, grad_norm: 0.531610
[[34m2025-10-04 12:13:23[0m] Step: 3062, Training Logs: loss_final: 0.906795, loss_mean: 0.860631, loss_mean_cls: 0.046164, grad_norm: 0.358808
[[34m2025-10-04 12:13:23[0m] Step: 3063, Training Logs: loss_final: 0.915835, loss_mean: 0.869600, loss_mean_cls: 0.046235, grad_norm: 0.361043
[[34m2025-10-04 12:13:23[0m] Step: 3064, Training Logs: loss_final: 0.931554, loss_mean: 0.884898, loss_mean_cls: 0.046656, grad_norm: 0.408747
[[34m2025-10-04 12:13:23[0m] Step: 3065, Training Logs: loss_final: 0.911888, loss_mean: 0.865184, loss_mean_cls: 0.046705, grad_norm: 0.382788
[[34m2025-10-04 12:13:24[0m] Step: 3066, Training Logs: loss_final: 0.900201, loss_mean: 0.853656, loss_mean_cls: 0.046545, grad_norm: 0.472992
[[34m2025-10-04 12:13:24[0m] Step: 3067, Training Logs: loss_final: 0.906333, loss_mean: 0.861611, loss_mean_cls: 0.044721, grad_norm: 0.358014
[[34m2025-10-04 12:13:24[0m] Step: 3068, Training Logs: loss_final: 0.903866, loss_mean: 0.857745, loss_mean_cls: 0.046121, grad_norm: 0.325822
[[34m2025-10-04 12:13:25[0m] Step: 3069, Training Logs: loss_final: 0.862650, loss_mean: 0.814685, loss_mean_cls: 0.047965, grad_norm: 0.453525
[[34m2025-10-04 12:13:25[0m] Step: 3070, Training Logs: loss_final: 0.910555, loss_mean: 0.863228, loss_mean_cls: 0.047327, grad_norm: 0.544044
[[34m2025-10-04 12:13:25[0m] Step: 3071, Training Logs: loss_final: 0.906867, loss_mean: 0.861039, loss_mean_cls: 0.045828, grad_norm: 0.550894
[[34m2025-10-04 12:13:25[0m] Step: 3072, Training Logs: loss_final: 0.919643, loss_mean: 0.873411, loss_mean_cls: 0.046232, grad_norm: 0.785556
[[34m2025-10-04 12:13:26[0m] Step: 3073, Training Logs: loss_final: 0.910219, loss_mean: 0.863548, loss_mean_cls: 0.046671, grad_norm: 0.671310
[[34m2025-10-04 12:13:26[0m] Step: 3074, Training Logs: loss_final: 0.910575, loss_mean: 0.863290, loss_mean_cls: 0.047284, grad_norm: 0.517605
[[34m2025-10-04 12:13:26[0m] Step: 3075, Training Logs: loss_final: 0.895410, loss_mean: 0.848918, loss_mean_cls: 0.046492, grad_norm: 0.674858
[[34m2025-10-04 12:13:27[0m] Step: 3076, Training Logs: loss_final: 0.900215, loss_mean: 0.853493, loss_mean_cls: 0.046722, grad_norm: 0.610659
[[34m2025-10-04 12:13:27[0m] Step: 3077, Training Logs: loss_final: 0.913518, loss_mean: 0.867709, loss_mean_cls: 0.045808, grad_norm: 0.461610
[[34m2025-10-04 12:13:27[0m] Step: 3078, Training Logs: loss_final: 0.898629, loss_mean: 0.851671, loss_mean_cls: 0.046958, grad_norm: 0.477387
[[34m2025-10-04 12:13:27[0m] Step: 3079, Training Logs: loss_final: 0.925441, loss_mean: 0.879918, loss_mean_cls: 0.045523, grad_norm: 0.415294
[[34m2025-10-04 12:13:28[0m] Step: 3080, Training Logs: loss_final: 0.910632, loss_mean: 0.863325, loss_mean_cls: 0.047307, grad_norm: 0.412786
[[34m2025-10-04 12:13:28[0m] Step: 3081, Training Logs: loss_final: 0.901913, loss_mean: 0.855076, loss_mean_cls: 0.046836, grad_norm: 0.426645
[[34m2025-10-04 12:13:28[0m] Step: 3082, Training Logs: loss_final: 0.895419, loss_mean: 0.849839, loss_mean_cls: 0.045581, grad_norm: 0.424525
[[34m2025-10-04 12:13:29[0m] Step: 3083, Training Logs: loss_final: 0.913710, loss_mean: 0.867502, loss_mean_cls: 0.046208, grad_norm: 0.359865
[[34m2025-10-04 12:13:29[0m] Step: 3084, Training Logs: loss_final: 0.903960, loss_mean: 0.856455, loss_mean_cls: 0.047506, grad_norm: 0.404375
[[34m2025-10-04 12:13:29[0m] Step: 3085, Training Logs: loss_final: 0.872227, loss_mean: 0.825343, loss_mean_cls: 0.046884, grad_norm: 0.425341
[[34m2025-10-04 12:13:30[0m] Step: 3086, Training Logs: loss_final: 0.896574, loss_mean: 0.849349, loss_mean_cls: 0.047225, grad_norm: 0.391677
[[34m2025-10-04 12:13:30[0m] Step: 3087, Training Logs: loss_final: 0.897116, loss_mean: 0.850969, loss_mean_cls: 0.046147, grad_norm: 0.371542
[[34m2025-10-04 12:13:30[0m] Step: 3088, Training Logs: loss_final: 0.909543, loss_mean: 0.863551, loss_mean_cls: 0.045993, grad_norm: 0.495795
[[34m2025-10-04 12:13:31[0m] Step: 3089, Training Logs: loss_final: 0.904240, loss_mean: 0.856506, loss_mean_cls: 0.047734, grad_norm: 0.428634
[[34m2025-10-04 12:13:31[0m] Step: 3090, Training Logs: loss_final: 0.895870, loss_mean: 0.849225, loss_mean_cls: 0.046645, grad_norm: 0.547646
[[34m2025-10-04 12:13:31[0m] Step: 3091, Training Logs: loss_final: 0.918439, loss_mean: 0.872050, loss_mean_cls: 0.046389, grad_norm: 0.595152
[[34m2025-10-04 12:13:31[0m] Step: 3092, Training Logs: loss_final: 0.889135, loss_mean: 0.842352, loss_mean_cls: 0.046783, grad_norm: 0.675818
[[34m2025-10-04 12:13:32[0m] Step: 3093, Training Logs: loss_final: 0.903961, loss_mean: 0.856968, loss_mean_cls: 0.046993, grad_norm: 0.542404
[[34m2025-10-04 12:13:32[0m] Step: 3094, Training Logs: loss_final: 0.915464, loss_mean: 0.869386, loss_mean_cls: 0.046078, grad_norm: 0.521403
[[34m2025-10-04 12:13:32[0m] Step: 3095, Training Logs: loss_final: 0.901806, loss_mean: 0.855580, loss_mean_cls: 0.046226, grad_norm: 0.516368
[[34m2025-10-04 12:13:33[0m] Step: 3096, Training Logs: loss_final: 0.909996, loss_mean: 0.863752, loss_mean_cls: 0.046245, grad_norm: 0.558660
[[34m2025-10-04 12:13:33[0m] Step: 3097, Training Logs: loss_final: 0.922104, loss_mean: 0.874788, loss_mean_cls: 0.047316, grad_norm: 0.468371
[[34m2025-10-04 12:13:33[0m] Step: 3098, Training Logs: loss_final: 0.914901, loss_mean: 0.867801, loss_mean_cls: 0.047100, grad_norm: 0.433099
[[34m2025-10-04 12:13:33[0m] Step: 3099, Training Logs: loss_final: 0.915547, loss_mean: 0.868786, loss_mean_cls: 0.046761, grad_norm: 0.536487
[[34m2025-10-04 12:13:34[0m] Step: 3100, Training Logs: loss_final: 0.911349, loss_mean: 0.863591, loss_mean_cls: 0.047758, grad_norm: 0.446647
[[34m2025-10-04 12:13:34[0m] Step: 3101, Training Logs: loss_final: 0.905295, loss_mean: 0.857041, loss_mean_cls: 0.048255, grad_norm: 0.415267
[[34m2025-10-04 12:13:34[0m] Step: 3102, Training Logs: loss_final: 0.902847, loss_mean: 0.856770, loss_mean_cls: 0.046077, grad_norm: 0.527129
[[34m2025-10-04 12:13:35[0m] Step: 3103, Training Logs: loss_final: 0.914452, loss_mean: 0.868751, loss_mean_cls: 0.045701, grad_norm: 0.301903
[[34m2025-10-04 12:13:35[0m] Step: 3104, Training Logs: loss_final: 0.880048, loss_mean: 0.832257, loss_mean_cls: 0.047792, grad_norm: 0.541700
[[34m2025-10-04 12:13:35[0m] Step: 3105, Training Logs: loss_final: 0.909134, loss_mean: 0.862421, loss_mean_cls: 0.046713, grad_norm: 0.457630
[[34m2025-10-04 12:13:36[0m] Step: 3106, Training Logs: loss_final: 0.911166, loss_mean: 0.865119, loss_mean_cls: 0.046047, grad_norm: 0.457059
[[34m2025-10-04 12:13:36[0m] Step: 3107, Training Logs: loss_final: 0.909731, loss_mean: 0.863446, loss_mean_cls: 0.046285, grad_norm: 0.383666
[[34m2025-10-04 12:13:36[0m] Step: 3108, Training Logs: loss_final: 0.908100, loss_mean: 0.860306, loss_mean_cls: 0.047794, grad_norm: 0.470839
[[34m2025-10-04 12:13:36[0m] Step: 3109, Training Logs: loss_final: 0.899066, loss_mean: 0.852327, loss_mean_cls: 0.046739, grad_norm: 0.483288
[[34m2025-10-04 12:13:37[0m] Step: 3110, Training Logs: loss_final: 0.910923, loss_mean: 0.864011, loss_mean_cls: 0.046912, grad_norm: 0.356899
[[34m2025-10-04 12:13:37[0m] Step: 3111, Training Logs: loss_final: 0.902259, loss_mean: 0.854992, loss_mean_cls: 0.047267, grad_norm: 0.497742
[[34m2025-10-04 12:13:37[0m] Step: 3112, Training Logs: loss_final: 0.905331, loss_mean: 0.858322, loss_mean_cls: 0.047010, grad_norm: 0.450600
[[34m2025-10-04 12:13:38[0m] Step: 3113, Training Logs: loss_final: 0.915172, loss_mean: 0.869075, loss_mean_cls: 0.046096, grad_norm: 0.494344
[[34m2025-10-04 12:13:38[0m] Step: 3114, Training Logs: loss_final: 0.896395, loss_mean: 0.849581, loss_mean_cls: 0.046814, grad_norm: 0.536375
[[34m2025-10-04 12:13:38[0m] Step: 3115, Training Logs: loss_final: 0.911883, loss_mean: 0.866766, loss_mean_cls: 0.045117, grad_norm: 0.409003
[[34m2025-10-04 12:13:39[0m] Step: 3116, Training Logs: loss_final: 0.901850, loss_mean: 0.853807, loss_mean_cls: 0.048043, grad_norm: 0.522491
[[34m2025-10-04 12:13:39[0m] Step: 3117, Training Logs: loss_final: 0.891117, loss_mean: 0.844935, loss_mean_cls: 0.046183, grad_norm: 0.490410
[[34m2025-10-04 12:13:39[0m] Step: 3118, Training Logs: loss_final: 0.903988, loss_mean: 0.858435, loss_mean_cls: 0.045554, grad_norm: 0.510596
[[34m2025-10-04 12:13:39[0m] Step: 3119, Training Logs: loss_final: 0.886908, loss_mean: 0.839803, loss_mean_cls: 0.047105, grad_norm: 0.512453
[[34m2025-10-04 12:13:40[0m] Step: 3120, Training Logs: loss_final: 0.894562, loss_mean: 0.847402, loss_mean_cls: 0.047160, grad_norm: 0.477176
[[34m2025-10-04 12:13:40[0m] Step: 3121, Training Logs: loss_final: 0.922739, loss_mean: 0.877038, loss_mean_cls: 0.045701, grad_norm: 0.460238
[[34m2025-10-04 12:13:40[0m] Step: 3122, Training Logs: loss_final: 0.899176, loss_mean: 0.851467, loss_mean_cls: 0.047709, grad_norm: 0.448375
[[34m2025-10-04 12:13:41[0m] Step: 3123, Training Logs: loss_final: 0.910047, loss_mean: 0.863799, loss_mean_cls: 0.046249, grad_norm: 0.625675
[[34m2025-10-04 12:13:41[0m] Step: 3124, Training Logs: loss_final: 0.889401, loss_mean: 0.842351, loss_mean_cls: 0.047050, grad_norm: 0.312896
[[34m2025-10-04 12:13:41[0m] Step: 3125, Training Logs: loss_final: 0.913243, loss_mean: 0.867690, loss_mean_cls: 0.045554, grad_norm: 0.473094
[[34m2025-10-04 12:13:41[0m] Step: 3126, Training Logs: loss_final: 0.928332, loss_mean: 0.882568, loss_mean_cls: 0.045764, grad_norm: 0.378643
[[34m2025-10-04 12:13:42[0m] Step: 3127, Training Logs: loss_final: 0.897978, loss_mean: 0.850996, loss_mean_cls: 0.046982, grad_norm: 0.435972
[[34m2025-10-04 12:13:42[0m] Step: 3128, Training Logs: loss_final: 0.906220, loss_mean: 0.859800, loss_mean_cls: 0.046420, grad_norm: 0.420622
[[34m2025-10-04 12:13:42[0m] Step: 3129, Training Logs: loss_final: 0.897484, loss_mean: 0.850665, loss_mean_cls: 0.046819, grad_norm: 0.330570
[[34m2025-10-04 12:13:43[0m] Step: 3130, Training Logs: loss_final: 0.881953, loss_mean: 0.835096, loss_mean_cls: 0.046857, grad_norm: 0.370145
[[34m2025-10-04 12:13:43[0m] Step: 3131, Training Logs: loss_final: 0.909367, loss_mean: 0.862771, loss_mean_cls: 0.046596, grad_norm: 0.410957
[[34m2025-10-04 12:13:43[0m] Step: 3132, Training Logs: loss_final: 0.930602, loss_mean: 0.884399, loss_mean_cls: 0.046204, grad_norm: 0.451067
[[34m2025-10-04 12:13:44[0m] Step: 3133, Training Logs: loss_final: 0.925609, loss_mean: 0.878744, loss_mean_cls: 0.046866, grad_norm: 0.607858
[[34m2025-10-04 12:13:44[0m] Step: 3134, Training Logs: loss_final: 0.909787, loss_mean: 0.862528, loss_mean_cls: 0.047260, grad_norm: 0.467093
[[34m2025-10-04 12:13:44[0m] Step: 3135, Training Logs: loss_final: 0.903527, loss_mean: 0.857388, loss_mean_cls: 0.046139, grad_norm: 0.383661
[[34m2025-10-04 12:13:44[0m] Step: 3136, Training Logs: loss_final: 0.900006, loss_mean: 0.854052, loss_mean_cls: 0.045954, grad_norm: 0.530850
[[34m2025-10-04 12:13:45[0m] Step: 3137, Training Logs: loss_final: 0.888492, loss_mean: 0.842369, loss_mean_cls: 0.046122, grad_norm: 0.378723
[[34m2025-10-04 12:13:45[0m] Step: 3138, Training Logs: loss_final: 0.902073, loss_mean: 0.854871, loss_mean_cls: 0.047202, grad_norm: 0.469952
[[34m2025-10-04 12:13:45[0m] Step: 3139, Training Logs: loss_final: 0.894838, loss_mean: 0.847394, loss_mean_cls: 0.047444, grad_norm: 0.422339
[[34m2025-10-04 12:13:46[0m] Step: 3140, Training Logs: loss_final: 0.885622, loss_mean: 0.838215, loss_mean_cls: 0.047407, grad_norm: 0.445962
[[34m2025-10-04 12:13:46[0m] Step: 3141, Training Logs: loss_final: 0.914581, loss_mean: 0.868844, loss_mean_cls: 0.045736, grad_norm: 0.502533
[[34m2025-10-04 12:13:46[0m] Step: 3142, Training Logs: loss_final: 0.913873, loss_mean: 0.867376, loss_mean_cls: 0.046498, grad_norm: 0.435702
[[34m2025-10-04 12:13:47[0m] Step: 3143, Training Logs: loss_final: 0.912634, loss_mean: 0.866319, loss_mean_cls: 0.046316, grad_norm: 0.377739
[[34m2025-10-04 12:13:47[0m] Step: 3144, Training Logs: loss_final: 0.896591, loss_mean: 0.850230, loss_mean_cls: 0.046361, grad_norm: 0.541995
[[34m2025-10-04 12:13:47[0m] Step: 3145, Training Logs: loss_final: 0.922371, loss_mean: 0.875581, loss_mean_cls: 0.046790, grad_norm: 0.591005
[[34m2025-10-04 12:13:47[0m] Step: 3146, Training Logs: loss_final: 0.881499, loss_mean: 0.834166, loss_mean_cls: 0.047333, grad_norm: 0.445960
[[34m2025-10-04 12:13:48[0m] Step: 3147, Training Logs: loss_final: 0.899418, loss_mean: 0.852918, loss_mean_cls: 0.046500, grad_norm: 0.500066
[[34m2025-10-04 12:13:48[0m] Step: 3148, Training Logs: loss_final: 0.912627, loss_mean: 0.866503, loss_mean_cls: 0.046124, grad_norm: 0.378333
[[34m2025-10-04 12:13:48[0m] Step: 3149, Training Logs: loss_final: 0.902329, loss_mean: 0.855949, loss_mean_cls: 0.046380, grad_norm: 0.452314
[[34m2025-10-04 12:13:49[0m] Step: 3150, Training Logs: loss_final: 0.896897, loss_mean: 0.850714, loss_mean_cls: 0.046183, grad_norm: 0.550256
[[34m2025-10-04 12:13:49[0m] Step: 3151, Training Logs: loss_final: 0.903031, loss_mean: 0.856092, loss_mean_cls: 0.046939, grad_norm: 0.388249
[[34m2025-10-04 12:13:49[0m] Step: 3152, Training Logs: loss_final: 0.910382, loss_mean: 0.862999, loss_mean_cls: 0.047383, grad_norm: 0.467512
[[34m2025-10-04 12:13:49[0m] Step: 3153, Training Logs: loss_final: 0.915813, loss_mean: 0.871138, loss_mean_cls: 0.044675, grad_norm: 0.480222
[[34m2025-10-04 12:13:50[0m] Step: 3154, Training Logs: loss_final: 0.910957, loss_mean: 0.862839, loss_mean_cls: 0.048118, grad_norm: 0.453210
[[34m2025-10-04 12:13:50[0m] Step: 3155, Training Logs: loss_final: 0.878824, loss_mean: 0.830898, loss_mean_cls: 0.047926, grad_norm: 0.584447
[[34m2025-10-04 12:13:50[0m] Step: 3156, Training Logs: loss_final: 0.895443, loss_mean: 0.849482, loss_mean_cls: 0.045961, grad_norm: 0.403823
[[34m2025-10-04 12:13:51[0m] Step: 3157, Training Logs: loss_final: 0.894436, loss_mean: 0.848431, loss_mean_cls: 0.046005, grad_norm: 0.554812
[[34m2025-10-04 12:13:51[0m] Step: 3158, Training Logs: loss_final: 0.914234, loss_mean: 0.867576, loss_mean_cls: 0.046658, grad_norm: 0.504322
[[34m2025-10-04 12:13:51[0m] Step: 3159, Training Logs: loss_final: 0.903717, loss_mean: 0.857806, loss_mean_cls: 0.045911, grad_norm: 0.662377
[[34m2025-10-04 12:13:52[0m] Step: 3160, Training Logs: loss_final: 0.915997, loss_mean: 0.868189, loss_mean_cls: 0.047808, grad_norm: 0.595824
[[34m2025-10-04 12:13:52[0m] Step: 3161, Training Logs: loss_final: 0.906494, loss_mean: 0.860031, loss_mean_cls: 0.046463, grad_norm: 0.547483
[[34m2025-10-04 12:13:52[0m] Step: 3162, Training Logs: loss_final: 0.932468, loss_mean: 0.886682, loss_mean_cls: 0.045786, grad_norm: 0.550928
[[34m2025-10-04 12:13:52[0m] Step: 3163, Training Logs: loss_final: 0.899497, loss_mean: 0.853928, loss_mean_cls: 0.045569, grad_norm: 0.476032
[[34m2025-10-04 12:13:53[0m] Step: 3164, Training Logs: loss_final: 0.928974, loss_mean: 0.883125, loss_mean_cls: 0.045849, grad_norm: 0.523893
[[34m2025-10-04 12:13:53[0m] Step: 3165, Training Logs: loss_final: 0.900736, loss_mean: 0.854066, loss_mean_cls: 0.046670, grad_norm: 0.692785
[[34m2025-10-04 12:13:53[0m] Step: 3166, Training Logs: loss_final: 0.895082, loss_mean: 0.847892, loss_mean_cls: 0.047190, grad_norm: 0.521687
[[34m2025-10-04 12:13:54[0m] Step: 3167, Training Logs: loss_final: 0.895401, loss_mean: 0.848360, loss_mean_cls: 0.047041, grad_norm: 0.634488
[[34m2025-10-04 12:13:54[0m] Step: 3168, Training Logs: loss_final: 0.934826, loss_mean: 0.888930, loss_mean_cls: 0.045896, grad_norm: 0.461694
[[34m2025-10-04 12:13:54[0m] Step: 3169, Training Logs: loss_final: 0.924110, loss_mean: 0.878512, loss_mean_cls: 0.045598, grad_norm: 0.691629
[[34m2025-10-04 12:13:54[0m] Step: 3170, Training Logs: loss_final: 0.901348, loss_mean: 0.854967, loss_mean_cls: 0.046381, grad_norm: 0.688299
[[34m2025-10-04 12:13:55[0m] Step: 3171, Training Logs: loss_final: 0.894075, loss_mean: 0.848169, loss_mean_cls: 0.045906, grad_norm: 0.482909
[[34m2025-10-04 12:13:55[0m] Step: 3172, Training Logs: loss_final: 0.891398, loss_mean: 0.845075, loss_mean_cls: 0.046322, grad_norm: 0.741848
[[34m2025-10-04 12:13:55[0m] Step: 3173, Training Logs: loss_final: 0.914993, loss_mean: 0.868804, loss_mean_cls: 0.046189, grad_norm: 0.449200
[[34m2025-10-04 12:13:56[0m] Step: 3174, Training Logs: loss_final: 0.916018, loss_mean: 0.869556, loss_mean_cls: 0.046462, grad_norm: 0.607571
[[34m2025-10-04 12:13:56[0m] Step: 3175, Training Logs: loss_final: 0.907685, loss_mean: 0.861862, loss_mean_cls: 0.045823, grad_norm: 0.761122
[[34m2025-10-04 12:13:56[0m] Step: 3176, Training Logs: loss_final: 0.924047, loss_mean: 0.877517, loss_mean_cls: 0.046530, grad_norm: 0.363559
[[34m2025-10-04 12:13:56[0m] Step: 3177, Training Logs: loss_final: 0.894243, loss_mean: 0.847640, loss_mean_cls: 0.046603, grad_norm: 0.506362
[[34m2025-10-04 12:13:57[0m] Step: 3178, Training Logs: loss_final: 0.919144, loss_mean: 0.872138, loss_mean_cls: 0.047006, grad_norm: 0.705726
[[34m2025-10-04 12:13:57[0m] Step: 3179, Training Logs: loss_final: 0.888145, loss_mean: 0.840818, loss_mean_cls: 0.047328, grad_norm: 0.506926
[[34m2025-10-04 12:13:57[0m] Step: 3180, Training Logs: loss_final: 0.917888, loss_mean: 0.871888, loss_mean_cls: 0.046000, grad_norm: 0.475717
[[34m2025-10-04 12:13:58[0m] Step: 3181, Training Logs: loss_final: 0.891310, loss_mean: 0.843963, loss_mean_cls: 0.047347, grad_norm: 0.504446
[[34m2025-10-04 12:13:58[0m] Step: 3182, Training Logs: loss_final: 0.882830, loss_mean: 0.836671, loss_mean_cls: 0.046159, grad_norm: 0.410790
[[34m2025-10-04 12:13:58[0m] Step: 3183, Training Logs: loss_final: 0.913880, loss_mean: 0.868530, loss_mean_cls: 0.045350, grad_norm: 0.525517
[[34m2025-10-04 12:13:58[0m] Step: 3184, Training Logs: loss_final: 0.898562, loss_mean: 0.851647, loss_mean_cls: 0.046915, grad_norm: 0.450403
[[34m2025-10-04 12:13:59[0m] Step: 3185, Training Logs: loss_final: 0.896490, loss_mean: 0.848849, loss_mean_cls: 0.047641, grad_norm: 0.497811
[[34m2025-10-04 12:13:59[0m] Step: 3186, Training Logs: loss_final: 0.884767, loss_mean: 0.838397, loss_mean_cls: 0.046370, grad_norm: 0.424845
[[34m2025-10-04 12:13:59[0m] Step: 3187, Training Logs: loss_final: 0.899613, loss_mean: 0.852929, loss_mean_cls: 0.046684, grad_norm: 0.390994
[[34m2025-10-04 12:14:00[0m] Step: 3188, Training Logs: loss_final: 0.918578, loss_mean: 0.871499, loss_mean_cls: 0.047079, grad_norm: 0.431448
[[34m2025-10-04 12:14:00[0m] Step: 3189, Training Logs: loss_final: 0.921153, loss_mean: 0.873888, loss_mean_cls: 0.047265, grad_norm: 0.359245
[[34m2025-10-04 12:14:00[0m] Step: 3190, Training Logs: loss_final: 0.880031, loss_mean: 0.832850, loss_mean_cls: 0.047181, grad_norm: 0.405259
[[34m2025-10-04 12:14:01[0m] Step: 3191, Training Logs: loss_final: 0.905429, loss_mean: 0.859010, loss_mean_cls: 0.046419, grad_norm: 0.489847
[[34m2025-10-04 12:14:01[0m] Step: 3192, Training Logs: loss_final: 0.900373, loss_mean: 0.853003, loss_mean_cls: 0.047370, grad_norm: 0.381002
[[34m2025-10-04 12:14:01[0m] Step: 3193, Training Logs: loss_final: 0.895157, loss_mean: 0.848784, loss_mean_cls: 0.046373, grad_norm: 0.389042
[[34m2025-10-04 12:14:01[0m] Step: 3194, Training Logs: loss_final: 0.889954, loss_mean: 0.844284, loss_mean_cls: 0.045670, grad_norm: 0.338378
[[34m2025-10-04 12:14:02[0m] Step: 3195, Training Logs: loss_final: 0.913370, loss_mean: 0.868272, loss_mean_cls: 0.045098, grad_norm: 0.420079
[[34m2025-10-04 12:14:02[0m] Step: 3196, Training Logs: loss_final: 0.908667, loss_mean: 0.863896, loss_mean_cls: 0.044770, grad_norm: 0.396999
[[34m2025-10-04 12:14:02[0m] Step: 3197, Training Logs: loss_final: 0.893476, loss_mean: 0.845905, loss_mean_cls: 0.047571, grad_norm: 0.371995
[[34m2025-10-04 12:14:03[0m] Step: 3198, Training Logs: loss_final: 0.897194, loss_mean: 0.849486, loss_mean_cls: 0.047708, grad_norm: 0.419179
[[34m2025-10-04 12:14:03[0m] Step: 3199, Training Logs: loss_final: 0.906071, loss_mean: 0.859767, loss_mean_cls: 0.046304, grad_norm: 0.361402
[[34m2025-10-04 12:14:03[0m] Step: 3200, Training Logs: loss_final: 0.888328, loss_mean: 0.841388, loss_mean_cls: 0.046940, grad_norm: 0.293965
[[34m2025-10-04 12:14:03[0m] Step: 3201, Training Logs: loss_final: 0.926575, loss_mean: 0.881166, loss_mean_cls: 0.045410, grad_norm: 0.387672
[[34m2025-10-04 12:14:04[0m] Step: 3202, Training Logs: loss_final: 0.887674, loss_mean: 0.841227, loss_mean_cls: 0.046447, grad_norm: 0.371445
[[34m2025-10-04 12:14:04[0m] Step: 3203, Training Logs: loss_final: 0.912485, loss_mean: 0.866813, loss_mean_cls: 0.045672, grad_norm: 0.442794
[[34m2025-10-04 12:14:04[0m] Step: 3204, Training Logs: loss_final: 0.895594, loss_mean: 0.848940, loss_mean_cls: 0.046654, grad_norm: 0.329725
[[34m2025-10-04 12:14:05[0m] Step: 3205, Training Logs: loss_final: 0.907526, loss_mean: 0.862347, loss_mean_cls: 0.045178, grad_norm: 0.515098
[[34m2025-10-04 12:14:05[0m] Step: 3206, Training Logs: loss_final: 0.905931, loss_mean: 0.858363, loss_mean_cls: 0.047568, grad_norm: 0.472532
[[34m2025-10-04 12:14:05[0m] Step: 3207, Training Logs: loss_final: 0.905877, loss_mean: 0.859240, loss_mean_cls: 0.046637, grad_norm: 0.504298
[[34m2025-10-04 12:14:06[0m] Step: 3208, Training Logs: loss_final: 0.882683, loss_mean: 0.836340, loss_mean_cls: 0.046343, grad_norm: 0.590679
[[34m2025-10-04 12:14:06[0m] Step: 3209, Training Logs: loss_final: 0.907497, loss_mean: 0.861345, loss_mean_cls: 0.046152, grad_norm: 0.520498
[[34m2025-10-04 12:14:06[0m] Step: 3210, Training Logs: loss_final: 0.900023, loss_mean: 0.853715, loss_mean_cls: 0.046307, grad_norm: 0.396976
[[34m2025-10-04 12:14:07[0m] Step: 3211, Training Logs: loss_final: 0.911067, loss_mean: 0.864768, loss_mean_cls: 0.046299, grad_norm: 0.507287
[[34m2025-10-04 12:14:07[0m] Step: 3212, Training Logs: loss_final: 0.900557, loss_mean: 0.854573, loss_mean_cls: 0.045984, grad_norm: 0.449661
[[34m2025-10-04 12:14:07[0m] Step: 3213, Training Logs: loss_final: 0.903344, loss_mean: 0.856384, loss_mean_cls: 0.046961, grad_norm: 0.539719
[[34m2025-10-04 12:14:07[0m] Step: 3214, Training Logs: loss_final: 0.911383, loss_mean: 0.864721, loss_mean_cls: 0.046662, grad_norm: 0.643089
[[34m2025-10-04 12:14:08[0m] Step: 3215, Training Logs: loss_final: 0.909235, loss_mean: 0.862650, loss_mean_cls: 0.046585, grad_norm: 0.429492
[[34m2025-10-04 12:14:08[0m] Step: 3216, Training Logs: loss_final: 0.917120, loss_mean: 0.871804, loss_mean_cls: 0.045315, grad_norm: 0.800465
[[34m2025-10-04 12:14:08[0m] Step: 3217, Training Logs: loss_final: 0.890845, loss_mean: 0.844291, loss_mean_cls: 0.046554, grad_norm: 0.613725
[[34m2025-10-04 12:14:09[0m] Step: 3218, Training Logs: loss_final: 0.901190, loss_mean: 0.854836, loss_mean_cls: 0.046355, grad_norm: 0.552096
[[34m2025-10-04 12:14:09[0m] Step: 3219, Training Logs: loss_final: 0.871839, loss_mean: 0.823809, loss_mean_cls: 0.048030, grad_norm: 0.652066
[[34m2025-10-04 12:14:09[0m] Step: 3220, Training Logs: loss_final: 0.906694, loss_mean: 0.860668, loss_mean_cls: 0.046026, grad_norm: 0.376566
[[34m2025-10-04 12:14:09[0m] Step: 3221, Training Logs: loss_final: 0.919777, loss_mean: 0.873188, loss_mean_cls: 0.046589, grad_norm: 0.640952
[[34m2025-10-04 12:14:10[0m] Step: 3222, Training Logs: loss_final: 0.895725, loss_mean: 0.848549, loss_mean_cls: 0.047176, grad_norm: 0.542196
[[34m2025-10-04 12:14:10[0m] Step: 3223, Training Logs: loss_final: 0.898225, loss_mean: 0.852102, loss_mean_cls: 0.046123, grad_norm: 0.442705
[[34m2025-10-04 12:14:10[0m] Step: 3224, Training Logs: loss_final: 0.882232, loss_mean: 0.835663, loss_mean_cls: 0.046570, grad_norm: 0.521613
[[34m2025-10-04 12:14:11[0m] Step: 3225, Training Logs: loss_final: 0.882259, loss_mean: 0.835568, loss_mean_cls: 0.046691, grad_norm: 0.534836
[[34m2025-10-04 12:14:11[0m] Step: 3226, Training Logs: loss_final: 0.904122, loss_mean: 0.858211, loss_mean_cls: 0.045912, grad_norm: 0.471713
[[34m2025-10-04 12:14:11[0m] Step: 3227, Training Logs: loss_final: 0.900078, loss_mean: 0.852437, loss_mean_cls: 0.047640, grad_norm: 0.696367
[[34m2025-10-04 12:14:11[0m] Step: 3228, Training Logs: loss_final: 0.900079, loss_mean: 0.853700, loss_mean_cls: 0.046379, grad_norm: 0.374038
[[34m2025-10-04 12:14:12[0m] Step: 3229, Training Logs: loss_final: 0.914354, loss_mean: 0.868163, loss_mean_cls: 0.046192, grad_norm: 0.450879
[[34m2025-10-04 12:14:12[0m] Step: 3230, Training Logs: loss_final: 0.910273, loss_mean: 0.863235, loss_mean_cls: 0.047038, grad_norm: 0.588894
[[34m2025-10-04 12:14:12[0m] Step: 3231, Training Logs: loss_final: 0.903877, loss_mean: 0.858303, loss_mean_cls: 0.045574, grad_norm: 0.427158
[[34m2025-10-04 12:14:13[0m] Step: 3232, Training Logs: loss_final: 0.924667, loss_mean: 0.878944, loss_mean_cls: 0.045723, grad_norm: 0.564911
[[34m2025-10-04 12:14:13[0m] Step: 3233, Training Logs: loss_final: 0.902265, loss_mean: 0.856090, loss_mean_cls: 0.046175, grad_norm: 0.680161
[[34m2025-10-04 12:14:13[0m] Step: 3234, Training Logs: loss_final: 0.907961, loss_mean: 0.861914, loss_mean_cls: 0.046047, grad_norm: 0.482581
[[34m2025-10-04 12:14:13[0m] Step: 3235, Training Logs: loss_final: 0.891886, loss_mean: 0.845430, loss_mean_cls: 0.046456, grad_norm: 0.530364
[[34m2025-10-04 12:14:14[0m] Step: 3236, Training Logs: loss_final: 0.906353, loss_mean: 0.860154, loss_mean_cls: 0.046199, grad_norm: 0.496873
[[34m2025-10-04 12:14:14[0m] Step: 3237, Training Logs: loss_final: 0.905265, loss_mean: 0.858031, loss_mean_cls: 0.047234, grad_norm: 0.465525
[[34m2025-10-04 12:14:14[0m] Step: 3238, Training Logs: loss_final: 0.909431, loss_mean: 0.862564, loss_mean_cls: 0.046866, grad_norm: 0.546175
[[34m2025-10-04 12:14:15[0m] Step: 3239, Training Logs: loss_final: 0.897475, loss_mean: 0.849320, loss_mean_cls: 0.048155, grad_norm: 0.471470
[[34m2025-10-04 12:14:15[0m] Step: 3240, Training Logs: loss_final: 0.890871, loss_mean: 0.845388, loss_mean_cls: 0.045482, grad_norm: 0.464194
[[34m2025-10-04 12:14:15[0m] Step: 3241, Training Logs: loss_final: 0.913208, loss_mean: 0.865505, loss_mean_cls: 0.047702, grad_norm: 0.345036
[[34m2025-10-04 12:14:16[0m] Step: 3242, Training Logs: loss_final: 0.890769, loss_mean: 0.845935, loss_mean_cls: 0.044834, grad_norm: 0.405781
[[34m2025-10-04 12:14:16[0m] Step: 3243, Training Logs: loss_final: 0.886819, loss_mean: 0.840049, loss_mean_cls: 0.046770, grad_norm: 0.312024
[[34m2025-10-04 12:14:16[0m] Step: 3244, Training Logs: loss_final: 0.914467, loss_mean: 0.868774, loss_mean_cls: 0.045694, grad_norm: 0.317791
[[34m2025-10-04 12:14:16[0m] Step: 3245, Training Logs: loss_final: 0.913095, loss_mean: 0.866733, loss_mean_cls: 0.046363, grad_norm: 0.440219
[[34m2025-10-04 12:14:17[0m] Step: 3246, Training Logs: loss_final: 0.915917, loss_mean: 0.870165, loss_mean_cls: 0.045752, grad_norm: 0.365899
[[34m2025-10-04 12:14:17[0m] Step: 3247, Training Logs: loss_final: 0.934557, loss_mean: 0.888639, loss_mean_cls: 0.045918, grad_norm: 0.371206
[[34m2025-10-04 12:14:17[0m] Step: 3248, Training Logs: loss_final: 0.903419, loss_mean: 0.856482, loss_mean_cls: 0.046937, grad_norm: 0.520941
[[34m2025-10-04 12:14:18[0m] Step: 3249, Training Logs: loss_final: 0.875756, loss_mean: 0.829028, loss_mean_cls: 0.046728, grad_norm: 0.360116
[[34m2025-10-04 12:14:18[0m] Step: 3250, Training Logs: loss_final: 0.905062, loss_mean: 0.859058, loss_mean_cls: 0.046004, grad_norm: 0.655991
[[34m2025-10-04 12:14:18[0m] Step: 3251, Training Logs: loss_final: 0.911004, loss_mean: 0.864173, loss_mean_cls: 0.046831, grad_norm: 0.467734
[[34m2025-10-04 12:14:19[0m] Step: 3252, Training Logs: loss_final: 0.914830, loss_mean: 0.868682, loss_mean_cls: 0.046148, grad_norm: 0.595303
[[34m2025-10-04 12:14:19[0m] Step: 3253, Training Logs: loss_final: 0.903063, loss_mean: 0.856346, loss_mean_cls: 0.046717, grad_norm: 0.340077
[[34m2025-10-04 12:14:19[0m] Step: 3254, Training Logs: loss_final: 0.906714, loss_mean: 0.859731, loss_mean_cls: 0.046983, grad_norm: 0.435739
[[34m2025-10-04 12:14:19[0m] Step: 3255, Training Logs: loss_final: 0.913678, loss_mean: 0.868702, loss_mean_cls: 0.044976, grad_norm: 0.450557
[[34m2025-10-04 12:14:20[0m] Step: 3256, Training Logs: loss_final: 0.908447, loss_mean: 0.861516, loss_mean_cls: 0.046930, grad_norm: 0.438478
[[34m2025-10-04 12:14:20[0m] Step: 3257, Training Logs: loss_final: 0.874696, loss_mean: 0.826948, loss_mean_cls: 0.047748, grad_norm: 0.490543
[[34m2025-10-04 12:14:20[0m] Step: 3258, Training Logs: loss_final: 0.901757, loss_mean: 0.854947, loss_mean_cls: 0.046809, grad_norm: 0.483586
[[34m2025-10-04 12:14:21[0m] Step: 3259, Training Logs: loss_final: 0.898919, loss_mean: 0.852049, loss_mean_cls: 0.046870, grad_norm: 0.406280
[[34m2025-10-04 12:14:21[0m] Step: 3260, Training Logs: loss_final: 0.904404, loss_mean: 0.858029, loss_mean_cls: 0.046375, grad_norm: 0.361190
[[34m2025-10-04 12:14:21[0m] Step: 3261, Training Logs: loss_final: 0.886970, loss_mean: 0.840645, loss_mean_cls: 0.046326, grad_norm: 0.499594
[[34m2025-10-04 12:14:22[0m] Step: 3262, Training Logs: loss_final: 0.890922, loss_mean: 0.845191, loss_mean_cls: 0.045731, grad_norm: 0.355283
[[34m2025-10-04 12:14:22[0m] Step: 3263, Training Logs: loss_final: 0.877180, loss_mean: 0.830874, loss_mean_cls: 0.046306, grad_norm: 0.452258
[[34m2025-10-04 12:14:22[0m] Step: 3264, Training Logs: loss_final: 0.905485, loss_mean: 0.859876, loss_mean_cls: 0.045609, grad_norm: 0.308022
[[34m2025-10-04 12:14:22[0m] Step: 3265, Training Logs: loss_final: 0.894431, loss_mean: 0.848120, loss_mean_cls: 0.046311, grad_norm: 0.415203
[[34m2025-10-04 12:14:23[0m] Step: 3266, Training Logs: loss_final: 0.895298, loss_mean: 0.849273, loss_mean_cls: 0.046025, grad_norm: 0.391818
[[34m2025-10-04 12:14:23[0m] Step: 3267, Training Logs: loss_final: 0.896314, loss_mean: 0.848846, loss_mean_cls: 0.047468, grad_norm: 0.478385
[[34m2025-10-04 12:14:23[0m] Step: 3268, Training Logs: loss_final: 0.919855, loss_mean: 0.873901, loss_mean_cls: 0.045955, grad_norm: 0.526895
[[34m2025-10-04 12:14:24[0m] Step: 3269, Training Logs: loss_final: 0.913191, loss_mean: 0.866631, loss_mean_cls: 0.046560, grad_norm: 0.373679
[[34m2025-10-04 12:14:24[0m] Step: 3270, Training Logs: loss_final: 0.906235, loss_mean: 0.861040, loss_mean_cls: 0.045195, grad_norm: 0.666355
[[34m2025-10-04 12:14:24[0m] Step: 3271, Training Logs: loss_final: 0.907089, loss_mean: 0.859874, loss_mean_cls: 0.047215, grad_norm: 0.435274
[[34m2025-10-04 12:14:25[0m] Step: 3272, Training Logs: loss_final: 0.897584, loss_mean: 0.852137, loss_mean_cls: 0.045447, grad_norm: 0.467777
[[34m2025-10-04 12:14:25[0m] Step: 3273, Training Logs: loss_final: 0.909113, loss_mean: 0.863098, loss_mean_cls: 0.046016, grad_norm: 0.567913
[[34m2025-10-04 12:14:25[0m] Step: 3274, Training Logs: loss_final: 0.905044, loss_mean: 0.859000, loss_mean_cls: 0.046044, grad_norm: 0.393654
[[34m2025-10-04 12:14:25[0m] Step: 3275, Training Logs: loss_final: 0.899030, loss_mean: 0.852240, loss_mean_cls: 0.046790, grad_norm: 0.503590
[[34m2025-10-04 12:14:26[0m] Step: 3276, Training Logs: loss_final: 0.904397, loss_mean: 0.857967, loss_mean_cls: 0.046430, grad_norm: 0.344625
[[34m2025-10-04 12:14:26[0m] Step: 3277, Training Logs: loss_final: 0.895431, loss_mean: 0.848429, loss_mean_cls: 0.047002, grad_norm: 0.381893
[[34m2025-10-04 12:14:26[0m] Step: 3278, Training Logs: loss_final: 0.922460, loss_mean: 0.875984, loss_mean_cls: 0.046476, grad_norm: 0.321709
[[34m2025-10-04 12:14:27[0m] Step: 3279, Training Logs: loss_final: 0.894658, loss_mean: 0.846997, loss_mean_cls: 0.047661, grad_norm: 0.287772
[[34m2025-10-04 12:14:27[0m] Step: 3280, Training Logs: loss_final: 0.899169, loss_mean: 0.852302, loss_mean_cls: 0.046867, grad_norm: 0.301376
[[34m2025-10-04 12:14:27[0m] Step: 3281, Training Logs: loss_final: 0.884806, loss_mean: 0.837180, loss_mean_cls: 0.047626, grad_norm: 0.395342
[[34m2025-10-04 12:14:27[0m] Step: 3282, Training Logs: loss_final: 0.890374, loss_mean: 0.844292, loss_mean_cls: 0.046082, grad_norm: 0.291170
[[34m2025-10-04 12:14:28[0m] Step: 3283, Training Logs: loss_final: 0.921376, loss_mean: 0.874961, loss_mean_cls: 0.046414, grad_norm: 0.516281
[[34m2025-10-04 12:14:28[0m] Step: 3284, Training Logs: loss_final: 0.905033, loss_mean: 0.857917, loss_mean_cls: 0.047117, grad_norm: 0.351566
[[34m2025-10-04 12:14:28[0m] Step: 3285, Training Logs: loss_final: 0.912624, loss_mean: 0.868022, loss_mean_cls: 0.044601, grad_norm: 0.458554
[[34m2025-10-04 12:14:29[0m] Step: 3286, Training Logs: loss_final: 0.884132, loss_mean: 0.837435, loss_mean_cls: 0.046697, grad_norm: 0.520255
[[34m2025-10-04 12:14:29[0m] Step: 3287, Training Logs: loss_final: 0.920915, loss_mean: 0.874729, loss_mean_cls: 0.046186, grad_norm: 0.444480
[[34m2025-10-04 12:14:29[0m] Step: 3288, Training Logs: loss_final: 0.897618, loss_mean: 0.851418, loss_mean_cls: 0.046200, grad_norm: 0.512873
[[34m2025-10-04 12:14:29[0m] Step: 3289, Training Logs: loss_final: 0.885428, loss_mean: 0.838324, loss_mean_cls: 0.047104, grad_norm: 0.437682
[[34m2025-10-04 12:14:30[0m] Step: 3290, Training Logs: loss_final: 0.903758, loss_mean: 0.857684, loss_mean_cls: 0.046074, grad_norm: 0.546212
[[34m2025-10-04 12:14:30[0m] Step: 3291, Training Logs: loss_final: 0.905894, loss_mean: 0.858847, loss_mean_cls: 0.047047, grad_norm: 0.355687
[[34m2025-10-04 12:14:30[0m] Step: 3292, Training Logs: loss_final: 0.884306, loss_mean: 0.837262, loss_mean_cls: 0.047043, grad_norm: 0.366697
[[34m2025-10-04 12:14:31[0m] Step: 3293, Training Logs: loss_final: 0.911021, loss_mean: 0.864585, loss_mean_cls: 0.046436, grad_norm: 0.462336
[[34m2025-10-04 12:14:31[0m] Step: 3294, Training Logs: loss_final: 0.896720, loss_mean: 0.851584, loss_mean_cls: 0.045136, grad_norm: 0.378933
[[34m2025-10-04 12:14:31[0m] Step: 3295, Training Logs: loss_final: 0.899040, loss_mean: 0.853540, loss_mean_cls: 0.045500, grad_norm: 0.314936
[[34m2025-10-04 12:14:32[0m] Step: 3296, Training Logs: loss_final: 0.899555, loss_mean: 0.853437, loss_mean_cls: 0.046118, grad_norm: 0.581318
[[34m2025-10-04 12:14:32[0m] Step: 3297, Training Logs: loss_final: 0.897925, loss_mean: 0.851009, loss_mean_cls: 0.046917, grad_norm: 0.458085
[[34m2025-10-04 12:14:32[0m] Step: 3298, Training Logs: loss_final: 0.890558, loss_mean: 0.844136, loss_mean_cls: 0.046423, grad_norm: 0.478836
[[34m2025-10-04 12:14:32[0m] Step: 3299, Training Logs: loss_final: 0.902542, loss_mean: 0.855069, loss_mean_cls: 0.047473, grad_norm: 0.480549
[[34m2025-10-04 12:14:33[0m] Step: 3300, Training Logs: loss_final: 0.905401, loss_mean: 0.859513, loss_mean_cls: 0.045888, grad_norm: 0.332653
[[34m2025-10-04 12:14:33[0m] Step: 3301, Training Logs: loss_final: 0.900099, loss_mean: 0.852974, loss_mean_cls: 0.047125, grad_norm: 0.386575
[[34m2025-10-04 12:14:33[0m] Step: 3302, Training Logs: loss_final: 0.881371, loss_mean: 0.834403, loss_mean_cls: 0.046968, grad_norm: 0.462721
[[34m2025-10-04 12:14:34[0m] Step: 3303, Training Logs: loss_final: 0.900708, loss_mean: 0.854852, loss_mean_cls: 0.045856, grad_norm: 0.333659
[[34m2025-10-04 12:14:34[0m] Step: 3304, Training Logs: loss_final: 0.913937, loss_mean: 0.867526, loss_mean_cls: 0.046411, grad_norm: 0.368210
[[34m2025-10-04 12:14:34[0m] Step: 3305, Training Logs: loss_final: 0.895912, loss_mean: 0.849426, loss_mean_cls: 0.046487, grad_norm: 0.394644
[[34m2025-10-04 12:14:34[0m] Step: 3306, Training Logs: loss_final: 0.902755, loss_mean: 0.855650, loss_mean_cls: 0.047106, grad_norm: 0.367933
[[34m2025-10-04 12:14:35[0m] Step: 3307, Training Logs: loss_final: 0.873639, loss_mean: 0.826463, loss_mean_cls: 0.047176, grad_norm: 0.431221
[[34m2025-10-04 12:14:35[0m] Step: 3308, Training Logs: loss_final: 0.889257, loss_mean: 0.842062, loss_mean_cls: 0.047195, grad_norm: 0.318395
[[34m2025-10-04 12:14:35[0m] Step: 3309, Training Logs: loss_final: 0.923061, loss_mean: 0.877602, loss_mean_cls: 0.045459, grad_norm: 0.385605
[[34m2025-10-04 12:14:36[0m] Step: 3310, Training Logs: loss_final: 0.893869, loss_mean: 0.847018, loss_mean_cls: 0.046851, grad_norm: 0.544776
[[34m2025-10-04 12:14:36[0m] Step: 3311, Training Logs: loss_final: 0.898292, loss_mean: 0.851583, loss_mean_cls: 0.046710, grad_norm: 0.439918
[[34m2025-10-04 12:14:36[0m] Step: 3312, Training Logs: loss_final: 0.909539, loss_mean: 0.864008, loss_mean_cls: 0.045531, grad_norm: 0.463123
[[34m2025-10-04 12:14:36[0m] Step: 3313, Training Logs: loss_final: 0.891775, loss_mean: 0.845686, loss_mean_cls: 0.046089, grad_norm: 0.507715
[[34m2025-10-04 12:14:37[0m] Step: 3314, Training Logs: loss_final: 0.899415, loss_mean: 0.851860, loss_mean_cls: 0.047554, grad_norm: 0.355490
[[34m2025-10-04 12:14:37[0m] Step: 3315, Training Logs: loss_final: 0.908199, loss_mean: 0.861168, loss_mean_cls: 0.047030, grad_norm: 0.529788
[[34m2025-10-04 12:14:37[0m] Step: 3316, Training Logs: loss_final: 0.912871, loss_mean: 0.865699, loss_mean_cls: 0.047172, grad_norm: 0.373133
[[34m2025-10-04 12:14:38[0m] Step: 3317, Training Logs: loss_final: 0.903559, loss_mean: 0.858224, loss_mean_cls: 0.045335, grad_norm: 0.465134
[[34m2025-10-04 12:14:38[0m] Step: 3318, Training Logs: loss_final: 0.898070, loss_mean: 0.850800, loss_mean_cls: 0.047270, grad_norm: 0.460705
[[34m2025-10-04 12:14:38[0m] Step: 3319, Training Logs: loss_final: 0.908092, loss_mean: 0.861802, loss_mean_cls: 0.046290, grad_norm: 0.508804
[[34m2025-10-04 12:14:39[0m] Step: 3320, Training Logs: loss_final: 0.878334, loss_mean: 0.831543, loss_mean_cls: 0.046791, grad_norm: 0.437957
[[34m2025-10-04 12:14:39[0m] Step: 3321, Training Logs: loss_final: 0.900051, loss_mean: 0.853496, loss_mean_cls: 0.046555, grad_norm: 0.622588
[[34m2025-10-04 12:14:39[0m] Step: 3322, Training Logs: loss_final: 0.893762, loss_mean: 0.847971, loss_mean_cls: 0.045791, grad_norm: 0.355061
[[34m2025-10-04 12:14:39[0m] Step: 3323, Training Logs: loss_final: 0.895019, loss_mean: 0.848952, loss_mean_cls: 0.046068, grad_norm: 0.574461
[[34m2025-10-04 12:14:40[0m] Step: 3324, Training Logs: loss_final: 0.909202, loss_mean: 0.862401, loss_mean_cls: 0.046800, grad_norm: 0.519089
[[34m2025-10-04 12:14:40[0m] Step: 3325, Training Logs: loss_final: 0.892178, loss_mean: 0.844750, loss_mean_cls: 0.047429, grad_norm: 0.330091
[[34m2025-10-04 12:14:40[0m] Step: 3326, Training Logs: loss_final: 0.911728, loss_mean: 0.864763, loss_mean_cls: 0.046965, grad_norm: 0.567480
[[34m2025-10-04 12:14:41[0m] Step: 3327, Training Logs: loss_final: 0.892347, loss_mean: 0.845359, loss_mean_cls: 0.046987, grad_norm: 0.445272
[[34m2025-10-04 12:14:41[0m] Step: 3328, Training Logs: loss_final: 0.908783, loss_mean: 0.863118, loss_mean_cls: 0.045665, grad_norm: 0.425231
[[34m2025-10-04 12:14:41[0m] Step: 3329, Training Logs: loss_final: 0.887611, loss_mean: 0.840607, loss_mean_cls: 0.047003, grad_norm: 0.562245
[[34m2025-10-04 12:14:41[0m] Step: 3330, Training Logs: loss_final: 0.900339, loss_mean: 0.852871, loss_mean_cls: 0.047468, grad_norm: 0.417919
[[34m2025-10-04 12:14:42[0m] Step: 3331, Training Logs: loss_final: 0.899004, loss_mean: 0.851533, loss_mean_cls: 0.047470, grad_norm: 0.375307
[[34m2025-10-04 12:14:42[0m] Step: 3332, Training Logs: loss_final: 0.924136, loss_mean: 0.878861, loss_mean_cls: 0.045274, grad_norm: 0.500235
[[34m2025-10-04 12:14:42[0m] Step: 3333, Training Logs: loss_final: 0.898892, loss_mean: 0.851991, loss_mean_cls: 0.046901, grad_norm: 0.507634
[[34m2025-10-04 12:14:43[0m] Step: 3334, Training Logs: loss_final: 0.931199, loss_mean: 0.886684, loss_mean_cls: 0.044515, grad_norm: 0.441701
[[34m2025-10-04 12:14:43[0m] Step: 3335, Training Logs: loss_final: 0.904995, loss_mean: 0.857812, loss_mean_cls: 0.047183, grad_norm: 0.520489
[[34m2025-10-04 12:14:43[0m] Step: 3336, Training Logs: loss_final: 0.907183, loss_mean: 0.861273, loss_mean_cls: 0.045910, grad_norm: 0.308358
[[34m2025-10-04 12:14:43[0m] Step: 3337, Training Logs: loss_final: 0.902591, loss_mean: 0.855960, loss_mean_cls: 0.046631, grad_norm: 0.578215
[[34m2025-10-04 12:14:44[0m] Step: 3338, Training Logs: loss_final: 0.884033, loss_mean: 0.837810, loss_mean_cls: 0.046223, grad_norm: 0.441538
[[34m2025-10-04 12:14:44[0m] Step: 3339, Training Logs: loss_final: 0.893167, loss_mean: 0.846626, loss_mean_cls: 0.046541, grad_norm: 0.514018
[[34m2025-10-04 12:14:44[0m] Step: 3340, Training Logs: loss_final: 0.897386, loss_mean: 0.851202, loss_mean_cls: 0.046184, grad_norm: 0.498477
[[34m2025-10-04 12:14:45[0m] Step: 3341, Training Logs: loss_final: 0.860963, loss_mean: 0.814306, loss_mean_cls: 0.046657, grad_norm: 0.393206
[[34m2025-10-04 12:14:45[0m] Step: 3342, Training Logs: loss_final: 0.906778, loss_mean: 0.860825, loss_mean_cls: 0.045952, grad_norm: 0.676334
[[34m2025-10-04 12:14:45[0m] Step: 3343, Training Logs: loss_final: 0.903794, loss_mean: 0.857137, loss_mean_cls: 0.046657, grad_norm: 0.405019
[[34m2025-10-04 12:14:46[0m] Step: 3344, Training Logs: loss_final: 0.900968, loss_mean: 0.855572, loss_mean_cls: 0.045396, grad_norm: 0.611075
[[34m2025-10-04 12:14:46[0m] Step: 3345, Training Logs: loss_final: 0.902094, loss_mean: 0.855201, loss_mean_cls: 0.046893, grad_norm: 0.506149
[[34m2025-10-04 12:14:46[0m] Step: 3346, Training Logs: loss_final: 0.901613, loss_mean: 0.854700, loss_mean_cls: 0.046912, grad_norm: 0.472767
[[34m2025-10-04 12:14:46[0m] Step: 3347, Training Logs: loss_final: 0.902791, loss_mean: 0.855700, loss_mean_cls: 0.047091, grad_norm: 0.640853
[[34m2025-10-04 12:14:47[0m] Step: 3348, Training Logs: loss_final: 0.900309, loss_mean: 0.853768, loss_mean_cls: 0.046541, grad_norm: 0.402154
[[34m2025-10-04 12:14:47[0m] Step: 3349, Training Logs: loss_final: 0.880462, loss_mean: 0.833254, loss_mean_cls: 0.047208, grad_norm: 0.703584
[[34m2025-10-04 12:14:47[0m] Step: 3350, Training Logs: loss_final: 0.907348, loss_mean: 0.861025, loss_mean_cls: 0.046323, grad_norm: 0.547524
[[34m2025-10-04 12:14:48[0m] Step: 3351, Training Logs: loss_final: 0.895863, loss_mean: 0.849971, loss_mean_cls: 0.045893, grad_norm: 0.577644
[[34m2025-10-04 12:14:48[0m] Step: 3352, Training Logs: loss_final: 0.891607, loss_mean: 0.845341, loss_mean_cls: 0.046266, grad_norm: 0.510811
[[34m2025-10-04 12:14:48[0m] Step: 3353, Training Logs: loss_final: 0.900686, loss_mean: 0.853702, loss_mean_cls: 0.046984, grad_norm: 0.470213
[[34m2025-10-04 12:14:48[0m] Step: 3354, Training Logs: loss_final: 0.908419, loss_mean: 0.862112, loss_mean_cls: 0.046308, grad_norm: 0.581942
[[34m2025-10-04 12:14:49[0m] Step: 3355, Training Logs: loss_final: 0.897501, loss_mean: 0.851048, loss_mean_cls: 0.046452, grad_norm: 0.694689
[[34m2025-10-04 12:14:49[0m] Step: 3356, Training Logs: loss_final: 0.910048, loss_mean: 0.863873, loss_mean_cls: 0.046175, grad_norm: 0.380068
[[34m2025-10-04 12:14:49[0m] Step: 3357, Training Logs: loss_final: 0.908655, loss_mean: 0.863329, loss_mean_cls: 0.045325, grad_norm: 0.461810
[[34m2025-10-04 12:14:50[0m] Step: 3358, Training Logs: loss_final: 0.893861, loss_mean: 0.847467, loss_mean_cls: 0.046393, grad_norm: 0.475242
[[34m2025-10-04 12:14:50[0m] Step: 3359, Training Logs: loss_final: 0.901212, loss_mean: 0.854278, loss_mean_cls: 0.046934, grad_norm: 0.442572
[[34m2025-10-04 12:14:50[0m] Step: 3360, Training Logs: loss_final: 0.905576, loss_mean: 0.860191, loss_mean_cls: 0.045385, grad_norm: 0.512476
[[34m2025-10-04 12:14:51[0m] Step: 3361, Training Logs: loss_final: 0.885602, loss_mean: 0.838361, loss_mean_cls: 0.047242, grad_norm: 0.435166
[[34m2025-10-04 12:14:51[0m] Step: 3362, Training Logs: loss_final: 0.904388, loss_mean: 0.857371, loss_mean_cls: 0.047017, grad_norm: 0.540081
[[34m2025-10-04 12:14:51[0m] Step: 3363, Training Logs: loss_final: 0.894402, loss_mean: 0.847888, loss_mean_cls: 0.046514, grad_norm: 0.381942
[[34m2025-10-04 12:14:51[0m] Step: 3364, Training Logs: loss_final: 0.894510, loss_mean: 0.849245, loss_mean_cls: 0.045265, grad_norm: 0.607660
[[34m2025-10-04 12:14:52[0m] Step: 3365, Training Logs: loss_final: 0.908054, loss_mean: 0.861814, loss_mean_cls: 0.046240, grad_norm: 0.391131
[[34m2025-10-04 12:14:52[0m] Step: 3366, Training Logs: loss_final: 0.909657, loss_mean: 0.862730, loss_mean_cls: 0.046927, grad_norm: 0.374386
[[34m2025-10-04 12:14:52[0m] Step: 3367, Training Logs: loss_final: 0.900597, loss_mean: 0.854284, loss_mean_cls: 0.046313, grad_norm: 0.521232
[[34m2025-10-04 12:14:53[0m] Step: 3368, Training Logs: loss_final: 0.896218, loss_mean: 0.849743, loss_mean_cls: 0.046475, grad_norm: 0.348451
[[34m2025-10-04 12:14:53[0m] Step: 3369, Training Logs: loss_final: 0.909824, loss_mean: 0.864405, loss_mean_cls: 0.045418, grad_norm: 0.477549
[[34m2025-10-04 12:14:53[0m] Step: 3370, Training Logs: loss_final: 0.904392, loss_mean: 0.858949, loss_mean_cls: 0.045443, grad_norm: 0.460303
[[34m2025-10-04 12:14:53[0m] Step: 3371, Training Logs: loss_final: 0.881132, loss_mean: 0.834396, loss_mean_cls: 0.046736, grad_norm: 0.387416
[[34m2025-10-04 12:14:54[0m] Step: 3372, Training Logs: loss_final: 0.898065, loss_mean: 0.850910, loss_mean_cls: 0.047155, grad_norm: 0.335168
[[34m2025-10-04 12:14:54[0m] Step: 3373, Training Logs: loss_final: 0.932262, loss_mean: 0.887353, loss_mean_cls: 0.044910, grad_norm: 0.469342
[[34m2025-10-04 12:14:54[0m] Step: 3374, Training Logs: loss_final: 0.886504, loss_mean: 0.840301, loss_mean_cls: 0.046203, grad_norm: 0.355032
[[34m2025-10-04 12:14:55[0m] Step: 3375, Training Logs: loss_final: 0.901218, loss_mean: 0.855572, loss_mean_cls: 0.045646, grad_norm: 0.452215
[[34m2025-10-04 12:14:55[0m] Step: 3376, Training Logs: loss_final: 0.890905, loss_mean: 0.844652, loss_mean_cls: 0.046253, grad_norm: 0.284429
[[34m2025-10-04 12:14:55[0m] Step: 3377, Training Logs: loss_final: 0.911259, loss_mean: 0.864736, loss_mean_cls: 0.046523, grad_norm: 0.387786
[[34m2025-10-04 12:14:56[0m] Step: 3378, Training Logs: loss_final: 0.900286, loss_mean: 0.853896, loss_mean_cls: 0.046390, grad_norm: 0.430003
[[34m2025-10-04 12:14:56[0m] Step: 3379, Training Logs: loss_final: 0.875517, loss_mean: 0.829662, loss_mean_cls: 0.045855, grad_norm: 0.381718
[[34m2025-10-04 12:14:56[0m] Step: 3380, Training Logs: loss_final: 0.916419, loss_mean: 0.871092, loss_mean_cls: 0.045327, grad_norm: 0.499408
[[34m2025-10-04 12:14:56[0m] Step: 3381, Training Logs: loss_final: 0.890539, loss_mean: 0.844369, loss_mean_cls: 0.046170, grad_norm: 0.466148
[[34m2025-10-04 12:14:57[0m] Step: 3382, Training Logs: loss_final: 0.906539, loss_mean: 0.860537, loss_mean_cls: 0.046001, grad_norm: 0.375906
[[34m2025-10-04 12:14:57[0m] Step: 3383, Training Logs: loss_final: 0.902475, loss_mean: 0.856617, loss_mean_cls: 0.045858, grad_norm: 0.536920
[[34m2025-10-04 12:14:57[0m] Step: 3384, Training Logs: loss_final: 0.881278, loss_mean: 0.833072, loss_mean_cls: 0.048206, grad_norm: 0.675999
[[34m2025-10-04 12:14:58[0m] Step: 3385, Training Logs: loss_final: 0.883438, loss_mean: 0.836218, loss_mean_cls: 0.047221, grad_norm: 0.392692
[[34m2025-10-04 12:14:58[0m] Step: 3386, Training Logs: loss_final: 0.901077, loss_mean: 0.855096, loss_mean_cls: 0.045981, grad_norm: 0.553151
[[34m2025-10-04 12:14:58[0m] Step: 3387, Training Logs: loss_final: 0.910015, loss_mean: 0.865560, loss_mean_cls: 0.044455, grad_norm: 0.479916
[[34m2025-10-04 12:14:59[0m] Step: 3388, Training Logs: loss_final: 0.897982, loss_mean: 0.851919, loss_mean_cls: 0.046063, grad_norm: 0.348281
[[34m2025-10-04 12:14:59[0m] Step: 3389, Training Logs: loss_final: 0.924636, loss_mean: 0.879668, loss_mean_cls: 0.044968, grad_norm: 0.581647
[[34m2025-10-04 12:14:59[0m] Step: 3390, Training Logs: loss_final: 0.885674, loss_mean: 0.839035, loss_mean_cls: 0.046639, grad_norm: 0.398158
[[34m2025-10-04 12:14:59[0m] Step: 3391, Training Logs: loss_final: 0.898175, loss_mean: 0.851566, loss_mean_cls: 0.046609, grad_norm: 0.460858
[[34m2025-10-04 12:15:00[0m] Step: 3392, Training Logs: loss_final: 0.880336, loss_mean: 0.835128, loss_mean_cls: 0.045207, grad_norm: 0.531433
[[34m2025-10-04 12:15:00[0m] Step: 3393, Training Logs: loss_final: 0.893034, loss_mean: 0.846558, loss_mean_cls: 0.046476, grad_norm: 0.537944
[[34m2025-10-04 12:15:00[0m] Step: 3394, Training Logs: loss_final: 0.881059, loss_mean: 0.833821, loss_mean_cls: 0.047238, grad_norm: 0.465119
[[34m2025-10-04 12:15:01[0m] Step: 3395, Training Logs: loss_final: 0.899670, loss_mean: 0.854063, loss_mean_cls: 0.045608, grad_norm: 0.510744
[[34m2025-10-04 12:15:01[0m] Step: 3396, Training Logs: loss_final: 0.891792, loss_mean: 0.845570, loss_mean_cls: 0.046222, grad_norm: 0.532700
[[34m2025-10-04 12:15:01[0m] Step: 3397, Training Logs: loss_final: 0.900686, loss_mean: 0.853597, loss_mean_cls: 0.047089, grad_norm: 0.407133
[[34m2025-10-04 12:15:01[0m] Step: 3398, Training Logs: loss_final: 0.903951, loss_mean: 0.856820, loss_mean_cls: 0.047131, grad_norm: 0.539118
[[34m2025-10-04 12:15:02[0m] Step: 3399, Training Logs: loss_final: 0.911769, loss_mean: 0.865447, loss_mean_cls: 0.046322, grad_norm: 0.531224
[[34m2025-10-04 12:15:02[0m] Step: 3400, Training Logs: loss_final: 0.922085, loss_mean: 0.876658, loss_mean_cls: 0.045427, grad_norm: 0.434207
[[34m2025-10-04 12:15:02[0m] Step: 3401, Training Logs: loss_final: 0.899382, loss_mean: 0.852450, loss_mean_cls: 0.046932, grad_norm: 0.458918
[[34m2025-10-04 12:15:03[0m] Step: 3402, Training Logs: loss_final: 0.908502, loss_mean: 0.861261, loss_mean_cls: 0.047241, grad_norm: 0.462210
[[34m2025-10-04 12:15:03[0m] Step: 3403, Training Logs: loss_final: 0.927268, loss_mean: 0.882113, loss_mean_cls: 0.045155, grad_norm: 0.406925
[[34m2025-10-04 12:15:03[0m] Step: 3404, Training Logs: loss_final: 0.897331, loss_mean: 0.852843, loss_mean_cls: 0.044489, grad_norm: 0.524586
[[34m2025-10-04 12:15:04[0m] Step: 3405, Training Logs: loss_final: 0.894567, loss_mean: 0.847760, loss_mean_cls: 0.046807, grad_norm: 0.330073
[[34m2025-10-04 12:15:04[0m] Step: 3406, Training Logs: loss_final: 0.913845, loss_mean: 0.868108, loss_mean_cls: 0.045737, grad_norm: 0.570386
[[34m2025-10-04 12:15:04[0m] Step: 3407, Training Logs: loss_final: 0.899022, loss_mean: 0.853050, loss_mean_cls: 0.045972, grad_norm: 0.444371
[[34m2025-10-04 12:15:04[0m] Step: 3408, Training Logs: loss_final: 0.903869, loss_mean: 0.858023, loss_mean_cls: 0.045846, grad_norm: 0.484747
[[34m2025-10-04 12:15:05[0m] Step: 3409, Training Logs: loss_final: 0.891335, loss_mean: 0.844542, loss_mean_cls: 0.046794, grad_norm: 0.408571
[[34m2025-10-04 12:15:05[0m] Step: 3410, Training Logs: loss_final: 0.898693, loss_mean: 0.852073, loss_mean_cls: 0.046621, grad_norm: 0.503708
[[34m2025-10-04 12:15:05[0m] Step: 3411, Training Logs: loss_final: 0.883900, loss_mean: 0.837522, loss_mean_cls: 0.046378, grad_norm: 0.552834
[[34m2025-10-04 12:15:06[0m] Step: 3412, Training Logs: loss_final: 0.879026, loss_mean: 0.832376, loss_mean_cls: 0.046650, grad_norm: 0.454586
[[34m2025-10-04 12:15:06[0m] Step: 3413, Training Logs: loss_final: 0.895560, loss_mean: 0.848644, loss_mean_cls: 0.046916, grad_norm: 0.581962
[[34m2025-10-04 12:15:06[0m] Step: 3414, Training Logs: loss_final: 0.886960, loss_mean: 0.840840, loss_mean_cls: 0.046120, grad_norm: 0.449193
[[34m2025-10-04 12:15:06[0m] Step: 3415, Training Logs: loss_final: 0.916705, loss_mean: 0.870486, loss_mean_cls: 0.046219, grad_norm: 0.427663
[[34m2025-10-04 12:15:07[0m] Step: 3416, Training Logs: loss_final: 0.894752, loss_mean: 0.847773, loss_mean_cls: 0.046979, grad_norm: 0.587395
[[34m2025-10-04 12:15:07[0m] Step: 3417, Training Logs: loss_final: 0.890099, loss_mean: 0.843839, loss_mean_cls: 0.046260, grad_norm: 0.583688
[[34m2025-10-04 12:15:07[0m] Step: 3418, Training Logs: loss_final: 0.912285, loss_mean: 0.866706, loss_mean_cls: 0.045579, grad_norm: 0.502831
[[34m2025-10-04 12:15:08[0m] Step: 3419, Training Logs: loss_final: 0.893525, loss_mean: 0.847857, loss_mean_cls: 0.045668, grad_norm: 0.415321
[[34m2025-10-04 12:15:08[0m] Step: 3420, Training Logs: loss_final: 0.906617, loss_mean: 0.859027, loss_mean_cls: 0.047590, grad_norm: 0.377783
[[34m2025-10-04 12:15:08[0m] Step: 3421, Training Logs: loss_final: 0.891442, loss_mean: 0.846286, loss_mean_cls: 0.045156, grad_norm: 0.476431
[[34m2025-10-04 12:15:09[0m] Step: 3422, Training Logs: loss_final: 0.917918, loss_mean: 0.870837, loss_mean_cls: 0.047081, grad_norm: 0.384826
[[34m2025-10-04 12:15:09[0m] Step: 3423, Training Logs: loss_final: 0.896818, loss_mean: 0.850077, loss_mean_cls: 0.046741, grad_norm: 0.410341
[[34m2025-10-04 12:15:09[0m] Step: 3424, Training Logs: loss_final: 0.906928, loss_mean: 0.860428, loss_mean_cls: 0.046500, grad_norm: 0.489000
[[34m2025-10-04 12:15:09[0m] Step: 3425, Training Logs: loss_final: 0.913617, loss_mean: 0.867256, loss_mean_cls: 0.046361, grad_norm: 0.272148
[[34m2025-10-04 12:15:10[0m] Step: 3426, Training Logs: loss_final: 0.889834, loss_mean: 0.842892, loss_mean_cls: 0.046942, grad_norm: 0.438129
[[34m2025-10-04 12:15:10[0m] Step: 3427, Training Logs: loss_final: 0.897668, loss_mean: 0.852941, loss_mean_cls: 0.044727, grad_norm: 0.380112
[[34m2025-10-04 12:15:10[0m] Step: 3428, Training Logs: loss_final: 0.896315, loss_mean: 0.849972, loss_mean_cls: 0.046343, grad_norm: 0.514777
[[34m2025-10-04 12:15:11[0m] Step: 3429, Training Logs: loss_final: 0.912746, loss_mean: 0.868006, loss_mean_cls: 0.044740, grad_norm: 0.359226
[[34m2025-10-04 12:15:11[0m] Step: 3430, Training Logs: loss_final: 0.903027, loss_mean: 0.856367, loss_mean_cls: 0.046660, grad_norm: 0.443544
[[34m2025-10-04 12:15:11[0m] Step: 3431, Training Logs: loss_final: 0.906182, loss_mean: 0.860458, loss_mean_cls: 0.045724, grad_norm: 0.507595
[[34m2025-10-04 12:15:11[0m] Step: 3432, Training Logs: loss_final: 0.904463, loss_mean: 0.858428, loss_mean_cls: 0.046035, grad_norm: 0.313725
[[34m2025-10-04 12:15:12[0m] Step: 3433, Training Logs: loss_final: 0.902819, loss_mean: 0.855581, loss_mean_cls: 0.047238, grad_norm: 0.533058
[[34m2025-10-04 12:15:12[0m] Step: 3434, Training Logs: loss_final: 0.909856, loss_mean: 0.863630, loss_mean_cls: 0.046226, grad_norm: 0.438955
[[34m2025-10-04 12:15:12[0m] Step: 3435, Training Logs: loss_final: 0.885776, loss_mean: 0.840158, loss_mean_cls: 0.045618, grad_norm: 0.369102
[[34m2025-10-04 12:15:13[0m] Step: 3436, Training Logs: loss_final: 0.922200, loss_mean: 0.875661, loss_mean_cls: 0.046539, grad_norm: 0.381698
[[34m2025-10-04 12:15:13[0m] Step: 3437, Training Logs: loss_final: 0.891721, loss_mean: 0.845189, loss_mean_cls: 0.046532, grad_norm: 0.289560
[[34m2025-10-04 12:15:13[0m] Step: 3438, Training Logs: loss_final: 0.907816, loss_mean: 0.860811, loss_mean_cls: 0.047005, grad_norm: 0.439202
[[34m2025-10-04 12:15:14[0m] Step: 3439, Training Logs: loss_final: 0.902884, loss_mean: 0.856909, loss_mean_cls: 0.045975, grad_norm: 0.461310
[[34m2025-10-04 12:15:14[0m] Step: 3440, Training Logs: loss_final: 0.900719, loss_mean: 0.854909, loss_mean_cls: 0.045810, grad_norm: 0.389900
[[34m2025-10-04 12:15:14[0m] Step: 3441, Training Logs: loss_final: 0.894818, loss_mean: 0.847884, loss_mean_cls: 0.046934, grad_norm: 0.553960
[[34m2025-10-04 12:15:14[0m] Step: 3442, Training Logs: loss_final: 0.890085, loss_mean: 0.844110, loss_mean_cls: 0.045976, grad_norm: 0.623482
[[34m2025-10-04 12:15:15[0m] Step: 3443, Training Logs: loss_final: 0.887935, loss_mean: 0.842564, loss_mean_cls: 0.045371, grad_norm: 0.441723
[[34m2025-10-04 12:15:15[0m] Step: 3444, Training Logs: loss_final: 0.902504, loss_mean: 0.856152, loss_mean_cls: 0.046352, grad_norm: 0.578319
[[34m2025-10-04 12:15:15[0m] Step: 3445, Training Logs: loss_final: 0.893000, loss_mean: 0.846897, loss_mean_cls: 0.046103, grad_norm: 0.443411
[[34m2025-10-04 12:15:16[0m] Step: 3446, Training Logs: loss_final: 0.882852, loss_mean: 0.836008, loss_mean_cls: 0.046845, grad_norm: 0.490239
[[34m2025-10-04 12:15:16[0m] Step: 3447, Training Logs: loss_final: 0.897056, loss_mean: 0.851112, loss_mean_cls: 0.045944, grad_norm: 0.339087
[[34m2025-10-04 12:15:16[0m] Step: 3448, Training Logs: loss_final: 0.908844, loss_mean: 0.862733, loss_mean_cls: 0.046111, grad_norm: 0.557594
[[34m2025-10-04 12:15:16[0m] Step: 3449, Training Logs: loss_final: 0.896624, loss_mean: 0.850635, loss_mean_cls: 0.045990, grad_norm: 0.415837
[[34m2025-10-04 12:15:17[0m] Step: 3450, Training Logs: loss_final: 0.901174, loss_mean: 0.854810, loss_mean_cls: 0.046364, grad_norm: 0.456441
[[34m2025-10-04 12:15:17[0m] Step: 3451, Training Logs: loss_final: 0.896770, loss_mean: 0.851074, loss_mean_cls: 0.045696, grad_norm: 0.493726
[[34m2025-10-04 12:15:17[0m] Step: 3452, Training Logs: loss_final: 0.916576, loss_mean: 0.870993, loss_mean_cls: 0.045583, grad_norm: 0.444457
[[34m2025-10-04 12:15:18[0m] Step: 3453, Training Logs: loss_final: 0.926495, loss_mean: 0.881029, loss_mean_cls: 0.045465, grad_norm: 0.438974
[[34m2025-10-04 12:15:18[0m] Step: 3454, Training Logs: loss_final: 0.889755, loss_mean: 0.844353, loss_mean_cls: 0.045402, grad_norm: 0.477326
[[34m2025-10-04 12:15:18[0m] Step: 3455, Training Logs: loss_final: 0.907272, loss_mean: 0.862535, loss_mean_cls: 0.044737, grad_norm: 0.386003
[[34m2025-10-04 12:15:18[0m] Step: 3456, Training Logs: loss_final: 0.882638, loss_mean: 0.836240, loss_mean_cls: 0.046398, grad_norm: 0.496885
[[34m2025-10-04 12:15:19[0m] Step: 3457, Training Logs: loss_final: 0.889249, loss_mean: 0.842059, loss_mean_cls: 0.047190, grad_norm: 0.400130
[[34m2025-10-04 12:15:19[0m] Step: 3458, Training Logs: loss_final: 0.886142, loss_mean: 0.841642, loss_mean_cls: 0.044500, grad_norm: 0.362989
[[34m2025-10-04 12:15:19[0m] Step: 3459, Training Logs: loss_final: 0.890186, loss_mean: 0.844208, loss_mean_cls: 0.045979, grad_norm: 0.401648
[[34m2025-10-04 12:15:20[0m] Step: 3460, Training Logs: loss_final: 0.902823, loss_mean: 0.855952, loss_mean_cls: 0.046871, grad_norm: 0.356736
[[34m2025-10-04 12:15:20[0m] Step: 3461, Training Logs: loss_final: 0.892885, loss_mean: 0.846000, loss_mean_cls: 0.046885, grad_norm: 0.402504
[[34m2025-10-04 12:15:20[0m] Step: 3462, Training Logs: loss_final: 0.922808, loss_mean: 0.878078, loss_mean_cls: 0.044730, grad_norm: 0.473087
[[34m2025-10-04 12:15:21[0m] Step: 3463, Training Logs: loss_final: 0.866879, loss_mean: 0.818748, loss_mean_cls: 0.048131, grad_norm: 0.359315
[[34m2025-10-04 12:15:21[0m] Step: 3464, Training Logs: loss_final: 0.910160, loss_mean: 0.863459, loss_mean_cls: 0.046701, grad_norm: 0.519536
[[34m2025-10-04 12:15:21[0m] Step: 3465, Training Logs: loss_final: 0.899139, loss_mean: 0.854215, loss_mean_cls: 0.044924, grad_norm: 0.386346
[[34m2025-10-04 12:15:21[0m] Step: 3466, Training Logs: loss_final: 0.899516, loss_mean: 0.852598, loss_mean_cls: 0.046918, grad_norm: 0.396077
[[34m2025-10-04 12:15:22[0m] Step: 3467, Training Logs: loss_final: 0.905108, loss_mean: 0.860467, loss_mean_cls: 0.044641, grad_norm: 0.318386
[[34m2025-10-04 12:15:22[0m] Step: 3468, Training Logs: loss_final: 0.909860, loss_mean: 0.863521, loss_mean_cls: 0.046340, grad_norm: 0.400307
[[34m2025-10-04 12:15:22[0m] Step: 3469, Training Logs: loss_final: 0.913111, loss_mean: 0.866524, loss_mean_cls: 0.046587, grad_norm: 0.420964
[[34m2025-10-04 12:15:23[0m] Step: 3470, Training Logs: loss_final: 0.916356, loss_mean: 0.870065, loss_mean_cls: 0.046291, grad_norm: 0.376515
[[34m2025-10-04 12:15:23[0m] Step: 3471, Training Logs: loss_final: 0.889659, loss_mean: 0.843427, loss_mean_cls: 0.046232, grad_norm: 0.373180
[[34m2025-10-04 12:15:23[0m] Step: 3472, Training Logs: loss_final: 0.906042, loss_mean: 0.859117, loss_mean_cls: 0.046924, grad_norm: 0.339782
[[34m2025-10-04 12:15:24[0m] Step: 3473, Training Logs: loss_final: 0.903247, loss_mean: 0.857759, loss_mean_cls: 0.045488, grad_norm: 0.402253
[[34m2025-10-04 12:15:24[0m] Step: 3474, Training Logs: loss_final: 0.902942, loss_mean: 0.857629, loss_mean_cls: 0.045313, grad_norm: 0.380809
[[34m2025-10-04 12:15:24[0m] Step: 3475, Training Logs: loss_final: 0.913503, loss_mean: 0.867165, loss_mean_cls: 0.046338, grad_norm: 0.355626
[[34m2025-10-04 12:15:24[0m] Step: 3476, Training Logs: loss_final: 0.901310, loss_mean: 0.854858, loss_mean_cls: 0.046452, grad_norm: 0.383802
[[34m2025-10-04 12:15:25[0m] Step: 3477, Training Logs: loss_final: 0.891605, loss_mean: 0.846317, loss_mean_cls: 0.045288, grad_norm: 0.727470
[[34m2025-10-04 12:15:25[0m] Step: 3478, Training Logs: loss_final: 0.898291, loss_mean: 0.852546, loss_mean_cls: 0.045745, grad_norm: 0.458666
[[34m2025-10-04 12:15:25[0m] Step: 3479, Training Logs: loss_final: 0.890059, loss_mean: 0.844286, loss_mean_cls: 0.045773, grad_norm: 0.508003
[[34m2025-10-04 12:15:26[0m] Step: 3480, Training Logs: loss_final: 0.915667, loss_mean: 0.868516, loss_mean_cls: 0.047151, grad_norm: 0.511204
[[34m2025-10-04 12:15:26[0m] Step: 3481, Training Logs: loss_final: 0.878749, loss_mean: 0.832611, loss_mean_cls: 0.046138, grad_norm: 0.396024
[[34m2025-10-04 12:15:26[0m] Step: 3482, Training Logs: loss_final: 0.907610, loss_mean: 0.861371, loss_mean_cls: 0.046239, grad_norm: 0.555290
[[34m2025-10-04 12:15:27[0m] Step: 3483, Training Logs: loss_final: 0.918255, loss_mean: 0.871692, loss_mean_cls: 0.046563, grad_norm: 0.467513
[[34m2025-10-04 12:15:27[0m] Step: 3484, Training Logs: loss_final: 0.905548, loss_mean: 0.859728, loss_mean_cls: 0.045820, grad_norm: 0.318976
[[34m2025-10-04 12:15:27[0m] Step: 3485, Training Logs: loss_final: 0.912733, loss_mean: 0.866832, loss_mean_cls: 0.045900, grad_norm: 0.379957
[[34m2025-10-04 12:15:27[0m] Step: 3486, Training Logs: loss_final: 0.905129, loss_mean: 0.858601, loss_mean_cls: 0.046528, grad_norm: 0.351999
[[34m2025-10-04 12:15:28[0m] Step: 3487, Training Logs: loss_final: 0.912436, loss_mean: 0.867716, loss_mean_cls: 0.044720, grad_norm: 0.271295
[[34m2025-10-04 12:15:28[0m] Step: 3488, Training Logs: loss_final: 0.921205, loss_mean: 0.875338, loss_mean_cls: 0.045866, grad_norm: 0.420848
[[34m2025-10-04 12:15:28[0m] Step: 3489, Training Logs: loss_final: 0.887833, loss_mean: 0.841446, loss_mean_cls: 0.046386, grad_norm: 0.475728
[[34m2025-10-04 12:15:29[0m] Step: 3490, Training Logs: loss_final: 0.890880, loss_mean: 0.843974, loss_mean_cls: 0.046906, grad_norm: 0.342202
[[34m2025-10-04 12:15:29[0m] Step: 3491, Training Logs: loss_final: 0.887532, loss_mean: 0.841542, loss_mean_cls: 0.045989, grad_norm: 0.514570
[[34m2025-10-04 12:15:29[0m] Step: 3492, Training Logs: loss_final: 0.899865, loss_mean: 0.854618, loss_mean_cls: 0.045247, grad_norm: 0.488583
[[34m2025-10-04 12:15:29[0m] Step: 3493, Training Logs: loss_final: 0.922888, loss_mean: 0.875317, loss_mean_cls: 0.047571, grad_norm: 0.394570
[[34m2025-10-04 12:15:30[0m] Step: 3494, Training Logs: loss_final: 0.909344, loss_mean: 0.865144, loss_mean_cls: 0.044200, grad_norm: 0.383288
[[34m2025-10-04 12:15:30[0m] Step: 3495, Training Logs: loss_final: 0.898099, loss_mean: 0.851563, loss_mean_cls: 0.046535, grad_norm: 0.344134
[[34m2025-10-04 12:15:30[0m] Step: 3496, Training Logs: loss_final: 0.890026, loss_mean: 0.844449, loss_mean_cls: 0.045577, grad_norm: 0.501508
[[34m2025-10-04 12:15:31[0m] Step: 3497, Training Logs: loss_final: 0.910083, loss_mean: 0.864475, loss_mean_cls: 0.045608, grad_norm: 0.572600
[[34m2025-10-04 12:15:31[0m] Step: 3498, Training Logs: loss_final: 0.885413, loss_mean: 0.838765, loss_mean_cls: 0.046649, grad_norm: 0.602169
[[34m2025-10-04 12:15:31[0m] Step: 3499, Training Logs: loss_final: 0.912487, loss_mean: 0.866603, loss_mean_cls: 0.045884, grad_norm: 0.498457
[[34m2025-10-04 12:15:31[0m] Step: 3500, Training Logs: loss_final: 0.894651, loss_mean: 0.849096, loss_mean_cls: 0.045555, grad_norm: 0.484335
[[34m2025-10-04 12:15:32[0m] Step: 3501, Training Logs: loss_final: 0.900612, loss_mean: 0.853604, loss_mean_cls: 0.047008, grad_norm: 0.453094
[[34m2025-10-04 12:15:32[0m] Step: 3502, Training Logs: loss_final: 0.891336, loss_mean: 0.845108, loss_mean_cls: 0.046228, grad_norm: 0.429769
[[34m2025-10-04 12:15:32[0m] Step: 3503, Training Logs: loss_final: 0.897998, loss_mean: 0.852394, loss_mean_cls: 0.045604, grad_norm: 0.351310
[[34m2025-10-04 12:15:33[0m] Step: 3504, Training Logs: loss_final: 0.888105, loss_mean: 0.841660, loss_mean_cls: 0.046444, grad_norm: 0.458427
[[34m2025-10-04 12:15:33[0m] Step: 3505, Training Logs: loss_final: 0.916829, loss_mean: 0.871996, loss_mean_cls: 0.044834, grad_norm: 0.370213
[[34m2025-10-04 12:15:33[0m] Step: 3506, Training Logs: loss_final: 0.903356, loss_mean: 0.858670, loss_mean_cls: 0.044685, grad_norm: 0.404750
[[34m2025-10-04 12:15:34[0m] Step: 3507, Training Logs: loss_final: 0.905671, loss_mean: 0.859976, loss_mean_cls: 0.045695, grad_norm: 0.386163
[[34m2025-10-04 12:15:34[0m] Step: 3508, Training Logs: loss_final: 0.888188, loss_mean: 0.842250, loss_mean_cls: 0.045938, grad_norm: 0.386777
[[34m2025-10-04 12:15:34[0m] Step: 3509, Training Logs: loss_final: 0.896695, loss_mean: 0.849332, loss_mean_cls: 0.047362, grad_norm: 0.392720
[[34m2025-10-04 12:15:34[0m] Step: 3510, Training Logs: loss_final: 0.910154, loss_mean: 0.865220, loss_mean_cls: 0.044934, grad_norm: 0.387872
[[34m2025-10-04 12:15:35[0m] Step: 3511, Training Logs: loss_final: 0.899906, loss_mean: 0.855077, loss_mean_cls: 0.044829, grad_norm: 0.373459
[[34m2025-10-04 12:15:35[0m] Step: 3512, Training Logs: loss_final: 0.883798, loss_mean: 0.836749, loss_mean_cls: 0.047049, grad_norm: 0.336197
[[34m2025-10-04 12:15:35[0m] Step: 3513, Training Logs: loss_final: 0.912409, loss_mean: 0.867224, loss_mean_cls: 0.045185, grad_norm: 0.353879
[[34m2025-10-04 12:15:36[0m] Step: 3514, Training Logs: loss_final: 0.908266, loss_mean: 0.860609, loss_mean_cls: 0.047658, grad_norm: 0.458368
[[34m2025-10-04 12:15:36[0m] Step: 3515, Training Logs: loss_final: 0.874772, loss_mean: 0.827694, loss_mean_cls: 0.047079, grad_norm: 0.309164
[[34m2025-10-04 12:15:36[0m] Step: 3516, Training Logs: loss_final: 0.910499, loss_mean: 0.864270, loss_mean_cls: 0.046228, grad_norm: 0.400001
[[34m2025-10-04 12:15:37[0m] Step: 3517, Training Logs: loss_final: 0.898635, loss_mean: 0.851480, loss_mean_cls: 0.047155, grad_norm: 0.576187
[[34m2025-10-04 12:15:37[0m] Step: 3518, Training Logs: loss_final: 0.902881, loss_mean: 0.855902, loss_mean_cls: 0.046979, grad_norm: 0.536511
[[34m2025-10-04 12:15:37[0m] Step: 3519, Training Logs: loss_final: 0.923566, loss_mean: 0.877252, loss_mean_cls: 0.046315, grad_norm: 0.705854
[[34m2025-10-04 12:15:37[0m] Step: 3520, Training Logs: loss_final: 0.893166, loss_mean: 0.846352, loss_mean_cls: 0.046814, grad_norm: 0.467520
[[34m2025-10-04 12:15:38[0m] Step: 3521, Training Logs: loss_final: 0.912112, loss_mean: 0.865844, loss_mean_cls: 0.046268, grad_norm: 0.528330
[[34m2025-10-04 12:15:38[0m] Step: 3522, Training Logs: loss_final: 0.902839, loss_mean: 0.857213, loss_mean_cls: 0.045625, grad_norm: 0.564945
[[34m2025-10-04 12:15:38[0m] Step: 3523, Training Logs: loss_final: 0.924202, loss_mean: 0.877471, loss_mean_cls: 0.046731, grad_norm: 0.445081
[[34m2025-10-04 12:15:39[0m] Step: 3524, Training Logs: loss_final: 0.917812, loss_mean: 0.871077, loss_mean_cls: 0.046735, grad_norm: 0.544275
[[34m2025-10-04 12:15:39[0m] Step: 3525, Training Logs: loss_final: 0.887502, loss_mean: 0.841530, loss_mean_cls: 0.045973, grad_norm: 0.524580
[[34m2025-10-04 12:15:39[0m] Step: 3526, Training Logs: loss_final: 0.871081, loss_mean: 0.824186, loss_mean_cls: 0.046895, grad_norm: 0.671876
[[34m2025-10-04 12:15:40[0m] Step: 3527, Training Logs: loss_final: 0.904950, loss_mean: 0.858532, loss_mean_cls: 0.046418, grad_norm: 0.627076
[[34m2025-10-04 12:15:40[0m] Step: 3528, Training Logs: loss_final: 0.900166, loss_mean: 0.854421, loss_mean_cls: 0.045746, grad_norm: 0.547244
[[34m2025-10-04 12:15:40[0m] Step: 3529, Training Logs: loss_final: 0.904352, loss_mean: 0.858347, loss_mean_cls: 0.046005, grad_norm: 0.544697
[[34m2025-10-04 12:15:40[0m] Step: 3530, Training Logs: loss_final: 0.900224, loss_mean: 0.853643, loss_mean_cls: 0.046582, grad_norm: 0.406256
[[34m2025-10-04 12:15:41[0m] Step: 3531, Training Logs: loss_final: 0.890931, loss_mean: 0.844215, loss_mean_cls: 0.046716, grad_norm: 0.475335
[[34m2025-10-04 12:15:41[0m] Step: 3532, Training Logs: loss_final: 0.910091, loss_mean: 0.864999, loss_mean_cls: 0.045092, grad_norm: 0.623459
[[34m2025-10-04 12:15:41[0m] Step: 3533, Training Logs: loss_final: 0.909157, loss_mean: 0.863427, loss_mean_cls: 0.045729, grad_norm: 0.556181
[[34m2025-10-04 12:15:42[0m] Step: 3534, Training Logs: loss_final: 0.905859, loss_mean: 0.859550, loss_mean_cls: 0.046309, grad_norm: 0.373834
[[34m2025-10-04 12:15:42[0m] Step: 3535, Training Logs: loss_final: 0.869614, loss_mean: 0.821779, loss_mean_cls: 0.047834, grad_norm: 0.568397
[[34m2025-10-04 12:15:42[0m] Step: 3536, Training Logs: loss_final: 0.909607, loss_mean: 0.863593, loss_mean_cls: 0.046014, grad_norm: 0.407067
[[34m2025-10-04 12:15:43[0m] Step: 3537, Training Logs: loss_final: 0.908236, loss_mean: 0.862946, loss_mean_cls: 0.045290, grad_norm: 0.372643
[[34m2025-10-04 12:15:43[0m] Step: 3538, Training Logs: loss_final: 0.888475, loss_mean: 0.842160, loss_mean_cls: 0.046315, grad_norm: 0.503193
[[34m2025-10-04 12:15:43[0m] Step: 3539, Training Logs: loss_final: 0.874547, loss_mean: 0.829039, loss_mean_cls: 0.045508, grad_norm: 0.410300
[[34m2025-10-04 12:15:44[0m] Step: 3540, Training Logs: loss_final: 0.894825, loss_mean: 0.848590, loss_mean_cls: 0.046235, grad_norm: 0.355612
[[34m2025-10-04 12:15:44[0m] Step: 3541, Training Logs: loss_final: 0.907300, loss_mean: 0.860906, loss_mean_cls: 0.046394, grad_norm: 0.575820
[[34m2025-10-04 12:15:44[0m] Step: 3542, Training Logs: loss_final: 0.886271, loss_mean: 0.840528, loss_mean_cls: 0.045744, grad_norm: 0.618782
[[34m2025-10-04 12:15:44[0m] Step: 3543, Training Logs: loss_final: 0.898130, loss_mean: 0.850825, loss_mean_cls: 0.047305, grad_norm: 0.357366
[[34m2025-10-04 12:15:45[0m] Step: 3544, Training Logs: loss_final: 0.926194, loss_mean: 0.880043, loss_mean_cls: 0.046151, grad_norm: 0.527083
[[34m2025-10-04 12:15:45[0m] Step: 3545, Training Logs: loss_final: 0.884593, loss_mean: 0.839009, loss_mean_cls: 0.045584, grad_norm: 0.425856
[[34m2025-10-04 12:15:45[0m] Step: 3546, Training Logs: loss_final: 0.898782, loss_mean: 0.851675, loss_mean_cls: 0.047107, grad_norm: 0.427226
[[34m2025-10-04 12:15:46[0m] Step: 3547, Training Logs: loss_final: 0.920634, loss_mean: 0.874988, loss_mean_cls: 0.045646, grad_norm: 0.438813
[[34m2025-10-04 12:15:46[0m] Step: 3548, Training Logs: loss_final: 0.885310, loss_mean: 0.839282, loss_mean_cls: 0.046029, grad_norm: 0.426940
[[34m2025-10-04 12:15:46[0m] Step: 3549, Training Logs: loss_final: 0.897767, loss_mean: 0.851212, loss_mean_cls: 0.046555, grad_norm: 0.380271
[[34m2025-10-04 12:15:47[0m] Step: 3550, Training Logs: loss_final: 0.903487, loss_mean: 0.858070, loss_mean_cls: 0.045416, grad_norm: 0.484179
[[34m2025-10-04 12:15:47[0m] Step: 3551, Training Logs: loss_final: 0.893862, loss_mean: 0.847536, loss_mean_cls: 0.046326, grad_norm: 0.369233
[[34m2025-10-04 12:15:47[0m] Step: 3552, Training Logs: loss_final: 0.899752, loss_mean: 0.854747, loss_mean_cls: 0.045005, grad_norm: 0.544814
[[34m2025-10-04 12:15:47[0m] Step: 3553, Training Logs: loss_final: 0.907594, loss_mean: 0.860926, loss_mean_cls: 0.046667, grad_norm: 0.428828
[[34m2025-10-04 12:15:48[0m] Step: 3554, Training Logs: loss_final: 0.907655, loss_mean: 0.861213, loss_mean_cls: 0.046441, grad_norm: 0.400724
[[34m2025-10-04 12:15:48[0m] Step: 3555, Training Logs: loss_final: 0.897305, loss_mean: 0.850387, loss_mean_cls: 0.046919, grad_norm: 0.694127
[[34m2025-10-04 12:15:48[0m] Step: 3556, Training Logs: loss_final: 0.897965, loss_mean: 0.850688, loss_mean_cls: 0.047277, grad_norm: 0.571232
[[34m2025-10-04 12:15:49[0m] Step: 3557, Training Logs: loss_final: 0.895736, loss_mean: 0.849829, loss_mean_cls: 0.045907, grad_norm: 0.457392
[[34m2025-10-04 12:15:49[0m] Step: 3558, Training Logs: loss_final: 0.891433, loss_mean: 0.843892, loss_mean_cls: 0.047541, grad_norm: 0.653510
[[34m2025-10-04 12:15:49[0m] Step: 3559, Training Logs: loss_final: 0.902140, loss_mean: 0.856533, loss_mean_cls: 0.045607, grad_norm: 0.676517
[[34m2025-10-04 12:15:49[0m] Step: 3560, Training Logs: loss_final: 0.880669, loss_mean: 0.833182, loss_mean_cls: 0.047487, grad_norm: 0.472366
[[34m2025-10-04 12:15:50[0m] Step: 3561, Training Logs: loss_final: 0.894613, loss_mean: 0.848029, loss_mean_cls: 0.046584, grad_norm: 0.485727
[[34m2025-10-04 12:15:50[0m] Step: 3562, Training Logs: loss_final: 0.885164, loss_mean: 0.838198, loss_mean_cls: 0.046966, grad_norm: 0.528359
[[34m2025-10-04 12:15:50[0m] Step: 3563, Training Logs: loss_final: 0.912244, loss_mean: 0.866159, loss_mean_cls: 0.046085, grad_norm: 0.535153
[[34m2025-10-04 12:15:51[0m] Step: 3564, Training Logs: loss_final: 0.901297, loss_mean: 0.855851, loss_mean_cls: 0.045446, grad_norm: 0.424715
[[34m2025-10-04 12:15:51[0m] Step: 3565, Training Logs: loss_final: 0.882740, loss_mean: 0.837405, loss_mean_cls: 0.045334, grad_norm: 0.437075
[[34m2025-10-04 12:15:51[0m] Step: 3566, Training Logs: loss_final: 0.883870, loss_mean: 0.837192, loss_mean_cls: 0.046678, grad_norm: 0.434786
[[34m2025-10-04 12:15:52[0m] Step: 3567, Training Logs: loss_final: 0.892497, loss_mean: 0.845712, loss_mean_cls: 0.046785, grad_norm: 0.346106
[[34m2025-10-04 12:15:52[0m] Step: 3568, Training Logs: loss_final: 0.902963, loss_mean: 0.856996, loss_mean_cls: 0.045967, grad_norm: 0.383791
[[34m2025-10-04 12:15:52[0m] Step: 3569, Training Logs: loss_final: 0.905595, loss_mean: 0.859469, loss_mean_cls: 0.046126, grad_norm: 0.403957
[[34m2025-10-04 12:15:52[0m] Step: 3570, Training Logs: loss_final: 0.904579, loss_mean: 0.858712, loss_mean_cls: 0.045867, grad_norm: 0.567316
[[34m2025-10-04 12:15:53[0m] Step: 3571, Training Logs: loss_final: 0.894951, loss_mean: 0.848405, loss_mean_cls: 0.046546, grad_norm: 0.599784
[[34m2025-10-04 12:15:53[0m] Step: 3572, Training Logs: loss_final: 0.875894, loss_mean: 0.828362, loss_mean_cls: 0.047532, grad_norm: 0.428402
[[34m2025-10-04 12:15:53[0m] Step: 3573, Training Logs: loss_final: 0.915462, loss_mean: 0.870199, loss_mean_cls: 0.045263, grad_norm: 0.703507
[[34m2025-10-04 12:15:54[0m] Step: 3574, Training Logs: loss_final: 0.892350, loss_mean: 0.847590, loss_mean_cls: 0.044760, grad_norm: 0.437237
[[34m2025-10-04 12:15:54[0m] Step: 3575, Training Logs: loss_final: 0.907599, loss_mean: 0.862593, loss_mean_cls: 0.045006, grad_norm: 0.499310
[[34m2025-10-04 12:15:54[0m] Step: 3576, Training Logs: loss_final: 0.898429, loss_mean: 0.852635, loss_mean_cls: 0.045794, grad_norm: 0.466688
[[34m2025-10-04 12:15:55[0m] Step: 3577, Training Logs: loss_final: 0.894936, loss_mean: 0.848339, loss_mean_cls: 0.046597, grad_norm: 0.584093
[[34m2025-10-04 12:15:55[0m] Step: 3578, Training Logs: loss_final: 0.891128, loss_mean: 0.845232, loss_mean_cls: 0.045896, grad_norm: 0.532744
[[34m2025-10-04 12:15:55[0m] Step: 3579, Training Logs: loss_final: 0.887251, loss_mean: 0.840941, loss_mean_cls: 0.046310, grad_norm: 0.525784
[[34m2025-10-04 12:15:55[0m] Step: 3580, Training Logs: loss_final: 0.903864, loss_mean: 0.857418, loss_mean_cls: 0.046446, grad_norm: 0.540189
[[34m2025-10-04 12:15:56[0m] Step: 3581, Training Logs: loss_final: 0.903476, loss_mean: 0.856409, loss_mean_cls: 0.047067, grad_norm: 0.610501
[[34m2025-10-04 12:15:56[0m] Step: 3582, Training Logs: loss_final: 0.893309, loss_mean: 0.848548, loss_mean_cls: 0.044761, grad_norm: 0.481394
[[34m2025-10-04 12:15:56[0m] Step: 3583, Training Logs: loss_final: 0.883978, loss_mean: 0.837306, loss_mean_cls: 0.046672, grad_norm: 0.370437
[[34m2025-10-04 12:15:57[0m] Step: 3584, Training Logs: loss_final: 0.874344, loss_mean: 0.828309, loss_mean_cls: 0.046034, grad_norm: 0.590863
[[34m2025-10-04 12:15:57[0m] Step: 3585, Training Logs: loss_final: 0.916052, loss_mean: 0.871151, loss_mean_cls: 0.044901, grad_norm: 0.526606
[[34m2025-10-04 12:15:57[0m] Step: 3586, Training Logs: loss_final: 0.899889, loss_mean: 0.854383, loss_mean_cls: 0.045507, grad_norm: 0.360510
[[34m2025-10-04 12:15:57[0m] Step: 3587, Training Logs: loss_final: 0.896808, loss_mean: 0.851401, loss_mean_cls: 0.045407, grad_norm: 0.468571
[[34m2025-10-04 12:15:58[0m] Step: 3588, Training Logs: loss_final: 0.900556, loss_mean: 0.853812, loss_mean_cls: 0.046744, grad_norm: 0.417952
[[34m2025-10-04 12:15:58[0m] Step: 3589, Training Logs: loss_final: 0.918643, loss_mean: 0.872478, loss_mean_cls: 0.046165, grad_norm: 0.682077
[[34m2025-10-04 12:15:58[0m] Step: 3590, Training Logs: loss_final: 0.902534, loss_mean: 0.857016, loss_mean_cls: 0.045518, grad_norm: 0.426095
[[34m2025-10-04 12:15:59[0m] Step: 3591, Training Logs: loss_final: 0.890229, loss_mean: 0.843716, loss_mean_cls: 0.046512, grad_norm: 0.547101
[[34m2025-10-04 12:15:59[0m] Step: 3592, Training Logs: loss_final: 0.901087, loss_mean: 0.854182, loss_mean_cls: 0.046905, grad_norm: 0.423128
[[34m2025-10-04 12:15:59[0m] Step: 3593, Training Logs: loss_final: 0.889008, loss_mean: 0.843920, loss_mean_cls: 0.045089, grad_norm: 0.417611
[[34m2025-10-04 12:16:00[0m] Step: 3594, Training Logs: loss_final: 0.881148, loss_mean: 0.833768, loss_mean_cls: 0.047380, grad_norm: 0.439138
[[34m2025-10-04 12:16:00[0m] Step: 3595, Training Logs: loss_final: 0.897171, loss_mean: 0.850029, loss_mean_cls: 0.047142, grad_norm: 0.330747
[[34m2025-10-04 12:16:00[0m] Step: 3596, Training Logs: loss_final: 0.894896, loss_mean: 0.848627, loss_mean_cls: 0.046268, grad_norm: 0.522294
[[34m2025-10-04 12:16:00[0m] Step: 3597, Training Logs: loss_final: 0.911601, loss_mean: 0.866061, loss_mean_cls: 0.045540, grad_norm: 0.292331
[[34m2025-10-04 12:16:01[0m] Step: 3598, Training Logs: loss_final: 0.885085, loss_mean: 0.838373, loss_mean_cls: 0.046712, grad_norm: 0.494644
[[34m2025-10-04 12:16:01[0m] Step: 3599, Training Logs: loss_final: 0.893398, loss_mean: 0.846239, loss_mean_cls: 0.047159, grad_norm: 0.397730
[[34m2025-10-04 12:16:01[0m] Step: 3600, Training Logs: loss_final: 0.913420, loss_mean: 0.866512, loss_mean_cls: 0.046908, grad_norm: 0.420829
[[34m2025-10-04 12:16:02[0m] Step: 3601, Training Logs: loss_final: 0.893003, loss_mean: 0.846631, loss_mean_cls: 0.046372, grad_norm: 0.464134
[[34m2025-10-04 12:16:02[0m] Step: 3602, Training Logs: loss_final: 0.884017, loss_mean: 0.838317, loss_mean_cls: 0.045701, grad_norm: 0.325865
[[34m2025-10-04 12:16:02[0m] Step: 3603, Training Logs: loss_final: 0.890257, loss_mean: 0.842912, loss_mean_cls: 0.047346, grad_norm: 0.471002
[[34m2025-10-04 12:16:02[0m] Step: 3604, Training Logs: loss_final: 0.899824, loss_mean: 0.854192, loss_mean_cls: 0.045632, grad_norm: 0.399160
[[34m2025-10-04 12:16:03[0m] Step: 3605, Training Logs: loss_final: 0.917706, loss_mean: 0.872081, loss_mean_cls: 0.045626, grad_norm: 0.561986
[[34m2025-10-04 12:16:03[0m] Step: 3606, Training Logs: loss_final: 0.890904, loss_mean: 0.844161, loss_mean_cls: 0.046742, grad_norm: 0.393090
[[34m2025-10-04 12:16:03[0m] Step: 3607, Training Logs: loss_final: 0.902122, loss_mean: 0.856292, loss_mean_cls: 0.045830, grad_norm: 0.438373
[[34m2025-10-04 12:16:04[0m] Step: 3608, Training Logs: loss_final: 0.899027, loss_mean: 0.854281, loss_mean_cls: 0.044746, grad_norm: 0.420614
[[34m2025-10-04 12:16:04[0m] Step: 3609, Training Logs: loss_final: 0.893294, loss_mean: 0.846670, loss_mean_cls: 0.046624, grad_norm: 0.470320
[[34m2025-10-04 12:16:04[0m] Step: 3610, Training Logs: loss_final: 0.890727, loss_mean: 0.843896, loss_mean_cls: 0.046830, grad_norm: 0.467965
[[34m2025-10-04 12:16:05[0m] Step: 3611, Training Logs: loss_final: 0.903133, loss_mean: 0.857105, loss_mean_cls: 0.046028, grad_norm: 0.494846
[[34m2025-10-04 12:16:05[0m] Step: 3612, Training Logs: loss_final: 0.912585, loss_mean: 0.866746, loss_mean_cls: 0.045839, grad_norm: 0.593357
[[34m2025-10-04 12:16:05[0m] Step: 3613, Training Logs: loss_final: 0.905962, loss_mean: 0.860386, loss_mean_cls: 0.045576, grad_norm: 0.629164
[[34m2025-10-04 12:16:05[0m] Step: 3614, Training Logs: loss_final: 0.920271, loss_mean: 0.874262, loss_mean_cls: 0.046009, grad_norm: 0.715074
[[34m2025-10-04 12:16:06[0m] Step: 3615, Training Logs: loss_final: 0.897002, loss_mean: 0.851323, loss_mean_cls: 0.045680, grad_norm: 0.543596
[[34m2025-10-04 12:16:06[0m] Step: 3616, Training Logs: loss_final: 0.910690, loss_mean: 0.863938, loss_mean_cls: 0.046752, grad_norm: 0.585090
[[34m2025-10-04 12:16:06[0m] Step: 3617, Training Logs: loss_final: 0.915736, loss_mean: 0.868628, loss_mean_cls: 0.047108, grad_norm: 0.631413
[[34m2025-10-04 12:16:07[0m] Step: 3618, Training Logs: loss_final: 0.890268, loss_mean: 0.843552, loss_mean_cls: 0.046717, grad_norm: 0.464540
[[34m2025-10-04 12:16:07[0m] Step: 3619, Training Logs: loss_final: 0.902455, loss_mean: 0.857290, loss_mean_cls: 0.045164, grad_norm: 0.752637
[[34m2025-10-04 12:16:07[0m] Step: 3620, Training Logs: loss_final: 0.897115, loss_mean: 0.849771, loss_mean_cls: 0.047343, grad_norm: 0.662273
[[34m2025-10-04 12:16:07[0m] Step: 3621, Training Logs: loss_final: 0.887856, loss_mean: 0.842078, loss_mean_cls: 0.045779, grad_norm: 0.526729
[[34m2025-10-04 12:16:08[0m] Step: 3622, Training Logs: loss_final: 0.901130, loss_mean: 0.855822, loss_mean_cls: 0.045309, grad_norm: 0.709171
[[34m2025-10-04 12:16:08[0m] Step: 3623, Training Logs: loss_final: 0.923235, loss_mean: 0.876993, loss_mean_cls: 0.046242, grad_norm: 0.456999
[[34m2025-10-04 12:16:08[0m] Step: 3624, Training Logs: loss_final: 0.905825, loss_mean: 0.859786, loss_mean_cls: 0.046039, grad_norm: 0.818150
[[34m2025-10-04 12:16:09[0m] Step: 3625, Training Logs: loss_final: 0.908894, loss_mean: 0.863218, loss_mean_cls: 0.045676, grad_norm: 0.496402
[[34m2025-10-04 12:16:09[0m] Step: 3626, Training Logs: loss_final: 0.901791, loss_mean: 0.856908, loss_mean_cls: 0.044883, grad_norm: 0.792855
[[34m2025-10-04 12:16:09[0m] Step: 3627, Training Logs: loss_final: 0.890873, loss_mean: 0.844620, loss_mean_cls: 0.046253, grad_norm: 0.415986
[[34m2025-10-04 12:16:09[0m] Step: 3628, Training Logs: loss_final: 0.914906, loss_mean: 0.868655, loss_mean_cls: 0.046251, grad_norm: 0.877111
[[34m2025-10-04 12:16:10[0m] Step: 3629, Training Logs: loss_final: 0.888146, loss_mean: 0.841373, loss_mean_cls: 0.046773, grad_norm: 0.555259
[[34m2025-10-04 12:16:10[0m] Step: 3630, Training Logs: loss_final: 0.922116, loss_mean: 0.875904, loss_mean_cls: 0.046212, grad_norm: 0.851480
[[34m2025-10-04 12:16:10[0m] Step: 3631, Training Logs: loss_final: 0.903955, loss_mean: 0.857214, loss_mean_cls: 0.046741, grad_norm: 0.658026
[[34m2025-10-04 12:16:11[0m] Step: 3632, Training Logs: loss_final: 0.882612, loss_mean: 0.837143, loss_mean_cls: 0.045470, grad_norm: 0.693903
[[34m2025-10-04 12:16:11[0m] Step: 3633, Training Logs: loss_final: 0.905075, loss_mean: 0.858624, loss_mean_cls: 0.046451, grad_norm: 0.521166
[[34m2025-10-04 12:16:11[0m] Step: 3634, Training Logs: loss_final: 0.937152, loss_mean: 0.893109, loss_mean_cls: 0.044043, grad_norm: 0.815090
[[34m2025-10-04 12:16:12[0m] Step: 3635, Training Logs: loss_final: 0.873716, loss_mean: 0.826409, loss_mean_cls: 0.047306, grad_norm: 0.499873
[[34m2025-10-04 12:16:12[0m] Step: 3636, Training Logs: loss_final: 0.919296, loss_mean: 0.874468, loss_mean_cls: 0.044828, grad_norm: 0.772327
[[34m2025-10-04 12:16:12[0m] Step: 3637, Training Logs: loss_final: 0.919843, loss_mean: 0.875188, loss_mean_cls: 0.044654, grad_norm: 0.652741
[[34m2025-10-04 12:16:12[0m] Step: 3638, Training Logs: loss_final: 0.910030, loss_mean: 0.864162, loss_mean_cls: 0.045868, grad_norm: 0.816167
[[34m2025-10-04 12:16:13[0m] Step: 3639, Training Logs: loss_final: 0.884627, loss_mean: 0.838721, loss_mean_cls: 0.045906, grad_norm: 0.581390
[[34m2025-10-04 12:16:13[0m] Step: 3640, Training Logs: loss_final: 0.913117, loss_mean: 0.866419, loss_mean_cls: 0.046698, grad_norm: 0.713614
[[34m2025-10-04 12:16:13[0m] Step: 3641, Training Logs: loss_final: 0.910367, loss_mean: 0.864363, loss_mean_cls: 0.046003, grad_norm: 0.820147
[[34m2025-10-04 12:16:14[0m] Step: 3642, Training Logs: loss_final: 0.909380, loss_mean: 0.863244, loss_mean_cls: 0.046136, grad_norm: 0.676749
[[34m2025-10-04 12:16:14[0m] Step: 3643, Training Logs: loss_final: 0.906496, loss_mean: 0.860326, loss_mean_cls: 0.046170, grad_norm: 0.589108
[[34m2025-10-04 12:16:14[0m] Step: 3644, Training Logs: loss_final: 0.900558, loss_mean: 0.855064, loss_mean_cls: 0.045494, grad_norm: 0.661915
[[34m2025-10-04 12:16:14[0m] Step: 3645, Training Logs: loss_final: 0.919763, loss_mean: 0.874561, loss_mean_cls: 0.045202, grad_norm: 0.607706
[[34m2025-10-04 12:16:15[0m] Step: 3646, Training Logs: loss_final: 0.894477, loss_mean: 0.848205, loss_mean_cls: 0.046272, grad_norm: 0.478687
[[34m2025-10-04 12:16:15[0m] Step: 3647, Training Logs: loss_final: 0.902020, loss_mean: 0.854989, loss_mean_cls: 0.047031, grad_norm: 0.631029
[[34m2025-10-04 12:16:15[0m] Step: 3648, Training Logs: loss_final: 0.885320, loss_mean: 0.839273, loss_mean_cls: 0.046047, grad_norm: 0.524370
[[34m2025-10-04 12:16:16[0m] Step: 3649, Training Logs: loss_final: 0.898900, loss_mean: 0.852573, loss_mean_cls: 0.046327, grad_norm: 0.800585
[[34m2025-10-04 12:16:16[0m] Step: 3650, Training Logs: loss_final: 0.900196, loss_mean: 0.853771, loss_mean_cls: 0.046425, grad_norm: 0.406791
[[34m2025-10-04 12:16:16[0m] Step: 3651, Training Logs: loss_final: 0.923527, loss_mean: 0.878175, loss_mean_cls: 0.045352, grad_norm: 0.774337
[[34m2025-10-04 12:16:17[0m] Step: 3652, Training Logs: loss_final: 0.896573, loss_mean: 0.850507, loss_mean_cls: 0.046066, grad_norm: 0.503656
[[34m2025-10-04 12:16:17[0m] Step: 3653, Training Logs: loss_final: 0.911169, loss_mean: 0.864440, loss_mean_cls: 0.046729, grad_norm: 0.469722
[[34m2025-10-04 12:16:17[0m] Step: 3654, Training Logs: loss_final: 0.902109, loss_mean: 0.855795, loss_mean_cls: 0.046314, grad_norm: 0.419734
[[34m2025-10-04 12:16:17[0m] Step: 3655, Training Logs: loss_final: 0.898300, loss_mean: 0.853254, loss_mean_cls: 0.045046, grad_norm: 0.532188
[[34m2025-10-04 12:16:18[0m] Step: 3656, Training Logs: loss_final: 0.876772, loss_mean: 0.831062, loss_mean_cls: 0.045710, grad_norm: 0.432825
[[34m2025-10-04 12:16:18[0m] Step: 3657, Training Logs: loss_final: 0.899452, loss_mean: 0.852410, loss_mean_cls: 0.047042, grad_norm: 0.362478
[[34m2025-10-04 12:16:18[0m] Step: 3658, Training Logs: loss_final: 0.896799, loss_mean: 0.850296, loss_mean_cls: 0.046503, grad_norm: 0.411732
[[34m2025-10-04 12:16:19[0m] Step: 3659, Training Logs: loss_final: 0.896210, loss_mean: 0.850646, loss_mean_cls: 0.045563, grad_norm: 0.333279
[[34m2025-10-04 12:16:19[0m] Step: 3660, Training Logs: loss_final: 0.904593, loss_mean: 0.858844, loss_mean_cls: 0.045750, grad_norm: 0.305340
[[34m2025-10-04 12:16:19[0m] Step: 3661, Training Logs: loss_final: 0.914115, loss_mean: 0.867937, loss_mean_cls: 0.046178, grad_norm: 0.376416
[[34m2025-10-04 12:16:19[0m] Step: 3662, Training Logs: loss_final: 0.910489, loss_mean: 0.864668, loss_mean_cls: 0.045822, grad_norm: 0.341066
[[34m2025-10-04 12:16:20[0m] Step: 3663, Training Logs: loss_final: 0.922254, loss_mean: 0.875670, loss_mean_cls: 0.046585, grad_norm: 0.338075
[[34m2025-10-04 12:16:20[0m] Step: 3664, Training Logs: loss_final: 0.896324, loss_mean: 0.850818, loss_mean_cls: 0.045505, grad_norm: 0.372798
[[34m2025-10-04 12:16:20[0m] Step: 3665, Training Logs: loss_final: 0.898603, loss_mean: 0.853575, loss_mean_cls: 0.045028, grad_norm: 0.339732
[[34m2025-10-04 12:16:21[0m] Step: 3666, Training Logs: loss_final: 0.916981, loss_mean: 0.872084, loss_mean_cls: 0.044897, grad_norm: 0.369543
[[34m2025-10-04 12:16:21[0m] Step: 3667, Training Logs: loss_final: 0.881388, loss_mean: 0.835252, loss_mean_cls: 0.046136, grad_norm: 0.478395
[[34m2025-10-04 12:16:21[0m] Step: 3668, Training Logs: loss_final: 0.894378, loss_mean: 0.847872, loss_mean_cls: 0.046506, grad_norm: 0.349337
[[34m2025-10-04 12:16:22[0m] Step: 3669, Training Logs: loss_final: 0.885008, loss_mean: 0.839538, loss_mean_cls: 0.045470, grad_norm: 0.518210
[[34m2025-10-04 12:16:22[0m] Step: 3670, Training Logs: loss_final: 0.897252, loss_mean: 0.851447, loss_mean_cls: 0.045805, grad_norm: 0.444036
[[34m2025-10-04 12:16:22[0m] Step: 3671, Training Logs: loss_final: 0.894645, loss_mean: 0.848091, loss_mean_cls: 0.046554, grad_norm: 0.458353
[[34m2025-10-04 12:16:22[0m] Step: 3672, Training Logs: loss_final: 0.926534, loss_mean: 0.880049, loss_mean_cls: 0.046485, grad_norm: 0.544206
[[34m2025-10-04 12:16:23[0m] Step: 3673, Training Logs: loss_final: 0.887719, loss_mean: 0.841835, loss_mean_cls: 0.045884, grad_norm: 0.337818
[[34m2025-10-04 12:16:23[0m] Step: 3674, Training Logs: loss_final: 0.904584, loss_mean: 0.857373, loss_mean_cls: 0.047211, grad_norm: 0.476772
[[34m2025-10-04 12:16:23[0m] Step: 3675, Training Logs: loss_final: 0.899898, loss_mean: 0.854147, loss_mean_cls: 0.045751, grad_norm: 0.489518
[[34m2025-10-04 12:16:24[0m] Step: 3676, Training Logs: loss_final: 0.924065, loss_mean: 0.878697, loss_mean_cls: 0.045369, grad_norm: 0.384781
[[34m2025-10-04 12:16:24[0m] Step: 3677, Training Logs: loss_final: 0.910430, loss_mean: 0.863913, loss_mean_cls: 0.046518, grad_norm: 0.493549
[[34m2025-10-04 12:16:24[0m] Step: 3678, Training Logs: loss_final: 0.911551, loss_mean: 0.866020, loss_mean_cls: 0.045531, grad_norm: 0.427528
[[34m2025-10-04 12:16:25[0m] Step: 3679, Training Logs: loss_final: 0.884111, loss_mean: 0.837237, loss_mean_cls: 0.046874, grad_norm: 0.538264
[[34m2025-10-04 12:16:25[0m] Step: 3680, Training Logs: loss_final: 0.908002, loss_mean: 0.861052, loss_mean_cls: 0.046950, grad_norm: 0.492699
[[34m2025-10-04 12:16:25[0m] Step: 3681, Training Logs: loss_final: 0.915945, loss_mean: 0.870940, loss_mean_cls: 0.045006, grad_norm: 0.399445
[[34m2025-10-04 12:16:25[0m] Step: 3682, Training Logs: loss_final: 0.899938, loss_mean: 0.854842, loss_mean_cls: 0.045097, grad_norm: 0.348733
[[34m2025-10-04 12:16:26[0m] Step: 3683, Training Logs: loss_final: 0.876690, loss_mean: 0.830160, loss_mean_cls: 0.046530, grad_norm: 0.456380
[[34m2025-10-04 12:16:26[0m] Step: 3684, Training Logs: loss_final: 0.903493, loss_mean: 0.857074, loss_mean_cls: 0.046419, grad_norm: 0.299973
[[34m2025-10-04 12:16:26[0m] Step: 3685, Training Logs: loss_final: 0.914093, loss_mean: 0.867670, loss_mean_cls: 0.046423, grad_norm: 0.560367
[[34m2025-10-04 12:16:27[0m] Step: 3686, Training Logs: loss_final: 0.899694, loss_mean: 0.854537, loss_mean_cls: 0.045156, grad_norm: 0.357121
[[34m2025-10-04 12:16:27[0m] Step: 3687, Training Logs: loss_final: 0.883420, loss_mean: 0.835529, loss_mean_cls: 0.047891, grad_norm: 0.439565
[[34m2025-10-04 12:16:27[0m] Step: 3688, Training Logs: loss_final: 0.899210, loss_mean: 0.853878, loss_mean_cls: 0.045332, grad_norm: 0.505644
[[34m2025-10-04 12:16:27[0m] Step: 3689, Training Logs: loss_final: 0.914444, loss_mean: 0.870438, loss_mean_cls: 0.044006, grad_norm: 0.454080
[[34m2025-10-04 12:16:28[0m] Step: 3690, Training Logs: loss_final: 0.906761, loss_mean: 0.860794, loss_mean_cls: 0.045967, grad_norm: 0.463357
[[34m2025-10-04 12:16:28[0m] Step: 3691, Training Logs: loss_final: 0.901609, loss_mean: 0.856016, loss_mean_cls: 0.045593, grad_norm: 0.412504
[[34m2025-10-04 12:16:28[0m] Step: 3692, Training Logs: loss_final: 0.914412, loss_mean: 0.868407, loss_mean_cls: 0.046005, grad_norm: 0.366960
[[34m2025-10-04 12:16:29[0m] Step: 3693, Training Logs: loss_final: 0.865636, loss_mean: 0.818974, loss_mean_cls: 0.046662, grad_norm: 0.392406
[[34m2025-10-04 12:16:29[0m] Step: 3694, Training Logs: loss_final: 0.875405, loss_mean: 0.828286, loss_mean_cls: 0.047119, grad_norm: 0.515603
[[34m2025-10-04 12:16:29[0m] Step: 3695, Training Logs: loss_final: 0.899155, loss_mean: 0.851576, loss_mean_cls: 0.047579, grad_norm: 0.595156
[[34m2025-10-04 12:16:30[0m] Step: 3696, Training Logs: loss_final: 0.866441, loss_mean: 0.819485, loss_mean_cls: 0.046956, grad_norm: 0.385965
[[34m2025-10-04 12:16:30[0m] Step: 3697, Training Logs: loss_final: 0.880535, loss_mean: 0.834278, loss_mean_cls: 0.046257, grad_norm: 0.459450
[[34m2025-10-04 12:16:30[0m] Step: 3698, Training Logs: loss_final: 0.898972, loss_mean: 0.853259, loss_mean_cls: 0.045714, grad_norm: 0.439549
[[34m2025-10-04 12:16:30[0m] Step: 3699, Training Logs: loss_final: 0.915264, loss_mean: 0.868173, loss_mean_cls: 0.047092, grad_norm: 0.425885
[[34m2025-10-04 12:16:31[0m] Step: 3700, Training Logs: loss_final: 0.911697, loss_mean: 0.866770, loss_mean_cls: 0.044927, grad_norm: 0.539673
[[34m2025-10-04 12:16:31[0m] Step: 3701, Training Logs: loss_final: 0.911491, loss_mean: 0.866698, loss_mean_cls: 0.044794, grad_norm: 0.540457
[[34m2025-10-04 12:16:31[0m] Step: 3702, Training Logs: loss_final: 0.873221, loss_mean: 0.825402, loss_mean_cls: 0.047819, grad_norm: 0.475909
[[34m2025-10-04 12:16:32[0m] Step: 3703, Training Logs: loss_final: 0.903309, loss_mean: 0.858136, loss_mean_cls: 0.045173, grad_norm: 0.520334
[[34m2025-10-04 12:16:32[0m] Step: 3704, Training Logs: loss_final: 0.914979, loss_mean: 0.870059, loss_mean_cls: 0.044920, grad_norm: 0.550173
[[34m2025-10-04 12:16:32[0m] Step: 3705, Training Logs: loss_final: 0.887472, loss_mean: 0.841186, loss_mean_cls: 0.046286, grad_norm: 0.339475
[[34m2025-10-04 12:16:33[0m] Step: 3706, Training Logs: loss_final: 0.895186, loss_mean: 0.850246, loss_mean_cls: 0.044940, grad_norm: 0.601213
[[34m2025-10-04 12:16:33[0m] Step: 3707, Training Logs: loss_final: 0.903146, loss_mean: 0.856545, loss_mean_cls: 0.046601, grad_norm: 0.435791
[[34m2025-10-04 12:16:33[0m] Step: 3708, Training Logs: loss_final: 0.910989, loss_mean: 0.864843, loss_mean_cls: 0.046145, grad_norm: 0.449396
[[34m2025-10-04 12:16:33[0m] Step: 3709, Training Logs: loss_final: 0.905010, loss_mean: 0.859016, loss_mean_cls: 0.045994, grad_norm: 0.400565
[[34m2025-10-04 12:16:34[0m] Step: 3710, Training Logs: loss_final: 0.908269, loss_mean: 0.861960, loss_mean_cls: 0.046309, grad_norm: 0.464596
[[34m2025-10-04 12:16:34[0m] Step: 3711, Training Logs: loss_final: 0.885559, loss_mean: 0.838262, loss_mean_cls: 0.047297, grad_norm: 0.400437
[[34m2025-10-04 12:16:34[0m] Step: 3712, Training Logs: loss_final: 0.919439, loss_mean: 0.873934, loss_mean_cls: 0.045504, grad_norm: 0.497734
[[34m2025-10-04 12:16:35[0m] Step: 3713, Training Logs: loss_final: 0.905061, loss_mean: 0.859504, loss_mean_cls: 0.045557, grad_norm: 0.480640
[[34m2025-10-04 12:16:35[0m] Step: 3714, Training Logs: loss_final: 0.911388, loss_mean: 0.865552, loss_mean_cls: 0.045835, grad_norm: 0.467818
[[34m2025-10-04 12:16:35[0m] Step: 3715, Training Logs: loss_final: 0.903350, loss_mean: 0.857792, loss_mean_cls: 0.045558, grad_norm: 0.601001
[[34m2025-10-04 12:16:35[0m] Step: 3716, Training Logs: loss_final: 0.891955, loss_mean: 0.846533, loss_mean_cls: 0.045422, grad_norm: 0.433432
[[34m2025-10-04 12:16:36[0m] Step: 3717, Training Logs: loss_final: 0.895229, loss_mean: 0.849495, loss_mean_cls: 0.045734, grad_norm: 0.460560
[[34m2025-10-04 12:16:36[0m] Step: 3718, Training Logs: loss_final: 0.902144, loss_mean: 0.857549, loss_mean_cls: 0.044595, grad_norm: 0.486712
[[34m2025-10-04 12:16:36[0m] Step: 3719, Training Logs: loss_final: 0.917172, loss_mean: 0.871251, loss_mean_cls: 0.045921, grad_norm: 0.506901
[[34m2025-10-04 12:16:37[0m] Step: 3720, Training Logs: loss_final: 0.889999, loss_mean: 0.843429, loss_mean_cls: 0.046570, grad_norm: 0.383133
[[34m2025-10-04 12:16:37[0m] Step: 3721, Training Logs: loss_final: 0.927231, loss_mean: 0.882996, loss_mean_cls: 0.044234, grad_norm: 0.386278
[[34m2025-10-04 12:16:37[0m] Step: 3722, Training Logs: loss_final: 0.900951, loss_mean: 0.856220, loss_mean_cls: 0.044732, grad_norm: 0.412100
[[34m2025-10-04 12:16:38[0m] Step: 3723, Training Logs: loss_final: 0.900245, loss_mean: 0.852663, loss_mean_cls: 0.047582, grad_norm: 0.373341
[[34m2025-10-04 12:16:38[0m] Step: 3724, Training Logs: loss_final: 0.915099, loss_mean: 0.869875, loss_mean_cls: 0.045224, grad_norm: 0.297781
[[34m2025-10-04 12:16:38[0m] Step: 3725, Training Logs: loss_final: 0.884957, loss_mean: 0.839329, loss_mean_cls: 0.045628, grad_norm: 0.387256
[[34m2025-10-04 12:16:38[0m] Step: 3726, Training Logs: loss_final: 0.894056, loss_mean: 0.847180, loss_mean_cls: 0.046876, grad_norm: 0.424457
[[34m2025-10-04 12:16:39[0m] Step: 3727, Training Logs: loss_final: 0.902787, loss_mean: 0.856583, loss_mean_cls: 0.046204, grad_norm: 0.438250
[[34m2025-10-04 12:16:39[0m] Step: 3728, Training Logs: loss_final: 0.868504, loss_mean: 0.822838, loss_mean_cls: 0.045666, grad_norm: 0.484940
[[34m2025-10-04 12:16:39[0m] Step: 3729, Training Logs: loss_final: 0.877286, loss_mean: 0.830972, loss_mean_cls: 0.046315, grad_norm: 0.448674
[[34m2025-10-04 12:16:40[0m] Step: 3730, Training Logs: loss_final: 0.899926, loss_mean: 0.854218, loss_mean_cls: 0.045708, grad_norm: 0.481211
[[34m2025-10-04 12:16:40[0m] Step: 3731, Training Logs: loss_final: 0.890922, loss_mean: 0.845705, loss_mean_cls: 0.045218, grad_norm: 0.401734
[[34m2025-10-04 12:16:40[0m] Step: 3732, Training Logs: loss_final: 0.918177, loss_mean: 0.872840, loss_mean_cls: 0.045337, grad_norm: 0.654491
[[34m2025-10-04 12:16:41[0m] Step: 3733, Training Logs: loss_final: 0.911968, loss_mean: 0.865474, loss_mean_cls: 0.046494, grad_norm: 0.391112
[[34m2025-10-04 12:16:41[0m] Step: 3734, Training Logs: loss_final: 0.899682, loss_mean: 0.853385, loss_mean_cls: 0.046297, grad_norm: 0.478746
[[34m2025-10-04 12:16:41[0m] Step: 3735, Training Logs: loss_final: 0.923840, loss_mean: 0.877299, loss_mean_cls: 0.046541, grad_norm: 0.476108
[[34m2025-10-04 12:16:41[0m] Step: 3736, Training Logs: loss_final: 0.907584, loss_mean: 0.861472, loss_mean_cls: 0.046112, grad_norm: 0.412968
[[34m2025-10-04 12:16:42[0m] Step: 3737, Training Logs: loss_final: 0.891171, loss_mean: 0.845104, loss_mean_cls: 0.046066, grad_norm: 0.401065
[[34m2025-10-04 12:16:42[0m] Step: 3738, Training Logs: loss_final: 0.902095, loss_mean: 0.855531, loss_mean_cls: 0.046564, grad_norm: 0.359407
[[34m2025-10-04 12:16:42[0m] Step: 3739, Training Logs: loss_final: 0.898665, loss_mean: 0.852029, loss_mean_cls: 0.046636, grad_norm: 0.393807
[[34m2025-10-04 12:16:43[0m] Step: 3740, Training Logs: loss_final: 0.880169, loss_mean: 0.833589, loss_mean_cls: 0.046581, grad_norm: 0.277812
[[34m2025-10-04 12:16:43[0m] Step: 3741, Training Logs: loss_final: 0.904999, loss_mean: 0.859883, loss_mean_cls: 0.045116, grad_norm: 0.447488
[[34m2025-10-04 12:16:43[0m] Step: 3742, Training Logs: loss_final: 0.903306, loss_mean: 0.857928, loss_mean_cls: 0.045378, grad_norm: 0.279174
[[34m2025-10-04 12:16:43[0m] Step: 3743, Training Logs: loss_final: 0.866899, loss_mean: 0.820317, loss_mean_cls: 0.046581, grad_norm: 0.473097
[[34m2025-10-04 12:16:44[0m] Step: 3744, Training Logs: loss_final: 0.913135, loss_mean: 0.867325, loss_mean_cls: 0.045810, grad_norm: 0.346957
[[34m2025-10-04 12:16:44[0m] Step: 3745, Training Logs: loss_final: 0.872278, loss_mean: 0.825109, loss_mean_cls: 0.047168, grad_norm: 0.376534
[[34m2025-10-04 12:16:44[0m] Step: 3746, Training Logs: loss_final: 0.871235, loss_mean: 0.824548, loss_mean_cls: 0.046688, grad_norm: 0.393215
[[34m2025-10-04 12:16:45[0m] Step: 3747, Training Logs: loss_final: 0.909013, loss_mean: 0.862701, loss_mean_cls: 0.046312, grad_norm: 0.390937
[[34m2025-10-04 12:16:45[0m] Step: 3748, Training Logs: loss_final: 0.907108, loss_mean: 0.861837, loss_mean_cls: 0.045270, grad_norm: 0.289464
[[34m2025-10-04 12:16:45[0m] Step: 3749, Training Logs: loss_final: 0.912843, loss_mean: 0.866903, loss_mean_cls: 0.045940, grad_norm: 0.474974
[[34m2025-10-04 12:16:46[0m] Step: 3750, Training Logs: loss_final: 0.882550, loss_mean: 0.836753, loss_mean_cls: 0.045797, grad_norm: 0.423390
[[34m2025-10-04 12:16:46[0m] Step: 3751, Training Logs: loss_final: 0.896004, loss_mean: 0.849428, loss_mean_cls: 0.046576, grad_norm: 0.320479
[[34m2025-10-04 12:16:46[0m] Step: 3752, Training Logs: loss_final: 0.884239, loss_mean: 0.836763, loss_mean_cls: 0.047476, grad_norm: 0.500590
[[34m2025-10-04 12:16:46[0m] Step: 3753, Training Logs: loss_final: 0.903119, loss_mean: 0.858289, loss_mean_cls: 0.044830, grad_norm: 0.356710
[[34m2025-10-04 12:16:47[0m] Step: 3754, Training Logs: loss_final: 0.894551, loss_mean: 0.848572, loss_mean_cls: 0.045979, grad_norm: 0.334236
[[34m2025-10-04 12:16:47[0m] Step: 3755, Training Logs: loss_final: 0.888022, loss_mean: 0.841385, loss_mean_cls: 0.046636, grad_norm: 0.472172
[[34m2025-10-04 12:16:47[0m] Step: 3756, Training Logs: loss_final: 0.903345, loss_mean: 0.858128, loss_mean_cls: 0.045216, grad_norm: 0.361722
[[34m2025-10-04 12:16:48[0m] Step: 3757, Training Logs: loss_final: 0.872381, loss_mean: 0.826437, loss_mean_cls: 0.045944, grad_norm: 0.461184
[[34m2025-10-04 12:16:48[0m] Step: 3758, Training Logs: loss_final: 0.885271, loss_mean: 0.839789, loss_mean_cls: 0.045482, grad_norm: 0.487968
[[34m2025-10-04 12:16:48[0m] Step: 3759, Training Logs: loss_final: 0.890987, loss_mean: 0.845559, loss_mean_cls: 0.045428, grad_norm: 0.341829
[[34m2025-10-04 12:16:48[0m] Step: 3760, Training Logs: loss_final: 0.905814, loss_mean: 0.858894, loss_mean_cls: 0.046920, grad_norm: 0.398402
[[34m2025-10-04 12:16:49[0m] Step: 3761, Training Logs: loss_final: 0.879183, loss_mean: 0.832920, loss_mean_cls: 0.046263, grad_norm: 0.522094
[[34m2025-10-04 12:16:49[0m] Step: 3762, Training Logs: loss_final: 0.922786, loss_mean: 0.877438, loss_mean_cls: 0.045348, grad_norm: 0.552564
[[34m2025-10-04 12:16:49[0m] Step: 3763, Training Logs: loss_final: 0.885897, loss_mean: 0.839747, loss_mean_cls: 0.046150, grad_norm: 0.437746
[[34m2025-10-04 12:16:50[0m] Step: 3764, Training Logs: loss_final: 0.895774, loss_mean: 0.849934, loss_mean_cls: 0.045841, grad_norm: 0.675403
[[34m2025-10-04 12:16:50[0m] Step: 3765, Training Logs: loss_final: 0.886255, loss_mean: 0.839874, loss_mean_cls: 0.046381, grad_norm: 0.519081
[[34m2025-10-04 12:16:50[0m] Step: 3766, Training Logs: loss_final: 0.884763, loss_mean: 0.839229, loss_mean_cls: 0.045534, grad_norm: 0.454790
[[34m2025-10-04 12:16:50[0m] Step: 3767, Training Logs: loss_final: 0.901301, loss_mean: 0.855724, loss_mean_cls: 0.045577, grad_norm: 0.581265
[[34m2025-10-04 12:16:51[0m] Step: 3768, Training Logs: loss_final: 0.910952, loss_mean: 0.863814, loss_mean_cls: 0.047138, grad_norm: 0.874067
[[34m2025-10-04 12:16:51[0m] Step: 3769, Training Logs: loss_final: 0.901478, loss_mean: 0.855450, loss_mean_cls: 0.046028, grad_norm: 0.538024
[[34m2025-10-04 12:16:51[0m] Step: 3770, Training Logs: loss_final: 0.905397, loss_mean: 0.857807, loss_mean_cls: 0.047590, grad_norm: 0.502620
[[34m2025-10-04 12:16:52[0m] Step: 3771, Training Logs: loss_final: 0.922700, loss_mean: 0.877992, loss_mean_cls: 0.044708, grad_norm: 0.720327
[[34m2025-10-04 12:16:52[0m] Step: 3772, Training Logs: loss_final: 0.881020, loss_mean: 0.835383, loss_mean_cls: 0.045637, grad_norm: 0.449376
[[34m2025-10-04 12:16:52[0m] Step: 3773, Training Logs: loss_final: 0.899987, loss_mean: 0.853473, loss_mean_cls: 0.046514, grad_norm: 0.429096
[[34m2025-10-04 12:16:53[0m] Step: 3774, Training Logs: loss_final: 0.898317, loss_mean: 0.852427, loss_mean_cls: 0.045890, grad_norm: 0.544190
[[34m2025-10-04 12:16:53[0m] Step: 3775, Training Logs: loss_final: 0.888446, loss_mean: 0.840984, loss_mean_cls: 0.047463, grad_norm: 0.347501
[[34m2025-10-04 12:16:53[0m] Step: 3776, Training Logs: loss_final: 0.899320, loss_mean: 0.853492, loss_mean_cls: 0.045828, grad_norm: 0.311342
[[34m2025-10-04 12:16:53[0m] Step: 3777, Training Logs: loss_final: 0.886164, loss_mean: 0.840478, loss_mean_cls: 0.045685, grad_norm: 0.418205
[[34m2025-10-04 12:16:54[0m] Step: 3778, Training Logs: loss_final: 0.905238, loss_mean: 0.859906, loss_mean_cls: 0.045333, grad_norm: 0.398204
[[34m2025-10-04 12:16:54[0m] Step: 3779, Training Logs: loss_final: 0.902324, loss_mean: 0.856172, loss_mean_cls: 0.046152, grad_norm: 0.412914
[[34m2025-10-04 12:16:54[0m] Step: 3780, Training Logs: loss_final: 0.889803, loss_mean: 0.843018, loss_mean_cls: 0.046785, grad_norm: 0.434824
[[34m2025-10-04 12:16:55[0m] Step: 3781, Training Logs: loss_final: 0.907105, loss_mean: 0.861144, loss_mean_cls: 0.045962, grad_norm: 0.383169
[[34m2025-10-04 12:16:55[0m] Step: 3782, Training Logs: loss_final: 0.909941, loss_mean: 0.863653, loss_mean_cls: 0.046288, grad_norm: 0.463657
[[34m2025-10-04 12:16:55[0m] Step: 3783, Training Logs: loss_final: 0.892696, loss_mean: 0.846729, loss_mean_cls: 0.045967, grad_norm: 0.557259
[[34m2025-10-04 12:16:55[0m] Step: 3784, Training Logs: loss_final: 0.897993, loss_mean: 0.851415, loss_mean_cls: 0.046578, grad_norm: 0.450232
[[34m2025-10-04 12:16:56[0m] Step: 3785, Training Logs: loss_final: 0.921230, loss_mean: 0.876076, loss_mean_cls: 0.045154, grad_norm: 0.462987
[[34m2025-10-04 12:16:56[0m] Step: 3786, Training Logs: loss_final: 0.895029, loss_mean: 0.848598, loss_mean_cls: 0.046431, grad_norm: 0.476747
[[34m2025-10-04 12:16:56[0m] Step: 3787, Training Logs: loss_final: 0.894433, loss_mean: 0.849077, loss_mean_cls: 0.045356, grad_norm: 0.329519
[[34m2025-10-04 12:16:57[0m] Step: 3788, Training Logs: loss_final: 0.901002, loss_mean: 0.855540, loss_mean_cls: 0.045462, grad_norm: 0.406009
[[34m2025-10-04 12:16:57[0m] Step: 3789, Training Logs: loss_final: 0.893895, loss_mean: 0.847508, loss_mean_cls: 0.046387, grad_norm: 0.627689
[[34m2025-10-04 12:16:57[0m] Step: 3790, Training Logs: loss_final: 0.890470, loss_mean: 0.844300, loss_mean_cls: 0.046170, grad_norm: 0.380752
[[34m2025-10-04 12:16:58[0m] Step: 3791, Training Logs: loss_final: 0.874703, loss_mean: 0.828491, loss_mean_cls: 0.046212, grad_norm: 0.536065
[[34m2025-10-04 12:16:58[0m] Step: 3792, Training Logs: loss_final: 0.920743, loss_mean: 0.875221, loss_mean_cls: 0.045521, grad_norm: 0.450891
[[34m2025-10-04 12:16:58[0m] Step: 3793, Training Logs: loss_final: 0.911200, loss_mean: 0.866014, loss_mean_cls: 0.045185, grad_norm: 0.462096
[[34m2025-10-04 12:16:58[0m] Step: 3794, Training Logs: loss_final: 0.898465, loss_mean: 0.853221, loss_mean_cls: 0.045243, grad_norm: 0.432361
[[34m2025-10-04 12:16:59[0m] Step: 3795, Training Logs: loss_final: 0.913021, loss_mean: 0.867563, loss_mean_cls: 0.045458, grad_norm: 0.433862
[[34m2025-10-04 12:16:59[0m] Step: 3796, Training Logs: loss_final: 0.887318, loss_mean: 0.842272, loss_mean_cls: 0.045046, grad_norm: 0.453848
[[34m2025-10-04 12:16:59[0m] Step: 3797, Training Logs: loss_final: 0.900111, loss_mean: 0.854372, loss_mean_cls: 0.045739, grad_norm: 0.367284
[[34m2025-10-04 12:17:00[0m] Step: 3798, Training Logs: loss_final: 0.896397, loss_mean: 0.851215, loss_mean_cls: 0.045182, grad_norm: 0.299008
[[34m2025-10-04 12:17:00[0m] Step: 3799, Training Logs: loss_final: 0.914244, loss_mean: 0.870324, loss_mean_cls: 0.043920, grad_norm: 0.487349
[[34m2025-10-04 12:17:00[0m] Step: 3800, Training Logs: loss_final: 0.888699, loss_mean: 0.841571, loss_mean_cls: 0.047128, grad_norm: 0.381789
[[34m2025-10-04 12:17:01[0m] Step: 3801, Training Logs: loss_final: 0.895641, loss_mean: 0.850179, loss_mean_cls: 0.045462, grad_norm: 0.264312
[[34m2025-10-04 12:17:01[0m] Step: 3802, Training Logs: loss_final: 0.876493, loss_mean: 0.829616, loss_mean_cls: 0.046877, grad_norm: 0.357988
[[34m2025-10-04 12:17:01[0m] Step: 3803, Training Logs: loss_final: 0.903826, loss_mean: 0.857808, loss_mean_cls: 0.046018, grad_norm: 0.330044
[[34m2025-10-04 12:17:01[0m] Step: 3804, Training Logs: loss_final: 0.896928, loss_mean: 0.850316, loss_mean_cls: 0.046612, grad_norm: 0.428504
[[34m2025-10-04 12:17:02[0m] Step: 3805, Training Logs: loss_final: 0.890331, loss_mean: 0.843736, loss_mean_cls: 0.046596, grad_norm: 0.374808
[[34m2025-10-04 12:17:02[0m] Step: 3806, Training Logs: loss_final: 0.897191, loss_mean: 0.852189, loss_mean_cls: 0.045003, grad_norm: 0.296460
[[34m2025-10-04 12:17:02[0m] Step: 3807, Training Logs: loss_final: 0.901937, loss_mean: 0.856883, loss_mean_cls: 0.045054, grad_norm: 0.422169
[[34m2025-10-04 12:17:03[0m] Step: 3808, Training Logs: loss_final: 0.887526, loss_mean: 0.840636, loss_mean_cls: 0.046890, grad_norm: 0.418289
[[34m2025-10-04 12:17:03[0m] Step: 3809, Training Logs: loss_final: 0.897558, loss_mean: 0.852616, loss_mean_cls: 0.044942, grad_norm: 0.479989
[[34m2025-10-04 12:17:03[0m] Step: 3810, Training Logs: loss_final: 0.914538, loss_mean: 0.869950, loss_mean_cls: 0.044588, grad_norm: 0.391426
[[34m2025-10-04 12:17:03[0m] Step: 3811, Training Logs: loss_final: 0.896786, loss_mean: 0.851132, loss_mean_cls: 0.045654, grad_norm: 0.385330
[[34m2025-10-04 12:17:04[0m] Step: 3812, Training Logs: loss_final: 0.898754, loss_mean: 0.853502, loss_mean_cls: 0.045252, grad_norm: 0.420129
[[34m2025-10-04 12:17:04[0m] Step: 3813, Training Logs: loss_final: 0.899087, loss_mean: 0.852975, loss_mean_cls: 0.046112, grad_norm: 0.462228
[[34m2025-10-04 12:17:04[0m] Step: 3814, Training Logs: loss_final: 0.902116, loss_mean: 0.857349, loss_mean_cls: 0.044767, grad_norm: 0.440978
[[34m2025-10-04 12:17:05[0m] Step: 3815, Training Logs: loss_final: 0.871407, loss_mean: 0.825076, loss_mean_cls: 0.046330, grad_norm: 0.439377
[[34m2025-10-04 12:17:05[0m] Step: 3816, Training Logs: loss_final: 0.907521, loss_mean: 0.862099, loss_mean_cls: 0.045423, grad_norm: 0.495214
[[34m2025-10-04 12:17:05[0m] Step: 3817, Training Logs: loss_final: 0.897238, loss_mean: 0.850683, loss_mean_cls: 0.046554, grad_norm: 0.458957
[[34m2025-10-04 12:17:06[0m] Step: 3818, Training Logs: loss_final: 0.908711, loss_mean: 0.863395, loss_mean_cls: 0.045316, grad_norm: 0.447136
[[34m2025-10-04 12:17:06[0m] Step: 3819, Training Logs: loss_final: 0.907852, loss_mean: 0.861944, loss_mean_cls: 0.045908, grad_norm: 0.608578
[[34m2025-10-04 12:17:06[0m] Step: 3820, Training Logs: loss_final: 0.870696, loss_mean: 0.825270, loss_mean_cls: 0.045426, grad_norm: 0.465649
[[34m2025-10-04 12:17:06[0m] Step: 3821, Training Logs: loss_final: 0.891403, loss_mean: 0.846115, loss_mean_cls: 0.045288, grad_norm: 0.421519
[[34m2025-10-04 12:17:07[0m] Step: 3822, Training Logs: loss_final: 0.907737, loss_mean: 0.861413, loss_mean_cls: 0.046324, grad_norm: 0.417061
[[34m2025-10-04 12:17:07[0m] Step: 3823, Training Logs: loss_final: 0.899291, loss_mean: 0.853495, loss_mean_cls: 0.045796, grad_norm: 0.434065
[[34m2025-10-04 12:17:07[0m] Step: 3824, Training Logs: loss_final: 0.890093, loss_mean: 0.844957, loss_mean_cls: 0.045136, grad_norm: 0.562313
[[34m2025-10-04 12:17:08[0m] Step: 3825, Training Logs: loss_final: 0.885528, loss_mean: 0.838346, loss_mean_cls: 0.047181, grad_norm: 0.365586
[[34m2025-10-04 12:17:08[0m] Step: 3826, Training Logs: loss_final: 0.880541, loss_mean: 0.834768, loss_mean_cls: 0.045773, grad_norm: 0.445638
[[34m2025-10-04 12:17:08[0m] Step: 3827, Training Logs: loss_final: 0.903217, loss_mean: 0.858469, loss_mean_cls: 0.044748, grad_norm: 0.373209
[[34m2025-10-04 12:17:09[0m] Step: 3828, Training Logs: loss_final: 0.885370, loss_mean: 0.839945, loss_mean_cls: 0.045425, grad_norm: 0.299144
[[34m2025-10-04 12:17:09[0m] Step: 3829, Training Logs: loss_final: 0.897772, loss_mean: 0.852844, loss_mean_cls: 0.044928, grad_norm: 0.375426
[[34m2025-10-04 12:17:09[0m] Step: 3830, Training Logs: loss_final: 0.895729, loss_mean: 0.850055, loss_mean_cls: 0.045675, grad_norm: 0.411768
[[34m2025-10-04 12:17:09[0m] Step: 3831, Training Logs: loss_final: 0.900204, loss_mean: 0.853635, loss_mean_cls: 0.046568, grad_norm: 0.379187
[[34m2025-10-04 12:17:10[0m] Step: 3832, Training Logs: loss_final: 0.881358, loss_mean: 0.835789, loss_mean_cls: 0.045570, grad_norm: 0.312721
[[34m2025-10-04 12:17:10[0m] Step: 3833, Training Logs: loss_final: 0.894818, loss_mean: 0.847912, loss_mean_cls: 0.046905, grad_norm: 0.383706
[[34m2025-10-04 12:17:10[0m] Step: 3834, Training Logs: loss_final: 0.885440, loss_mean: 0.837947, loss_mean_cls: 0.047493, grad_norm: 0.564351
[[34m2025-10-04 12:17:11[0m] Step: 3835, Training Logs: loss_final: 0.901857, loss_mean: 0.856538, loss_mean_cls: 0.045319, grad_norm: 0.426903
[[34m2025-10-04 12:17:11[0m] Step: 3836, Training Logs: loss_final: 0.899631, loss_mean: 0.854004, loss_mean_cls: 0.045628, grad_norm: 0.429982
[[34m2025-10-04 12:17:11[0m] Step: 3837, Training Logs: loss_final: 0.899289, loss_mean: 0.853837, loss_mean_cls: 0.045453, grad_norm: 0.574058
[[34m2025-10-04 12:17:12[0m] Step: 3838, Training Logs: loss_final: 0.894934, loss_mean: 0.848697, loss_mean_cls: 0.046237, grad_norm: 0.387426
[[34m2025-10-04 12:17:12[0m] Step: 3839, Training Logs: loss_final: 0.903566, loss_mean: 0.858523, loss_mean_cls: 0.045044, grad_norm: 0.559834
[[34m2025-10-04 12:17:12[0m] Step: 3840, Training Logs: loss_final: 0.907900, loss_mean: 0.863313, loss_mean_cls: 0.044587, grad_norm: 0.454614
[[34m2025-10-04 12:17:12[0m] Step: 3841, Training Logs: loss_final: 0.915877, loss_mean: 0.870267, loss_mean_cls: 0.045609, grad_norm: 0.341527
[[34m2025-10-04 12:17:13[0m] Step: 3842, Training Logs: loss_final: 0.896151, loss_mean: 0.849456, loss_mean_cls: 0.046695, grad_norm: 0.350079
[[34m2025-10-04 12:17:13[0m] Step: 3843, Training Logs: loss_final: 0.903648, loss_mean: 0.857740, loss_mean_cls: 0.045908, grad_norm: 0.361877
[[34m2025-10-04 12:17:13[0m] Step: 3844, Training Logs: loss_final: 0.899046, loss_mean: 0.853906, loss_mean_cls: 0.045139, grad_norm: 0.359006
[[34m2025-10-04 12:17:14[0m] Step: 3845, Training Logs: loss_final: 0.909888, loss_mean: 0.864783, loss_mean_cls: 0.045105, grad_norm: 0.402197
[[34m2025-10-04 12:17:14[0m] Step: 3846, Training Logs: loss_final: 0.908950, loss_mean: 0.863674, loss_mean_cls: 0.045276, grad_norm: 0.443587
[[34m2025-10-04 12:17:14[0m] Step: 3847, Training Logs: loss_final: 0.874883, loss_mean: 0.829251, loss_mean_cls: 0.045632, grad_norm: 0.376781
[[34m2025-10-04 12:17:14[0m] Step: 3848, Training Logs: loss_final: 0.901197, loss_mean: 0.854691, loss_mean_cls: 0.046506, grad_norm: 0.523931
[[34m2025-10-04 12:17:15[0m] Step: 3849, Training Logs: loss_final: 0.909570, loss_mean: 0.864396, loss_mean_cls: 0.045174, grad_norm: 0.409601
[[34m2025-10-04 12:17:15[0m] Step: 3850, Training Logs: loss_final: 0.923524, loss_mean: 0.878679, loss_mean_cls: 0.044845, grad_norm: 0.497427
[[34m2025-10-04 12:17:15[0m] Step: 3851, Training Logs: loss_final: 0.889526, loss_mean: 0.844302, loss_mean_cls: 0.045224, grad_norm: 0.469843
[[34m2025-10-04 12:17:16[0m] Step: 3852, Training Logs: loss_final: 0.904237, loss_mean: 0.859161, loss_mean_cls: 0.045076, grad_norm: 0.529619
[[34m2025-10-04 12:17:16[0m] Step: 3853, Training Logs: loss_final: 0.904880, loss_mean: 0.858546, loss_mean_cls: 0.046333, grad_norm: 0.491304
[[34m2025-10-04 12:17:16[0m] Step: 3854, Training Logs: loss_final: 0.876561, loss_mean: 0.830034, loss_mean_cls: 0.046527, grad_norm: 0.531272
[[34m2025-10-04 12:17:17[0m] Step: 3855, Training Logs: loss_final: 0.888586, loss_mean: 0.842734, loss_mean_cls: 0.045852, grad_norm: 0.384814
[[34m2025-10-04 12:17:17[0m] Step: 3856, Training Logs: loss_final: 0.897118, loss_mean: 0.851427, loss_mean_cls: 0.045691, grad_norm: 0.516838
[[34m2025-10-04 12:17:17[0m] Step: 3857, Training Logs: loss_final: 0.889230, loss_mean: 0.843801, loss_mean_cls: 0.045429, grad_norm: 0.473267
[[34m2025-10-04 12:17:17[0m] Step: 3858, Training Logs: loss_final: 0.901524, loss_mean: 0.856361, loss_mean_cls: 0.045162, grad_norm: 0.358799
[[34m2025-10-04 12:17:18[0m] Step: 3859, Training Logs: loss_final: 0.885955, loss_mean: 0.840251, loss_mean_cls: 0.045704, grad_norm: 0.441914
[[34m2025-10-04 12:17:18[0m] Step: 3860, Training Logs: loss_final: 0.903685, loss_mean: 0.857540, loss_mean_cls: 0.046144, grad_norm: 0.419913
[[34m2025-10-04 12:17:18[0m] Step: 3861, Training Logs: loss_final: 0.904487, loss_mean: 0.858598, loss_mean_cls: 0.045889, grad_norm: 0.297453
[[34m2025-10-04 12:17:19[0m] Step: 3862, Training Logs: loss_final: 0.889289, loss_mean: 0.843620, loss_mean_cls: 0.045669, grad_norm: 0.499932
[[34m2025-10-04 12:17:19[0m] Step: 3863, Training Logs: loss_final: 0.896381, loss_mean: 0.850600, loss_mean_cls: 0.045781, grad_norm: 0.388855
[[34m2025-10-04 12:17:19[0m] Step: 3864, Training Logs: loss_final: 0.891578, loss_mean: 0.845575, loss_mean_cls: 0.046003, grad_norm: 0.313417
[[34m2025-10-04 12:17:19[0m] Step: 3865, Training Logs: loss_final: 0.922981, loss_mean: 0.877552, loss_mean_cls: 0.045429, grad_norm: 0.527235
[[34m2025-10-04 12:17:20[0m] Step: 3866, Training Logs: loss_final: 0.895298, loss_mean: 0.850902, loss_mean_cls: 0.044397, grad_norm: 0.406350
[[34m2025-10-04 12:17:20[0m] Step: 3867, Training Logs: loss_final: 0.907152, loss_mean: 0.861922, loss_mean_cls: 0.045230, grad_norm: 0.455442
[[34m2025-10-04 12:17:20[0m] Step: 3868, Training Logs: loss_final: 0.882115, loss_mean: 0.835325, loss_mean_cls: 0.046790, grad_norm: 0.369045
[[34m2025-10-04 12:17:21[0m] Step: 3869, Training Logs: loss_final: 0.903686, loss_mean: 0.857645, loss_mean_cls: 0.046041, grad_norm: 0.466344
[[34m2025-10-04 12:17:21[0m] Step: 3870, Training Logs: loss_final: 0.877259, loss_mean: 0.830143, loss_mean_cls: 0.047116, grad_norm: 0.320289
[[34m2025-10-04 12:17:21[0m] Step: 3871, Training Logs: loss_final: 0.904902, loss_mean: 0.859175, loss_mean_cls: 0.045727, grad_norm: 0.374562
[[34m2025-10-04 12:17:22[0m] Step: 3872, Training Logs: loss_final: 0.898451, loss_mean: 0.852981, loss_mean_cls: 0.045471, grad_norm: 0.368864
[[34m2025-10-04 12:17:22[0m] Step: 3873, Training Logs: loss_final: 0.902970, loss_mean: 0.858423, loss_mean_cls: 0.044547, grad_norm: 0.335212
[[34m2025-10-04 12:17:22[0m] Step: 3874, Training Logs: loss_final: 0.889290, loss_mean: 0.843648, loss_mean_cls: 0.045642, grad_norm: 0.269701
[[34m2025-10-04 12:17:22[0m] Step: 3875, Training Logs: loss_final: 0.900224, loss_mean: 0.854832, loss_mean_cls: 0.045392, grad_norm: 0.416500
[[34m2025-10-04 12:17:23[0m] Step: 3876, Training Logs: loss_final: 0.908031, loss_mean: 0.862031, loss_mean_cls: 0.046000, grad_norm: 0.396109
[[34m2025-10-04 12:17:23[0m] Step: 3877, Training Logs: loss_final: 0.875695, loss_mean: 0.829629, loss_mean_cls: 0.046065, grad_norm: 0.268006
[[34m2025-10-04 12:17:23[0m] Step: 3878, Training Logs: loss_final: 0.885343, loss_mean: 0.839283, loss_mean_cls: 0.046060, grad_norm: 0.344819
[[34m2025-10-04 12:17:24[0m] Step: 3879, Training Logs: loss_final: 0.876597, loss_mean: 0.831399, loss_mean_cls: 0.045198, grad_norm: 0.299373
[[34m2025-10-04 12:17:24[0m] Step: 3880, Training Logs: loss_final: 0.903666, loss_mean: 0.858585, loss_mean_cls: 0.045081, grad_norm: 0.422746
[[34m2025-10-04 12:17:24[0m] Step: 3881, Training Logs: loss_final: 0.879529, loss_mean: 0.833696, loss_mean_cls: 0.045833, grad_norm: 0.316156
[[34m2025-10-04 12:17:25[0m] Step: 3882, Training Logs: loss_final: 0.880662, loss_mean: 0.834992, loss_mean_cls: 0.045670, grad_norm: 0.383865
[[34m2025-10-04 12:17:25[0m] Step: 3883, Training Logs: loss_final: 0.901346, loss_mean: 0.855463, loss_mean_cls: 0.045883, grad_norm: 0.490793
[[34m2025-10-04 12:17:25[0m] Step: 3884, Training Logs: loss_final: 0.886871, loss_mean: 0.841448, loss_mean_cls: 0.045423, grad_norm: 0.318460
[[34m2025-10-04 12:17:25[0m] Step: 3885, Training Logs: loss_final: 0.893438, loss_mean: 0.847798, loss_mean_cls: 0.045641, grad_norm: 0.381092
[[34m2025-10-04 12:17:26[0m] Step: 3886, Training Logs: loss_final: 0.905760, loss_mean: 0.861774, loss_mean_cls: 0.043986, grad_norm: 0.415271
[[34m2025-10-04 12:17:26[0m] Step: 3887, Training Logs: loss_final: 0.931790, loss_mean: 0.886152, loss_mean_cls: 0.045638, grad_norm: 0.382011
[[34m2025-10-04 12:17:26[0m] Step: 3888, Training Logs: loss_final: 0.895856, loss_mean: 0.850880, loss_mean_cls: 0.044975, grad_norm: 0.533936
[[34m2025-10-04 12:17:27[0m] Step: 3889, Training Logs: loss_final: 0.877905, loss_mean: 0.831550, loss_mean_cls: 0.046355, grad_norm: 0.423424
[[34m2025-10-04 12:17:27[0m] Step: 3890, Training Logs: loss_final: 0.915248, loss_mean: 0.870682, loss_mean_cls: 0.044565, grad_norm: 0.643673
[[34m2025-10-04 12:17:27[0m] Step: 3891, Training Logs: loss_final: 0.914207, loss_mean: 0.869451, loss_mean_cls: 0.044756, grad_norm: 0.359422
[[34m2025-10-04 12:17:28[0m] Step: 3892, Training Logs: loss_final: 0.897730, loss_mean: 0.852101, loss_mean_cls: 0.045629, grad_norm: 0.461087
[[34m2025-10-04 12:17:28[0m] Step: 3893, Training Logs: loss_final: 0.898221, loss_mean: 0.852117, loss_mean_cls: 0.046104, grad_norm: 0.634843
[[34m2025-10-04 12:17:28[0m] Step: 3894, Training Logs: loss_final: 0.900041, loss_mean: 0.854846, loss_mean_cls: 0.045194, grad_norm: 0.446615
[[34m2025-10-04 12:17:28[0m] Step: 3895, Training Logs: loss_final: 0.910707, loss_mean: 0.865331, loss_mean_cls: 0.045376, grad_norm: 0.721656
[[34m2025-10-04 12:17:29[0m] Step: 3896, Training Logs: loss_final: 0.905790, loss_mean: 0.860633, loss_mean_cls: 0.045157, grad_norm: 0.593280
[[34m2025-10-04 12:17:29[0m] Step: 3897, Training Logs: loss_final: 0.875540, loss_mean: 0.829396, loss_mean_cls: 0.046144, grad_norm: 0.349182
[[34m2025-10-04 12:17:29[0m] Step: 3898, Training Logs: loss_final: 0.905365, loss_mean: 0.860059, loss_mean_cls: 0.045306, grad_norm: 0.571621
[[34m2025-10-04 12:17:30[0m] Step: 3899, Training Logs: loss_final: 0.907349, loss_mean: 0.861549, loss_mean_cls: 0.045800, grad_norm: 0.653780
[[34m2025-10-04 12:17:30[0m] Step: 3900, Training Logs: loss_final: 0.882213, loss_mean: 0.836564, loss_mean_cls: 0.045649, grad_norm: 0.296638
[[34m2025-10-04 12:17:30[0m] Step: 3901, Training Logs: loss_final: 0.887970, loss_mean: 0.842818, loss_mean_cls: 0.045153, grad_norm: 0.433779
[[34m2025-10-04 12:17:30[0m] Step: 3902, Training Logs: loss_final: 0.905645, loss_mean: 0.860132, loss_mean_cls: 0.045513, grad_norm: 0.440793
[[34m2025-10-04 12:17:31[0m] Step: 3903, Training Logs: loss_final: 0.921920, loss_mean: 0.877029, loss_mean_cls: 0.044891, grad_norm: 0.338828
[[34m2025-10-04 12:17:31[0m] Step: 3904, Training Logs: loss_final: 0.903214, loss_mean: 0.856972, loss_mean_cls: 0.046242, grad_norm: 0.319861
[[34m2025-10-04 12:17:31[0m] Step: 3905, Training Logs: loss_final: 0.910610, loss_mean: 0.864306, loss_mean_cls: 0.046304, grad_norm: 0.446909
[[34m2025-10-04 12:17:32[0m] Step: 3906, Training Logs: loss_final: 0.891774, loss_mean: 0.845398, loss_mean_cls: 0.046377, grad_norm: 0.519429
[[34m2025-10-04 12:17:32[0m] Step: 3907, Training Logs: loss_final: 0.893615, loss_mean: 0.846742, loss_mean_cls: 0.046874, grad_norm: 0.441522
[[34m2025-10-04 12:17:32[0m] Step: 3908, Training Logs: loss_final: 0.893533, loss_mean: 0.847501, loss_mean_cls: 0.046032, grad_norm: 0.543545
[[34m2025-10-04 12:17:33[0m] Step: 3909, Training Logs: loss_final: 0.891053, loss_mean: 0.845172, loss_mean_cls: 0.045880, grad_norm: 0.519505
[[34m2025-10-04 12:17:33[0m] Step: 3910, Training Logs: loss_final: 0.892809, loss_mean: 0.846480, loss_mean_cls: 0.046330, grad_norm: 0.491778
[[34m2025-10-04 12:17:33[0m] Step: 3911, Training Logs: loss_final: 0.915526, loss_mean: 0.869545, loss_mean_cls: 0.045981, grad_norm: 0.766888
[[34m2025-10-04 12:17:33[0m] Step: 3912, Training Logs: loss_final: 0.901531, loss_mean: 0.855466, loss_mean_cls: 0.046065, grad_norm: 0.472784
[[34m2025-10-04 12:17:34[0m] Step: 3913, Training Logs: loss_final: 0.894614, loss_mean: 0.850393, loss_mean_cls: 0.044221, grad_norm: 0.479224
[[34m2025-10-04 12:17:34[0m] Step: 3914, Training Logs: loss_final: 0.907670, loss_mean: 0.861071, loss_mean_cls: 0.046599, grad_norm: 0.656216
[[34m2025-10-04 12:17:34[0m] Step: 3915, Training Logs: loss_final: 0.904868, loss_mean: 0.859970, loss_mean_cls: 0.044898, grad_norm: 0.359077
[[34m2025-10-04 12:17:35[0m] Step: 3916, Training Logs: loss_final: 0.889221, loss_mean: 0.843440, loss_mean_cls: 0.045781, grad_norm: 0.402070
[[34m2025-10-04 12:17:35[0m] Step: 3917, Training Logs: loss_final: 0.899857, loss_mean: 0.853420, loss_mean_cls: 0.046436, grad_norm: 0.527606
[[34m2025-10-04 12:17:35[0m] Step: 3918, Training Logs: loss_final: 0.922358, loss_mean: 0.876144, loss_mean_cls: 0.046214, grad_norm: 0.400304
[[34m2025-10-04 12:17:35[0m] Step: 3919, Training Logs: loss_final: 0.887213, loss_mean: 0.840199, loss_mean_cls: 0.047014, grad_norm: 0.465162
[[34m2025-10-04 12:17:36[0m] Step: 3920, Training Logs: loss_final: 0.906054, loss_mean: 0.859705, loss_mean_cls: 0.046349, grad_norm: 0.377499
[[34m2025-10-04 12:17:36[0m] Step: 3921, Training Logs: loss_final: 0.880971, loss_mean: 0.833815, loss_mean_cls: 0.047156, grad_norm: 0.328630
[[34m2025-10-04 12:17:36[0m] Step: 3922, Training Logs: loss_final: 0.890079, loss_mean: 0.842562, loss_mean_cls: 0.047517, grad_norm: 0.551745
[[34m2025-10-04 12:17:37[0m] Step: 3923, Training Logs: loss_final: 0.896886, loss_mean: 0.852422, loss_mean_cls: 0.044463, grad_norm: 0.389598
[[34m2025-10-04 12:17:37[0m] Step: 3924, Training Logs: loss_final: 0.900954, loss_mean: 0.854160, loss_mean_cls: 0.046795, grad_norm: 0.528271
[[34m2025-10-04 12:17:37[0m] Step: 3925, Training Logs: loss_final: 0.891879, loss_mean: 0.845950, loss_mean_cls: 0.045929, grad_norm: 0.404635
[[34m2025-10-04 12:17:38[0m] Step: 3926, Training Logs: loss_final: 0.899016, loss_mean: 0.852808, loss_mean_cls: 0.046208, grad_norm: 0.461272
[[34m2025-10-04 12:17:38[0m] Step: 3927, Training Logs: loss_final: 0.860662, loss_mean: 0.814127, loss_mean_cls: 0.046535, grad_norm: 0.407991
[[34m2025-10-04 12:17:38[0m] Step: 3928, Training Logs: loss_final: 0.887515, loss_mean: 0.842166, loss_mean_cls: 0.045349, grad_norm: 0.443779
[[34m2025-10-04 12:17:38[0m] Step: 3929, Training Logs: loss_final: 0.898141, loss_mean: 0.853870, loss_mean_cls: 0.044271, grad_norm: 0.410028
[[34m2025-10-04 12:17:39[0m] Step: 3930, Training Logs: loss_final: 0.878566, loss_mean: 0.832149, loss_mean_cls: 0.046417, grad_norm: 0.456392
[[34m2025-10-04 12:17:39[0m] Step: 3931, Training Logs: loss_final: 0.891169, loss_mean: 0.845186, loss_mean_cls: 0.045983, grad_norm: 0.391395
[[34m2025-10-04 12:17:39[0m] Step: 3932, Training Logs: loss_final: 0.922695, loss_mean: 0.876951, loss_mean_cls: 0.045744, grad_norm: 0.363501
[[34m2025-10-04 12:17:40[0m] Step: 3933, Training Logs: loss_final: 0.897110, loss_mean: 0.851407, loss_mean_cls: 0.045703, grad_norm: 0.418984
[[34m2025-10-04 12:17:40[0m] Step: 3934, Training Logs: loss_final: 0.900113, loss_mean: 0.855567, loss_mean_cls: 0.044546, grad_norm: 0.382354
[[34m2025-10-04 12:17:40[0m] Step: 3935, Training Logs: loss_final: 0.890900, loss_mean: 0.844533, loss_mean_cls: 0.046367, grad_norm: 0.429405
[[34m2025-10-04 12:17:40[0m] Step: 3936, Training Logs: loss_final: 0.899344, loss_mean: 0.854412, loss_mean_cls: 0.044932, grad_norm: 0.412946
[[34m2025-10-04 12:17:41[0m] Step: 3937, Training Logs: loss_final: 0.889676, loss_mean: 0.843074, loss_mean_cls: 0.046602, grad_norm: 0.441062
[[34m2025-10-04 12:17:41[0m] Step: 3938, Training Logs: loss_final: 0.906918, loss_mean: 0.860828, loss_mean_cls: 0.046090, grad_norm: 0.399255
[[34m2025-10-04 12:17:41[0m] Step: 3939, Training Logs: loss_final: 0.905809, loss_mean: 0.859888, loss_mean_cls: 0.045921, grad_norm: 0.411752
[[34m2025-10-04 12:17:42[0m] Step: 3940, Training Logs: loss_final: 0.894095, loss_mean: 0.848651, loss_mean_cls: 0.045444, grad_norm: 0.403624
[[34m2025-10-04 12:17:42[0m] Step: 3941, Training Logs: loss_final: 0.877541, loss_mean: 0.830330, loss_mean_cls: 0.047211, grad_norm: 0.452270
[[34m2025-10-04 12:17:42[0m] Step: 3942, Training Logs: loss_final: 0.918094, loss_mean: 0.873467, loss_mean_cls: 0.044627, grad_norm: 0.358739
[[34m2025-10-04 12:17:43[0m] Step: 3943, Training Logs: loss_final: 0.890006, loss_mean: 0.843208, loss_mean_cls: 0.046798, grad_norm: 0.419879
[[34m2025-10-04 12:17:43[0m] Step: 3944, Training Logs: loss_final: 0.894266, loss_mean: 0.848078, loss_mean_cls: 0.046188, grad_norm: 0.309984
[[34m2025-10-04 12:17:43[0m] Step: 3945, Training Logs: loss_final: 0.883727, loss_mean: 0.838424, loss_mean_cls: 0.045303, grad_norm: 0.312938
[[34m2025-10-04 12:17:43[0m] Step: 3946, Training Logs: loss_final: 0.886880, loss_mean: 0.841155, loss_mean_cls: 0.045725, grad_norm: 0.429762
[[34m2025-10-04 12:17:44[0m] Step: 3947, Training Logs: loss_final: 0.898787, loss_mean: 0.853063, loss_mean_cls: 0.045724, grad_norm: 0.330082
[[34m2025-10-04 12:17:44[0m] Step: 3948, Training Logs: loss_final: 0.887215, loss_mean: 0.841532, loss_mean_cls: 0.045683, grad_norm: 0.377129
[[34m2025-10-04 12:17:44[0m] Step: 3949, Training Logs: loss_final: 0.904422, loss_mean: 0.859380, loss_mean_cls: 0.045041, grad_norm: 0.298829
[[34m2025-10-04 12:17:45[0m] Step: 3950, Training Logs: loss_final: 0.901273, loss_mean: 0.854631, loss_mean_cls: 0.046641, grad_norm: 0.379749
[[34m2025-10-04 12:17:45[0m] Step: 3951, Training Logs: loss_final: 0.911338, loss_mean: 0.866421, loss_mean_cls: 0.044917, grad_norm: 0.450754
[[34m2025-10-04 12:17:45[0m] Step: 3952, Training Logs: loss_final: 0.911397, loss_mean: 0.864703, loss_mean_cls: 0.046694, grad_norm: 0.462199
[[34m2025-10-04 12:17:45[0m] Step: 3953, Training Logs: loss_final: 0.897397, loss_mean: 0.851826, loss_mean_cls: 0.045571, grad_norm: 0.301102
[[34m2025-10-04 12:17:46[0m] Step: 3954, Training Logs: loss_final: 0.891212, loss_mean: 0.845604, loss_mean_cls: 0.045609, grad_norm: 0.416486
[[34m2025-10-04 12:17:46[0m] Step: 3955, Training Logs: loss_final: 0.898341, loss_mean: 0.852608, loss_mean_cls: 0.045733, grad_norm: 0.308084
[[34m2025-10-04 12:17:46[0m] Step: 3956, Training Logs: loss_final: 0.905981, loss_mean: 0.860063, loss_mean_cls: 0.045918, grad_norm: 0.375988
[[34m2025-10-04 12:17:47[0m] Step: 3957, Training Logs: loss_final: 0.888158, loss_mean: 0.842495, loss_mean_cls: 0.045663, grad_norm: 0.365154
[[34m2025-10-04 12:17:47[0m] Step: 3958, Training Logs: loss_final: 0.899067, loss_mean: 0.853735, loss_mean_cls: 0.045332, grad_norm: 0.347762
[[34m2025-10-04 12:17:47[0m] Step: 3959, Training Logs: loss_final: 0.888844, loss_mean: 0.841651, loss_mean_cls: 0.047193, grad_norm: 0.428399
[[34m2025-10-04 12:17:48[0m] Step: 3960, Training Logs: loss_final: 0.913970, loss_mean: 0.869476, loss_mean_cls: 0.044494, grad_norm: 0.352092
[[34m2025-10-04 12:17:48[0m] Step: 3961, Training Logs: loss_final: 0.889680, loss_mean: 0.844441, loss_mean_cls: 0.045240, grad_norm: 0.417409
[[34m2025-10-04 12:17:48[0m] Step: 3962, Training Logs: loss_final: 0.914321, loss_mean: 0.870051, loss_mean_cls: 0.044270, grad_norm: 0.358731
[[34m2025-10-04 12:17:48[0m] Step: 3963, Training Logs: loss_final: 0.893590, loss_mean: 0.846580, loss_mean_cls: 0.047011, grad_norm: 0.322993
[[34m2025-10-04 12:17:49[0m] Step: 3964, Training Logs: loss_final: 0.905164, loss_mean: 0.857426, loss_mean_cls: 0.047739, grad_norm: 0.489174
[[34m2025-10-04 12:17:49[0m] Step: 3965, Training Logs: loss_final: 0.918865, loss_mean: 0.872954, loss_mean_cls: 0.045911, grad_norm: 0.354142
[[34m2025-10-04 12:17:49[0m] Step: 3966, Training Logs: loss_final: 0.917677, loss_mean: 0.871838, loss_mean_cls: 0.045839, grad_norm: 0.456876
[[34m2025-10-04 12:17:50[0m] Step: 3967, Training Logs: loss_final: 0.891997, loss_mean: 0.845372, loss_mean_cls: 0.046625, grad_norm: 0.502713
[[34m2025-10-04 12:17:50[0m] Step: 3968, Training Logs: loss_final: 0.889728, loss_mean: 0.843597, loss_mean_cls: 0.046131, grad_norm: 0.286139
[[34m2025-10-04 12:17:50[0m] Step: 3969, Training Logs: loss_final: 0.893726, loss_mean: 0.848372, loss_mean_cls: 0.045354, grad_norm: 0.557209
[[34m2025-10-04 12:17:51[0m] Step: 3970, Training Logs: loss_final: 0.909171, loss_mean: 0.863632, loss_mean_cls: 0.045538, grad_norm: 0.325088
[[34m2025-10-04 12:17:51[0m] Step: 3971, Training Logs: loss_final: 0.912199, loss_mean: 0.866111, loss_mean_cls: 0.046088, grad_norm: 0.548967
[[34m2025-10-04 12:17:51[0m] Step: 3972, Training Logs: loss_final: 0.902228, loss_mean: 0.855828, loss_mean_cls: 0.046400, grad_norm: 0.476987
[[34m2025-10-04 12:17:51[0m] Step: 3973, Training Logs: loss_final: 0.889974, loss_mean: 0.844213, loss_mean_cls: 0.045761, grad_norm: 0.357746
[[34m2025-10-04 12:17:52[0m] Step: 3974, Training Logs: loss_final: 0.908002, loss_mean: 0.863135, loss_mean_cls: 0.044867, grad_norm: 0.437750
[[34m2025-10-04 12:17:52[0m] Step: 3975, Training Logs: loss_final: 0.888542, loss_mean: 0.843095, loss_mean_cls: 0.045447, grad_norm: 0.490268
[[34m2025-10-04 12:17:52[0m] Step: 3976, Training Logs: loss_final: 0.894088, loss_mean: 0.848705, loss_mean_cls: 0.045383, grad_norm: 0.361305
[[34m2025-10-04 12:17:53[0m] Step: 3977, Training Logs: loss_final: 0.893804, loss_mean: 0.848375, loss_mean_cls: 0.045429, grad_norm: 0.512216
[[34m2025-10-04 12:17:53[0m] Step: 3978, Training Logs: loss_final: 0.899771, loss_mean: 0.854165, loss_mean_cls: 0.045606, grad_norm: 0.407439
[[34m2025-10-04 12:17:53[0m] Step: 3979, Training Logs: loss_final: 0.910028, loss_mean: 0.863600, loss_mean_cls: 0.046428, grad_norm: 0.438076
[[34m2025-10-04 12:17:54[0m] Step: 3980, Training Logs: loss_final: 0.894743, loss_mean: 0.848988, loss_mean_cls: 0.045755, grad_norm: 0.444178
[[34m2025-10-04 12:17:54[0m] Step: 3981, Training Logs: loss_final: 0.886610, loss_mean: 0.840733, loss_mean_cls: 0.045877, grad_norm: 0.367050
[[34m2025-10-04 12:17:54[0m] Step: 3982, Training Logs: loss_final: 0.887816, loss_mean: 0.841402, loss_mean_cls: 0.046413, grad_norm: 0.350414
[[34m2025-10-04 12:17:54[0m] Step: 3983, Training Logs: loss_final: 0.886048, loss_mean: 0.840371, loss_mean_cls: 0.045677, grad_norm: 0.412492
[[34m2025-10-04 12:17:55[0m] Step: 3984, Training Logs: loss_final: 0.910865, loss_mean: 0.866003, loss_mean_cls: 0.044862, grad_norm: 0.295389
[[34m2025-10-04 12:17:55[0m] Step: 3985, Training Logs: loss_final: 0.896310, loss_mean: 0.850531, loss_mean_cls: 0.045779, grad_norm: 0.286086
[[34m2025-10-04 12:17:55[0m] Step: 3986, Training Logs: loss_final: 0.910243, loss_mean: 0.864912, loss_mean_cls: 0.045331, grad_norm: 0.356874
[[34m2025-10-04 12:17:56[0m] Step: 3987, Training Logs: loss_final: 0.893890, loss_mean: 0.847677, loss_mean_cls: 0.046213, grad_norm: 0.386551
[[34m2025-10-04 12:17:56[0m] Step: 3988, Training Logs: loss_final: 0.886395, loss_mean: 0.840509, loss_mean_cls: 0.045886, grad_norm: 0.310997
[[34m2025-10-04 12:17:56[0m] Step: 3989, Training Logs: loss_final: 0.896231, loss_mean: 0.851668, loss_mean_cls: 0.044563, grad_norm: 0.396237
[[34m2025-10-04 12:17:56[0m] Step: 3990, Training Logs: loss_final: 0.880847, loss_mean: 0.835081, loss_mean_cls: 0.045765, grad_norm: 0.343396
[[34m2025-10-04 12:17:57[0m] Step: 3991, Training Logs: loss_final: 0.883320, loss_mean: 0.836834, loss_mean_cls: 0.046486, grad_norm: 0.346569
[[34m2025-10-04 12:17:57[0m] Step: 3992, Training Logs: loss_final: 0.896604, loss_mean: 0.850434, loss_mean_cls: 0.046170, grad_norm: 0.385562
[[34m2025-10-04 12:17:57[0m] Step: 3993, Training Logs: loss_final: 0.898694, loss_mean: 0.853740, loss_mean_cls: 0.044954, grad_norm: 0.434723
[[34m2025-10-04 12:17:58[0m] Step: 3994, Training Logs: loss_final: 0.917220, loss_mean: 0.872939, loss_mean_cls: 0.044281, grad_norm: 0.438187
[[34m2025-10-04 12:17:58[0m] Step: 3995, Training Logs: loss_final: 0.898746, loss_mean: 0.852875, loss_mean_cls: 0.045871, grad_norm: 0.433736
[[34m2025-10-04 12:17:58[0m] Step: 3996, Training Logs: loss_final: 0.896288, loss_mean: 0.848854, loss_mean_cls: 0.047434, grad_norm: 0.401250
[[34m2025-10-04 12:17:59[0m] Step: 3997, Training Logs: loss_final: 0.879997, loss_mean: 0.834434, loss_mean_cls: 0.045564, grad_norm: 0.364446
[[34m2025-10-04 12:17:59[0m] Step: 3998, Training Logs: loss_final: 0.900993, loss_mean: 0.854261, loss_mean_cls: 0.046732, grad_norm: 0.452819
[[34m2025-10-04 12:17:59[0m] Step: 3999, Training Logs: loss_final: 0.872758, loss_mean: 0.826687, loss_mean_cls: 0.046070, grad_norm: 0.332869
[[34m2025-10-04 12:17:59[0m] Step: 4000, Training Logs: loss_final: 0.906015, loss_mean: 0.860786, loss_mean_cls: 0.045229, grad_norm: 0.430347
[[34m2025-10-04 12:18:00[0m] Step: 4001, Training Logs: loss_final: 0.911934, loss_mean: 0.866874, loss_mean_cls: 0.045060, grad_norm: 0.323395
[[34m2025-10-04 12:18:00[0m] Step: 4002, Training Logs: loss_final: 0.905167, loss_mean: 0.859855, loss_mean_cls: 0.045312, grad_norm: 0.295771
[[34m2025-10-04 12:18:00[0m] Step: 4003, Training Logs: loss_final: 0.886796, loss_mean: 0.840912, loss_mean_cls: 0.045885, grad_norm: 0.343883
[[34m2025-10-04 12:18:01[0m] Step: 4004, Training Logs: loss_final: 0.888523, loss_mean: 0.843005, loss_mean_cls: 0.045518, grad_norm: 0.283174
[[34m2025-10-04 12:18:01[0m] Step: 4005, Training Logs: loss_final: 0.881453, loss_mean: 0.834962, loss_mean_cls: 0.046490, grad_norm: 0.360099
[[34m2025-10-04 12:18:01[0m] Step: 4006, Training Logs: loss_final: 0.907697, loss_mean: 0.862515, loss_mean_cls: 0.045182, grad_norm: 0.422150
[[34m2025-10-04 12:18:01[0m] Step: 4007, Training Logs: loss_final: 0.897241, loss_mean: 0.852005, loss_mean_cls: 0.045236, grad_norm: 0.484016
[[34m2025-10-04 12:18:02[0m] Step: 4008, Training Logs: loss_final: 0.894306, loss_mean: 0.848268, loss_mean_cls: 0.046037, grad_norm: 0.470851
[[34m2025-10-04 12:18:02[0m] Step: 4009, Training Logs: loss_final: 0.900707, loss_mean: 0.854600, loss_mean_cls: 0.046107, grad_norm: 0.477282
[[34m2025-10-04 12:18:02[0m] Step: 4010, Training Logs: loss_final: 0.898641, loss_mean: 0.853813, loss_mean_cls: 0.044828, grad_norm: 0.420697
[[34m2025-10-04 12:18:03[0m] Step: 4011, Training Logs: loss_final: 0.907306, loss_mean: 0.862323, loss_mean_cls: 0.044984, grad_norm: 0.472351
[[34m2025-10-04 12:18:03[0m] Step: 4012, Training Logs: loss_final: 0.917054, loss_mean: 0.872685, loss_mean_cls: 0.044369, grad_norm: 0.394447
[[34m2025-10-04 12:18:03[0m] Step: 4013, Training Logs: loss_final: 0.912530, loss_mean: 0.867032, loss_mean_cls: 0.045498, grad_norm: 0.487264
[[34m2025-10-04 12:18:04[0m] Step: 4014, Training Logs: loss_final: 0.902095, loss_mean: 0.856648, loss_mean_cls: 0.045447, grad_norm: 0.344518
[[34m2025-10-04 12:18:04[0m] Step: 4015, Training Logs: loss_final: 0.889741, loss_mean: 0.843730, loss_mean_cls: 0.046011, grad_norm: 0.433891
[[34m2025-10-04 12:18:04[0m] Step: 4016, Training Logs: loss_final: 0.882837, loss_mean: 0.837117, loss_mean_cls: 0.045720, grad_norm: 0.450052
[[34m2025-10-04 12:18:04[0m] Step: 4017, Training Logs: loss_final: 0.906216, loss_mean: 0.859375, loss_mean_cls: 0.046841, grad_norm: 0.313328
[[34m2025-10-04 12:18:05[0m] Step: 4018, Training Logs: loss_final: 0.887004, loss_mean: 0.842222, loss_mean_cls: 0.044782, grad_norm: 0.511207
[[34m2025-10-04 12:18:05[0m] Step: 4019, Training Logs: loss_final: 0.907910, loss_mean: 0.861656, loss_mean_cls: 0.046254, grad_norm: 0.319626
[[34m2025-10-04 12:18:05[0m] Step: 4020, Training Logs: loss_final: 0.912440, loss_mean: 0.867336, loss_mean_cls: 0.045104, grad_norm: 0.520869
[[34m2025-10-04 12:18:06[0m] Step: 4021, Training Logs: loss_final: 0.906966, loss_mean: 0.861772, loss_mean_cls: 0.045194, grad_norm: 0.351335
[[34m2025-10-04 12:18:06[0m] Step: 4022, Training Logs: loss_final: 0.886175, loss_mean: 0.840793, loss_mean_cls: 0.045382, grad_norm: 0.583120
[[34m2025-10-04 12:18:06[0m] Step: 4023, Training Logs: loss_final: 0.891895, loss_mean: 0.846381, loss_mean_cls: 0.045514, grad_norm: 0.557367
[[34m2025-10-04 12:18:07[0m] Step: 4024, Training Logs: loss_final: 0.885562, loss_mean: 0.840296, loss_mean_cls: 0.045266, grad_norm: 0.443044
[[34m2025-10-04 12:18:07[0m] Step: 4025, Training Logs: loss_final: 0.889152, loss_mean: 0.843806, loss_mean_cls: 0.045345, grad_norm: 0.374760
[[34m2025-10-04 12:18:07[0m] Step: 4026, Training Logs: loss_final: 0.887311, loss_mean: 0.842822, loss_mean_cls: 0.044488, grad_norm: 0.342026
[[34m2025-10-04 12:18:07[0m] Step: 4027, Training Logs: loss_final: 0.910787, loss_mean: 0.865607, loss_mean_cls: 0.045180, grad_norm: 0.378997
[[34m2025-10-04 12:18:08[0m] Step: 4028, Training Logs: loss_final: 0.910570, loss_mean: 0.864752, loss_mean_cls: 0.045818, grad_norm: 0.400483
[[34m2025-10-04 12:18:08[0m] Step: 4029, Training Logs: loss_final: 0.920618, loss_mean: 0.874403, loss_mean_cls: 0.046215, grad_norm: 0.446618
[[34m2025-10-04 12:18:08[0m] Step: 4030, Training Logs: loss_final: 0.896413, loss_mean: 0.850654, loss_mean_cls: 0.045759, grad_norm: 0.345847
[[34m2025-10-04 12:18:09[0m] Step: 4031, Training Logs: loss_final: 0.899150, loss_mean: 0.853731, loss_mean_cls: 0.045420, grad_norm: 0.542864
[[34m2025-10-04 12:18:09[0m] Step: 4032, Training Logs: loss_final: 0.892829, loss_mean: 0.846069, loss_mean_cls: 0.046760, grad_norm: 0.447118
[[34m2025-10-04 12:18:09[0m] Step: 4033, Training Logs: loss_final: 0.917311, loss_mean: 0.872327, loss_mean_cls: 0.044984, grad_norm: 0.586637
[[34m2025-10-04 12:18:10[0m] Step: 4034, Training Logs: loss_final: 0.909615, loss_mean: 0.864286, loss_mean_cls: 0.045328, grad_norm: 0.531287
[[34m2025-10-04 12:18:10[0m] Step: 4035, Training Logs: loss_final: 0.884731, loss_mean: 0.838879, loss_mean_cls: 0.045851, grad_norm: 0.405553
[[34m2025-10-04 12:18:10[0m] Step: 4036, Training Logs: loss_final: 0.900683, loss_mean: 0.854696, loss_mean_cls: 0.045986, grad_norm: 0.357598
[[34m2025-10-04 12:18:10[0m] Step: 4037, Training Logs: loss_final: 0.885711, loss_mean: 0.840998, loss_mean_cls: 0.044714, grad_norm: 0.640027
[[34m2025-10-04 12:18:11[0m] Step: 4038, Training Logs: loss_final: 0.878863, loss_mean: 0.833257, loss_mean_cls: 0.045606, grad_norm: 0.420685
[[34m2025-10-04 12:18:11[0m] Step: 4039, Training Logs: loss_final: 0.896671, loss_mean: 0.851426, loss_mean_cls: 0.045246, grad_norm: 0.380907
[[34m2025-10-04 12:18:11[0m] Step: 4040, Training Logs: loss_final: 0.909316, loss_mean: 0.864065, loss_mean_cls: 0.045252, grad_norm: 0.410138
[[34m2025-10-04 12:18:12[0m] Step: 4041, Training Logs: loss_final: 0.874285, loss_mean: 0.828335, loss_mean_cls: 0.045950, grad_norm: 0.353932
[[34m2025-10-04 12:18:12[0m] Step: 4042, Training Logs: loss_final: 0.894753, loss_mean: 0.848672, loss_mean_cls: 0.046081, grad_norm: 0.363822
[[34m2025-10-04 12:18:12[0m] Step: 4043, Training Logs: loss_final: 0.871203, loss_mean: 0.824380, loss_mean_cls: 0.046823, grad_norm: 0.447319
[[34m2025-10-04 12:18:12[0m] Step: 4044, Training Logs: loss_final: 0.916069, loss_mean: 0.870357, loss_mean_cls: 0.045713, grad_norm: 0.360970
[[34m2025-10-04 12:18:13[0m] Step: 4045, Training Logs: loss_final: 0.893061, loss_mean: 0.846203, loss_mean_cls: 0.046858, grad_norm: 0.285973
[[34m2025-10-04 12:18:13[0m] Step: 4046, Training Logs: loss_final: 0.911825, loss_mean: 0.865103, loss_mean_cls: 0.046721, grad_norm: 0.444443
[[34m2025-10-04 12:18:13[0m] Step: 4047, Training Logs: loss_final: 0.879165, loss_mean: 0.834668, loss_mean_cls: 0.044497, grad_norm: 0.325418
[[34m2025-10-04 12:18:14[0m] Step: 4048, Training Logs: loss_final: 0.901813, loss_mean: 0.856990, loss_mean_cls: 0.044823, grad_norm: 0.474386
[[34m2025-10-04 12:18:14[0m] Step: 4049, Training Logs: loss_final: 0.903642, loss_mean: 0.857341, loss_mean_cls: 0.046301, grad_norm: 0.447098
[[34m2025-10-04 12:18:14[0m] Step: 4050, Training Logs: loss_final: 0.891443, loss_mean: 0.844946, loss_mean_cls: 0.046497, grad_norm: 0.278559
[[34m2025-10-04 12:18:15[0m] Step: 4051, Training Logs: loss_final: 0.906638, loss_mean: 0.861836, loss_mean_cls: 0.044802, grad_norm: 0.455392
[[34m2025-10-04 12:18:15[0m] Step: 4052, Training Logs: loss_final: 0.885918, loss_mean: 0.841115, loss_mean_cls: 0.044803, grad_norm: 0.294426
[[34m2025-10-04 12:18:15[0m] Step: 4053, Training Logs: loss_final: 0.891020, loss_mean: 0.845271, loss_mean_cls: 0.045749, grad_norm: 0.514715
[[34m2025-10-04 12:18:15[0m] Step: 4054, Training Logs: loss_final: 0.891792, loss_mean: 0.845837, loss_mean_cls: 0.045956, grad_norm: 0.334662
[[34m2025-10-04 12:18:16[0m] Step: 4055, Training Logs: loss_final: 0.892652, loss_mean: 0.847401, loss_mean_cls: 0.045252, grad_norm: 0.409877
[[34m2025-10-04 12:18:16[0m] Step: 4056, Training Logs: loss_final: 0.904851, loss_mean: 0.860924, loss_mean_cls: 0.043927, grad_norm: 0.349435
[[34m2025-10-04 12:18:16[0m] Step: 4057, Training Logs: loss_final: 0.904549, loss_mean: 0.860294, loss_mean_cls: 0.044254, grad_norm: 0.435182
[[34m2025-10-04 12:18:17[0m] Step: 4058, Training Logs: loss_final: 0.884142, loss_mean: 0.839365, loss_mean_cls: 0.044777, grad_norm: 0.393156
[[34m2025-10-04 12:18:17[0m] Step: 4059, Training Logs: loss_final: 0.902033, loss_mean: 0.856209, loss_mean_cls: 0.045824, grad_norm: 0.389737
[[34m2025-10-04 12:18:17[0m] Step: 4060, Training Logs: loss_final: 0.889488, loss_mean: 0.843311, loss_mean_cls: 0.046177, grad_norm: 0.691420
[[34m2025-10-04 12:18:17[0m] Step: 4061, Training Logs: loss_final: 0.887519, loss_mean: 0.841103, loss_mean_cls: 0.046416, grad_norm: 0.376851
[[34m2025-10-04 12:18:18[0m] Step: 4062, Training Logs: loss_final: 0.892974, loss_mean: 0.847780, loss_mean_cls: 0.045194, grad_norm: 0.673990
[[34m2025-10-04 12:18:18[0m] Step: 4063, Training Logs: loss_final: 0.906782, loss_mean: 0.861681, loss_mean_cls: 0.045101, grad_norm: 0.527655
[[34m2025-10-04 12:18:18[0m] Step: 4064, Training Logs: loss_final: 0.897223, loss_mean: 0.852060, loss_mean_cls: 0.045163, grad_norm: 0.528456
[[34m2025-10-04 12:18:19[0m] Step: 4065, Training Logs: loss_final: 0.893228, loss_mean: 0.847567, loss_mean_cls: 0.045661, grad_norm: 0.575546
[[34m2025-10-04 12:18:19[0m] Step: 4066, Training Logs: loss_final: 0.886498, loss_mean: 0.841847, loss_mean_cls: 0.044651, grad_norm: 0.476175
[[34m2025-10-04 12:18:19[0m] Step: 4067, Training Logs: loss_final: 0.884427, loss_mean: 0.838739, loss_mean_cls: 0.045688, grad_norm: 0.621025
[[34m2025-10-04 12:18:20[0m] Step: 4068, Training Logs: loss_final: 0.905934, loss_mean: 0.860992, loss_mean_cls: 0.044942, grad_norm: 0.435707
[[34m2025-10-04 12:18:20[0m] Step: 4069, Training Logs: loss_final: 0.881666, loss_mean: 0.836292, loss_mean_cls: 0.045374, grad_norm: 0.336785
[[34m2025-10-04 12:18:20[0m] Step: 4070, Training Logs: loss_final: 0.883488, loss_mean: 0.838721, loss_mean_cls: 0.044767, grad_norm: 0.575512
[[34m2025-10-04 12:18:20[0m] Step: 4071, Training Logs: loss_final: 0.878282, loss_mean: 0.832839, loss_mean_cls: 0.045443, grad_norm: 0.387120
[[34m2025-10-04 12:18:21[0m] Step: 4072, Training Logs: loss_final: 0.920687, loss_mean: 0.874981, loss_mean_cls: 0.045707, grad_norm: 0.469453
[[34m2025-10-04 12:18:21[0m] Step: 4073, Training Logs: loss_final: 0.892861, loss_mean: 0.847064, loss_mean_cls: 0.045797, grad_norm: 0.397413
[[34m2025-10-04 12:18:21[0m] Step: 4074, Training Logs: loss_final: 0.904894, loss_mean: 0.859997, loss_mean_cls: 0.044897, grad_norm: 0.354594
[[34m2025-10-04 12:18:22[0m] Step: 4075, Training Logs: loss_final: 0.908134, loss_mean: 0.862395, loss_mean_cls: 0.045740, grad_norm: 0.399004
[[34m2025-10-04 12:18:22[0m] Step: 4076, Training Logs: loss_final: 0.880277, loss_mean: 0.833020, loss_mean_cls: 0.047257, grad_norm: 0.479650
[[34m2025-10-04 12:18:22[0m] Step: 4077, Training Logs: loss_final: 0.895194, loss_mean: 0.849197, loss_mean_cls: 0.045997, grad_norm: 0.372528
[[34m2025-10-04 12:18:23[0m] Step: 4078, Training Logs: loss_final: 0.896812, loss_mean: 0.853264, loss_mean_cls: 0.043549, grad_norm: 0.451901
[[34m2025-10-04 12:18:23[0m] Step: 4079, Training Logs: loss_final: 0.907197, loss_mean: 0.862154, loss_mean_cls: 0.045044, grad_norm: 0.445753
[[34m2025-10-04 12:18:23[0m] Step: 4080, Training Logs: loss_final: 0.876038, loss_mean: 0.829332, loss_mean_cls: 0.046705, grad_norm: 0.341988
[[34m2025-10-04 12:18:23[0m] Step: 4081, Training Logs: loss_final: 0.916628, loss_mean: 0.870599, loss_mean_cls: 0.046028, grad_norm: 0.399264
[[34m2025-10-04 12:18:24[0m] Step: 4082, Training Logs: loss_final: 0.909784, loss_mean: 0.863984, loss_mean_cls: 0.045800, grad_norm: 0.390008
[[34m2025-10-04 12:18:24[0m] Step: 4083, Training Logs: loss_final: 0.880031, loss_mean: 0.834155, loss_mean_cls: 0.045876, grad_norm: 0.426214
[[34m2025-10-04 12:18:24[0m] Step: 4084, Training Logs: loss_final: 0.871060, loss_mean: 0.824739, loss_mean_cls: 0.046322, grad_norm: 0.269990
[[34m2025-10-04 12:18:25[0m] Step: 4085, Training Logs: loss_final: 0.894165, loss_mean: 0.849673, loss_mean_cls: 0.044492, grad_norm: 0.507871
[[34m2025-10-04 12:18:25[0m] Step: 4086, Training Logs: loss_final: 0.908742, loss_mean: 0.863753, loss_mean_cls: 0.044989, grad_norm: 0.456012
[[34m2025-10-04 12:18:25[0m] Step: 4087, Training Logs: loss_final: 0.886889, loss_mean: 0.840894, loss_mean_cls: 0.045995, grad_norm: 0.289514
[[34m2025-10-04 12:18:26[0m] Step: 4088, Training Logs: loss_final: 0.897096, loss_mean: 0.851277, loss_mean_cls: 0.045819, grad_norm: 0.450887
[[34m2025-10-04 12:18:26[0m] Step: 4089, Training Logs: loss_final: 0.890256, loss_mean: 0.845645, loss_mean_cls: 0.044611, grad_norm: 0.389004
[[34m2025-10-04 12:18:26[0m] Step: 4090, Training Logs: loss_final: 0.888613, loss_mean: 0.843675, loss_mean_cls: 0.044938, grad_norm: 0.426287
[[34m2025-10-04 12:18:26[0m] Step: 4091, Training Logs: loss_final: 0.890959, loss_mean: 0.844439, loss_mean_cls: 0.046520, grad_norm: 0.405326
[[34m2025-10-04 12:18:27[0m] Step: 4092, Training Logs: loss_final: 0.878819, loss_mean: 0.833451, loss_mean_cls: 0.045368, grad_norm: 0.334394
[[34m2025-10-04 12:18:27[0m] Step: 4093, Training Logs: loss_final: 0.889444, loss_mean: 0.845056, loss_mean_cls: 0.044388, grad_norm: 0.408058
[[34m2025-10-04 12:18:27[0m] Step: 4094, Training Logs: loss_final: 0.898982, loss_mean: 0.852993, loss_mean_cls: 0.045989, grad_norm: 0.275549
[[34m2025-10-04 12:18:28[0m] Step: 4095, Training Logs: loss_final: 0.888266, loss_mean: 0.843038, loss_mean_cls: 0.045227, grad_norm: 0.378351
[[34m2025-10-04 12:18:28[0m] Step: 4096, Training Logs: loss_final: 0.886662, loss_mean: 0.840941, loss_mean_cls: 0.045721, grad_norm: 0.498576
[[34m2025-10-04 12:18:28[0m] Step: 4097, Training Logs: loss_final: 0.885943, loss_mean: 0.841054, loss_mean_cls: 0.044889, grad_norm: 0.342306
[[34m2025-10-04 12:18:28[0m] Step: 4098, Training Logs: loss_final: 0.914876, loss_mean: 0.869924, loss_mean_cls: 0.044952, grad_norm: 0.313731
[[34m2025-10-04 12:18:29[0m] Step: 4099, Training Logs: loss_final: 0.901474, loss_mean: 0.856284, loss_mean_cls: 0.045190, grad_norm: 0.649768
[[34m2025-10-04 12:18:29[0m] Step: 4100, Training Logs: loss_final: 0.889597, loss_mean: 0.842681, loss_mean_cls: 0.046916, grad_norm: 0.301856
[[34m2025-10-04 12:18:29[0m] Step: 4101, Training Logs: loss_final: 0.877002, loss_mean: 0.832699, loss_mean_cls: 0.044303, grad_norm: 0.322977
[[34m2025-10-04 12:18:30[0m] Step: 4102, Training Logs: loss_final: 0.892970, loss_mean: 0.846614, loss_mean_cls: 0.046356, grad_norm: 0.513606
[[34m2025-10-04 12:18:30[0m] Step: 4103, Training Logs: loss_final: 0.912944, loss_mean: 0.868638, loss_mean_cls: 0.044306, grad_norm: 0.374337
[[34m2025-10-04 12:18:30[0m] Step: 4104, Training Logs: loss_final: 0.916490, loss_mean: 0.871762, loss_mean_cls: 0.044729, grad_norm: 0.346184
[[34m2025-10-04 12:18:31[0m] Step: 4105, Training Logs: loss_final: 0.906502, loss_mean: 0.860338, loss_mean_cls: 0.046164, grad_norm: 0.358602
[[34m2025-10-04 12:18:31[0m] Step: 4106, Training Logs: loss_final: 0.889409, loss_mean: 0.844838, loss_mean_cls: 0.044572, grad_norm: 0.362638
[[34m2025-10-04 12:18:31[0m] Step: 4107, Training Logs: loss_final: 0.892824, loss_mean: 0.847165, loss_mean_cls: 0.045659, grad_norm: 0.339130
[[34m2025-10-04 12:18:31[0m] Step: 4108, Training Logs: loss_final: 0.881677, loss_mean: 0.835975, loss_mean_cls: 0.045702, grad_norm: 0.330737
[[34m2025-10-04 12:18:32[0m] Step: 4109, Training Logs: loss_final: 0.900819, loss_mean: 0.855785, loss_mean_cls: 0.045033, grad_norm: 0.393571
[[34m2025-10-04 12:18:32[0m] Step: 4110, Training Logs: loss_final: 0.918635, loss_mean: 0.873874, loss_mean_cls: 0.044761, grad_norm: 0.485809
[[34m2025-10-04 12:18:32[0m] Step: 4111, Training Logs: loss_final: 0.896313, loss_mean: 0.850862, loss_mean_cls: 0.045451, grad_norm: 0.396214
[[34m2025-10-04 12:18:33[0m] Step: 4112, Training Logs: loss_final: 0.877537, loss_mean: 0.831204, loss_mean_cls: 0.046333, grad_norm: 0.421285
[[34m2025-10-04 12:18:33[0m] Step: 4113, Training Logs: loss_final: 0.909508, loss_mean: 0.864346, loss_mean_cls: 0.045162, grad_norm: 0.572781
[[34m2025-10-04 12:18:33[0m] Step: 4114, Training Logs: loss_final: 0.911363, loss_mean: 0.866711, loss_mean_cls: 0.044651, grad_norm: 0.459875
[[34m2025-10-04 12:18:34[0m] Step: 4115, Training Logs: loss_final: 0.897311, loss_mean: 0.852083, loss_mean_cls: 0.045227, grad_norm: 0.287920
[[34m2025-10-04 12:18:34[0m] Step: 4116, Training Logs: loss_final: 0.922353, loss_mean: 0.876789, loss_mean_cls: 0.045564, grad_norm: 0.320097
[[34m2025-10-04 12:18:34[0m] Step: 4117, Training Logs: loss_final: 0.897845, loss_mean: 0.852431, loss_mean_cls: 0.045414, grad_norm: 0.349595
[[34m2025-10-04 12:18:34[0m] Step: 4118, Training Logs: loss_final: 0.897947, loss_mean: 0.851418, loss_mean_cls: 0.046530, grad_norm: 0.356346
[[34m2025-10-04 12:18:35[0m] Step: 4119, Training Logs: loss_final: 0.899716, loss_mean: 0.854920, loss_mean_cls: 0.044795, grad_norm: 0.343579
[[34m2025-10-04 12:18:35[0m] Step: 4120, Training Logs: loss_final: 0.865067, loss_mean: 0.818394, loss_mean_cls: 0.046673, grad_norm: 0.409509
[[34m2025-10-04 12:18:35[0m] Step: 4121, Training Logs: loss_final: 0.891795, loss_mean: 0.844836, loss_mean_cls: 0.046959, grad_norm: 0.332177
[[34m2025-10-04 12:18:36[0m] Step: 4122, Training Logs: loss_final: 0.905271, loss_mean: 0.859944, loss_mean_cls: 0.045326, grad_norm: 0.304108
[[34m2025-10-04 12:18:36[0m] Step: 4123, Training Logs: loss_final: 0.892187, loss_mean: 0.846243, loss_mean_cls: 0.045944, grad_norm: 0.446533
[[34m2025-10-04 12:18:36[0m] Step: 4124, Training Logs: loss_final: 0.905668, loss_mean: 0.859566, loss_mean_cls: 0.046101, grad_norm: 0.298711
[[34m2025-10-04 12:18:36[0m] Step: 4125, Training Logs: loss_final: 0.882615, loss_mean: 0.837595, loss_mean_cls: 0.045020, grad_norm: 0.513544
[[34m2025-10-04 12:18:37[0m] Step: 4126, Training Logs: loss_final: 0.898600, loss_mean: 0.852868, loss_mean_cls: 0.045732, grad_norm: 0.321485
[[34m2025-10-04 12:18:37[0m] Step: 4127, Training Logs: loss_final: 0.900042, loss_mean: 0.854276, loss_mean_cls: 0.045766, grad_norm: 0.506836
[[34m2025-10-04 12:18:37[0m] Step: 4128, Training Logs: loss_final: 0.913907, loss_mean: 0.869553, loss_mean_cls: 0.044354, grad_norm: 0.494671
[[34m2025-10-04 12:18:38[0m] Step: 4129, Training Logs: loss_final: 0.908169, loss_mean: 0.863132, loss_mean_cls: 0.045037, grad_norm: 0.471954
[[34m2025-10-04 12:18:38[0m] Step: 4130, Training Logs: loss_final: 0.894444, loss_mean: 0.849088, loss_mean_cls: 0.045355, grad_norm: 0.406319
[[34m2025-10-04 12:18:38[0m] Step: 4131, Training Logs: loss_final: 0.889434, loss_mean: 0.842216, loss_mean_cls: 0.047218, grad_norm: 0.454435
[[34m2025-10-04 12:18:38[0m] Step: 4132, Training Logs: loss_final: 0.887614, loss_mean: 0.841806, loss_mean_cls: 0.045808, grad_norm: 0.416979
[[34m2025-10-04 12:18:39[0m] Step: 4133, Training Logs: loss_final: 0.901308, loss_mean: 0.856008, loss_mean_cls: 0.045300, grad_norm: 0.551305
[[34m2025-10-04 12:18:39[0m] Step: 4134, Training Logs: loss_final: 0.902842, loss_mean: 0.856596, loss_mean_cls: 0.046246, grad_norm: 0.511689
[[34m2025-10-04 12:18:39[0m] Step: 4135, Training Logs: loss_final: 0.900189, loss_mean: 0.854618, loss_mean_cls: 0.045571, grad_norm: 0.391349
[[34m2025-10-04 12:18:40[0m] Step: 4136, Training Logs: loss_final: 0.860627, loss_mean: 0.814833, loss_mean_cls: 0.045794, grad_norm: 0.492822
[[34m2025-10-04 12:18:40[0m] Step: 4137, Training Logs: loss_final: 0.884354, loss_mean: 0.838363, loss_mean_cls: 0.045992, grad_norm: 0.439211
[[34m2025-10-04 12:18:40[0m] Step: 4138, Training Logs: loss_final: 0.885717, loss_mean: 0.839074, loss_mean_cls: 0.046643, grad_norm: 0.436867
[[34m2025-10-04 12:18:41[0m] Step: 4139, Training Logs: loss_final: 0.904627, loss_mean: 0.859196, loss_mean_cls: 0.045431, grad_norm: 0.509707
[[34m2025-10-04 12:18:41[0m] Step: 4140, Training Logs: loss_final: 0.902644, loss_mean: 0.856131, loss_mean_cls: 0.046514, grad_norm: 0.484615
[[34m2025-10-04 12:18:41[0m] Step: 4141, Training Logs: loss_final: 0.898839, loss_mean: 0.853565, loss_mean_cls: 0.045274, grad_norm: 0.517623
[[34m2025-10-04 12:18:41[0m] Step: 4142, Training Logs: loss_final: 0.873304, loss_mean: 0.828285, loss_mean_cls: 0.045019, grad_norm: 0.504419
[[34m2025-10-04 12:18:42[0m] Step: 4143, Training Logs: loss_final: 0.890550, loss_mean: 0.844096, loss_mean_cls: 0.046455, grad_norm: 0.551274
[[34m2025-10-04 12:18:42[0m] Step: 4144, Training Logs: loss_final: 0.902135, loss_mean: 0.856418, loss_mean_cls: 0.045717, grad_norm: 0.407347
[[34m2025-10-04 12:18:42[0m] Step: 4145, Training Logs: loss_final: 0.880514, loss_mean: 0.835011, loss_mean_cls: 0.045503, grad_norm: 0.401827
[[34m2025-10-04 12:18:43[0m] Step: 4146, Training Logs: loss_final: 0.903675, loss_mean: 0.858312, loss_mean_cls: 0.045364, grad_norm: 0.396495
[[34m2025-10-04 12:18:43[0m] Step: 4147, Training Logs: loss_final: 0.904462, loss_mean: 0.859803, loss_mean_cls: 0.044659, grad_norm: 0.385192
[[34m2025-10-04 12:18:43[0m] Step: 4148, Training Logs: loss_final: 0.912676, loss_mean: 0.867851, loss_mean_cls: 0.044825, grad_norm: 0.418553
[[34m2025-10-04 12:18:43[0m] Step: 4149, Training Logs: loss_final: 0.912873, loss_mean: 0.867710, loss_mean_cls: 0.045162, grad_norm: 0.326187
[[34m2025-10-04 12:18:44[0m] Step: 4150, Training Logs: loss_final: 0.876324, loss_mean: 0.832077, loss_mean_cls: 0.044247, grad_norm: 0.373633
[[34m2025-10-04 12:18:44[0m] Step: 4151, Training Logs: loss_final: 0.912696, loss_mean: 0.866604, loss_mean_cls: 0.046092, grad_norm: 0.401998
[[34m2025-10-04 12:18:44[0m] Step: 4152, Training Logs: loss_final: 0.894927, loss_mean: 0.850480, loss_mean_cls: 0.044447, grad_norm: 0.277159
[[34m2025-10-04 12:18:45[0m] Step: 4153, Training Logs: loss_final: 0.897476, loss_mean: 0.850922, loss_mean_cls: 0.046554, grad_norm: 0.397621
[[34m2025-10-04 12:18:45[0m] Step: 4154, Training Logs: loss_final: 0.913773, loss_mean: 0.869584, loss_mean_cls: 0.044190, grad_norm: 0.506382
[[34m2025-10-04 12:18:45[0m] Step: 4155, Training Logs: loss_final: 0.882697, loss_mean: 0.837258, loss_mean_cls: 0.045438, grad_norm: 0.371897
[[34m2025-10-04 12:18:46[0m] Step: 4156, Training Logs: loss_final: 0.898269, loss_mean: 0.853022, loss_mean_cls: 0.045246, grad_norm: 0.300404
[[34m2025-10-04 12:18:46[0m] Step: 4157, Training Logs: loss_final: 0.908673, loss_mean: 0.862527, loss_mean_cls: 0.046146, grad_norm: 0.441555
[[34m2025-10-04 12:18:46[0m] Step: 4158, Training Logs: loss_final: 0.912482, loss_mean: 0.867942, loss_mean_cls: 0.044540, grad_norm: 0.335430
[[34m2025-10-04 12:18:46[0m] Step: 4159, Training Logs: loss_final: 0.917984, loss_mean: 0.873010, loss_mean_cls: 0.044974, grad_norm: 0.332464
[[34m2025-10-04 12:18:47[0m] Step: 4160, Training Logs: loss_final: 0.905162, loss_mean: 0.858679, loss_mean_cls: 0.046483, grad_norm: 0.415732
[[34m2025-10-04 12:18:47[0m] Step: 4161, Training Logs: loss_final: 0.899154, loss_mean: 0.853770, loss_mean_cls: 0.045383, grad_norm: 0.297905
[[34m2025-10-04 12:18:47[0m] Step: 4162, Training Logs: loss_final: 0.895213, loss_mean: 0.849406, loss_mean_cls: 0.045806, grad_norm: 0.435432
[[34m2025-10-04 12:18:48[0m] Step: 4163, Training Logs: loss_final: 0.918180, loss_mean: 0.872706, loss_mean_cls: 0.045474, grad_norm: 0.355527
[[34m2025-10-04 12:18:48[0m] Step: 4164, Training Logs: loss_final: 0.895061, loss_mean: 0.850333, loss_mean_cls: 0.044728, grad_norm: 0.391735
[[34m2025-10-04 12:18:48[0m] Step: 4165, Training Logs: loss_final: 0.907547, loss_mean: 0.862745, loss_mean_cls: 0.044803, grad_norm: 0.357695
[[34m2025-10-04 12:18:49[0m] Step: 4166, Training Logs: loss_final: 0.901369, loss_mean: 0.855101, loss_mean_cls: 0.046268, grad_norm: 0.286536
[[34m2025-10-04 12:18:49[0m] Step: 4167, Training Logs: loss_final: 0.874373, loss_mean: 0.828656, loss_mean_cls: 0.045716, grad_norm: 0.433960
[[34m2025-10-04 12:18:49[0m] Step: 4168, Training Logs: loss_final: 0.909800, loss_mean: 0.863828, loss_mean_cls: 0.045972, grad_norm: 0.332442
[[34m2025-10-04 12:18:49[0m] Step: 4169, Training Logs: loss_final: 0.897519, loss_mean: 0.851224, loss_mean_cls: 0.046295, grad_norm: 0.383854
[[34m2025-10-04 12:18:50[0m] Step: 4170, Training Logs: loss_final: 0.899995, loss_mean: 0.855360, loss_mean_cls: 0.044635, grad_norm: 0.409546
[[34m2025-10-04 12:18:50[0m] Step: 4171, Training Logs: loss_final: 0.890242, loss_mean: 0.843975, loss_mean_cls: 0.046267, grad_norm: 0.361124
[[34m2025-10-04 12:18:50[0m] Step: 4172, Training Logs: loss_final: 0.910807, loss_mean: 0.864584, loss_mean_cls: 0.046222, grad_norm: 0.384159
[[34m2025-10-04 12:18:51[0m] Step: 4173, Training Logs: loss_final: 0.888883, loss_mean: 0.843607, loss_mean_cls: 0.045276, grad_norm: 0.462501
[[34m2025-10-04 12:18:51[0m] Step: 4174, Training Logs: loss_final: 0.880004, loss_mean: 0.835310, loss_mean_cls: 0.044694, grad_norm: 0.379348
[[34m2025-10-04 12:18:51[0m] Step: 4175, Training Logs: loss_final: 0.901945, loss_mean: 0.857178, loss_mean_cls: 0.044767, grad_norm: 0.492345
[[34m2025-10-04 12:18:52[0m] Step: 4176, Training Logs: loss_final: 0.906051, loss_mean: 0.860862, loss_mean_cls: 0.045189, grad_norm: 0.345032
[[34m2025-10-04 12:18:52[0m] Step: 4177, Training Logs: loss_final: 0.882100, loss_mean: 0.835197, loss_mean_cls: 0.046903, grad_norm: 0.448844
[[34m2025-10-04 12:18:52[0m] Step: 4178, Training Logs: loss_final: 0.897948, loss_mean: 0.852192, loss_mean_cls: 0.045757, grad_norm: 0.419328
[[34m2025-10-04 12:18:52[0m] Step: 4179, Training Logs: loss_final: 0.908124, loss_mean: 0.861983, loss_mean_cls: 0.046141, grad_norm: 0.529406
[[34m2025-10-04 12:18:53[0m] Step: 4180, Training Logs: loss_final: 0.898956, loss_mean: 0.854218, loss_mean_cls: 0.044738, grad_norm: 0.452415
[[34m2025-10-04 12:18:53[0m] Step: 4181, Training Logs: loss_final: 0.912717, loss_mean: 0.868051, loss_mean_cls: 0.044666, grad_norm: 0.356694
[[34m2025-10-04 12:18:53[0m] Step: 4182, Training Logs: loss_final: 0.904900, loss_mean: 0.859353, loss_mean_cls: 0.045547, grad_norm: 0.492072
[[34m2025-10-04 12:18:54[0m] Step: 4183, Training Logs: loss_final: 0.923640, loss_mean: 0.879360, loss_mean_cls: 0.044281, grad_norm: 0.297586
[[34m2025-10-04 12:18:54[0m] Step: 4184, Training Logs: loss_final: 0.866222, loss_mean: 0.820342, loss_mean_cls: 0.045880, grad_norm: 0.511171
[[34m2025-10-04 12:18:54[0m] Step: 4185, Training Logs: loss_final: 0.900874, loss_mean: 0.854253, loss_mean_cls: 0.046621, grad_norm: 0.396294
[[34m2025-10-04 12:18:54[0m] Step: 4186, Training Logs: loss_final: 0.895244, loss_mean: 0.850097, loss_mean_cls: 0.045148, grad_norm: 0.387589
[[34m2025-10-04 12:18:55[0m] Step: 4187, Training Logs: loss_final: 0.869377, loss_mean: 0.824364, loss_mean_cls: 0.045013, grad_norm: 0.336381
[[34m2025-10-04 12:18:55[0m] Step: 4188, Training Logs: loss_final: 0.911389, loss_mean: 0.865549, loss_mean_cls: 0.045840, grad_norm: 0.385954
[[34m2025-10-04 12:18:55[0m] Step: 4189, Training Logs: loss_final: 0.926482, loss_mean: 0.881299, loss_mean_cls: 0.045183, grad_norm: 0.427547
[[34m2025-10-04 12:18:56[0m] Step: 4190, Training Logs: loss_final: 0.870236, loss_mean: 0.825282, loss_mean_cls: 0.044954, grad_norm: 0.295963
[[34m2025-10-04 12:18:56[0m] Step: 4191, Training Logs: loss_final: 0.894982, loss_mean: 0.849954, loss_mean_cls: 0.045027, grad_norm: 0.377211
[[34m2025-10-04 12:18:56[0m] Step: 4192, Training Logs: loss_final: 0.883454, loss_mean: 0.837593, loss_mean_cls: 0.045861, grad_norm: 0.474765
[[34m2025-10-04 12:18:56[0m] Step: 4193, Training Logs: loss_final: 0.886660, loss_mean: 0.841311, loss_mean_cls: 0.045348, grad_norm: 0.535724
[[34m2025-10-04 12:18:57[0m] Step: 4194, Training Logs: loss_final: 0.899267, loss_mean: 0.853874, loss_mean_cls: 0.045393, grad_norm: 0.434533
[[34m2025-10-04 12:18:57[0m] Step: 4195, Training Logs: loss_final: 0.900605, loss_mean: 0.856070, loss_mean_cls: 0.044535, grad_norm: 0.428090
[[34m2025-10-04 12:18:57[0m] Step: 4196, Training Logs: loss_final: 0.892623, loss_mean: 0.847136, loss_mean_cls: 0.045487, grad_norm: 0.490738
[[34m2025-10-04 12:18:58[0m] Step: 4197, Training Logs: loss_final: 0.892865, loss_mean: 0.846419, loss_mean_cls: 0.046446, grad_norm: 0.405101
[[34m2025-10-04 12:18:58[0m] Step: 4198, Training Logs: loss_final: 0.883239, loss_mean: 0.836873, loss_mean_cls: 0.046366, grad_norm: 0.378906
[[34m2025-10-04 12:18:58[0m] Step: 4199, Training Logs: loss_final: 0.905081, loss_mean: 0.859755, loss_mean_cls: 0.045326, grad_norm: 0.429539
[[34m2025-10-04 12:18:59[0m] Step: 4200, Training Logs: loss_final: 0.891415, loss_mean: 0.846936, loss_mean_cls: 0.044480, grad_norm: 0.355678
[[34m2025-10-04 12:18:59[0m] Step: 4201, Training Logs: loss_final: 0.913832, loss_mean: 0.868900, loss_mean_cls: 0.044932, grad_norm: 0.326201
[[34m2025-10-04 12:18:59[0m] Step: 4202, Training Logs: loss_final: 0.892806, loss_mean: 0.847319, loss_mean_cls: 0.045487, grad_norm: 0.336817
[[34m2025-10-04 12:18:59[0m] Step: 4203, Training Logs: loss_final: 0.891273, loss_mean: 0.847065, loss_mean_cls: 0.044208, grad_norm: 0.453698
[[34m2025-10-04 12:19:00[0m] Step: 4204, Training Logs: loss_final: 0.894079, loss_mean: 0.849015, loss_mean_cls: 0.045064, grad_norm: 0.374611
[[34m2025-10-04 12:19:00[0m] Step: 4205, Training Logs: loss_final: 0.870807, loss_mean: 0.825187, loss_mean_cls: 0.045620, grad_norm: 0.381903
[[34m2025-10-04 12:19:00[0m] Step: 4206, Training Logs: loss_final: 0.892660, loss_mean: 0.847334, loss_mean_cls: 0.045326, grad_norm: 0.453950
[[34m2025-10-04 12:19:01[0m] Step: 4207, Training Logs: loss_final: 0.888645, loss_mean: 0.843227, loss_mean_cls: 0.045418, grad_norm: 0.364576
[[34m2025-10-04 12:19:01[0m] Step: 4208, Training Logs: loss_final: 0.905884, loss_mean: 0.860753, loss_mean_cls: 0.045131, grad_norm: 0.481583
[[34m2025-10-04 12:19:01[0m] Step: 4209, Training Logs: loss_final: 0.913900, loss_mean: 0.868337, loss_mean_cls: 0.045563, grad_norm: 0.359836
[[34m2025-10-04 12:19:02[0m] Step: 4210, Training Logs: loss_final: 0.908150, loss_mean: 0.861650, loss_mean_cls: 0.046500, grad_norm: 0.457976
[[34m2025-10-04 12:19:02[0m] Step: 4211, Training Logs: loss_final: 0.904919, loss_mean: 0.859059, loss_mean_cls: 0.045860, grad_norm: 0.464089
[[34m2025-10-04 12:19:02[0m] Step: 4212, Training Logs: loss_final: 0.902323, loss_mean: 0.857619, loss_mean_cls: 0.044704, grad_norm: 0.532118
[[34m2025-10-04 12:19:02[0m] Step: 4213, Training Logs: loss_final: 0.900716, loss_mean: 0.855831, loss_mean_cls: 0.044885, grad_norm: 0.475377
[[34m2025-10-04 12:19:03[0m] Step: 4214, Training Logs: loss_final: 0.893347, loss_mean: 0.846963, loss_mean_cls: 0.046384, grad_norm: 0.471741
[[34m2025-10-04 12:19:03[0m] Step: 4215, Training Logs: loss_final: 0.876809, loss_mean: 0.830677, loss_mean_cls: 0.046132, grad_norm: 0.635979
[[34m2025-10-04 12:19:03[0m] Step: 4216, Training Logs: loss_final: 0.907850, loss_mean: 0.862071, loss_mean_cls: 0.045778, grad_norm: 0.427088
[[34m2025-10-04 12:19:04[0m] Step: 4217, Training Logs: loss_final: 0.869212, loss_mean: 0.823254, loss_mean_cls: 0.045959, grad_norm: 0.363198
[[34m2025-10-04 12:19:04[0m] Step: 4218, Training Logs: loss_final: 0.892531, loss_mean: 0.847003, loss_mean_cls: 0.045529, grad_norm: 0.531551
[[34m2025-10-04 12:19:04[0m] Step: 4219, Training Logs: loss_final: 0.897322, loss_mean: 0.851745, loss_mean_cls: 0.045577, grad_norm: 0.397935
[[34m2025-10-04 12:19:05[0m] Step: 4220, Training Logs: loss_final: 0.897140, loss_mean: 0.851221, loss_mean_cls: 0.045919, grad_norm: 0.253116
[[34m2025-10-04 12:19:05[0m] Step: 4221, Training Logs: loss_final: 0.903575, loss_mean: 0.857713, loss_mean_cls: 0.045863, grad_norm: 0.442262
[[34m2025-10-04 12:19:05[0m] Step: 4222, Training Logs: loss_final: 0.871098, loss_mean: 0.825461, loss_mean_cls: 0.045636, grad_norm: 0.449940
[[34m2025-10-04 12:19:05[0m] Step: 4223, Training Logs: loss_final: 0.862296, loss_mean: 0.815234, loss_mean_cls: 0.047062, grad_norm: 0.328629
[[34m2025-10-04 12:19:06[0m] Step: 4224, Training Logs: loss_final: 0.902256, loss_mean: 0.858080, loss_mean_cls: 0.044176, grad_norm: 0.426366
[[34m2025-10-04 12:19:06[0m] Step: 4225, Training Logs: loss_final: 0.906154, loss_mean: 0.860698, loss_mean_cls: 0.045456, grad_norm: 0.400776
[[34m2025-10-04 12:19:06[0m] Step: 4226, Training Logs: loss_final: 0.868591, loss_mean: 0.823738, loss_mean_cls: 0.044854, grad_norm: 0.366305
[[34m2025-10-04 12:19:07[0m] Step: 4227, Training Logs: loss_final: 0.884117, loss_mean: 0.839604, loss_mean_cls: 0.044513, grad_norm: 0.492387
[[34m2025-10-04 12:19:07[0m] Step: 4228, Training Logs: loss_final: 0.910212, loss_mean: 0.864688, loss_mean_cls: 0.045524, grad_norm: 0.361186
[[34m2025-10-04 12:19:07[0m] Step: 4229, Training Logs: loss_final: 0.894106, loss_mean: 0.849169, loss_mean_cls: 0.044937, grad_norm: 0.303186
[[34m2025-10-04 12:19:07[0m] Step: 4230, Training Logs: loss_final: 0.874504, loss_mean: 0.827944, loss_mean_cls: 0.046561, grad_norm: 0.415584
[[34m2025-10-04 12:19:08[0m] Step: 4231, Training Logs: loss_final: 0.888826, loss_mean: 0.842140, loss_mean_cls: 0.046687, grad_norm: 0.453037
[[34m2025-10-04 12:19:08[0m] Step: 4232, Training Logs: loss_final: 0.899480, loss_mean: 0.854904, loss_mean_cls: 0.044576, grad_norm: 0.317525
[[34m2025-10-04 12:19:08[0m] Step: 4233, Training Logs: loss_final: 0.886571, loss_mean: 0.841132, loss_mean_cls: 0.045439, grad_norm: 0.286629
[[34m2025-10-04 12:19:09[0m] Step: 4234, Training Logs: loss_final: 0.919743, loss_mean: 0.874533, loss_mean_cls: 0.045210, grad_norm: 0.395852
[[34m2025-10-04 12:19:09[0m] Step: 4235, Training Logs: loss_final: 0.880908, loss_mean: 0.835937, loss_mean_cls: 0.044971, grad_norm: 0.473632
[[34m2025-10-04 12:19:09[0m] Step: 4236, Training Logs: loss_final: 0.881540, loss_mean: 0.835040, loss_mean_cls: 0.046500, grad_norm: 0.421925
[[34m2025-10-04 12:19:10[0m] Step: 4237, Training Logs: loss_final: 0.892090, loss_mean: 0.846151, loss_mean_cls: 0.045939, grad_norm: 0.462304
[[34m2025-10-04 12:19:10[0m] Step: 4238, Training Logs: loss_final: 0.902131, loss_mean: 0.856678, loss_mean_cls: 0.045452, grad_norm: 0.493965
[[34m2025-10-04 12:19:10[0m] Step: 4239, Training Logs: loss_final: 0.879439, loss_mean: 0.834698, loss_mean_cls: 0.044741, grad_norm: 0.361711
[[34m2025-10-04 12:19:10[0m] Step: 4240, Training Logs: loss_final: 0.892465, loss_mean: 0.846398, loss_mean_cls: 0.046067, grad_norm: 0.359119
[[34m2025-10-04 12:19:11[0m] Step: 4241, Training Logs: loss_final: 0.891413, loss_mean: 0.846843, loss_mean_cls: 0.044569, grad_norm: 0.515595
[[34m2025-10-04 12:19:11[0m] Step: 4242, Training Logs: loss_final: 0.897208, loss_mean: 0.853251, loss_mean_cls: 0.043957, grad_norm: 0.337171
[[34m2025-10-04 12:19:11[0m] Step: 4243, Training Logs: loss_final: 0.886464, loss_mean: 0.841685, loss_mean_cls: 0.044778, grad_norm: 0.280534
[[34m2025-10-04 12:19:12[0m] Step: 4244, Training Logs: loss_final: 0.917757, loss_mean: 0.871781, loss_mean_cls: 0.045976, grad_norm: 0.512513
[[34m2025-10-04 12:19:12[0m] Step: 4245, Training Logs: loss_final: 0.901440, loss_mean: 0.856284, loss_mean_cls: 0.045156, grad_norm: 0.409183
[[34m2025-10-04 12:19:12[0m] Step: 4246, Training Logs: loss_final: 0.897579, loss_mean: 0.852694, loss_mean_cls: 0.044885, grad_norm: 0.340827
[[34m2025-10-04 12:19:13[0m] Step: 4247, Training Logs: loss_final: 0.891362, loss_mean: 0.846314, loss_mean_cls: 0.045048, grad_norm: 0.425071
[[34m2025-10-04 12:19:13[0m] Step: 4248, Training Logs: loss_final: 0.883328, loss_mean: 0.836839, loss_mean_cls: 0.046489, grad_norm: 0.489978
[[34m2025-10-04 12:19:13[0m] Step: 4249, Training Logs: loss_final: 0.895614, loss_mean: 0.850885, loss_mean_cls: 0.044728, grad_norm: 0.305145
[[34m2025-10-04 12:19:13[0m] Step: 4250, Training Logs: loss_final: 0.911346, loss_mean: 0.866239, loss_mean_cls: 0.045108, grad_norm: 0.443582
[[34m2025-10-04 12:19:14[0m] Step: 4251, Training Logs: loss_final: 0.918798, loss_mean: 0.874907, loss_mean_cls: 0.043891, grad_norm: 0.473184
[[34m2025-10-04 12:19:14[0m] Step: 4252, Training Logs: loss_final: 0.893595, loss_mean: 0.847570, loss_mean_cls: 0.046025, grad_norm: 0.326952
[[34m2025-10-04 12:19:14[0m] Step: 4253, Training Logs: loss_final: 0.891403, loss_mean: 0.845911, loss_mean_cls: 0.045492, grad_norm: 0.407013
[[34m2025-10-04 12:19:15[0m] Step: 4254, Training Logs: loss_final: 0.897356, loss_mean: 0.852193, loss_mean_cls: 0.045163, grad_norm: 0.513402
[[34m2025-10-04 12:19:15[0m] Step: 4255, Training Logs: loss_final: 0.912398, loss_mean: 0.867560, loss_mean_cls: 0.044838, grad_norm: 0.329177
[[34m2025-10-04 12:19:15[0m] Step: 4256, Training Logs: loss_final: 0.897497, loss_mean: 0.851414, loss_mean_cls: 0.046084, grad_norm: 0.492099
[[34m2025-10-04 12:19:15[0m] Step: 4257, Training Logs: loss_final: 0.907784, loss_mean: 0.863073, loss_mean_cls: 0.044712, grad_norm: 0.419406
[[34m2025-10-04 12:19:16[0m] Step: 4258, Training Logs: loss_final: 0.907395, loss_mean: 0.863002, loss_mean_cls: 0.044393, grad_norm: 0.447060
[[34m2025-10-04 12:19:16[0m] Step: 4259, Training Logs: loss_final: 0.896912, loss_mean: 0.851272, loss_mean_cls: 0.045640, grad_norm: 0.410138
[[34m2025-10-04 12:19:16[0m] Step: 4260, Training Logs: loss_final: 0.881936, loss_mean: 0.836256, loss_mean_cls: 0.045680, grad_norm: 0.382704
[[34m2025-10-04 12:19:17[0m] Step: 4261, Training Logs: loss_final: 0.889209, loss_mean: 0.843481, loss_mean_cls: 0.045728, grad_norm: 0.367156
[[34m2025-10-04 12:19:17[0m] Step: 4262, Training Logs: loss_final: 0.892264, loss_mean: 0.845735, loss_mean_cls: 0.046528, grad_norm: 0.339902
[[34m2025-10-04 12:19:17[0m] Step: 4263, Training Logs: loss_final: 0.889717, loss_mean: 0.845899, loss_mean_cls: 0.043818, grad_norm: 0.401055
[[34m2025-10-04 12:19:17[0m] Step: 4264, Training Logs: loss_final: 0.885493, loss_mean: 0.839238, loss_mean_cls: 0.046254, grad_norm: 0.254796
[[34m2025-10-04 12:19:18[0m] Step: 4265, Training Logs: loss_final: 0.891237, loss_mean: 0.844779, loss_mean_cls: 0.046458, grad_norm: 0.343197
[[34m2025-10-04 12:19:18[0m] Step: 4266, Training Logs: loss_final: 0.907310, loss_mean: 0.862441, loss_mean_cls: 0.044869, grad_norm: 0.476562
[[34m2025-10-04 12:19:18[0m] Step: 4267, Training Logs: loss_final: 0.867628, loss_mean: 0.820904, loss_mean_cls: 0.046723, grad_norm: 0.307999
[[34m2025-10-04 12:19:19[0m] Step: 4268, Training Logs: loss_final: 0.906616, loss_mean: 0.860426, loss_mean_cls: 0.046190, grad_norm: 0.539934
[[34m2025-10-04 12:19:19[0m] Step: 4269, Training Logs: loss_final: 0.892931, loss_mean: 0.848700, loss_mean_cls: 0.044231, grad_norm: 0.549485
[[34m2025-10-04 12:19:19[0m] Step: 4270, Training Logs: loss_final: 0.879678, loss_mean: 0.833811, loss_mean_cls: 0.045867, grad_norm: 0.299666
[[34m2025-10-04 12:19:20[0m] Step: 4271, Training Logs: loss_final: 0.884786, loss_mean: 0.839057, loss_mean_cls: 0.045729, grad_norm: 0.380973
[[34m2025-10-04 12:19:20[0m] Step: 4272, Training Logs: loss_final: 0.883043, loss_mean: 0.838374, loss_mean_cls: 0.044669, grad_norm: 0.440381
[[34m2025-10-04 12:19:20[0m] Step: 4273, Training Logs: loss_final: 0.888654, loss_mean: 0.842367, loss_mean_cls: 0.046287, grad_norm: 0.306676
[[34m2025-10-04 12:19:20[0m] Step: 4274, Training Logs: loss_final: 0.906407, loss_mean: 0.860943, loss_mean_cls: 0.045464, grad_norm: 0.331439
[[34m2025-10-04 12:19:21[0m] Step: 4275, Training Logs: loss_final: 0.875513, loss_mean: 0.829720, loss_mean_cls: 0.045793, grad_norm: 0.527530
[[34m2025-10-04 12:19:21[0m] Step: 4276, Training Logs: loss_final: 0.891755, loss_mean: 0.846107, loss_mean_cls: 0.045648, grad_norm: 0.280222
[[34m2025-10-04 12:19:21[0m] Step: 4277, Training Logs: loss_final: 0.889065, loss_mean: 0.843655, loss_mean_cls: 0.045410, grad_norm: 0.506652
[[34m2025-10-04 12:19:22[0m] Step: 4278, Training Logs: loss_final: 0.886497, loss_mean: 0.840921, loss_mean_cls: 0.045576, grad_norm: 0.396960
[[34m2025-10-04 12:19:22[0m] Step: 4279, Training Logs: loss_final: 0.880639, loss_mean: 0.834239, loss_mean_cls: 0.046399, grad_norm: 0.391828
[[34m2025-10-04 12:19:22[0m] Step: 4280, Training Logs: loss_final: 0.877107, loss_mean: 0.831492, loss_mean_cls: 0.045615, grad_norm: 0.412734
[[34m2025-10-04 12:19:22[0m] Step: 4281, Training Logs: loss_final: 0.880701, loss_mean: 0.835312, loss_mean_cls: 0.045389, grad_norm: 0.545001
[[34m2025-10-04 12:19:23[0m] Step: 4282, Training Logs: loss_final: 0.888368, loss_mean: 0.842868, loss_mean_cls: 0.045501, grad_norm: 0.388294
[[34m2025-10-04 12:19:23[0m] Step: 4283, Training Logs: loss_final: 0.917772, loss_mean: 0.872238, loss_mean_cls: 0.045534, grad_norm: 0.375165
[[34m2025-10-04 12:19:23[0m] Step: 4284, Training Logs: loss_final: 0.903818, loss_mean: 0.858176, loss_mean_cls: 0.045642, grad_norm: 0.475651
[[34m2025-10-04 12:19:24[0m] Step: 4285, Training Logs: loss_final: 0.916108, loss_mean: 0.871809, loss_mean_cls: 0.044298, grad_norm: 0.395210
[[34m2025-10-04 12:19:24[0m] Step: 4286, Training Logs: loss_final: 0.881896, loss_mean: 0.835934, loss_mean_cls: 0.045962, grad_norm: 0.340893
[[34m2025-10-04 12:19:24[0m] Step: 4287, Training Logs: loss_final: 0.906286, loss_mean: 0.860968, loss_mean_cls: 0.045318, grad_norm: 0.396653
[[34m2025-10-04 12:19:25[0m] Step: 4288, Training Logs: loss_final: 0.913149, loss_mean: 0.867549, loss_mean_cls: 0.045600, grad_norm: 0.463541
[[34m2025-10-04 12:19:25[0m] Step: 4289, Training Logs: loss_final: 0.901371, loss_mean: 0.857703, loss_mean_cls: 0.043668, grad_norm: 0.350166
[[34m2025-10-04 12:19:25[0m] Step: 4290, Training Logs: loss_final: 0.889547, loss_mean: 0.844197, loss_mean_cls: 0.045351, grad_norm: 0.355600
[[34m2025-10-04 12:19:25[0m] Step: 4291, Training Logs: loss_final: 0.901074, loss_mean: 0.856302, loss_mean_cls: 0.044771, grad_norm: 0.413907
[[34m2025-10-04 12:19:26[0m] Step: 4292, Training Logs: loss_final: 0.901806, loss_mean: 0.856286, loss_mean_cls: 0.045521, grad_norm: 0.348173
[[34m2025-10-04 12:19:26[0m] Step: 4293, Training Logs: loss_final: 0.893945, loss_mean: 0.849382, loss_mean_cls: 0.044563, grad_norm: 0.352609
[[34m2025-10-04 12:19:26[0m] Step: 4294, Training Logs: loss_final: 0.894114, loss_mean: 0.848490, loss_mean_cls: 0.045624, grad_norm: 0.403768
[[34m2025-10-04 12:19:27[0m] Step: 4295, Training Logs: loss_final: 0.884717, loss_mean: 0.838632, loss_mean_cls: 0.046085, grad_norm: 0.311688
[[34m2025-10-04 12:19:27[0m] Step: 4296, Training Logs: loss_final: 0.899368, loss_mean: 0.855241, loss_mean_cls: 0.044127, grad_norm: 0.337525
[[34m2025-10-04 12:19:27[0m] Step: 4297, Training Logs: loss_final: 0.899025, loss_mean: 0.854281, loss_mean_cls: 0.044744, grad_norm: 0.296939
[[34m2025-10-04 12:19:27[0m] Step: 4298, Training Logs: loss_final: 0.890295, loss_mean: 0.845088, loss_mean_cls: 0.045206, grad_norm: 0.505385
[[34m2025-10-04 12:19:28[0m] Step: 4299, Training Logs: loss_final: 0.902394, loss_mean: 0.857289, loss_mean_cls: 0.045105, grad_norm: 0.390939
[[34m2025-10-04 12:19:28[0m] Step: 4300, Training Logs: loss_final: 0.878212, loss_mean: 0.833066, loss_mean_cls: 0.045146, grad_norm: 0.463325
[[34m2025-10-04 12:19:28[0m] Step: 4301, Training Logs: loss_final: 0.874563, loss_mean: 0.828211, loss_mean_cls: 0.046352, grad_norm: 0.322571
[[34m2025-10-04 12:19:29[0m] Step: 4302, Training Logs: loss_final: 0.869575, loss_mean: 0.823202, loss_mean_cls: 0.046373, grad_norm: 0.290422
[[34m2025-10-04 12:19:29[0m] Step: 4303, Training Logs: loss_final: 0.885661, loss_mean: 0.840035, loss_mean_cls: 0.045626, grad_norm: 0.460620
[[34m2025-10-04 12:19:29[0m] Step: 4304, Training Logs: loss_final: 0.895308, loss_mean: 0.848980, loss_mean_cls: 0.046328, grad_norm: 0.466248
[[34m2025-10-04 12:19:30[0m] Step: 4305, Training Logs: loss_final: 0.878075, loss_mean: 0.832451, loss_mean_cls: 0.045624, grad_norm: 0.339893
[[34m2025-10-04 12:19:30[0m] Step: 4306, Training Logs: loss_final: 0.908349, loss_mean: 0.862057, loss_mean_cls: 0.046292, grad_norm: 0.305541
[[34m2025-10-04 12:19:30[0m] Step: 4307, Training Logs: loss_final: 0.895086, loss_mean: 0.850762, loss_mean_cls: 0.044324, grad_norm: 0.432098
[[34m2025-10-04 12:19:30[0m] Step: 4308, Training Logs: loss_final: 0.878811, loss_mean: 0.833158, loss_mean_cls: 0.045653, grad_norm: 0.305701
[[34m2025-10-04 12:19:31[0m] Step: 4309, Training Logs: loss_final: 0.907497, loss_mean: 0.862937, loss_mean_cls: 0.044560, grad_norm: 0.307991
[[34m2025-10-04 12:19:31[0m] Step: 4310, Training Logs: loss_final: 0.909669, loss_mean: 0.863993, loss_mean_cls: 0.045675, grad_norm: 0.432782
[[34m2025-10-04 12:19:31[0m] Step: 4311, Training Logs: loss_final: 0.897823, loss_mean: 0.852806, loss_mean_cls: 0.045017, grad_norm: 0.361414
[[34m2025-10-04 12:19:32[0m] Step: 4312, Training Logs: loss_final: 0.883940, loss_mean: 0.837857, loss_mean_cls: 0.046084, grad_norm: 0.347471
[[34m2025-10-04 12:19:32[0m] Step: 4313, Training Logs: loss_final: 0.887732, loss_mean: 0.840652, loss_mean_cls: 0.047080, grad_norm: 0.439046
[[34m2025-10-04 12:19:32[0m] Step: 4314, Training Logs: loss_final: 0.883117, loss_mean: 0.838127, loss_mean_cls: 0.044990, grad_norm: 0.367185
[[34m2025-10-04 12:19:32[0m] Step: 4315, Training Logs: loss_final: 0.912448, loss_mean: 0.868209, loss_mean_cls: 0.044239, grad_norm: 0.404938
[[34m2025-10-04 12:19:33[0m] Step: 4316, Training Logs: loss_final: 0.880045, loss_mean: 0.833659, loss_mean_cls: 0.046386, grad_norm: 0.426316
[[34m2025-10-04 12:19:33[0m] Step: 4317, Training Logs: loss_final: 0.901686, loss_mean: 0.857097, loss_mean_cls: 0.044590, grad_norm: 0.386226
[[34m2025-10-04 12:19:33[0m] Step: 4318, Training Logs: loss_final: 0.897047, loss_mean: 0.851604, loss_mean_cls: 0.045444, grad_norm: 0.346839
[[34m2025-10-04 12:19:34[0m] Step: 4319, Training Logs: loss_final: 0.889705, loss_mean: 0.844310, loss_mean_cls: 0.045395, grad_norm: 0.338498
[[34m2025-10-04 12:19:34[0m] Step: 4320, Training Logs: loss_final: 0.906857, loss_mean: 0.861821, loss_mean_cls: 0.045036, grad_norm: 0.479367
[[34m2025-10-04 12:19:34[0m] Step: 4321, Training Logs: loss_final: 0.894621, loss_mean: 0.849853, loss_mean_cls: 0.044768, grad_norm: 0.407334
[[34m2025-10-04 12:19:35[0m] Step: 4322, Training Logs: loss_final: 0.885743, loss_mean: 0.840672, loss_mean_cls: 0.045071, grad_norm: 0.370024
[[34m2025-10-04 12:19:35[0m] Step: 4323, Training Logs: loss_final: 0.907035, loss_mean: 0.861987, loss_mean_cls: 0.045048, grad_norm: 0.385656
[[34m2025-10-04 12:19:35[0m] Step: 4324, Training Logs: loss_final: 0.894498, loss_mean: 0.849602, loss_mean_cls: 0.044896, grad_norm: 0.412461
[[34m2025-10-04 12:19:35[0m] Step: 4325, Training Logs: loss_final: 0.917978, loss_mean: 0.872336, loss_mean_cls: 0.045641, grad_norm: 0.438577
[[34m2025-10-04 12:19:36[0m] Step: 4326, Training Logs: loss_final: 0.885622, loss_mean: 0.841133, loss_mean_cls: 0.044489, grad_norm: 0.392896
[[34m2025-10-04 12:19:36[0m] Step: 4327, Training Logs: loss_final: 0.886588, loss_mean: 0.841196, loss_mean_cls: 0.045392, grad_norm: 0.313002
[[34m2025-10-04 12:19:36[0m] Step: 4328, Training Logs: loss_final: 0.894403, loss_mean: 0.849090, loss_mean_cls: 0.045314, grad_norm: 0.372845
[[34m2025-10-04 12:19:37[0m] Step: 4329, Training Logs: loss_final: 0.883489, loss_mean: 0.838880, loss_mean_cls: 0.044609, grad_norm: 0.334458
[[34m2025-10-04 12:19:37[0m] Step: 4330, Training Logs: loss_final: 0.882714, loss_mean: 0.838123, loss_mean_cls: 0.044590, grad_norm: 0.322452
[[34m2025-10-04 12:19:37[0m] Step: 4331, Training Logs: loss_final: 0.895833, loss_mean: 0.850761, loss_mean_cls: 0.045072, grad_norm: 0.457088
[[34m2025-10-04 12:19:37[0m] Step: 4332, Training Logs: loss_final: 0.884047, loss_mean: 0.838019, loss_mean_cls: 0.046029, grad_norm: 0.639296
[[34m2025-10-04 12:19:38[0m] Step: 4333, Training Logs: loss_final: 0.907151, loss_mean: 0.862104, loss_mean_cls: 0.045048, grad_norm: 0.401873
[[34m2025-10-04 12:19:38[0m] Step: 4334, Training Logs: loss_final: 0.880591, loss_mean: 0.835170, loss_mean_cls: 0.045420, grad_norm: 0.401006
[[34m2025-10-04 12:19:38[0m] Step: 4335, Training Logs: loss_final: 0.898827, loss_mean: 0.854053, loss_mean_cls: 0.044775, grad_norm: 0.494293
[[34m2025-10-04 12:19:39[0m] Step: 4336, Training Logs: loss_final: 0.884922, loss_mean: 0.838596, loss_mean_cls: 0.046326, grad_norm: 0.444161
[[34m2025-10-04 12:19:39[0m] Step: 4337, Training Logs: loss_final: 0.902306, loss_mean: 0.856975, loss_mean_cls: 0.045331, grad_norm: 0.306191
[[34m2025-10-04 12:19:39[0m] Step: 4338, Training Logs: loss_final: 0.888426, loss_mean: 0.841570, loss_mean_cls: 0.046856, grad_norm: 0.418592
[[34m2025-10-04 12:19:39[0m] Step: 4339, Training Logs: loss_final: 0.891884, loss_mean: 0.845826, loss_mean_cls: 0.046058, grad_norm: 0.350382
[[34m2025-10-04 12:19:40[0m] Step: 4340, Training Logs: loss_final: 0.887065, loss_mean: 0.842884, loss_mean_cls: 0.044181, grad_norm: 0.333430
[[34m2025-10-04 12:19:40[0m] Step: 4341, Training Logs: loss_final: 0.909757, loss_mean: 0.863464, loss_mean_cls: 0.046293, grad_norm: 0.346202
[[34m2025-10-04 12:19:40[0m] Step: 4342, Training Logs: loss_final: 0.897990, loss_mean: 0.851464, loss_mean_cls: 0.046526, grad_norm: 0.609699
[[34m2025-10-04 12:19:41[0m] Step: 4343, Training Logs: loss_final: 0.895978, loss_mean: 0.851186, loss_mean_cls: 0.044791, grad_norm: 0.619641
[[34m2025-10-04 12:19:41[0m] Step: 4344, Training Logs: loss_final: 0.892702, loss_mean: 0.847329, loss_mean_cls: 0.045373, grad_norm: 0.317908
[[34m2025-10-04 12:19:41[0m] Step: 4345, Training Logs: loss_final: 0.893667, loss_mean: 0.848076, loss_mean_cls: 0.045591, grad_norm: 0.423688
[[34m2025-10-04 12:19:42[0m] Step: 4346, Training Logs: loss_final: 0.887338, loss_mean: 0.841702, loss_mean_cls: 0.045635, grad_norm: 0.646382
[[34m2025-10-04 12:19:42[0m] Step: 4347, Training Logs: loss_final: 0.903225, loss_mean: 0.858791, loss_mean_cls: 0.044435, grad_norm: 0.395497
[[34m2025-10-04 12:19:42[0m] Step: 4348, Training Logs: loss_final: 0.895239, loss_mean: 0.848436, loss_mean_cls: 0.046803, grad_norm: 0.437929
[[34m2025-10-04 12:19:42[0m] Step: 4349, Training Logs: loss_final: 0.896537, loss_mean: 0.851897, loss_mean_cls: 0.044640, grad_norm: 0.539383
[[34m2025-10-04 12:19:43[0m] Step: 4350, Training Logs: loss_final: 0.904207, loss_mean: 0.859731, loss_mean_cls: 0.044476, grad_norm: 0.473165
[[34m2025-10-04 12:19:43[0m] Step: 4351, Training Logs: loss_final: 0.887746, loss_mean: 0.842861, loss_mean_cls: 0.044885, grad_norm: 0.273441
[[34m2025-10-04 12:19:43[0m] Step: 4352, Training Logs: loss_final: 0.901167, loss_mean: 0.854224, loss_mean_cls: 0.046943, grad_norm: 0.495566
[[34m2025-10-04 12:19:44[0m] Step: 4353, Training Logs: loss_final: 0.901446, loss_mean: 0.855182, loss_mean_cls: 0.046264, grad_norm: 0.515954
[[34m2025-10-04 12:19:44[0m] Step: 4354, Training Logs: loss_final: 0.894519, loss_mean: 0.848492, loss_mean_cls: 0.046028, grad_norm: 0.313558
[[34m2025-10-04 12:19:44[0m] Step: 4355, Training Logs: loss_final: 0.917185, loss_mean: 0.871793, loss_mean_cls: 0.045392, grad_norm: 0.583806
[[34m2025-10-04 12:19:45[0m] Step: 4356, Training Logs: loss_final: 0.884246, loss_mean: 0.838897, loss_mean_cls: 0.045348, grad_norm: 0.323248
[[34m2025-10-04 12:19:45[0m] Step: 4357, Training Logs: loss_final: 0.878863, loss_mean: 0.833787, loss_mean_cls: 0.045077, grad_norm: 0.469311
[[34m2025-10-04 12:19:45[0m] Step: 4358, Training Logs: loss_final: 0.902599, loss_mean: 0.857999, loss_mean_cls: 0.044600, grad_norm: 0.295347
[[34m2025-10-04 12:19:45[0m] Step: 4359, Training Logs: loss_final: 0.896264, loss_mean: 0.850932, loss_mean_cls: 0.045333, grad_norm: 0.341125
[[34m2025-10-04 12:19:46[0m] Step: 4360, Training Logs: loss_final: 0.898378, loss_mean: 0.853590, loss_mean_cls: 0.044788, grad_norm: 0.399088
[[34m2025-10-04 12:19:46[0m] Step: 4361, Training Logs: loss_final: 0.880309, loss_mean: 0.834329, loss_mean_cls: 0.045980, grad_norm: 0.287243
[[34m2025-10-04 12:19:46[0m] Step: 4362, Training Logs: loss_final: 0.899159, loss_mean: 0.854308, loss_mean_cls: 0.044851, grad_norm: 0.466729
[[34m2025-10-04 12:19:47[0m] Step: 4363, Training Logs: loss_final: 0.899182, loss_mean: 0.853850, loss_mean_cls: 0.045332, grad_norm: 0.283697
[[34m2025-10-04 12:19:47[0m] Step: 4364, Training Logs: loss_final: 0.887353, loss_mean: 0.842784, loss_mean_cls: 0.044570, grad_norm: 0.362644
[[34m2025-10-04 12:19:47[0m] Step: 4365, Training Logs: loss_final: 0.891676, loss_mean: 0.846479, loss_mean_cls: 0.045198, grad_norm: 0.336267
[[34m2025-10-04 12:19:47[0m] Step: 4366, Training Logs: loss_final: 0.915213, loss_mean: 0.870766, loss_mean_cls: 0.044447, grad_norm: 0.294412
[[34m2025-10-04 12:19:48[0m] Step: 4367, Training Logs: loss_final: 0.902284, loss_mean: 0.857878, loss_mean_cls: 0.044406, grad_norm: 0.301200
[[34m2025-10-04 12:19:48[0m] Step: 4368, Training Logs: loss_final: 0.884475, loss_mean: 0.838927, loss_mean_cls: 0.045548, grad_norm: 0.433937
[[34m2025-10-04 12:19:48[0m] Step: 4369, Training Logs: loss_final: 0.886900, loss_mean: 0.841284, loss_mean_cls: 0.045616, grad_norm: 0.335266
[[34m2025-10-04 12:19:49[0m] Step: 4370, Training Logs: loss_final: 0.861396, loss_mean: 0.816502, loss_mean_cls: 0.044895, grad_norm: 0.436503
[[34m2025-10-04 12:19:49[0m] Step: 4371, Training Logs: loss_final: 0.883729, loss_mean: 0.837222, loss_mean_cls: 0.046507, grad_norm: 0.377283
[[34m2025-10-04 12:19:49[0m] Step: 4372, Training Logs: loss_final: 0.904586, loss_mean: 0.859862, loss_mean_cls: 0.044724, grad_norm: 0.278128
[[34m2025-10-04 12:19:50[0m] Step: 4373, Training Logs: loss_final: 0.892072, loss_mean: 0.846809, loss_mean_cls: 0.045263, grad_norm: 0.590264
[[34m2025-10-04 12:19:50[0m] Step: 4374, Training Logs: loss_final: 0.900377, loss_mean: 0.856178, loss_mean_cls: 0.044199, grad_norm: 0.337250
[[34m2025-10-04 12:19:50[0m] Step: 4375, Training Logs: loss_final: 0.891891, loss_mean: 0.845260, loss_mean_cls: 0.046631, grad_norm: 0.636566
[[34m2025-10-04 12:19:50[0m] Step: 4376, Training Logs: loss_final: 0.889125, loss_mean: 0.842589, loss_mean_cls: 0.046536, grad_norm: 0.400234
[[34m2025-10-04 12:19:51[0m] Step: 4377, Training Logs: loss_final: 0.897648, loss_mean: 0.852333, loss_mean_cls: 0.045315, grad_norm: 0.467972
[[34m2025-10-04 12:19:51[0m] Step: 4378, Training Logs: loss_final: 0.892932, loss_mean: 0.846806, loss_mean_cls: 0.046126, grad_norm: 0.554394
[[34m2025-10-04 12:19:51[0m] Step: 4379, Training Logs: loss_final: 0.886068, loss_mean: 0.840530, loss_mean_cls: 0.045538, grad_norm: 0.389069
[[34m2025-10-04 12:19:52[0m] Step: 4380, Training Logs: loss_final: 0.893730, loss_mean: 0.848245, loss_mean_cls: 0.045485, grad_norm: 0.457718
[[34m2025-10-04 12:19:52[0m] Step: 4381, Training Logs: loss_final: 0.883960, loss_mean: 0.838812, loss_mean_cls: 0.045149, grad_norm: 0.384077
[[34m2025-10-04 12:19:52[0m] Step: 4382, Training Logs: loss_final: 0.908965, loss_mean: 0.863377, loss_mean_cls: 0.045588, grad_norm: 0.392029
[[34m2025-10-04 12:19:53[0m] Step: 4383, Training Logs: loss_final: 0.889667, loss_mean: 0.844918, loss_mean_cls: 0.044749, grad_norm: 0.460296
[[34m2025-10-04 12:19:53[0m] Step: 4384, Training Logs: loss_final: 0.883867, loss_mean: 0.838462, loss_mean_cls: 0.045405, grad_norm: 0.373172
[[34m2025-10-04 12:19:53[0m] Step: 4385, Training Logs: loss_final: 0.873986, loss_mean: 0.828869, loss_mean_cls: 0.045117, grad_norm: 0.399027
[[34m2025-10-04 12:19:53[0m] Step: 4386, Training Logs: loss_final: 0.895023, loss_mean: 0.848578, loss_mean_cls: 0.046445, grad_norm: 0.321709
[[34m2025-10-04 12:19:54[0m] Step: 4387, Training Logs: loss_final: 0.897389, loss_mean: 0.853116, loss_mean_cls: 0.044274, grad_norm: 0.438819
[[34m2025-10-04 12:19:54[0m] Step: 4388, Training Logs: loss_final: 0.890398, loss_mean: 0.844019, loss_mean_cls: 0.046379, grad_norm: 0.366242
[[34m2025-10-04 12:19:54[0m] Step: 4389, Training Logs: loss_final: 0.896917, loss_mean: 0.851171, loss_mean_cls: 0.045745, grad_norm: 0.334273
[[34m2025-10-04 12:19:55[0m] Step: 4390, Training Logs: loss_final: 0.893989, loss_mean: 0.849273, loss_mean_cls: 0.044716, grad_norm: 0.399219
[[34m2025-10-04 12:19:55[0m] Step: 4391, Training Logs: loss_final: 0.894647, loss_mean: 0.849907, loss_mean_cls: 0.044740, grad_norm: 0.458193
[[34m2025-10-04 12:19:55[0m] Step: 4392, Training Logs: loss_final: 0.901891, loss_mean: 0.857365, loss_mean_cls: 0.044527, grad_norm: 0.295634
[[34m2025-10-04 12:19:55[0m] Step: 4393, Training Logs: loss_final: 0.894857, loss_mean: 0.849069, loss_mean_cls: 0.045788, grad_norm: 0.276829
[[34m2025-10-04 12:19:56[0m] Step: 4394, Training Logs: loss_final: 0.899665, loss_mean: 0.856466, loss_mean_cls: 0.043199, grad_norm: 0.391834
[[34m2025-10-04 12:19:56[0m] Step: 4395, Training Logs: loss_final: 0.896858, loss_mean: 0.852434, loss_mean_cls: 0.044424, grad_norm: 0.292664
[[34m2025-10-04 12:19:56[0m] Step: 4396, Training Logs: loss_final: 0.893297, loss_mean: 0.847564, loss_mean_cls: 0.045733, grad_norm: 0.312805
[[34m2025-10-04 12:19:57[0m] Step: 4397, Training Logs: loss_final: 0.882901, loss_mean: 0.837398, loss_mean_cls: 0.045503, grad_norm: 0.398741
[[34m2025-10-04 12:19:57[0m] Step: 4398, Training Logs: loss_final: 0.885041, loss_mean: 0.838675, loss_mean_cls: 0.046367, grad_norm: 0.335920
[[34m2025-10-04 12:19:57[0m] Step: 4399, Training Logs: loss_final: 0.902092, loss_mean: 0.857028, loss_mean_cls: 0.045065, grad_norm: 0.274530
[[34m2025-10-04 12:19:58[0m] Step: 4400, Training Logs: loss_final: 0.879591, loss_mean: 0.833748, loss_mean_cls: 0.045843, grad_norm: 0.371046
[[34m2025-10-04 12:19:58[0m] Step: 4401, Training Logs: loss_final: 0.872049, loss_mean: 0.826060, loss_mean_cls: 0.045989, grad_norm: 0.430565
[[34m2025-10-04 12:19:58[0m] Step: 4402, Training Logs: loss_final: 0.903670, loss_mean: 0.857200, loss_mean_cls: 0.046470, grad_norm: 0.333896
[[34m2025-10-04 12:19:58[0m] Step: 4403, Training Logs: loss_final: 0.902774, loss_mean: 0.856087, loss_mean_cls: 0.046688, grad_norm: 0.371945
[[34m2025-10-04 12:19:59[0m] Step: 4404, Training Logs: loss_final: 0.917976, loss_mean: 0.873575, loss_mean_cls: 0.044401, grad_norm: 0.335687
[[34m2025-10-04 12:19:59[0m] Step: 4405, Training Logs: loss_final: 0.894051, loss_mean: 0.848701, loss_mean_cls: 0.045350, grad_norm: 0.358325
[[34m2025-10-04 12:19:59[0m] Step: 4406, Training Logs: loss_final: 0.881436, loss_mean: 0.835995, loss_mean_cls: 0.045441, grad_norm: 0.318815
[[34m2025-10-04 12:20:00[0m] Step: 4407, Training Logs: loss_final: 0.884087, loss_mean: 0.838554, loss_mean_cls: 0.045534, grad_norm: 0.381778
[[34m2025-10-04 12:20:00[0m] Step: 4408, Training Logs: loss_final: 0.881666, loss_mean: 0.836573, loss_mean_cls: 0.045093, grad_norm: 0.403254
[[34m2025-10-04 12:20:00[0m] Step: 4409, Training Logs: loss_final: 0.891456, loss_mean: 0.845829, loss_mean_cls: 0.045627, grad_norm: 0.252030
[[34m2025-10-04 12:20:00[0m] Step: 4410, Training Logs: loss_final: 0.896838, loss_mean: 0.850817, loss_mean_cls: 0.046022, grad_norm: 0.426887
[[34m2025-10-04 12:20:01[0m] Step: 4411, Training Logs: loss_final: 0.900826, loss_mean: 0.855583, loss_mean_cls: 0.045243, grad_norm: 0.424851
[[34m2025-10-04 12:20:01[0m] Step: 4412, Training Logs: loss_final: 0.905183, loss_mean: 0.860358, loss_mean_cls: 0.044825, grad_norm: 0.318014
[[34m2025-10-04 12:20:01[0m] Step: 4413, Training Logs: loss_final: 0.876286, loss_mean: 0.830266, loss_mean_cls: 0.046020, grad_norm: 0.354288
[[34m2025-10-04 12:20:02[0m] Step: 4414, Training Logs: loss_final: 0.906087, loss_mean: 0.860715, loss_mean_cls: 0.045372, grad_norm: 0.340143
[[34m2025-10-04 12:20:02[0m] Step: 4415, Training Logs: loss_final: 0.889603, loss_mean: 0.844481, loss_mean_cls: 0.045122, grad_norm: 0.335999
[[34m2025-10-04 12:20:02[0m] Step: 4416, Training Logs: loss_final: 0.891851, loss_mean: 0.846368, loss_mean_cls: 0.045483, grad_norm: 0.328937
[[34m2025-10-04 12:20:03[0m] Step: 4417, Training Logs: loss_final: 0.866079, loss_mean: 0.819669, loss_mean_cls: 0.046409, grad_norm: 0.504519
[[34m2025-10-04 12:20:03[0m] Step: 4418, Training Logs: loss_final: 0.896637, loss_mean: 0.850415, loss_mean_cls: 0.046222, grad_norm: 0.386364
[[34m2025-10-04 12:20:03[0m] Step: 4419, Training Logs: loss_final: 0.883486, loss_mean: 0.837451, loss_mean_cls: 0.046035, grad_norm: 0.349491
[[34m2025-10-04 12:20:03[0m] Step: 4420, Training Logs: loss_final: 0.902865, loss_mean: 0.858325, loss_mean_cls: 0.044540, grad_norm: 0.315995
[[34m2025-10-04 12:20:04[0m] Step: 4421, Training Logs: loss_final: 0.895084, loss_mean: 0.849870, loss_mean_cls: 0.045214, grad_norm: 0.356406
[[34m2025-10-04 12:20:04[0m] Step: 4422, Training Logs: loss_final: 0.897299, loss_mean: 0.853589, loss_mean_cls: 0.043710, grad_norm: 0.295462
[[34m2025-10-04 12:20:04[0m] Step: 4423, Training Logs: loss_final: 0.913355, loss_mean: 0.867807, loss_mean_cls: 0.045548, grad_norm: 0.360364
[[34m2025-10-04 12:20:05[0m] Step: 4424, Training Logs: loss_final: 0.895656, loss_mean: 0.850014, loss_mean_cls: 0.045642, grad_norm: 0.324303
[[34m2025-10-04 12:20:05[0m] Step: 4425, Training Logs: loss_final: 0.882371, loss_mean: 0.837279, loss_mean_cls: 0.045091, grad_norm: 0.303319
[[34m2025-10-04 12:20:05[0m] Step: 4426, Training Logs: loss_final: 0.902840, loss_mean: 0.858173, loss_mean_cls: 0.044666, grad_norm: 0.266379
[[34m2025-10-04 12:20:06[0m] Step: 4427, Training Logs: loss_final: 0.886868, loss_mean: 0.842745, loss_mean_cls: 0.044124, grad_norm: 0.455149
[[34m2025-10-04 12:20:06[0m] Step: 4428, Training Logs: loss_final: 0.901024, loss_mean: 0.855520, loss_mean_cls: 0.045504, grad_norm: 0.319780
[[34m2025-10-04 12:20:06[0m] Step: 4429, Training Logs: loss_final: 0.886071, loss_mean: 0.839949, loss_mean_cls: 0.046121, grad_norm: 0.506546
[[34m2025-10-04 12:20:06[0m] Step: 4430, Training Logs: loss_final: 0.885487, loss_mean: 0.840624, loss_mean_cls: 0.044863, grad_norm: 0.426584
[[34m2025-10-04 12:20:07[0m] Step: 4431, Training Logs: loss_final: 0.895755, loss_mean: 0.849993, loss_mean_cls: 0.045763, grad_norm: 0.354476
[[34m2025-10-04 12:20:07[0m] Step: 4432, Training Logs: loss_final: 0.893955, loss_mean: 0.847570, loss_mean_cls: 0.046385, grad_norm: 0.345138
[[34m2025-10-04 12:20:07[0m] Step: 4433, Training Logs: loss_final: 0.904720, loss_mean: 0.859671, loss_mean_cls: 0.045049, grad_norm: 0.258458
[[34m2025-10-04 12:20:08[0m] Step: 4434, Training Logs: loss_final: 0.892115, loss_mean: 0.846961, loss_mean_cls: 0.045154, grad_norm: 0.340456
[[34m2025-10-04 12:20:08[0m] Step: 4435, Training Logs: loss_final: 0.889794, loss_mean: 0.844039, loss_mean_cls: 0.045756, grad_norm: 0.300899
[[34m2025-10-04 12:20:08[0m] Step: 4436, Training Logs: loss_final: 0.897122, loss_mean: 0.851653, loss_mean_cls: 0.045470, grad_norm: 0.302278
[[34m2025-10-04 12:20:08[0m] Step: 4437, Training Logs: loss_final: 0.911295, loss_mean: 0.866641, loss_mean_cls: 0.044654, grad_norm: 0.366717
[[34m2025-10-04 12:20:09[0m] Step: 4438, Training Logs: loss_final: 0.914351, loss_mean: 0.868813, loss_mean_cls: 0.045539, grad_norm: 0.376911
[[34m2025-10-04 12:20:09[0m] Step: 4439, Training Logs: loss_final: 0.890826, loss_mean: 0.845185, loss_mean_cls: 0.045641, grad_norm: 0.512367
[[34m2025-10-04 12:20:09[0m] Step: 4440, Training Logs: loss_final: 0.887855, loss_mean: 0.842165, loss_mean_cls: 0.045690, grad_norm: 0.399501
[[34m2025-10-04 12:20:10[0m] Step: 4441, Training Logs: loss_final: 0.910726, loss_mean: 0.866009, loss_mean_cls: 0.044717, grad_norm: 0.369697
[[34m2025-10-04 12:20:10[0m] Step: 4442, Training Logs: loss_final: 0.891507, loss_mean: 0.846976, loss_mean_cls: 0.044531, grad_norm: 0.321235
[[34m2025-10-04 12:20:10[0m] Step: 4443, Training Logs: loss_final: 0.869949, loss_mean: 0.823881, loss_mean_cls: 0.046068, grad_norm: 0.383143
[[34m2025-10-04 12:20:11[0m] Step: 4444, Training Logs: loss_final: 0.885697, loss_mean: 0.840697, loss_mean_cls: 0.045000, grad_norm: 0.423579
[[34m2025-10-04 12:20:11[0m] Step: 4445, Training Logs: loss_final: 0.897549, loss_mean: 0.852079, loss_mean_cls: 0.045470, grad_norm: 0.425024
[[34m2025-10-04 12:20:11[0m] Step: 4446, Training Logs: loss_final: 0.899325, loss_mean: 0.855184, loss_mean_cls: 0.044142, grad_norm: 0.447729
[[34m2025-10-04 12:20:12[0m] Step: 4447, Training Logs: loss_final: 0.897218, loss_mean: 0.850821, loss_mean_cls: 0.046397, grad_norm: 0.437254
[[34m2025-10-04 12:20:12[0m] Step: 4448, Training Logs: loss_final: 0.894302, loss_mean: 0.850098, loss_mean_cls: 0.044203, grad_norm: 0.431515
[[34m2025-10-04 12:20:12[0m] Step: 4449, Training Logs: loss_final: 0.900676, loss_mean: 0.855116, loss_mean_cls: 0.045560, grad_norm: 0.475314
[[34m2025-10-04 12:20:12[0m] Step: 4450, Training Logs: loss_final: 0.888110, loss_mean: 0.842725, loss_mean_cls: 0.045385, grad_norm: 0.419294
[[34m2025-10-04 12:20:13[0m] Step: 4451, Training Logs: loss_final: 0.886393, loss_mean: 0.841447, loss_mean_cls: 0.044946, grad_norm: 0.613828
[[34m2025-10-04 12:20:13[0m] Step: 4452, Training Logs: loss_final: 0.904898, loss_mean: 0.860296, loss_mean_cls: 0.044602, grad_norm: 0.420130
[[34m2025-10-04 12:20:13[0m] Step: 4453, Training Logs: loss_final: 0.892304, loss_mean: 0.847909, loss_mean_cls: 0.044394, grad_norm: 0.419067
[[34m2025-10-04 12:20:14[0m] Step: 4454, Training Logs: loss_final: 0.914727, loss_mean: 0.868776, loss_mean_cls: 0.045951, grad_norm: 0.478539
[[34m2025-10-04 12:20:14[0m] Step: 4455, Training Logs: loss_final: 0.892631, loss_mean: 0.847609, loss_mean_cls: 0.045022, grad_norm: 0.360285
[[34m2025-10-04 12:20:14[0m] Step: 4456, Training Logs: loss_final: 0.888555, loss_mean: 0.843295, loss_mean_cls: 0.045261, grad_norm: 0.459463
[[34m2025-10-04 12:20:14[0m] Step: 4457, Training Logs: loss_final: 0.888597, loss_mean: 0.843318, loss_mean_cls: 0.045279, grad_norm: 0.392581
[[34m2025-10-04 12:20:15[0m] Step: 4458, Training Logs: loss_final: 0.900828, loss_mean: 0.855986, loss_mean_cls: 0.044842, grad_norm: 0.371099
[[34m2025-10-04 12:20:15[0m] Step: 4459, Training Logs: loss_final: 0.914835, loss_mean: 0.868685, loss_mean_cls: 0.046149, grad_norm: 0.419404
[[34m2025-10-04 12:20:15[0m] Step: 4460, Training Logs: loss_final: 0.905945, loss_mean: 0.860441, loss_mean_cls: 0.045503, grad_norm: 0.535445
[[34m2025-10-04 12:20:16[0m] Step: 4461, Training Logs: loss_final: 0.910669, loss_mean: 0.865909, loss_mean_cls: 0.044760, grad_norm: 0.439220
[[34m2025-10-04 12:20:16[0m] Step: 4462, Training Logs: loss_final: 0.885584, loss_mean: 0.840138, loss_mean_cls: 0.045446, grad_norm: 0.531918
[[34m2025-10-04 12:20:16[0m] Step: 4463, Training Logs: loss_final: 0.903369, loss_mean: 0.858004, loss_mean_cls: 0.045365, grad_norm: 0.335574
[[34m2025-10-04 12:20:17[0m] Step: 4464, Training Logs: loss_final: 0.876934, loss_mean: 0.831615, loss_mean_cls: 0.045319, grad_norm: 0.418855
[[34m2025-10-04 12:20:17[0m] Step: 4465, Training Logs: loss_final: 0.898615, loss_mean: 0.852938, loss_mean_cls: 0.045677, grad_norm: 0.425576
[[34m2025-10-04 12:20:17[0m] Step: 4466, Training Logs: loss_final: 0.878972, loss_mean: 0.833531, loss_mean_cls: 0.045441, grad_norm: 0.491199
[[34m2025-10-04 12:20:17[0m] Step: 4467, Training Logs: loss_final: 0.905675, loss_mean: 0.859658, loss_mean_cls: 0.046016, grad_norm: 0.345249
[[34m2025-10-04 12:20:18[0m] Step: 4468, Training Logs: loss_final: 0.890709, loss_mean: 0.845260, loss_mean_cls: 0.045448, grad_norm: 0.479049
[[34m2025-10-04 12:20:18[0m] Step: 4469, Training Logs: loss_final: 0.909022, loss_mean: 0.864008, loss_mean_cls: 0.045014, grad_norm: 0.353377
[[34m2025-10-04 12:20:18[0m] Step: 4470, Training Logs: loss_final: 0.894069, loss_mean: 0.849482, loss_mean_cls: 0.044587, grad_norm: 0.493900
[[34m2025-10-04 12:20:19[0m] Step: 4471, Training Logs: loss_final: 0.918812, loss_mean: 0.874773, loss_mean_cls: 0.044039, grad_norm: 0.505838
[[34m2025-10-04 12:20:19[0m] Step: 4472, Training Logs: loss_final: 0.887483, loss_mean: 0.841911, loss_mean_cls: 0.045572, grad_norm: 0.422839
[[34m2025-10-04 12:20:19[0m] Step: 4473, Training Logs: loss_final: 0.892412, loss_mean: 0.847894, loss_mean_cls: 0.044518, grad_norm: 0.316983
[[34m2025-10-04 12:20:19[0m] Step: 4474, Training Logs: loss_final: 0.895520, loss_mean: 0.850317, loss_mean_cls: 0.045204, grad_norm: 0.308729
[[34m2025-10-04 12:20:20[0m] Step: 4475, Training Logs: loss_final: 0.870687, loss_mean: 0.824467, loss_mean_cls: 0.046219, grad_norm: 0.420237
[[34m2025-10-04 12:20:20[0m] Step: 4476, Training Logs: loss_final: 0.901501, loss_mean: 0.855859, loss_mean_cls: 0.045642, grad_norm: 0.255045
[[34m2025-10-04 12:20:20[0m] Step: 4477, Training Logs: loss_final: 0.875969, loss_mean: 0.829891, loss_mean_cls: 0.046078, grad_norm: 0.520023
[[34m2025-10-04 12:20:21[0m] Step: 4478, Training Logs: loss_final: 0.898799, loss_mean: 0.854214, loss_mean_cls: 0.044586, grad_norm: 0.264560
[[34m2025-10-04 12:20:21[0m] Step: 4479, Training Logs: loss_final: 0.904435, loss_mean: 0.858686, loss_mean_cls: 0.045749, grad_norm: 0.488320
[[34m2025-10-04 12:20:21[0m] Step: 4480, Training Logs: loss_final: 0.884095, loss_mean: 0.838808, loss_mean_cls: 0.045286, grad_norm: 0.406620
[[34m2025-10-04 12:20:22[0m] Step: 4481, Training Logs: loss_final: 0.884289, loss_mean: 0.838984, loss_mean_cls: 0.045305, grad_norm: 0.277008
[[34m2025-10-04 12:20:22[0m] Step: 4482, Training Logs: loss_final: 0.896673, loss_mean: 0.851555, loss_mean_cls: 0.045119, grad_norm: 0.479791
[[34m2025-10-04 12:20:22[0m] Step: 4483, Training Logs: loss_final: 0.906451, loss_mean: 0.860407, loss_mean_cls: 0.046044, grad_norm: 0.376742
[[34m2025-10-04 12:20:22[0m] Step: 4484, Training Logs: loss_final: 0.893281, loss_mean: 0.848931, loss_mean_cls: 0.044349, grad_norm: 0.331280
[[34m2025-10-04 12:20:23[0m] Step: 4485, Training Logs: loss_final: 0.890716, loss_mean: 0.844293, loss_mean_cls: 0.046424, grad_norm: 0.360106
[[34m2025-10-04 12:20:23[0m] Step: 4486, Training Logs: loss_final: 0.901536, loss_mean: 0.856059, loss_mean_cls: 0.045477, grad_norm: 0.319920
[[34m2025-10-04 12:20:23[0m] Step: 4487, Training Logs: loss_final: 0.889016, loss_mean: 0.843063, loss_mean_cls: 0.045953, grad_norm: 0.289901
[[34m2025-10-04 12:20:24[0m] Step: 4488, Training Logs: loss_final: 0.876171, loss_mean: 0.829855, loss_mean_cls: 0.046316, grad_norm: 0.289485
[[34m2025-10-04 12:20:24[0m] Step: 4489, Training Logs: loss_final: 0.866750, loss_mean: 0.820993, loss_mean_cls: 0.045757, grad_norm: 0.325786
[[34m2025-10-04 12:20:24[0m] Step: 4490, Training Logs: loss_final: 0.901097, loss_mean: 0.855895, loss_mean_cls: 0.045203, grad_norm: 0.350505
[[34m2025-10-04 12:20:24[0m] Step: 4491, Training Logs: loss_final: 0.883102, loss_mean: 0.838172, loss_mean_cls: 0.044930, grad_norm: 0.393636
[[34m2025-10-04 12:20:25[0m] Step: 4492, Training Logs: loss_final: 0.906951, loss_mean: 0.862129, loss_mean_cls: 0.044822, grad_norm: 0.300921
[[34m2025-10-04 12:20:25[0m] Step: 4493, Training Logs: loss_final: 0.898435, loss_mean: 0.852883, loss_mean_cls: 0.045552, grad_norm: 0.401599
[[34m2025-10-04 12:20:25[0m] Step: 4494, Training Logs: loss_final: 0.876144, loss_mean: 0.831105, loss_mean_cls: 0.045039, grad_norm: 0.489556
[[34m2025-10-04 12:20:26[0m] Step: 4495, Training Logs: loss_final: 0.882447, loss_mean: 0.838032, loss_mean_cls: 0.044415, grad_norm: 0.460892
[[34m2025-10-04 12:20:26[0m] Step: 4496, Training Logs: loss_final: 0.901815, loss_mean: 0.856415, loss_mean_cls: 0.045400, grad_norm: 0.452403
[[34m2025-10-04 12:20:26[0m] Step: 4497, Training Logs: loss_final: 0.906861, loss_mean: 0.862033, loss_mean_cls: 0.044828, grad_norm: 0.478475
[[34m2025-10-04 12:20:26[0m] Step: 4498, Training Logs: loss_final: 0.912992, loss_mean: 0.868279, loss_mean_cls: 0.044712, grad_norm: 0.534287
[[34m2025-10-04 12:20:27[0m] Step: 4499, Training Logs: loss_final: 0.895622, loss_mean: 0.849851, loss_mean_cls: 0.045771, grad_norm: 0.435119
[[34m2025-10-04 12:20:27[0m] Step: 4500, Training Logs: loss_final: 0.901307, loss_mean: 0.856014, loss_mean_cls: 0.045293, grad_norm: 0.373931
[[34m2025-10-04 12:20:27[0m] Step: 4501, Training Logs: loss_final: 0.882967, loss_mean: 0.837462, loss_mean_cls: 0.045506, grad_norm: 0.473271
[[34m2025-10-04 12:20:28[0m] Step: 4502, Training Logs: loss_final: 0.878948, loss_mean: 0.831887, loss_mean_cls: 0.047062, grad_norm: 0.473083
[[34m2025-10-04 12:20:28[0m] Step: 4503, Training Logs: loss_final: 0.881686, loss_mean: 0.837408, loss_mean_cls: 0.044278, grad_norm: 0.448867
[[34m2025-10-04 12:20:28[0m] Step: 4504, Training Logs: loss_final: 0.912627, loss_mean: 0.868347, loss_mean_cls: 0.044281, grad_norm: 0.293379
[[34m2025-10-04 12:20:29[0m] Step: 4505, Training Logs: loss_final: 0.905123, loss_mean: 0.860741, loss_mean_cls: 0.044382, grad_norm: 0.454656
[[34m2025-10-04 12:20:29[0m] Step: 4506, Training Logs: loss_final: 0.907041, loss_mean: 0.862173, loss_mean_cls: 0.044868, grad_norm: 0.571494
[[34m2025-10-04 12:20:29[0m] Step: 4507, Training Logs: loss_final: 0.897243, loss_mean: 0.851681, loss_mean_cls: 0.045561, grad_norm: 0.285129
[[34m2025-10-04 12:20:29[0m] Step: 4508, Training Logs: loss_final: 0.900455, loss_mean: 0.854137, loss_mean_cls: 0.046318, grad_norm: 0.446553
[[34m2025-10-04 12:20:30[0m] Step: 4509, Training Logs: loss_final: 0.876559, loss_mean: 0.829916, loss_mean_cls: 0.046643, grad_norm: 0.382963
[[34m2025-10-04 12:20:30[0m] Step: 4510, Training Logs: loss_final: 0.905180, loss_mean: 0.860209, loss_mean_cls: 0.044972, grad_norm: 0.519525
[[34m2025-10-04 12:20:30[0m] Step: 4511, Training Logs: loss_final: 0.905706, loss_mean: 0.860491, loss_mean_cls: 0.045215, grad_norm: 0.457318
[[34m2025-10-04 12:20:31[0m] Step: 4512, Training Logs: loss_final: 0.902951, loss_mean: 0.857780, loss_mean_cls: 0.045171, grad_norm: 0.278396
[[34m2025-10-04 12:20:31[0m] Step: 4513, Training Logs: loss_final: 0.889265, loss_mean: 0.842892, loss_mean_cls: 0.046372, grad_norm: 0.427440
[[34m2025-10-04 12:20:31[0m] Step: 4514, Training Logs: loss_final: 0.903762, loss_mean: 0.858190, loss_mean_cls: 0.045572, grad_norm: 0.325612
[[34m2025-10-04 12:20:31[0m] Step: 4515, Training Logs: loss_final: 0.886160, loss_mean: 0.841211, loss_mean_cls: 0.044949, grad_norm: 0.449679
[[34m2025-10-04 12:20:32[0m] Step: 4516, Training Logs: loss_final: 0.910218, loss_mean: 0.865683, loss_mean_cls: 0.044535, grad_norm: 0.361357
[[34m2025-10-04 12:20:32[0m] Step: 4517, Training Logs: loss_final: 0.884711, loss_mean: 0.838705, loss_mean_cls: 0.046006, grad_norm: 0.263650
[[34m2025-10-04 12:20:32[0m] Step: 4518, Training Logs: loss_final: 0.901451, loss_mean: 0.856208, loss_mean_cls: 0.045243, grad_norm: 0.514383
[[34m2025-10-04 12:20:33[0m] Step: 4519, Training Logs: loss_final: 0.886984, loss_mean: 0.840676, loss_mean_cls: 0.046308, grad_norm: 0.342630
[[34m2025-10-04 12:20:33[0m] Step: 4520, Training Logs: loss_final: 0.903713, loss_mean: 0.858167, loss_mean_cls: 0.045546, grad_norm: 0.413113
[[34m2025-10-04 12:20:33[0m] Step: 4521, Training Logs: loss_final: 0.884921, loss_mean: 0.839795, loss_mean_cls: 0.045126, grad_norm: 0.365278
[[34m2025-10-04 12:20:33[0m] Step: 4522, Training Logs: loss_final: 0.897682, loss_mean: 0.853274, loss_mean_cls: 0.044409, grad_norm: 0.304619
[[34m2025-10-04 12:20:34[0m] Step: 4523, Training Logs: loss_final: 0.908490, loss_mean: 0.862362, loss_mean_cls: 0.046129, grad_norm: 0.339738
[[34m2025-10-04 12:20:34[0m] Step: 4524, Training Logs: loss_final: 0.882407, loss_mean: 0.837369, loss_mean_cls: 0.045038, grad_norm: 0.483124
[[34m2025-10-04 12:20:34[0m] Step: 4525, Training Logs: loss_final: 0.882817, loss_mean: 0.836693, loss_mean_cls: 0.046124, grad_norm: 0.314222
[[34m2025-10-04 12:20:35[0m] Step: 4526, Training Logs: loss_final: 0.874446, loss_mean: 0.829908, loss_mean_cls: 0.044538, grad_norm: 0.327330
[[34m2025-10-04 12:20:35[0m] Step: 4527, Training Logs: loss_final: 0.889422, loss_mean: 0.843493, loss_mean_cls: 0.045928, grad_norm: 0.329120
[[34m2025-10-04 12:20:35[0m] Step: 4528, Training Logs: loss_final: 0.898018, loss_mean: 0.852530, loss_mean_cls: 0.045488, grad_norm: 0.294721
[[34m2025-10-04 12:20:36[0m] Step: 4529, Training Logs: loss_final: 0.884240, loss_mean: 0.839680, loss_mean_cls: 0.044560, grad_norm: 0.299362
[[34m2025-10-04 12:20:36[0m] Step: 4530, Training Logs: loss_final: 0.908642, loss_mean: 0.863638, loss_mean_cls: 0.045004, grad_norm: 0.403142
[[34m2025-10-04 12:20:36[0m] Step: 4531, Training Logs: loss_final: 0.905455, loss_mean: 0.861306, loss_mean_cls: 0.044149, grad_norm: 0.337697
[[34m2025-10-04 12:20:36[0m] Step: 4532, Training Logs: loss_final: 0.876521, loss_mean: 0.831206, loss_mean_cls: 0.045315, grad_norm: 0.377469
[[34m2025-10-04 12:20:37[0m] Step: 4533, Training Logs: loss_final: 0.911811, loss_mean: 0.868318, loss_mean_cls: 0.043494, grad_norm: 0.296859
[[34m2025-10-04 12:20:37[0m] Step: 4534, Training Logs: loss_final: 0.887897, loss_mean: 0.842572, loss_mean_cls: 0.045325, grad_norm: 0.391224
[[34m2025-10-04 12:20:37[0m] Step: 4535, Training Logs: loss_final: 0.866233, loss_mean: 0.820144, loss_mean_cls: 0.046089, grad_norm: 0.396856
[[34m2025-10-04 12:20:38[0m] Step: 4536, Training Logs: loss_final: 0.891961, loss_mean: 0.846061, loss_mean_cls: 0.045900, grad_norm: 0.347526
[[34m2025-10-04 12:20:38[0m] Step: 4537, Training Logs: loss_final: 0.881381, loss_mean: 0.835596, loss_mean_cls: 0.045785, grad_norm: 0.339439
[[34m2025-10-04 12:20:38[0m] Step: 4538, Training Logs: loss_final: 0.886584, loss_mean: 0.842364, loss_mean_cls: 0.044220, grad_norm: 0.372345
[[34m2025-10-04 12:20:38[0m] Step: 4539, Training Logs: loss_final: 0.899343, loss_mean: 0.853657, loss_mean_cls: 0.045686, grad_norm: 0.302098
[[34m2025-10-04 12:20:39[0m] Step: 4540, Training Logs: loss_final: 0.884784, loss_mean: 0.837956, loss_mean_cls: 0.046828, grad_norm: 0.252831
[[34m2025-10-04 12:20:39[0m] Step: 4541, Training Logs: loss_final: 0.904112, loss_mean: 0.859030, loss_mean_cls: 0.045083, grad_norm: 0.341953
[[34m2025-10-04 12:20:39[0m] Step: 4542, Training Logs: loss_final: 0.907984, loss_mean: 0.863895, loss_mean_cls: 0.044089, grad_norm: 0.368520
[[34m2025-10-04 12:20:40[0m] Step: 4543, Training Logs: loss_final: 0.906285, loss_mean: 0.861705, loss_mean_cls: 0.044580, grad_norm: 0.373323
[[34m2025-10-04 12:20:40[0m] Step: 4544, Training Logs: loss_final: 0.886028, loss_mean: 0.841160, loss_mean_cls: 0.044868, grad_norm: 0.301046
[[34m2025-10-04 12:20:40[0m] Step: 4545, Training Logs: loss_final: 0.886530, loss_mean: 0.840841, loss_mean_cls: 0.045689, grad_norm: 0.436930
[[34m2025-10-04 12:20:41[0m] Step: 4546, Training Logs: loss_final: 0.890384, loss_mean: 0.845678, loss_mean_cls: 0.044706, grad_norm: 0.445216
[[34m2025-10-04 12:20:41[0m] Step: 4547, Training Logs: loss_final: 0.905383, loss_mean: 0.860647, loss_mean_cls: 0.044736, grad_norm: 0.324507
[[34m2025-10-04 12:20:41[0m] Step: 4548, Training Logs: loss_final: 0.904360, loss_mean: 0.860358, loss_mean_cls: 0.044002, grad_norm: 0.541323
[[34m2025-10-04 12:20:41[0m] Step: 4549, Training Logs: loss_final: 0.882963, loss_mean: 0.838539, loss_mean_cls: 0.044424, grad_norm: 0.308755
[[34m2025-10-04 12:20:42[0m] Step: 4550, Training Logs: loss_final: 0.878581, loss_mean: 0.834487, loss_mean_cls: 0.044094, grad_norm: 0.346373
[[34m2025-10-04 12:20:42[0m] Step: 4551, Training Logs: loss_final: 0.894454, loss_mean: 0.849705, loss_mean_cls: 0.044749, grad_norm: 0.374726
[[34m2025-10-04 12:20:42[0m] Step: 4552, Training Logs: loss_final: 0.896318, loss_mean: 0.850516, loss_mean_cls: 0.045802, grad_norm: 0.494597
[[34m2025-10-04 12:20:43[0m] Step: 4553, Training Logs: loss_final: 0.901893, loss_mean: 0.856786, loss_mean_cls: 0.045107, grad_norm: 0.367921
[[34m2025-10-04 12:20:43[0m] Step: 4554, Training Logs: loss_final: 0.895475, loss_mean: 0.849333, loss_mean_cls: 0.046142, grad_norm: 0.320884
[[34m2025-10-04 12:20:43[0m] Step: 4555, Training Logs: loss_final: 0.885825, loss_mean: 0.839891, loss_mean_cls: 0.045934, grad_norm: 0.454279
[[34m2025-10-04 12:20:43[0m] Step: 4556, Training Logs: loss_final: 0.887612, loss_mean: 0.842378, loss_mean_cls: 0.045234, grad_norm: 0.298624
[[34m2025-10-04 12:20:44[0m] Step: 4557, Training Logs: loss_final: 0.877613, loss_mean: 0.830815, loss_mean_cls: 0.046798, grad_norm: 0.401387
[[34m2025-10-04 12:20:44[0m] Step: 4558, Training Logs: loss_final: 0.879626, loss_mean: 0.834041, loss_mean_cls: 0.045585, grad_norm: 0.414334
[[34m2025-10-04 12:20:44[0m] Step: 4559, Training Logs: loss_final: 0.902119, loss_mean: 0.857223, loss_mean_cls: 0.044895, grad_norm: 0.427635
[[34m2025-10-04 12:20:45[0m] Step: 4560, Training Logs: loss_final: 0.892459, loss_mean: 0.846874, loss_mean_cls: 0.045585, grad_norm: 0.347830
[[34m2025-10-04 12:20:45[0m] Step: 4561, Training Logs: loss_final: 0.889867, loss_mean: 0.844243, loss_mean_cls: 0.045624, grad_norm: 0.261272
[[34m2025-10-04 12:20:45[0m] Step: 4562, Training Logs: loss_final: 0.898730, loss_mean: 0.853481, loss_mean_cls: 0.045249, grad_norm: 0.456118
[[34m2025-10-04 12:20:46[0m] Step: 4563, Training Logs: loss_final: 0.901483, loss_mean: 0.856717, loss_mean_cls: 0.044766, grad_norm: 0.313070
[[34m2025-10-04 12:20:46[0m] Step: 4564, Training Logs: loss_final: 0.872953, loss_mean: 0.827314, loss_mean_cls: 0.045639, grad_norm: 0.282293
[[34m2025-10-04 12:20:46[0m] Step: 4565, Training Logs: loss_final: 0.880397, loss_mean: 0.834995, loss_mean_cls: 0.045402, grad_norm: 0.347777
[[34m2025-10-04 12:20:46[0m] Step: 4566, Training Logs: loss_final: 0.874827, loss_mean: 0.828605, loss_mean_cls: 0.046221, grad_norm: 0.352046
[[34m2025-10-04 12:20:47[0m] Step: 4567, Training Logs: loss_final: 0.874825, loss_mean: 0.828763, loss_mean_cls: 0.046063, grad_norm: 0.411958
[[34m2025-10-04 12:20:47[0m] Step: 4568, Training Logs: loss_final: 0.896888, loss_mean: 0.850657, loss_mean_cls: 0.046231, grad_norm: 0.350614
[[34m2025-10-04 12:20:47[0m] Step: 4569, Training Logs: loss_final: 0.876959, loss_mean: 0.831565, loss_mean_cls: 0.045394, grad_norm: 0.432985
[[34m2025-10-04 12:20:48[0m] Step: 4570, Training Logs: loss_final: 0.884114, loss_mean: 0.838277, loss_mean_cls: 0.045837, grad_norm: 0.409174
[[34m2025-10-04 12:20:48[0m] Step: 4571, Training Logs: loss_final: 0.899523, loss_mean: 0.853377, loss_mean_cls: 0.046146, grad_norm: 0.428891
[[34m2025-10-04 12:20:48[0m] Step: 4572, Training Logs: loss_final: 0.879820, loss_mean: 0.834503, loss_mean_cls: 0.045317, grad_norm: 0.437202
[[34m2025-10-04 12:20:48[0m] Step: 4573, Training Logs: loss_final: 0.896079, loss_mean: 0.851513, loss_mean_cls: 0.044566, grad_norm: 0.586811
[[34m2025-10-04 12:20:49[0m] Step: 4574, Training Logs: loss_final: 0.898593, loss_mean: 0.852332, loss_mean_cls: 0.046260, grad_norm: 0.361067
[[34m2025-10-04 12:20:49[0m] Step: 4575, Training Logs: loss_final: 0.887698, loss_mean: 0.842941, loss_mean_cls: 0.044757, grad_norm: 0.477873
[[34m2025-10-04 12:20:49[0m] Step: 4576, Training Logs: loss_final: 0.868501, loss_mean: 0.821595, loss_mean_cls: 0.046907, grad_norm: 0.435408
[[34m2025-10-04 12:20:50[0m] Step: 4577, Training Logs: loss_final: 0.895630, loss_mean: 0.851571, loss_mean_cls: 0.044059, grad_norm: 0.356938
[[34m2025-10-04 12:20:50[0m] Step: 4578, Training Logs: loss_final: 0.874562, loss_mean: 0.827796, loss_mean_cls: 0.046767, grad_norm: 0.514502
[[34m2025-10-04 12:20:50[0m] Step: 4579, Training Logs: loss_final: 0.887454, loss_mean: 0.841680, loss_mean_cls: 0.045774, grad_norm: 0.344160
[[34m2025-10-04 12:20:51[0m] Step: 4580, Training Logs: loss_final: 0.898475, loss_mean: 0.854873, loss_mean_cls: 0.043602, grad_norm: 0.392169
[[34m2025-10-04 12:20:51[0m] Step: 4581, Training Logs: loss_final: 0.893116, loss_mean: 0.848096, loss_mean_cls: 0.045020, grad_norm: 0.290407
[[34m2025-10-04 12:20:51[0m] Step: 4582, Training Logs: loss_final: 0.879887, loss_mean: 0.834314, loss_mean_cls: 0.045573, grad_norm: 0.391657
[[34m2025-10-04 12:20:51[0m] Step: 4583, Training Logs: loss_final: 0.886084, loss_mean: 0.840135, loss_mean_cls: 0.045949, grad_norm: 0.377050
[[34m2025-10-04 12:20:52[0m] Step: 4584, Training Logs: loss_final: 0.902015, loss_mean: 0.856874, loss_mean_cls: 0.045141, grad_norm: 0.275197
[[34m2025-10-04 12:20:52[0m] Step: 4585, Training Logs: loss_final: 0.891512, loss_mean: 0.845416, loss_mean_cls: 0.046096, grad_norm: 0.368821
[[34m2025-10-04 12:20:52[0m] Step: 4586, Training Logs: loss_final: 0.881613, loss_mean: 0.837024, loss_mean_cls: 0.044589, grad_norm: 0.394074
[[34m2025-10-04 12:20:53[0m] Step: 4587, Training Logs: loss_final: 0.894543, loss_mean: 0.849161, loss_mean_cls: 0.045382, grad_norm: 0.316616
[[34m2025-10-04 12:20:53[0m] Step: 4588, Training Logs: loss_final: 0.904554, loss_mean: 0.860390, loss_mean_cls: 0.044164, grad_norm: 0.423331
[[34m2025-10-04 12:20:53[0m] Step: 4589, Training Logs: loss_final: 0.889850, loss_mean: 0.843367, loss_mean_cls: 0.046483, grad_norm: 0.412487
[[34m2025-10-04 12:20:53[0m] Step: 4590, Training Logs: loss_final: 0.901320, loss_mean: 0.856721, loss_mean_cls: 0.044599, grad_norm: 0.255333
[[34m2025-10-04 12:20:54[0m] Step: 4591, Training Logs: loss_final: 0.866861, loss_mean: 0.821183, loss_mean_cls: 0.045679, grad_norm: 0.314681
[[34m2025-10-04 12:20:54[0m] Step: 4592, Training Logs: loss_final: 0.899137, loss_mean: 0.853898, loss_mean_cls: 0.045239, grad_norm: 0.428412
[[34m2025-10-04 12:20:54[0m] Step: 4593, Training Logs: loss_final: 0.862673, loss_mean: 0.815617, loss_mean_cls: 0.047056, grad_norm: 0.331377
[[34m2025-10-04 12:20:55[0m] Step: 4594, Training Logs: loss_final: 0.886010, loss_mean: 0.841070, loss_mean_cls: 0.044941, grad_norm: 0.419639
[[34m2025-10-04 12:20:55[0m] Step: 4595, Training Logs: loss_final: 0.905540, loss_mean: 0.860962, loss_mean_cls: 0.044578, grad_norm: 0.352736
[[34m2025-10-04 12:20:55[0m] Step: 4596, Training Logs: loss_final: 0.893198, loss_mean: 0.848538, loss_mean_cls: 0.044660, grad_norm: 0.290715
[[34m2025-10-04 12:20:55[0m] Step: 4597, Training Logs: loss_final: 0.890042, loss_mean: 0.846261, loss_mean_cls: 0.043781, grad_norm: 0.395841
[[34m2025-10-04 12:20:56[0m] Step: 4598, Training Logs: loss_final: 0.879598, loss_mean: 0.835314, loss_mean_cls: 0.044285, grad_norm: 0.380390
[[34m2025-10-04 12:20:56[0m] Step: 4599, Training Logs: loss_final: 0.886031, loss_mean: 0.839405, loss_mean_cls: 0.046627, grad_norm: 0.356527
[[34m2025-10-04 12:20:56[0m] Step: 4600, Training Logs: loss_final: 0.902232, loss_mean: 0.856599, loss_mean_cls: 0.045633, grad_norm: 0.348124
[[34m2025-10-04 12:20:57[0m] Step: 4601, Training Logs: loss_final: 0.875418, loss_mean: 0.830103, loss_mean_cls: 0.045315, grad_norm: 0.423650
[[34m2025-10-04 12:20:57[0m] Step: 4602, Training Logs: loss_final: 0.888981, loss_mean: 0.842677, loss_mean_cls: 0.046304, grad_norm: 0.449672
[[34m2025-10-04 12:20:57[0m] Step: 4603, Training Logs: loss_final: 0.880478, loss_mean: 0.835469, loss_mean_cls: 0.045009, grad_norm: 0.418098
[[34m2025-10-04 12:20:57[0m] Step: 4604, Training Logs: loss_final: 0.878059, loss_mean: 0.832934, loss_mean_cls: 0.045125, grad_norm: 0.431695
[[34m2025-10-04 12:20:58[0m] Step: 4605, Training Logs: loss_final: 0.881944, loss_mean: 0.836617, loss_mean_cls: 0.045327, grad_norm: 0.385289
[[34m2025-10-04 12:20:58[0m] Step: 4606, Training Logs: loss_final: 0.887935, loss_mean: 0.842928, loss_mean_cls: 0.045007, grad_norm: 0.492593
[[34m2025-10-04 12:20:58[0m] Step: 4607, Training Logs: loss_final: 0.882536, loss_mean: 0.836791, loss_mean_cls: 0.045745, grad_norm: 0.394270
[[34m2025-10-04 12:20:59[0m] Step: 4608, Training Logs: loss_final: 0.887694, loss_mean: 0.841880, loss_mean_cls: 0.045814, grad_norm: 0.392465
[[34m2025-10-04 12:20:59[0m] Step: 4609, Training Logs: loss_final: 0.883361, loss_mean: 0.837219, loss_mean_cls: 0.046142, grad_norm: 0.538472
[[34m2025-10-04 12:20:59[0m] Step: 4610, Training Logs: loss_final: 0.896410, loss_mean: 0.850548, loss_mean_cls: 0.045862, grad_norm: 0.366462
[[34m2025-10-04 12:20:59[0m] Step: 4611, Training Logs: loss_final: 0.909397, loss_mean: 0.864339, loss_mean_cls: 0.045057, grad_norm: 0.334833
[[34m2025-10-04 12:21:00[0m] Step: 4612, Training Logs: loss_final: 0.879141, loss_mean: 0.833887, loss_mean_cls: 0.045254, grad_norm: 0.359236
[[34m2025-10-04 12:21:00[0m] Step: 4613, Training Logs: loss_final: 0.907569, loss_mean: 0.862951, loss_mean_cls: 0.044618, grad_norm: 0.338633
[[34m2025-10-04 12:21:00[0m] Step: 4614, Training Logs: loss_final: 0.911887, loss_mean: 0.867496, loss_mean_cls: 0.044391, grad_norm: 0.502286
[[34m2025-10-04 12:21:01[0m] Step: 4615, Training Logs: loss_final: 0.894780, loss_mean: 0.850518, loss_mean_cls: 0.044262, grad_norm: 0.531945
[[34m2025-10-04 12:21:01[0m] Step: 4616, Training Logs: loss_final: 0.887376, loss_mean: 0.839899, loss_mean_cls: 0.047476, grad_norm: 0.360782
[[34m2025-10-04 12:21:01[0m] Step: 4617, Training Logs: loss_final: 0.903473, loss_mean: 0.859768, loss_mean_cls: 0.043706, grad_norm: 0.352139
[[34m2025-10-04 12:21:02[0m] Step: 4618, Training Logs: loss_final: 0.894547, loss_mean: 0.849847, loss_mean_cls: 0.044700, grad_norm: 0.362726
[[34m2025-10-04 12:21:02[0m] Step: 4619, Training Logs: loss_final: 0.895096, loss_mean: 0.850266, loss_mean_cls: 0.044830, grad_norm: 0.360377
[[34m2025-10-04 12:21:02[0m] Step: 4620, Training Logs: loss_final: 0.898867, loss_mean: 0.853613, loss_mean_cls: 0.045253, grad_norm: 0.614402
[[34m2025-10-04 12:21:02[0m] Step: 4621, Training Logs: loss_final: 0.907519, loss_mean: 0.863590, loss_mean_cls: 0.043929, grad_norm: 0.311717
[[34m2025-10-04 12:21:03[0m] Step: 4622, Training Logs: loss_final: 0.881458, loss_mean: 0.836688, loss_mean_cls: 0.044770, grad_norm: 0.472319
[[34m2025-10-04 12:21:03[0m] Step: 4623, Training Logs: loss_final: 0.889766, loss_mean: 0.844051, loss_mean_cls: 0.045715, grad_norm: 0.439007
[[34m2025-10-04 12:21:03[0m] Step: 4624, Training Logs: loss_final: 0.879267, loss_mean: 0.833729, loss_mean_cls: 0.045537, grad_norm: 0.624021
[[34m2025-10-04 12:21:04[0m] Step: 4625, Training Logs: loss_final: 0.885859, loss_mean: 0.839919, loss_mean_cls: 0.045939, grad_norm: 0.696898
[[34m2025-10-04 12:21:04[0m] Step: 4626, Training Logs: loss_final: 0.886937, loss_mean: 0.841400, loss_mean_cls: 0.045537, grad_norm: 0.331210
[[34m2025-10-04 12:21:04[0m] Step: 4627, Training Logs: loss_final: 0.892849, loss_mean: 0.847555, loss_mean_cls: 0.045294, grad_norm: 0.325049
[[34m2025-10-04 12:21:04[0m] Step: 4628, Training Logs: loss_final: 0.898051, loss_mean: 0.852185, loss_mean_cls: 0.045866, grad_norm: 0.273007
[[34m2025-10-04 12:21:05[0m] Step: 4629, Training Logs: loss_final: 0.913714, loss_mean: 0.869689, loss_mean_cls: 0.044025, grad_norm: 0.424310
[[34m2025-10-04 12:21:05[0m] Step: 4630, Training Logs: loss_final: 0.888775, loss_mean: 0.843936, loss_mean_cls: 0.044839, grad_norm: 0.407522
[[34m2025-10-04 12:21:05[0m] Step: 4631, Training Logs: loss_final: 0.862937, loss_mean: 0.817322, loss_mean_cls: 0.045615, grad_norm: 0.339525
[[34m2025-10-04 12:21:06[0m] Step: 4632, Training Logs: loss_final: 0.877015, loss_mean: 0.830434, loss_mean_cls: 0.046581, grad_norm: 0.452659
[[34m2025-10-04 12:21:06[0m] Step: 4633, Training Logs: loss_final: 0.902951, loss_mean: 0.857836, loss_mean_cls: 0.045115, grad_norm: 0.308515
[[34m2025-10-04 12:21:06[0m] Step: 4634, Training Logs: loss_final: 0.911416, loss_mean: 0.866186, loss_mean_cls: 0.045230, grad_norm: 0.413219
[[34m2025-10-04 12:21:06[0m] Step: 4635, Training Logs: loss_final: 0.872313, loss_mean: 0.826274, loss_mean_cls: 0.046039, grad_norm: 0.413995
[[34m2025-10-04 12:21:07[0m] Step: 4636, Training Logs: loss_final: 0.888094, loss_mean: 0.842023, loss_mean_cls: 0.046071, grad_norm: 0.370817
[[34m2025-10-04 12:21:07[0m] Step: 4637, Training Logs: loss_final: 0.867073, loss_mean: 0.822024, loss_mean_cls: 0.045049, grad_norm: 0.430441
[[34m2025-10-04 12:21:07[0m] Step: 4638, Training Logs: loss_final: 0.900331, loss_mean: 0.856018, loss_mean_cls: 0.044313, grad_norm: 0.341730
[[34m2025-10-04 12:21:08[0m] Step: 4639, Training Logs: loss_final: 0.918867, loss_mean: 0.874104, loss_mean_cls: 0.044763, grad_norm: 0.463935
[[34m2025-10-04 12:21:08[0m] Step: 4640, Training Logs: loss_final: 0.869963, loss_mean: 0.823726, loss_mean_cls: 0.046237, grad_norm: 0.337028
[[34m2025-10-04 12:21:08[0m] Step: 4641, Training Logs: loss_final: 0.899610, loss_mean: 0.854504, loss_mean_cls: 0.045106, grad_norm: 0.502280
[[34m2025-10-04 12:21:09[0m] Step: 4642, Training Logs: loss_final: 0.892775, loss_mean: 0.847811, loss_mean_cls: 0.044964, grad_norm: 0.347757
[[34m2025-10-04 12:21:09[0m] Step: 4643, Training Logs: loss_final: 0.888444, loss_mean: 0.843449, loss_mean_cls: 0.044996, grad_norm: 0.475363
[[34m2025-10-04 12:21:09[0m] Step: 4644, Training Logs: loss_final: 0.884382, loss_mean: 0.838690, loss_mean_cls: 0.045692, grad_norm: 0.344460
[[34m2025-10-04 12:21:09[0m] Step: 4645, Training Logs: loss_final: 0.888228, loss_mean: 0.842504, loss_mean_cls: 0.045724, grad_norm: 0.315188
[[34m2025-10-04 12:21:10[0m] Step: 4646, Training Logs: loss_final: 0.889212, loss_mean: 0.844569, loss_mean_cls: 0.044643, grad_norm: 0.359369
[[34m2025-10-04 12:21:10[0m] Step: 4647, Training Logs: loss_final: 0.894386, loss_mean: 0.848989, loss_mean_cls: 0.045397, grad_norm: 0.373892
[[34m2025-10-04 12:21:10[0m] Step: 4648, Training Logs: loss_final: 0.913893, loss_mean: 0.868817, loss_mean_cls: 0.045076, grad_norm: 0.302341
[[34m2025-10-04 12:21:11[0m] Step: 4649, Training Logs: loss_final: 0.892469, loss_mean: 0.847720, loss_mean_cls: 0.044750, grad_norm: 0.379223
[[34m2025-10-04 12:21:11[0m] Step: 4650, Training Logs: loss_final: 0.879972, loss_mean: 0.834258, loss_mean_cls: 0.045714, grad_norm: 0.309119
[[34m2025-10-04 12:21:11[0m] Step: 4651, Training Logs: loss_final: 0.924462, loss_mean: 0.878925, loss_mean_cls: 0.045537, grad_norm: 0.256644
[[34m2025-10-04 12:21:11[0m] Step: 4652, Training Logs: loss_final: 0.903534, loss_mean: 0.858627, loss_mean_cls: 0.044907, grad_norm: 0.317475
[[34m2025-10-04 12:21:12[0m] Step: 4653, Training Logs: loss_final: 0.895035, loss_mean: 0.850598, loss_mean_cls: 0.044437, grad_norm: 0.291739
[[34m2025-10-04 12:21:12[0m] Step: 4654, Training Logs: loss_final: 0.868854, loss_mean: 0.823590, loss_mean_cls: 0.045263, grad_norm: 0.263701
[[34m2025-10-04 12:21:12[0m] Step: 4655, Training Logs: loss_final: 0.896459, loss_mean: 0.851281, loss_mean_cls: 0.045178, grad_norm: 0.415102
[[34m2025-10-04 12:21:13[0m] Step: 4656, Training Logs: loss_final: 0.909053, loss_mean: 0.863844, loss_mean_cls: 0.045209, grad_norm: 0.314000
[[34m2025-10-04 12:21:13[0m] Step: 4657, Training Logs: loss_final: 0.903149, loss_mean: 0.858679, loss_mean_cls: 0.044470, grad_norm: 0.450733
[[34m2025-10-04 12:21:13[0m] Step: 4658, Training Logs: loss_final: 0.899791, loss_mean: 0.854664, loss_mean_cls: 0.045128, grad_norm: 0.274093
[[34m2025-10-04 12:21:13[0m] Step: 4659, Training Logs: loss_final: 0.880235, loss_mean: 0.835820, loss_mean_cls: 0.044415, grad_norm: 0.472557
[[34m2025-10-04 12:21:14[0m] Step: 4660, Training Logs: loss_final: 0.868187, loss_mean: 0.822539, loss_mean_cls: 0.045648, grad_norm: 0.483130
[[34m2025-10-04 12:21:14[0m] Step: 4661, Training Logs: loss_final: 0.913152, loss_mean: 0.867835, loss_mean_cls: 0.045318, grad_norm: 0.321884
[[34m2025-10-04 12:21:14[0m] Step: 4662, Training Logs: loss_final: 0.881312, loss_mean: 0.835819, loss_mean_cls: 0.045493, grad_norm: 0.369710
[[34m2025-10-04 12:21:15[0m] Step: 4663, Training Logs: loss_final: 0.879947, loss_mean: 0.835406, loss_mean_cls: 0.044541, grad_norm: 0.347656
[[34m2025-10-04 12:21:15[0m] Step: 4664, Training Logs: loss_final: 0.914280, loss_mean: 0.869451, loss_mean_cls: 0.044829, grad_norm: 0.335271
[[34m2025-10-04 12:21:15[0m] Step: 4665, Training Logs: loss_final: 0.878786, loss_mean: 0.834207, loss_mean_cls: 0.044579, grad_norm: 0.398315
[[34m2025-10-04 12:21:16[0m] Step: 4666, Training Logs: loss_final: 0.889883, loss_mean: 0.844056, loss_mean_cls: 0.045827, grad_norm: 0.331926
[[34m2025-10-04 12:21:16[0m] Step: 4667, Training Logs: loss_final: 0.904183, loss_mean: 0.859332, loss_mean_cls: 0.044851, grad_norm: 0.318749
[[34m2025-10-04 12:21:16[0m] Step: 4668, Training Logs: loss_final: 0.884466, loss_mean: 0.839158, loss_mean_cls: 0.045308, grad_norm: 0.364394
[[34m2025-10-04 12:21:16[0m] Step: 4669, Training Logs: loss_final: 0.901684, loss_mean: 0.855872, loss_mean_cls: 0.045812, grad_norm: 0.349346
[[34m2025-10-04 12:21:17[0m] Step: 4670, Training Logs: loss_final: 0.891187, loss_mean: 0.846300, loss_mean_cls: 0.044887, grad_norm: 0.261427
[[34m2025-10-04 12:21:17[0m] Step: 4671, Training Logs: loss_final: 0.882098, loss_mean: 0.836272, loss_mean_cls: 0.045827, grad_norm: 0.434728
[[34m2025-10-04 12:21:17[0m] Step: 4672, Training Logs: loss_final: 0.877916, loss_mean: 0.832176, loss_mean_cls: 0.045741, grad_norm: 0.396198
[[34m2025-10-04 12:21:18[0m] Step: 4673, Training Logs: loss_final: 0.899443, loss_mean: 0.853720, loss_mean_cls: 0.045722, grad_norm: 0.398775
[[34m2025-10-04 12:21:18[0m] Step: 4674, Training Logs: loss_final: 0.906243, loss_mean: 0.862491, loss_mean_cls: 0.043752, grad_norm: 0.359598
[[34m2025-10-04 12:21:18[0m] Step: 4675, Training Logs: loss_final: 0.894394, loss_mean: 0.849623, loss_mean_cls: 0.044770, grad_norm: 0.333037
[[34m2025-10-04 12:21:18[0m] Step: 4676, Training Logs: loss_final: 0.880722, loss_mean: 0.835336, loss_mean_cls: 0.045385, grad_norm: 0.291611
[[34m2025-10-04 12:21:19[0m] Step: 4677, Training Logs: loss_final: 0.892708, loss_mean: 0.847736, loss_mean_cls: 0.044972, grad_norm: 0.364971
[[34m2025-10-04 12:21:19[0m] Step: 4678, Training Logs: loss_final: 0.886979, loss_mean: 0.842219, loss_mean_cls: 0.044761, grad_norm: 0.401105
[[34m2025-10-04 12:21:19[0m] Step: 4679, Training Logs: loss_final: 0.894550, loss_mean: 0.849881, loss_mean_cls: 0.044669, grad_norm: 0.481977
[[34m2025-10-04 12:21:20[0m] Step: 4680, Training Logs: loss_final: 0.913707, loss_mean: 0.868924, loss_mean_cls: 0.044783, grad_norm: 0.318460
[[34m2025-10-04 12:21:20[0m] Step: 4681, Training Logs: loss_final: 0.880107, loss_mean: 0.834862, loss_mean_cls: 0.045245, grad_norm: 0.311665
[[34m2025-10-04 12:21:20[0m] Step: 4682, Training Logs: loss_final: 0.869055, loss_mean: 0.823636, loss_mean_cls: 0.045419, grad_norm: 0.304510
[[34m2025-10-04 12:21:20[0m] Step: 4683, Training Logs: loss_final: 0.902518, loss_mean: 0.857168, loss_mean_cls: 0.045351, grad_norm: 0.383825
[[34m2025-10-04 12:21:21[0m] Step: 4684, Training Logs: loss_final: 0.906059, loss_mean: 0.861117, loss_mean_cls: 0.044942, grad_norm: 0.362732
[[34m2025-10-04 12:21:21[0m] Step: 4685, Training Logs: loss_final: 0.899604, loss_mean: 0.853248, loss_mean_cls: 0.046356, grad_norm: 0.444768
[[34m2025-10-04 12:21:21[0m] Step: 4686, Training Logs: loss_final: 0.883410, loss_mean: 0.838570, loss_mean_cls: 0.044840, grad_norm: 0.318678
[[34m2025-10-04 12:21:22[0m] Step: 4687, Training Logs: loss_final: 0.883373, loss_mean: 0.837403, loss_mean_cls: 0.045970, grad_norm: 0.486213
[[34m2025-10-04 12:21:22[0m] Step: 4688, Training Logs: loss_final: 0.884443, loss_mean: 0.839940, loss_mean_cls: 0.044503, grad_norm: 0.372437
[[34m2025-10-04 12:21:22[0m] Step: 4689, Training Logs: loss_final: 0.902517, loss_mean: 0.857224, loss_mean_cls: 0.045293, grad_norm: 0.436476
[[34m2025-10-04 12:21:22[0m] Step: 4690, Training Logs: loss_final: 0.893920, loss_mean: 0.848166, loss_mean_cls: 0.045753, grad_norm: 0.365455
[[34m2025-10-04 12:21:23[0m] Step: 4691, Training Logs: loss_final: 0.905872, loss_mean: 0.861403, loss_mean_cls: 0.044469, grad_norm: 0.477688
[[34m2025-10-04 12:21:23[0m] Step: 4692, Training Logs: loss_final: 0.892335, loss_mean: 0.847161, loss_mean_cls: 0.045174, grad_norm: 0.618565
[[34m2025-10-04 12:21:23[0m] Step: 4693, Training Logs: loss_final: 0.883759, loss_mean: 0.837931, loss_mean_cls: 0.045828, grad_norm: 0.417790
[[34m2025-10-04 12:21:24[0m] Step: 4694, Training Logs: loss_final: 0.887067, loss_mean: 0.842414, loss_mean_cls: 0.044654, grad_norm: 0.462517
[[34m2025-10-04 12:21:24[0m] Step: 4695, Training Logs: loss_final: 0.900430, loss_mean: 0.854623, loss_mean_cls: 0.045808, grad_norm: 0.366166
[[34m2025-10-04 12:21:24[0m] Step: 4696, Training Logs: loss_final: 0.889092, loss_mean: 0.843724, loss_mean_cls: 0.045369, grad_norm: 0.445285
[[34m2025-10-04 12:21:25[0m] Step: 4697, Training Logs: loss_final: 0.902955, loss_mean: 0.857012, loss_mean_cls: 0.045943, grad_norm: 0.498046
[[34m2025-10-04 12:21:25[0m] Step: 4698, Training Logs: loss_final: 0.900797, loss_mean: 0.855761, loss_mean_cls: 0.045036, grad_norm: 0.306125
[[34m2025-10-04 12:21:25[0m] Step: 4699, Training Logs: loss_final: 0.871408, loss_mean: 0.826890, loss_mean_cls: 0.044519, grad_norm: 0.462104
[[34m2025-10-04 12:21:25[0m] Step: 4700, Training Logs: loss_final: 0.881180, loss_mean: 0.836630, loss_mean_cls: 0.044550, grad_norm: 0.309833
[[34m2025-10-04 12:21:26[0m] Step: 4701, Training Logs: loss_final: 0.867065, loss_mean: 0.822403, loss_mean_cls: 0.044662, grad_norm: 0.430959
[[34m2025-10-04 12:21:26[0m] Step: 4702, Training Logs: loss_final: 0.887018, loss_mean: 0.842872, loss_mean_cls: 0.044146, grad_norm: 0.497430
[[34m2025-10-04 12:21:26[0m] Step: 4703, Training Logs: loss_final: 0.905213, loss_mean: 0.860547, loss_mean_cls: 0.044666, grad_norm: 0.352631
[[34m2025-10-04 12:21:27[0m] Step: 4704, Training Logs: loss_final: 0.898537, loss_mean: 0.853661, loss_mean_cls: 0.044876, grad_norm: 0.424452
[[34m2025-10-04 12:21:27[0m] Step: 4705, Training Logs: loss_final: 0.882292, loss_mean: 0.837684, loss_mean_cls: 0.044608, grad_norm: 0.369436
[[34m2025-10-04 12:21:27[0m] Step: 4706, Training Logs: loss_final: 0.870503, loss_mean: 0.824663, loss_mean_cls: 0.045840, grad_norm: 0.427484
[[34m2025-10-04 12:21:28[0m] Step: 4707, Training Logs: loss_final: 0.877784, loss_mean: 0.833585, loss_mean_cls: 0.044199, grad_norm: 0.453052
[[34m2025-10-04 12:21:28[0m] Step: 4708, Training Logs: loss_final: 0.898896, loss_mean: 0.854647, loss_mean_cls: 0.044249, grad_norm: 0.326737
[[34m2025-10-04 12:21:28[0m] Step: 4709, Training Logs: loss_final: 0.905408, loss_mean: 0.861160, loss_mean_cls: 0.044248, grad_norm: 0.300576
[[34m2025-10-04 12:21:28[0m] Step: 4710, Training Logs: loss_final: 0.888605, loss_mean: 0.843601, loss_mean_cls: 0.045004, grad_norm: 0.335571
[[34m2025-10-04 12:21:29[0m] Step: 4711, Training Logs: loss_final: 0.890119, loss_mean: 0.845345, loss_mean_cls: 0.044775, grad_norm: 0.364480
[[34m2025-10-04 12:21:29[0m] Step: 4712, Training Logs: loss_final: 0.880618, loss_mean: 0.836419, loss_mean_cls: 0.044199, grad_norm: 0.535376
[[34m2025-10-04 12:21:29[0m] Step: 4713, Training Logs: loss_final: 0.888382, loss_mean: 0.842838, loss_mean_cls: 0.045544, grad_norm: 0.330778
[[34m2025-10-04 12:21:30[0m] Step: 4714, Training Logs: loss_final: 0.896731, loss_mean: 0.850949, loss_mean_cls: 0.045782, grad_norm: 0.483356
[[34m2025-10-04 12:21:30[0m] Step: 4715, Training Logs: loss_final: 0.863703, loss_mean: 0.817831, loss_mean_cls: 0.045872, grad_norm: 0.347668
[[34m2025-10-04 12:21:30[0m] Step: 4716, Training Logs: loss_final: 0.901827, loss_mean: 0.857700, loss_mean_cls: 0.044126, grad_norm: 0.290005
[[34m2025-10-04 12:21:30[0m] Step: 4717, Training Logs: loss_final: 0.882153, loss_mean: 0.837052, loss_mean_cls: 0.045101, grad_norm: 0.484509
[[34m2025-10-04 12:21:31[0m] Step: 4718, Training Logs: loss_final: 0.912268, loss_mean: 0.867645, loss_mean_cls: 0.044623, grad_norm: 0.372733
[[34m2025-10-04 12:21:31[0m] Step: 4719, Training Logs: loss_final: 0.881518, loss_mean: 0.837139, loss_mean_cls: 0.044379, grad_norm: 0.374593
[[34m2025-10-04 12:21:31[0m] Step: 4720, Training Logs: loss_final: 0.886037, loss_mean: 0.840762, loss_mean_cls: 0.045275, grad_norm: 0.352888
[[34m2025-10-04 12:21:32[0m] Step: 4721, Training Logs: loss_final: 0.885964, loss_mean: 0.840566, loss_mean_cls: 0.045398, grad_norm: 0.334250
[[34m2025-10-04 12:21:32[0m] Step: 4722, Training Logs: loss_final: 0.902346, loss_mean: 0.858382, loss_mean_cls: 0.043964, grad_norm: 0.482585
[[34m2025-10-04 12:21:32[0m] Step: 4723, Training Logs: loss_final: 0.880212, loss_mean: 0.834733, loss_mean_cls: 0.045480, grad_norm: 0.355576
[[34m2025-10-04 12:21:32[0m] Step: 4724, Training Logs: loss_final: 0.894774, loss_mean: 0.848525, loss_mean_cls: 0.046249, grad_norm: 0.332793
[[34m2025-10-04 12:21:33[0m] Step: 4725, Training Logs: loss_final: 0.881029, loss_mean: 0.836530, loss_mean_cls: 0.044499, grad_norm: 0.396014
[[34m2025-10-04 12:21:33[0m] Step: 4726, Training Logs: loss_final: 0.886233, loss_mean: 0.840757, loss_mean_cls: 0.045476, grad_norm: 0.343147
[[34m2025-10-04 12:21:33[0m] Step: 4727, Training Logs: loss_final: 0.889315, loss_mean: 0.843564, loss_mean_cls: 0.045751, grad_norm: 0.494691
[[34m2025-10-04 12:21:34[0m] Step: 4728, Training Logs: loss_final: 0.877552, loss_mean: 0.833089, loss_mean_cls: 0.044463, grad_norm: 0.498096
[[34m2025-10-04 12:21:34[0m] Step: 4729, Training Logs: loss_final: 0.868778, loss_mean: 0.823002, loss_mean_cls: 0.045776, grad_norm: 0.346340
[[34m2025-10-04 12:21:34[0m] Step: 4730, Training Logs: loss_final: 0.878350, loss_mean: 0.833332, loss_mean_cls: 0.045018, grad_norm: 0.415915
[[34m2025-10-04 12:21:35[0m] Step: 4731, Training Logs: loss_final: 0.878837, loss_mean: 0.832929, loss_mean_cls: 0.045908, grad_norm: 0.421435
[[34m2025-10-04 12:21:35[0m] Step: 4732, Training Logs: loss_final: 0.896718, loss_mean: 0.852157, loss_mean_cls: 0.044561, grad_norm: 0.232700
[[34m2025-10-04 12:21:35[0m] Step: 4733, Training Logs: loss_final: 0.894563, loss_mean: 0.849464, loss_mean_cls: 0.045099, grad_norm: 0.409142
[[34m2025-10-04 12:21:35[0m] Step: 4734, Training Logs: loss_final: 0.890792, loss_mean: 0.845289, loss_mean_cls: 0.045504, grad_norm: 0.301816
[[34m2025-10-04 12:21:36[0m] Step: 4735, Training Logs: loss_final: 0.863266, loss_mean: 0.816673, loss_mean_cls: 0.046593, grad_norm: 0.361442
[[34m2025-10-04 12:21:36[0m] Step: 4736, Training Logs: loss_final: 0.882953, loss_mean: 0.837084, loss_mean_cls: 0.045869, grad_norm: 0.299974
[[34m2025-10-04 12:21:36[0m] Step: 4737, Training Logs: loss_final: 0.876713, loss_mean: 0.830991, loss_mean_cls: 0.045722, grad_norm: 0.277845
[[34m2025-10-04 12:21:37[0m] Step: 4738, Training Logs: loss_final: 0.899511, loss_mean: 0.853976, loss_mean_cls: 0.045535, grad_norm: 0.311505
[[34m2025-10-04 12:21:37[0m] Step: 4739, Training Logs: loss_final: 0.888027, loss_mean: 0.843127, loss_mean_cls: 0.044900, grad_norm: 0.449310
[[34m2025-10-04 12:21:37[0m] Step: 4740, Training Logs: loss_final: 0.883704, loss_mean: 0.838635, loss_mean_cls: 0.045069, grad_norm: 0.272293
[[34m2025-10-04 12:21:37[0m] Step: 4741, Training Logs: loss_final: 0.878927, loss_mean: 0.833521, loss_mean_cls: 0.045406, grad_norm: 0.362202
[[34m2025-10-04 12:21:38[0m] Step: 4742, Training Logs: loss_final: 0.886876, loss_mean: 0.841648, loss_mean_cls: 0.045228, grad_norm: 0.351619
[[34m2025-10-04 12:21:38[0m] Step: 4743, Training Logs: loss_final: 0.892590, loss_mean: 0.848203, loss_mean_cls: 0.044388, grad_norm: 0.312145
[[34m2025-10-04 12:21:38[0m] Step: 4744, Training Logs: loss_final: 0.904277, loss_mean: 0.857948, loss_mean_cls: 0.046329, grad_norm: 0.321938
[[34m2025-10-04 12:21:39[0m] Step: 4745, Training Logs: loss_final: 0.899592, loss_mean: 0.854850, loss_mean_cls: 0.044743, grad_norm: 0.236891
[[34m2025-10-04 12:21:39[0m] Step: 4746, Training Logs: loss_final: 0.891512, loss_mean: 0.845494, loss_mean_cls: 0.046018, grad_norm: 0.341979
[[34m2025-10-04 12:21:39[0m] Step: 4747, Training Logs: loss_final: 0.872989, loss_mean: 0.827761, loss_mean_cls: 0.045228, grad_norm: 0.396775
[[34m2025-10-04 12:21:39[0m] Step: 4748, Training Logs: loss_final: 0.878889, loss_mean: 0.833482, loss_mean_cls: 0.045406, grad_norm: 0.415891
[[34m2025-10-04 12:21:40[0m] Step: 4749, Training Logs: loss_final: 0.903261, loss_mean: 0.858046, loss_mean_cls: 0.045214, grad_norm: 0.381776
[[34m2025-10-04 12:21:40[0m] Step: 4750, Training Logs: loss_final: 0.879348, loss_mean: 0.834665, loss_mean_cls: 0.044683, grad_norm: 0.383496
[[34m2025-10-04 12:21:40[0m] Step: 4751, Training Logs: loss_final: 0.897013, loss_mean: 0.852533, loss_mean_cls: 0.044480, grad_norm: 0.412456
[[34m2025-10-04 12:21:41[0m] Step: 4752, Training Logs: loss_final: 0.900598, loss_mean: 0.855416, loss_mean_cls: 0.045182, grad_norm: 0.460819
[[34m2025-10-04 12:21:41[0m] Step: 4753, Training Logs: loss_final: 0.912161, loss_mean: 0.865959, loss_mean_cls: 0.046201, grad_norm: 0.371245
[[34m2025-10-04 12:21:41[0m] Step: 4754, Training Logs: loss_final: 0.891855, loss_mean: 0.847372, loss_mean_cls: 0.044483, grad_norm: 0.296393
[[34m2025-10-04 12:21:41[0m] Step: 4755, Training Logs: loss_final: 0.920948, loss_mean: 0.876779, loss_mean_cls: 0.044169, grad_norm: 0.386239
[[34m2025-10-04 12:21:42[0m] Step: 4756, Training Logs: loss_final: 0.871184, loss_mean: 0.825699, loss_mean_cls: 0.045484, grad_norm: 0.271268
[[34m2025-10-04 12:21:42[0m] Step: 4757, Training Logs: loss_final: 0.916415, loss_mean: 0.871163, loss_mean_cls: 0.045253, grad_norm: 0.492692
[[34m2025-10-04 12:21:42[0m] Step: 4758, Training Logs: loss_final: 0.898627, loss_mean: 0.852710, loss_mean_cls: 0.045917, grad_norm: 0.309142
[[34m2025-10-04 12:21:43[0m] Step: 4759, Training Logs: loss_final: 0.865673, loss_mean: 0.819324, loss_mean_cls: 0.046349, grad_norm: 0.292392
[[34m2025-10-04 12:21:43[0m] Step: 4760, Training Logs: loss_final: 0.902019, loss_mean: 0.857213, loss_mean_cls: 0.044806, grad_norm: 0.355251
[[34m2025-10-04 12:21:43[0m] Step: 4761, Training Logs: loss_final: 0.898361, loss_mean: 0.852924, loss_mean_cls: 0.045437, grad_norm: 0.368811
[[34m2025-10-04 12:21:44[0m] Step: 4762, Training Logs: loss_final: 0.880806, loss_mean: 0.835123, loss_mean_cls: 0.045683, grad_norm: 0.349503
[[34m2025-10-04 12:21:44[0m] Step: 4763, Training Logs: loss_final: 0.890119, loss_mean: 0.844709, loss_mean_cls: 0.045410, grad_norm: 0.405664
[[34m2025-10-04 12:21:44[0m] Step: 4764, Training Logs: loss_final: 0.884228, loss_mean: 0.839170, loss_mean_cls: 0.045058, grad_norm: 0.279751
[[34m2025-10-04 12:21:44[0m] Step: 4765, Training Logs: loss_final: 0.904136, loss_mean: 0.858948, loss_mean_cls: 0.045188, grad_norm: 0.358910
[[34m2025-10-04 12:21:45[0m] Step: 4766, Training Logs: loss_final: 0.904401, loss_mean: 0.859587, loss_mean_cls: 0.044814, grad_norm: 0.406611
[[34m2025-10-04 12:21:45[0m] Step: 4767, Training Logs: loss_final: 0.895198, loss_mean: 0.849360, loss_mean_cls: 0.045838, grad_norm: 0.316630
[[34m2025-10-04 12:21:45[0m] Step: 4768, Training Logs: loss_final: 0.876534, loss_mean: 0.831573, loss_mean_cls: 0.044962, grad_norm: 0.427174
[[34m2025-10-04 12:21:46[0m] Step: 4769, Training Logs: loss_final: 0.886021, loss_mean: 0.842142, loss_mean_cls: 0.043879, grad_norm: 0.350940
[[34m2025-10-04 12:21:46[0m] Step: 4770, Training Logs: loss_final: 0.879666, loss_mean: 0.834440, loss_mean_cls: 0.045225, grad_norm: 0.299133
[[34m2025-10-04 12:21:46[0m] Step: 4771, Training Logs: loss_final: 0.870046, loss_mean: 0.823430, loss_mean_cls: 0.046616, grad_norm: 0.331900
[[34m2025-10-04 12:21:46[0m] Step: 4772, Training Logs: loss_final: 0.905210, loss_mean: 0.859248, loss_mean_cls: 0.045962, grad_norm: 0.316070
[[34m2025-10-04 12:21:47[0m] Step: 4773, Training Logs: loss_final: 0.892702, loss_mean: 0.847255, loss_mean_cls: 0.045447, grad_norm: 0.443952
[[34m2025-10-04 12:21:47[0m] Step: 4774, Training Logs: loss_final: 0.882974, loss_mean: 0.837622, loss_mean_cls: 0.045351, grad_norm: 0.420781
[[34m2025-10-04 12:21:47[0m] Step: 4775, Training Logs: loss_final: 0.897684, loss_mean: 0.851811, loss_mean_cls: 0.045874, grad_norm: 0.412000
[[34m2025-10-04 12:21:48[0m] Step: 4776, Training Logs: loss_final: 0.886496, loss_mean: 0.841501, loss_mean_cls: 0.044996, grad_norm: 0.380586
[[34m2025-10-04 12:21:48[0m] Step: 4777, Training Logs: loss_final: 0.896675, loss_mean: 0.851772, loss_mean_cls: 0.044903, grad_norm: 0.386871
[[34m2025-10-04 12:21:48[0m] Step: 4778, Training Logs: loss_final: 0.895076, loss_mean: 0.851212, loss_mean_cls: 0.043864, grad_norm: 0.335115
[[34m2025-10-04 12:21:49[0m] Step: 4779, Training Logs: loss_final: 0.885402, loss_mean: 0.839392, loss_mean_cls: 0.046010, grad_norm: 0.467467
[[34m2025-10-04 12:21:49[0m] Step: 4780, Training Logs: loss_final: 0.870696, loss_mean: 0.824811, loss_mean_cls: 0.045885, grad_norm: 0.547314
[[34m2025-10-04 12:21:49[0m] Step: 4781, Training Logs: loss_final: 0.904586, loss_mean: 0.860818, loss_mean_cls: 0.043769, grad_norm: 0.346177
[[34m2025-10-04 12:21:49[0m] Step: 4782, Training Logs: loss_final: 0.874035, loss_mean: 0.827852, loss_mean_cls: 0.046182, grad_norm: 0.465919
[[34m2025-10-04 12:21:50[0m] Step: 4783, Training Logs: loss_final: 0.897157, loss_mean: 0.851586, loss_mean_cls: 0.045571, grad_norm: 0.307129
[[34m2025-10-04 12:21:50[0m] Step: 4784, Training Logs: loss_final: 0.871782, loss_mean: 0.826125, loss_mean_cls: 0.045657, grad_norm: 0.410438
[[34m2025-10-04 12:21:50[0m] Step: 4785, Training Logs: loss_final: 0.901139, loss_mean: 0.856524, loss_mean_cls: 0.044614, grad_norm: 0.478887
[[34m2025-10-04 12:21:51[0m] Step: 4786, Training Logs: loss_final: 0.870847, loss_mean: 0.826012, loss_mean_cls: 0.044835, grad_norm: 0.293856
[[34m2025-10-04 12:21:51[0m] Step: 4787, Training Logs: loss_final: 0.885895, loss_mean: 0.840149, loss_mean_cls: 0.045746, grad_norm: 0.353792
[[34m2025-10-04 12:21:51[0m] Step: 4788, Training Logs: loss_final: 0.909969, loss_mean: 0.865005, loss_mean_cls: 0.044964, grad_norm: 0.383358
[[34m2025-10-04 12:21:51[0m] Step: 4789, Training Logs: loss_final: 0.887122, loss_mean: 0.841991, loss_mean_cls: 0.045131, grad_norm: 0.318394
[[34m2025-10-04 12:21:52[0m] Step: 4790, Training Logs: loss_final: 0.863729, loss_mean: 0.818603, loss_mean_cls: 0.045126, grad_norm: 0.226501
[[34m2025-10-04 12:21:52[0m] Step: 4791, Training Logs: loss_final: 0.889140, loss_mean: 0.844859, loss_mean_cls: 0.044281, grad_norm: 0.309782
[[34m2025-10-04 12:21:52[0m] Step: 4792, Training Logs: loss_final: 0.875584, loss_mean: 0.830481, loss_mean_cls: 0.045103, grad_norm: 0.308410
[[34m2025-10-04 12:21:53[0m] Step: 4793, Training Logs: loss_final: 0.889927, loss_mean: 0.844121, loss_mean_cls: 0.045806, grad_norm: 0.308215
[[34m2025-10-04 12:21:53[0m] Step: 4794, Training Logs: loss_final: 0.880993, loss_mean: 0.835484, loss_mean_cls: 0.045509, grad_norm: 0.275837
[[34m2025-10-04 12:21:53[0m] Step: 4795, Training Logs: loss_final: 0.902670, loss_mean: 0.858398, loss_mean_cls: 0.044272, grad_norm: 0.279560
[[34m2025-10-04 12:21:54[0m] Step: 4796, Training Logs: loss_final: 0.873147, loss_mean: 0.827248, loss_mean_cls: 0.045899, grad_norm: 0.271643
[[34m2025-10-04 12:21:54[0m] Step: 4797, Training Logs: loss_final: 0.905561, loss_mean: 0.860786, loss_mean_cls: 0.044775, grad_norm: 0.261679
[[34m2025-10-04 12:21:54[0m] Step: 4798, Training Logs: loss_final: 0.903614, loss_mean: 0.858987, loss_mean_cls: 0.044627, grad_norm: 0.346510
[[34m2025-10-04 12:21:54[0m] Step: 4799, Training Logs: loss_final: 0.890872, loss_mean: 0.844998, loss_mean_cls: 0.045875, grad_norm: 0.395619
[[34m2025-10-04 12:21:55[0m] Step: 4800, Training Logs: loss_final: 0.905254, loss_mean: 0.860337, loss_mean_cls: 0.044917, grad_norm: 0.257783
[[34m2025-10-04 12:21:55[0m] Step: 4801, Training Logs: loss_final: 0.914511, loss_mean: 0.869669, loss_mean_cls: 0.044842, grad_norm: 0.404638
[[34m2025-10-04 12:21:55[0m] Step: 4802, Training Logs: loss_final: 0.880627, loss_mean: 0.834003, loss_mean_cls: 0.046625, grad_norm: 0.363028
[[34m2025-10-04 12:21:56[0m] Step: 4803, Training Logs: loss_final: 0.908055, loss_mean: 0.863208, loss_mean_cls: 0.044847, grad_norm: 0.290104
[[34m2025-10-04 12:21:56[0m] Step: 4804, Training Logs: loss_final: 0.894235, loss_mean: 0.849901, loss_mean_cls: 0.044334, grad_norm: 0.381275
[[34m2025-10-04 12:21:56[0m] Step: 4805, Training Logs: loss_final: 0.875022, loss_mean: 0.829061, loss_mean_cls: 0.045961, grad_norm: 0.305753
[[34m2025-10-04 12:21:56[0m] Step: 4806, Training Logs: loss_final: 0.898675, loss_mean: 0.852477, loss_mean_cls: 0.046198, grad_norm: 0.286183
[[34m2025-10-04 12:21:57[0m] Step: 4807, Training Logs: loss_final: 0.883682, loss_mean: 0.837968, loss_mean_cls: 0.045714, grad_norm: 0.441657
[[34m2025-10-04 12:21:57[0m] Step: 4808, Training Logs: loss_final: 0.894530, loss_mean: 0.850007, loss_mean_cls: 0.044523, grad_norm: 0.304411
[[34m2025-10-04 12:21:57[0m] Step: 4809, Training Logs: loss_final: 0.870051, loss_mean: 0.823125, loss_mean_cls: 0.046927, grad_norm: 0.338873
[[34m2025-10-04 12:21:58[0m] Step: 4810, Training Logs: loss_final: 0.875437, loss_mean: 0.830442, loss_mean_cls: 0.044994, grad_norm: 0.331731
[[34m2025-10-04 12:21:58[0m] Step: 4811, Training Logs: loss_final: 0.884465, loss_mean: 0.839427, loss_mean_cls: 0.045038, grad_norm: 0.386899
[[34m2025-10-04 12:21:58[0m] Step: 4812, Training Logs: loss_final: 0.904844, loss_mean: 0.860508, loss_mean_cls: 0.044336, grad_norm: 0.327288
[[34m2025-10-04 12:21:59[0m] Step: 4813, Training Logs: loss_final: 0.884386, loss_mean: 0.839318, loss_mean_cls: 0.045068, grad_norm: 0.376410
[[34m2025-10-04 12:21:59[0m] Step: 4814, Training Logs: loss_final: 0.907323, loss_mean: 0.862706, loss_mean_cls: 0.044617, grad_norm: 0.380818
[[34m2025-10-04 12:21:59[0m] Step: 4815, Training Logs: loss_final: 0.861374, loss_mean: 0.815175, loss_mean_cls: 0.046198, grad_norm: 0.335119
[[34m2025-10-04 12:21:59[0m] Step: 4816, Training Logs: loss_final: 0.878950, loss_mean: 0.834339, loss_mean_cls: 0.044610, grad_norm: 0.463777
[[34m2025-10-04 12:22:00[0m] Step: 4817, Training Logs: loss_final: 0.903015, loss_mean: 0.858908, loss_mean_cls: 0.044107, grad_norm: 0.339899
[[34m2025-10-04 12:22:00[0m] Step: 4818, Training Logs: loss_final: 0.891550, loss_mean: 0.848499, loss_mean_cls: 0.043051, grad_norm: 0.255372
[[34m2025-10-04 12:22:00[0m] Step: 4819, Training Logs: loss_final: 0.879188, loss_mean: 0.832209, loss_mean_cls: 0.046979, grad_norm: 0.312228
[[34m2025-10-04 12:22:01[0m] Step: 4820, Training Logs: loss_final: 0.891153, loss_mean: 0.846238, loss_mean_cls: 0.044915, grad_norm: 0.446569
[[34m2025-10-04 12:22:01[0m] Step: 4821, Training Logs: loss_final: 0.875508, loss_mean: 0.828762, loss_mean_cls: 0.046746, grad_norm: 0.278553
[[34m2025-10-04 12:22:01[0m] Step: 4822, Training Logs: loss_final: 0.897024, loss_mean: 0.853388, loss_mean_cls: 0.043637, grad_norm: 0.338450
[[34m2025-10-04 12:22:01[0m] Step: 4823, Training Logs: loss_final: 0.901289, loss_mean: 0.856520, loss_mean_cls: 0.044769, grad_norm: 0.301864
[[34m2025-10-04 12:22:02[0m] Step: 4824, Training Logs: loss_final: 0.864594, loss_mean: 0.819111, loss_mean_cls: 0.045483, grad_norm: 0.317987
[[34m2025-10-04 12:22:02[0m] Step: 4825, Training Logs: loss_final: 0.891035, loss_mean: 0.848146, loss_mean_cls: 0.042888, grad_norm: 0.306668
[[34m2025-10-04 12:22:02[0m] Step: 4826, Training Logs: loss_final: 0.896349, loss_mean: 0.851284, loss_mean_cls: 0.045065, grad_norm: 0.343350
[[34m2025-10-04 12:22:03[0m] Step: 4827, Training Logs: loss_final: 0.897640, loss_mean: 0.851357, loss_mean_cls: 0.046283, grad_norm: 0.340250
[[34m2025-10-04 12:22:03[0m] Step: 4828, Training Logs: loss_final: 0.892513, loss_mean: 0.847016, loss_mean_cls: 0.045497, grad_norm: 0.263061
[[34m2025-10-04 12:22:03[0m] Step: 4829, Training Logs: loss_final: 0.887565, loss_mean: 0.840859, loss_mean_cls: 0.046705, grad_norm: 0.320068
[[34m2025-10-04 12:22:04[0m] Step: 4830, Training Logs: loss_final: 0.884881, loss_mean: 0.840732, loss_mean_cls: 0.044149, grad_norm: 0.283589
[[34m2025-10-04 12:22:04[0m] Step: 4831, Training Logs: loss_final: 0.871056, loss_mean: 0.824833, loss_mean_cls: 0.046224, grad_norm: 0.296297
[[34m2025-10-04 12:22:04[0m] Step: 4832, Training Logs: loss_final: 0.901619, loss_mean: 0.856995, loss_mean_cls: 0.044624, grad_norm: 0.290819
[[34m2025-10-04 12:22:04[0m] Step: 4833, Training Logs: loss_final: 0.895335, loss_mean: 0.850652, loss_mean_cls: 0.044683, grad_norm: 0.370727
[[34m2025-10-04 12:22:05[0m] Step: 4834, Training Logs: loss_final: 0.910287, loss_mean: 0.864866, loss_mean_cls: 0.045420, grad_norm: 0.314336
[[34m2025-10-04 12:22:05[0m] Step: 4835, Training Logs: loss_final: 0.902817, loss_mean: 0.856794, loss_mean_cls: 0.046023, grad_norm: 0.349854
[[34m2025-10-04 12:22:05[0m] Step: 4836, Training Logs: loss_final: 0.885154, loss_mean: 0.841755, loss_mean_cls: 0.043399, grad_norm: 0.385594
[[34m2025-10-04 12:22:06[0m] Step: 4837, Training Logs: loss_final: 0.901573, loss_mean: 0.858162, loss_mean_cls: 0.043411, grad_norm: 0.296437
[[34m2025-10-04 12:22:06[0m] Step: 4838, Training Logs: loss_final: 0.886555, loss_mean: 0.842617, loss_mean_cls: 0.043938, grad_norm: 0.332500
[[34m2025-10-04 12:22:06[0m] Step: 4839, Training Logs: loss_final: 0.897605, loss_mean: 0.851040, loss_mean_cls: 0.046565, grad_norm: 0.359381
[[34m2025-10-04 12:22:06[0m] Step: 4840, Training Logs: loss_final: 0.896670, loss_mean: 0.852198, loss_mean_cls: 0.044473, grad_norm: 0.283718
[[34m2025-10-04 12:22:07[0m] Step: 4841, Training Logs: loss_final: 0.878266, loss_mean: 0.832971, loss_mean_cls: 0.045295, grad_norm: 0.340417
[[34m2025-10-04 12:22:07[0m] Step: 4842, Training Logs: loss_final: 0.889674, loss_mean: 0.844418, loss_mean_cls: 0.045256, grad_norm: 0.334486
[[34m2025-10-04 12:22:07[0m] Step: 4843, Training Logs: loss_final: 0.883765, loss_mean: 0.837503, loss_mean_cls: 0.046261, grad_norm: 0.396626
[[34m2025-10-04 12:22:08[0m] Step: 4844, Training Logs: loss_final: 0.895832, loss_mean: 0.850600, loss_mean_cls: 0.045232, grad_norm: 0.284023
[[34m2025-10-04 12:22:08[0m] Step: 4845, Training Logs: loss_final: 0.891264, loss_mean: 0.846231, loss_mean_cls: 0.045032, grad_norm: 0.368848
[[34m2025-10-04 12:22:08[0m] Step: 4846, Training Logs: loss_final: 0.880961, loss_mean: 0.836469, loss_mean_cls: 0.044492, grad_norm: 0.362454
[[34m2025-10-04 12:22:08[0m] Step: 4847, Training Logs: loss_final: 0.887324, loss_mean: 0.842000, loss_mean_cls: 0.045324, grad_norm: 0.290544
[[34m2025-10-04 12:22:09[0m] Step: 4848, Training Logs: loss_final: 0.899827, loss_mean: 0.854767, loss_mean_cls: 0.045060, grad_norm: 0.349144
[[34m2025-10-04 12:22:09[0m] Step: 4849, Training Logs: loss_final: 0.877307, loss_mean: 0.830650, loss_mean_cls: 0.046657, grad_norm: 0.284938
[[34m2025-10-04 12:22:09[0m] Step: 4850, Training Logs: loss_final: 0.878694, loss_mean: 0.834838, loss_mean_cls: 0.043856, grad_norm: 0.445784
[[34m2025-10-04 12:22:10[0m] Step: 4851, Training Logs: loss_final: 0.879673, loss_mean: 0.836097, loss_mean_cls: 0.043576, grad_norm: 0.369015
[[34m2025-10-04 12:22:10[0m] Step: 4852, Training Logs: loss_final: 0.884341, loss_mean: 0.838950, loss_mean_cls: 0.045391, grad_norm: 0.410977
[[34m2025-10-04 12:22:10[0m] Step: 4853, Training Logs: loss_final: 0.886055, loss_mean: 0.842344, loss_mean_cls: 0.043712, grad_norm: 0.320931
[[34m2025-10-04 12:22:10[0m] Step: 4854, Training Logs: loss_final: 0.896194, loss_mean: 0.851586, loss_mean_cls: 0.044608, grad_norm: 0.577863
[[34m2025-10-04 12:22:11[0m] Step: 4855, Training Logs: loss_final: 0.897613, loss_mean: 0.853009, loss_mean_cls: 0.044604, grad_norm: 0.303441
[[34m2025-10-04 12:22:11[0m] Step: 4856, Training Logs: loss_final: 0.879077, loss_mean: 0.833194, loss_mean_cls: 0.045883, grad_norm: 0.438691
[[34m2025-10-04 12:22:11[0m] Step: 4857, Training Logs: loss_final: 0.889158, loss_mean: 0.844932, loss_mean_cls: 0.044226, grad_norm: 0.487032
[[34m2025-10-04 12:22:12[0m] Step: 4858, Training Logs: loss_final: 0.886369, loss_mean: 0.841627, loss_mean_cls: 0.044742, grad_norm: 0.330370
[[34m2025-10-04 12:22:12[0m] Step: 4859, Training Logs: loss_final: 0.892686, loss_mean: 0.846757, loss_mean_cls: 0.045929, grad_norm: 0.411478
[[34m2025-10-04 12:22:12[0m] Step: 4860, Training Logs: loss_final: 0.894762, loss_mean: 0.849819, loss_mean_cls: 0.044943, grad_norm: 0.388232
[[34m2025-10-04 12:22:13[0m] Step: 4861, Training Logs: loss_final: 0.879543, loss_mean: 0.834245, loss_mean_cls: 0.045297, grad_norm: 0.351362
[[34m2025-10-04 12:22:13[0m] Step: 4862, Training Logs: loss_final: 0.886191, loss_mean: 0.841223, loss_mean_cls: 0.044968, grad_norm: 0.430677
[[34m2025-10-04 12:22:13[0m] Step: 4863, Training Logs: loss_final: 0.898740, loss_mean: 0.853355, loss_mean_cls: 0.045386, grad_norm: 0.289528
[[34m2025-10-04 12:22:13[0m] Step: 4864, Training Logs: loss_final: 0.905429, loss_mean: 0.860129, loss_mean_cls: 0.045300, grad_norm: 0.407357
[[34m2025-10-04 12:22:14[0m] Step: 4865, Training Logs: loss_final: 0.884763, loss_mean: 0.838737, loss_mean_cls: 0.046026, grad_norm: 0.360714
[[34m2025-10-04 12:22:14[0m] Step: 4866, Training Logs: loss_final: 0.883065, loss_mean: 0.837522, loss_mean_cls: 0.045543, grad_norm: 0.285689
[[34m2025-10-04 12:22:14[0m] Step: 4867, Training Logs: loss_final: 0.900477, loss_mean: 0.855513, loss_mean_cls: 0.044965, grad_norm: 0.418851
[[34m2025-10-04 12:22:15[0m] Step: 4868, Training Logs: loss_final: 0.898936, loss_mean: 0.854283, loss_mean_cls: 0.044653, grad_norm: 0.365733
[[34m2025-10-04 12:22:15[0m] Step: 4869, Training Logs: loss_final: 0.898545, loss_mean: 0.852650, loss_mean_cls: 0.045895, grad_norm: 0.301675
[[34m2025-10-04 12:22:15[0m] Step: 4870, Training Logs: loss_final: 0.892300, loss_mean: 0.847133, loss_mean_cls: 0.045167, grad_norm: 0.305653
[[34m2025-10-04 12:22:15[0m] Step: 4871, Training Logs: loss_final: 0.902758, loss_mean: 0.858141, loss_mean_cls: 0.044617, grad_norm: 0.355274
[[34m2025-10-04 12:22:16[0m] Step: 4872, Training Logs: loss_final: 0.912395, loss_mean: 0.867592, loss_mean_cls: 0.044803, grad_norm: 0.315783
[[34m2025-10-04 12:22:16[0m] Step: 4873, Training Logs: loss_final: 0.882717, loss_mean: 0.837478, loss_mean_cls: 0.045239, grad_norm: 0.331957
[[34m2025-10-04 12:22:16[0m] Step: 4874, Training Logs: loss_final: 0.873900, loss_mean: 0.827735, loss_mean_cls: 0.046165, grad_norm: 0.263346
[[34m2025-10-04 12:22:17[0m] Step: 4875, Training Logs: loss_final: 0.887472, loss_mean: 0.842024, loss_mean_cls: 0.045448, grad_norm: 0.286833
[[34m2025-10-04 12:22:17[0m] Step: 4876, Training Logs: loss_final: 0.889208, loss_mean: 0.843898, loss_mean_cls: 0.045311, grad_norm: 0.368642
[[34m2025-10-04 12:22:17[0m] Step: 4877, Training Logs: loss_final: 0.896457, loss_mean: 0.851215, loss_mean_cls: 0.045242, grad_norm: 0.289696
[[34m2025-10-04 12:22:18[0m] Step: 4878, Training Logs: loss_final: 0.899801, loss_mean: 0.853807, loss_mean_cls: 0.045994, grad_norm: 0.409752
[[34m2025-10-04 12:22:18[0m] Step: 4879, Training Logs: loss_final: 0.915211, loss_mean: 0.869991, loss_mean_cls: 0.045220, grad_norm: 0.502489
[[34m2025-10-04 12:22:18[0m] Step: 4880, Training Logs: loss_final: 0.912446, loss_mean: 0.867581, loss_mean_cls: 0.044865, grad_norm: 0.284609
[[34m2025-10-04 12:22:18[0m] Step: 4881, Training Logs: loss_final: 0.893221, loss_mean: 0.849223, loss_mean_cls: 0.043998, grad_norm: 0.262161
[[34m2025-10-04 12:22:19[0m] Step: 4882, Training Logs: loss_final: 0.879901, loss_mean: 0.835895, loss_mean_cls: 0.044006, grad_norm: 0.325807
[[34m2025-10-04 12:22:19[0m] Step: 4883, Training Logs: loss_final: 0.892432, loss_mean: 0.847886, loss_mean_cls: 0.044546, grad_norm: 0.328877
[[34m2025-10-04 12:22:19[0m] Step: 4884, Training Logs: loss_final: 0.889047, loss_mean: 0.844556, loss_mean_cls: 0.044491, grad_norm: 0.443890
[[34m2025-10-04 12:22:20[0m] Step: 4885, Training Logs: loss_final: 0.909497, loss_mean: 0.865712, loss_mean_cls: 0.043785, grad_norm: 0.352041
[[34m2025-10-04 12:22:20[0m] Step: 4886, Training Logs: loss_final: 0.895236, loss_mean: 0.850883, loss_mean_cls: 0.044354, grad_norm: 0.452051
[[34m2025-10-04 12:22:20[0m] Step: 4887, Training Logs: loss_final: 0.882847, loss_mean: 0.838408, loss_mean_cls: 0.044439, grad_norm: 0.337866
[[34m2025-10-04 12:22:21[0m] Step: 4888, Training Logs: loss_final: 0.886879, loss_mean: 0.841446, loss_mean_cls: 0.045433, grad_norm: 0.303880
[[34m2025-10-04 12:22:21[0m] Step: 4889, Training Logs: loss_final: 0.884575, loss_mean: 0.840500, loss_mean_cls: 0.044076, grad_norm: 0.423728
[[34m2025-10-04 12:22:21[0m] Step: 4890, Training Logs: loss_final: 0.897201, loss_mean: 0.852764, loss_mean_cls: 0.044437, grad_norm: 0.251678
[[34m2025-10-04 12:22:21[0m] Step: 4891, Training Logs: loss_final: 0.888737, loss_mean: 0.843651, loss_mean_cls: 0.045087, grad_norm: 0.486202
[[34m2025-10-04 12:22:22[0m] Step: 4892, Training Logs: loss_final: 0.875380, loss_mean: 0.829537, loss_mean_cls: 0.045843, grad_norm: 0.385428
[[34m2025-10-04 12:22:22[0m] Step: 4893, Training Logs: loss_final: 0.881541, loss_mean: 0.836032, loss_mean_cls: 0.045509, grad_norm: 0.407220
[[34m2025-10-04 12:22:22[0m] Step: 4894, Training Logs: loss_final: 0.869164, loss_mean: 0.824384, loss_mean_cls: 0.044780, grad_norm: 0.311731
[[34m2025-10-04 12:22:23[0m] Step: 4895, Training Logs: loss_final: 0.891283, loss_mean: 0.846407, loss_mean_cls: 0.044876, grad_norm: 0.529966
[[34m2025-10-04 12:22:23[0m] Step: 4896, Training Logs: loss_final: 0.912817, loss_mean: 0.868405, loss_mean_cls: 0.044412, grad_norm: 0.406448
[[34m2025-10-04 12:22:23[0m] Step: 4897, Training Logs: loss_final: 0.911711, loss_mean: 0.868100, loss_mean_cls: 0.043611, grad_norm: 0.370951
[[34m2025-10-04 12:22:23[0m] Step: 4898, Training Logs: loss_final: 0.890678, loss_mean: 0.845830, loss_mean_cls: 0.044847, grad_norm: 0.379272
[[34m2025-10-04 12:22:24[0m] Step: 4899, Training Logs: loss_final: 0.887717, loss_mean: 0.842194, loss_mean_cls: 0.045524, grad_norm: 0.276767
[[34m2025-10-04 12:22:24[0m] Step: 4900, Training Logs: loss_final: 0.883894, loss_mean: 0.837973, loss_mean_cls: 0.045921, grad_norm: 0.451743
[[34m2025-10-04 12:22:24[0m] Step: 4901, Training Logs: loss_final: 0.891499, loss_mean: 0.845363, loss_mean_cls: 0.046136, grad_norm: 0.333292
[[34m2025-10-04 12:22:25[0m] Step: 4902, Training Logs: loss_final: 0.893895, loss_mean: 0.849867, loss_mean_cls: 0.044028, grad_norm: 0.509463
[[34m2025-10-04 12:22:25[0m] Step: 4903, Training Logs: loss_final: 0.899093, loss_mean: 0.854541, loss_mean_cls: 0.044551, grad_norm: 0.653659
[[34m2025-10-04 12:22:25[0m] Step: 4904, Training Logs: loss_final: 0.857769, loss_mean: 0.812941, loss_mean_cls: 0.044828, grad_norm: 0.304237
[[34m2025-10-04 12:22:25[0m] Step: 4905, Training Logs: loss_final: 0.894323, loss_mean: 0.848486, loss_mean_cls: 0.045837, grad_norm: 0.643084
[[34m2025-10-04 12:22:26[0m] Step: 4906, Training Logs: loss_final: 0.894852, loss_mean: 0.850190, loss_mean_cls: 0.044663, grad_norm: 0.490512
[[34m2025-10-04 12:22:26[0m] Step: 4907, Training Logs: loss_final: 0.907424, loss_mean: 0.862234, loss_mean_cls: 0.045190, grad_norm: 0.480402
[[34m2025-10-04 12:22:26[0m] Step: 4908, Training Logs: loss_final: 0.879780, loss_mean: 0.834279, loss_mean_cls: 0.045500, grad_norm: 0.810123
[[34m2025-10-04 12:22:27[0m] Step: 4909, Training Logs: loss_final: 0.870097, loss_mean: 0.823112, loss_mean_cls: 0.046985, grad_norm: 0.235105
[[34m2025-10-04 12:22:27[0m] Step: 4910, Training Logs: loss_final: 0.880575, loss_mean: 0.834963, loss_mean_cls: 0.045612, grad_norm: 0.756143
[[34m2025-10-04 12:22:27[0m] Step: 4911, Training Logs: loss_final: 0.895005, loss_mean: 0.849849, loss_mean_cls: 0.045156, grad_norm: 0.510569
[[34m2025-10-04 12:22:28[0m] Step: 4912, Training Logs: loss_final: 0.888639, loss_mean: 0.843709, loss_mean_cls: 0.044931, grad_norm: 0.452589
[[34m2025-10-04 12:22:28[0m] Step: 4913, Training Logs: loss_final: 0.892000, loss_mean: 0.847625, loss_mean_cls: 0.044375, grad_norm: 0.572163
[[34m2025-10-04 12:22:28[0m] Step: 4914, Training Logs: loss_final: 0.898885, loss_mean: 0.854298, loss_mean_cls: 0.044587, grad_norm: 0.311254
[[34m2025-10-04 12:22:28[0m] Step: 4915, Training Logs: loss_final: 0.897011, loss_mean: 0.853044, loss_mean_cls: 0.043967, grad_norm: 0.531234
[[34m2025-10-04 12:22:29[0m] Step: 4916, Training Logs: loss_final: 0.905898, loss_mean: 0.860928, loss_mean_cls: 0.044970, grad_norm: 0.393714
[[34m2025-10-04 12:22:29[0m] Step: 4917, Training Logs: loss_final: 0.891521, loss_mean: 0.847426, loss_mean_cls: 0.044095, grad_norm: 0.412243
[[34m2025-10-04 12:22:29[0m] Step: 4918, Training Logs: loss_final: 0.890680, loss_mean: 0.845022, loss_mean_cls: 0.045658, grad_norm: 0.392861
[[34m2025-10-04 12:22:30[0m] Step: 4919, Training Logs: loss_final: 0.889209, loss_mean: 0.844681, loss_mean_cls: 0.044528, grad_norm: 0.566323
[[34m2025-10-04 12:22:30[0m] Step: 4920, Training Logs: loss_final: 0.902922, loss_mean: 0.856301, loss_mean_cls: 0.046620, grad_norm: 0.474244
[[34m2025-10-04 12:22:30[0m] Step: 4921, Training Logs: loss_final: 0.901945, loss_mean: 0.857641, loss_mean_cls: 0.044304, grad_norm: 0.383195
[[34m2025-10-04 12:22:30[0m] Step: 4922, Training Logs: loss_final: 0.900768, loss_mean: 0.855191, loss_mean_cls: 0.045577, grad_norm: 0.344532
[[34m2025-10-04 12:22:31[0m] Step: 4923, Training Logs: loss_final: 0.897840, loss_mean: 0.853472, loss_mean_cls: 0.044367, grad_norm: 0.346721
[[34m2025-10-04 12:22:31[0m] Step: 4924, Training Logs: loss_final: 0.893499, loss_mean: 0.847802, loss_mean_cls: 0.045698, grad_norm: 0.448788
[[34m2025-10-04 12:22:31[0m] Step: 4925, Training Logs: loss_final: 0.868586, loss_mean: 0.824793, loss_mean_cls: 0.043793, grad_norm: 0.320045
[[34m2025-10-04 12:22:32[0m] Step: 4926, Training Logs: loss_final: 0.897125, loss_mean: 0.853493, loss_mean_cls: 0.043633, grad_norm: 0.442396
[[34m2025-10-04 12:22:32[0m] Step: 4927, Training Logs: loss_final: 0.897434, loss_mean: 0.853746, loss_mean_cls: 0.043687, grad_norm: 0.373948
[[34m2025-10-04 12:22:32[0m] Step: 4928, Training Logs: loss_final: 0.887761, loss_mean: 0.843338, loss_mean_cls: 0.044423, grad_norm: 0.345195
[[34m2025-10-04 12:22:32[0m] Step: 4929, Training Logs: loss_final: 0.899740, loss_mean: 0.855023, loss_mean_cls: 0.044717, grad_norm: 0.382769
[[34m2025-10-04 12:22:33[0m] Step: 4930, Training Logs: loss_final: 0.890548, loss_mean: 0.845930, loss_mean_cls: 0.044618, grad_norm: 0.366119
[[34m2025-10-04 12:22:33[0m] Step: 4931, Training Logs: loss_final: 0.874462, loss_mean: 0.830296, loss_mean_cls: 0.044166, grad_norm: 0.409487
[[34m2025-10-04 12:22:33[0m] Step: 4932, Training Logs: loss_final: 0.883240, loss_mean: 0.839460, loss_mean_cls: 0.043780, grad_norm: 0.313332
[[34m2025-10-04 12:22:34[0m] Step: 4933, Training Logs: loss_final: 0.883047, loss_mean: 0.838839, loss_mean_cls: 0.044207, grad_norm: 0.419738
[[34m2025-10-04 12:22:34[0m] Step: 4934, Training Logs: loss_final: 0.890464, loss_mean: 0.846427, loss_mean_cls: 0.044037, grad_norm: 0.319387
[[34m2025-10-04 12:22:34[0m] Step: 4935, Training Logs: loss_final: 0.903103, loss_mean: 0.859303, loss_mean_cls: 0.043800, grad_norm: 0.378980
[[34m2025-10-04 12:22:34[0m] Step: 4936, Training Logs: loss_final: 0.889491, loss_mean: 0.844506, loss_mean_cls: 0.044985, grad_norm: 0.429991
[[34m2025-10-04 12:22:35[0m] Step: 4937, Training Logs: loss_final: 0.883581, loss_mean: 0.838786, loss_mean_cls: 0.044794, grad_norm: 0.351263
[[34m2025-10-04 12:22:35[0m] Step: 4938, Training Logs: loss_final: 0.883748, loss_mean: 0.839830, loss_mean_cls: 0.043918, grad_norm: 0.316214
[[34m2025-10-04 12:22:35[0m] Step: 4939, Training Logs: loss_final: 0.900814, loss_mean: 0.856642, loss_mean_cls: 0.044172, grad_norm: 0.287525
[[34m2025-10-04 12:22:36[0m] Step: 4940, Training Logs: loss_final: 0.896497, loss_mean: 0.851597, loss_mean_cls: 0.044900, grad_norm: 0.331092
[[34m2025-10-04 12:22:36[0m] Step: 4941, Training Logs: loss_final: 0.897711, loss_mean: 0.852851, loss_mean_cls: 0.044860, grad_norm: 0.373998
[[34m2025-10-04 12:22:36[0m] Step: 4942, Training Logs: loss_final: 0.892756, loss_mean: 0.845626, loss_mean_cls: 0.047131, grad_norm: 0.336057
[[34m2025-10-04 12:22:37[0m] Step: 4943, Training Logs: loss_final: 0.895184, loss_mean: 0.850731, loss_mean_cls: 0.044453, grad_norm: 0.426927
[[34m2025-10-04 12:22:37[0m] Step: 4944, Training Logs: loss_final: 0.878071, loss_mean: 0.833962, loss_mean_cls: 0.044109, grad_norm: 0.346103
[[34m2025-10-04 12:22:37[0m] Step: 4945, Training Logs: loss_final: 0.889533, loss_mean: 0.845266, loss_mean_cls: 0.044267, grad_norm: 0.361744
[[34m2025-10-04 12:22:37[0m] Step: 4946, Training Logs: loss_final: 0.881083, loss_mean: 0.834892, loss_mean_cls: 0.046190, grad_norm: 0.296305
[[34m2025-10-04 12:22:38[0m] Step: 4947, Training Logs: loss_final: 0.889929, loss_mean: 0.844610, loss_mean_cls: 0.045319, grad_norm: 0.430127
[[34m2025-10-04 12:22:38[0m] Step: 4948, Training Logs: loss_final: 0.876823, loss_mean: 0.831368, loss_mean_cls: 0.045455, grad_norm: 0.279944
[[34m2025-10-04 12:22:38[0m] Step: 4949, Training Logs: loss_final: 0.900669, loss_mean: 0.856882, loss_mean_cls: 0.043787, grad_norm: 0.344186
[[34m2025-10-04 12:22:39[0m] Step: 4950, Training Logs: loss_final: 0.871393, loss_mean: 0.827597, loss_mean_cls: 0.043797, grad_norm: 0.365242
[[34m2025-10-04 12:22:39[0m] Step: 4951, Training Logs: loss_final: 0.894096, loss_mean: 0.849589, loss_mean_cls: 0.044507, grad_norm: 0.316155
[[34m2025-10-04 12:22:39[0m] Step: 4952, Training Logs: loss_final: 0.875668, loss_mean: 0.830362, loss_mean_cls: 0.045306, grad_norm: 0.415271
[[34m2025-10-04 12:22:39[0m] Step: 4953, Training Logs: loss_final: 0.890778, loss_mean: 0.845785, loss_mean_cls: 0.044993, grad_norm: 0.417699
[[34m2025-10-04 12:22:40[0m] Step: 4954, Training Logs: loss_final: 0.880160, loss_mean: 0.835943, loss_mean_cls: 0.044217, grad_norm: 0.327166
[[34m2025-10-04 12:22:40[0m] Step: 4955, Training Logs: loss_final: 0.900883, loss_mean: 0.856179, loss_mean_cls: 0.044704, grad_norm: 0.346975
[[34m2025-10-04 12:22:40[0m] Step: 4956, Training Logs: loss_final: 0.891807, loss_mean: 0.847884, loss_mean_cls: 0.043923, grad_norm: 0.285233
[[34m2025-10-04 12:22:41[0m] Step: 4957, Training Logs: loss_final: 0.893456, loss_mean: 0.849541, loss_mean_cls: 0.043915, grad_norm: 0.350800
[[34m2025-10-04 12:22:41[0m] Step: 4958, Training Logs: loss_final: 0.889813, loss_mean: 0.845292, loss_mean_cls: 0.044521, grad_norm: 0.335134
[[34m2025-10-04 12:22:41[0m] Step: 4959, Training Logs: loss_final: 0.889133, loss_mean: 0.845021, loss_mean_cls: 0.044112, grad_norm: 0.306827
[[34m2025-10-04 12:22:42[0m] Step: 4960, Training Logs: loss_final: 0.890252, loss_mean: 0.845271, loss_mean_cls: 0.044981, grad_norm: 0.490188
[[34m2025-10-04 12:22:42[0m] Step: 4961, Training Logs: loss_final: 0.868557, loss_mean: 0.822952, loss_mean_cls: 0.045605, grad_norm: 0.329552
[[34m2025-10-04 12:22:42[0m] Step: 4962, Training Logs: loss_final: 0.884157, loss_mean: 0.838667, loss_mean_cls: 0.045489, grad_norm: 0.495908
[[34m2025-10-04 12:22:42[0m] Step: 4963, Training Logs: loss_final: 0.879514, loss_mean: 0.831896, loss_mean_cls: 0.047618, grad_norm: 0.393666
[[34m2025-10-04 12:22:43[0m] Step: 4964, Training Logs: loss_final: 0.876771, loss_mean: 0.832483, loss_mean_cls: 0.044288, grad_norm: 0.299566
[[34m2025-10-04 12:22:43[0m] Step: 4965, Training Logs: loss_final: 0.870896, loss_mean: 0.826052, loss_mean_cls: 0.044844, grad_norm: 0.441753
[[34m2025-10-04 12:22:43[0m] Step: 4966, Training Logs: loss_final: 0.889366, loss_mean: 0.844532, loss_mean_cls: 0.044834, grad_norm: 0.495196
[[34m2025-10-04 12:22:44[0m] Step: 4967, Training Logs: loss_final: 0.889450, loss_mean: 0.844456, loss_mean_cls: 0.044995, grad_norm: 0.337310
[[34m2025-10-04 12:22:44[0m] Step: 4968, Training Logs: loss_final: 0.888483, loss_mean: 0.845002, loss_mean_cls: 0.043481, grad_norm: 0.352148
[[34m2025-10-04 12:22:44[0m] Step: 4969, Training Logs: loss_final: 0.875580, loss_mean: 0.830054, loss_mean_cls: 0.045526, grad_norm: 0.485958
[[34m2025-10-04 12:22:45[0m] Step: 4970, Training Logs: loss_final: 0.888659, loss_mean: 0.844108, loss_mean_cls: 0.044551, grad_norm: 0.404627
[[34m2025-10-04 12:22:45[0m] Step: 4971, Training Logs: loss_final: 0.895630, loss_mean: 0.851105, loss_mean_cls: 0.044525, grad_norm: 0.302478
[[34m2025-10-04 12:22:45[0m] Step: 4972, Training Logs: loss_final: 0.913399, loss_mean: 0.869174, loss_mean_cls: 0.044225, grad_norm: 0.406730
[[34m2025-10-04 12:22:45[0m] Step: 4973, Training Logs: loss_final: 0.872403, loss_mean: 0.827967, loss_mean_cls: 0.044435, grad_norm: 0.470445
[[34m2025-10-04 12:22:46[0m] Step: 4974, Training Logs: loss_final: 0.914238, loss_mean: 0.869922, loss_mean_cls: 0.044316, grad_norm: 0.387091
[[34m2025-10-04 12:22:46[0m] Step: 4975, Training Logs: loss_final: 0.899640, loss_mean: 0.855081, loss_mean_cls: 0.044559, grad_norm: 0.370424
[[34m2025-10-04 12:22:46[0m] Step: 4976, Training Logs: loss_final: 0.861538, loss_mean: 0.815920, loss_mean_cls: 0.045618, grad_norm: 0.443697
[[34m2025-10-04 12:22:47[0m] Step: 4977, Training Logs: loss_final: 0.898909, loss_mean: 0.854261, loss_mean_cls: 0.044648, grad_norm: 0.646008
[[34m2025-10-04 12:22:47[0m] Step: 4978, Training Logs: loss_final: 0.884970, loss_mean: 0.840231, loss_mean_cls: 0.044739, grad_norm: 0.435065
[[34m2025-10-04 12:22:47[0m] Step: 4979, Training Logs: loss_final: 0.873299, loss_mean: 0.827934, loss_mean_cls: 0.045366, grad_norm: 0.421010
[[34m2025-10-04 12:22:47[0m] Step: 4980, Training Logs: loss_final: 0.904561, loss_mean: 0.859498, loss_mean_cls: 0.045064, grad_norm: 0.509945
[[34m2025-10-04 12:22:48[0m] Step: 4981, Training Logs: loss_final: 0.861299, loss_mean: 0.817090, loss_mean_cls: 0.044209, grad_norm: 0.410456
[[34m2025-10-04 12:22:48[0m] Step: 4982, Training Logs: loss_final: 0.868280, loss_mean: 0.822101, loss_mean_cls: 0.046179, grad_norm: 0.367793
[[34m2025-10-04 12:22:48[0m] Step: 4983, Training Logs: loss_final: 0.867413, loss_mean: 0.821839, loss_mean_cls: 0.045574, grad_norm: 0.465171
[[34m2025-10-04 12:22:49[0m] Step: 4984, Training Logs: loss_final: 0.886323, loss_mean: 0.841421, loss_mean_cls: 0.044902, grad_norm: 0.363544
[[34m2025-10-04 12:22:49[0m] Step: 4985, Training Logs: loss_final: 0.906796, loss_mean: 0.861651, loss_mean_cls: 0.045145, grad_norm: 0.366836
[[34m2025-10-04 12:22:49[0m] Step: 4986, Training Logs: loss_final: 0.857342, loss_mean: 0.811950, loss_mean_cls: 0.045392, grad_norm: 0.299428
[[34m2025-10-04 12:22:50[0m] Step: 4987, Training Logs: loss_final: 0.892298, loss_mean: 0.845750, loss_mean_cls: 0.046549, grad_norm: 0.362827
[[34m2025-10-04 12:22:50[0m] Step: 4988, Training Logs: loss_final: 0.884683, loss_mean: 0.839862, loss_mean_cls: 0.044821, grad_norm: 0.399690
[[34m2025-10-04 12:22:50[0m] Step: 4989, Training Logs: loss_final: 0.894313, loss_mean: 0.850022, loss_mean_cls: 0.044291, grad_norm: 0.300690
[[34m2025-10-04 12:22:50[0m] Step: 4990, Training Logs: loss_final: 0.884334, loss_mean: 0.839069, loss_mean_cls: 0.045266, grad_norm: 0.442421
[[34m2025-10-04 12:22:51[0m] Step: 4991, Training Logs: loss_final: 0.882111, loss_mean: 0.837497, loss_mean_cls: 0.044614, grad_norm: 0.362504
[[34m2025-10-04 12:22:51[0m] Step: 4992, Training Logs: loss_final: 0.875723, loss_mean: 0.830076, loss_mean_cls: 0.045648, grad_norm: 0.284767
[[34m2025-10-04 12:22:51[0m] Step: 4993, Training Logs: loss_final: 0.897407, loss_mean: 0.852814, loss_mean_cls: 0.044593, grad_norm: 0.348201
[[34m2025-10-04 12:22:52[0m] Step: 4994, Training Logs: loss_final: 0.909384, loss_mean: 0.864236, loss_mean_cls: 0.045148, grad_norm: 0.317898
[[34m2025-10-04 12:22:52[0m] Step: 4995, Training Logs: loss_final: 0.884932, loss_mean: 0.839491, loss_mean_cls: 0.045440, grad_norm: 0.429789
[[34m2025-10-04 12:22:52[0m] Step: 4996, Training Logs: loss_final: 0.883006, loss_mean: 0.838010, loss_mean_cls: 0.044997, grad_norm: 0.268080
[[34m2025-10-04 12:22:53[0m] Step: 4997, Training Logs: loss_final: 0.889333, loss_mean: 0.843415, loss_mean_cls: 0.045918, grad_norm: 0.370301
[[34m2025-10-04 12:22:53[0m] Step: 4998, Training Logs: loss_final: 0.890481, loss_mean: 0.845525, loss_mean_cls: 0.044956, grad_norm: 0.427628
[[34m2025-10-04 12:22:53[0m] Step: 4999, Training Logs: loss_final: 0.899320, loss_mean: 0.854569, loss_mean_cls: 0.044751, grad_norm: 0.337961
[[34m2025-10-04 12:22:54[0m] Step: 5000, Training Logs: loss_final: 0.889892, loss_mean: 0.845378, loss_mean_cls: 0.044514, grad_norm: 0.283304
[[34m2025-10-04 12:22:56[0m] Step: 5001, Training Logs: loss_final: 0.887896, loss_mean: 0.842665, loss_mean_cls: 0.045231, grad_norm: 0.382052
[[34m2025-10-04 12:22:57[0m] Step: 5002, Training Logs: loss_final: 0.876842, loss_mean: 0.832223, loss_mean_cls: 0.044620, grad_norm: 0.349873
[[34m2025-10-04 12:22:57[0m] Step: 5003, Training Logs: loss_final: 0.903280, loss_mean: 0.859907, loss_mean_cls: 0.043373, grad_norm: 0.292287
[[34m2025-10-04 12:22:57[0m] Step: 5004, Training Logs: loss_final: 0.890892, loss_mean: 0.846322, loss_mean_cls: 0.044569, grad_norm: 0.411170
[[34m2025-10-04 12:22:57[0m] Step: 5005, Training Logs: loss_final: 0.888225, loss_mean: 0.842532, loss_mean_cls: 0.045694, grad_norm: 0.292815
[[34m2025-10-04 12:22:58[0m] Step: 5006, Training Logs: loss_final: 0.849332, loss_mean: 0.803591, loss_mean_cls: 0.045741, grad_norm: 0.341605
[[34m2025-10-04 12:22:58[0m] Step: 5007, Training Logs: loss_final: 0.889755, loss_mean: 0.844776, loss_mean_cls: 0.044979, grad_norm: 0.275567
[[34m2025-10-04 12:22:58[0m] Step: 5008, Training Logs: loss_final: 0.898959, loss_mean: 0.854383, loss_mean_cls: 0.044576, grad_norm: 0.456576
[[34m2025-10-04 12:22:59[0m] Step: 5009, Training Logs: loss_final: 0.879749, loss_mean: 0.834077, loss_mean_cls: 0.045672, grad_norm: 0.333684
[[34m2025-10-04 12:22:59[0m] Step: 5010, Training Logs: loss_final: 0.902619, loss_mean: 0.859025, loss_mean_cls: 0.043593, grad_norm: 0.243782
[[34m2025-10-04 12:22:59[0m] Step: 5011, Training Logs: loss_final: 0.869468, loss_mean: 0.823867, loss_mean_cls: 0.045601, grad_norm: 0.329756
[[34m2025-10-04 12:23:00[0m] Step: 5012, Training Logs: loss_final: 0.904376, loss_mean: 0.859571, loss_mean_cls: 0.044805, grad_norm: 0.312450
[[34m2025-10-04 12:23:00[0m] Step: 5013, Training Logs: loss_final: 0.894449, loss_mean: 0.850593, loss_mean_cls: 0.043855, grad_norm: 0.326643
[[34m2025-10-04 12:23:00[0m] Step: 5014, Training Logs: loss_final: 0.895232, loss_mean: 0.850998, loss_mean_cls: 0.044233, grad_norm: 0.261176
[[34m2025-10-04 12:23:00[0m] Step: 5015, Training Logs: loss_final: 0.884243, loss_mean: 0.840129, loss_mean_cls: 0.044114, grad_norm: 0.281782
[[34m2025-10-04 12:23:01[0m] Step: 5016, Training Logs: loss_final: 0.905961, loss_mean: 0.861241, loss_mean_cls: 0.044719, grad_norm: 0.318384
[[34m2025-10-04 12:23:01[0m] Step: 5017, Training Logs: loss_final: 0.872930, loss_mean: 0.827567, loss_mean_cls: 0.045363, grad_norm: 0.299792
[[34m2025-10-04 12:23:01[0m] Step: 5018, Training Logs: loss_final: 0.894557, loss_mean: 0.850140, loss_mean_cls: 0.044417, grad_norm: 0.339291
[[34m2025-10-04 12:23:02[0m] Step: 5019, Training Logs: loss_final: 0.905179, loss_mean: 0.860903, loss_mean_cls: 0.044276, grad_norm: 0.242808
[[34m2025-10-04 12:23:02[0m] Step: 5020, Training Logs: loss_final: 0.873199, loss_mean: 0.827318, loss_mean_cls: 0.045881, grad_norm: 0.358193
[[34m2025-10-04 12:23:02[0m] Step: 5021, Training Logs: loss_final: 0.895888, loss_mean: 0.850772, loss_mean_cls: 0.045117, grad_norm: 0.263545
[[34m2025-10-04 12:23:02[0m] Step: 5022, Training Logs: loss_final: 0.895685, loss_mean: 0.850453, loss_mean_cls: 0.045233, grad_norm: 0.403170
[[34m2025-10-04 12:23:03[0m] Step: 5023, Training Logs: loss_final: 0.868183, loss_mean: 0.823646, loss_mean_cls: 0.044537, grad_norm: 0.422361
[[34m2025-10-04 12:23:03[0m] Step: 5024, Training Logs: loss_final: 0.885105, loss_mean: 0.840274, loss_mean_cls: 0.044831, grad_norm: 0.500973
[[34m2025-10-04 12:23:03[0m] Step: 5025, Training Logs: loss_final: 0.889516, loss_mean: 0.844122, loss_mean_cls: 0.045394, grad_norm: 0.395871
[[34m2025-10-04 12:23:04[0m] Step: 5026, Training Logs: loss_final: 0.886479, loss_mean: 0.842262, loss_mean_cls: 0.044217, grad_norm: 0.328457
[[34m2025-10-04 12:23:04[0m] Step: 5027, Training Logs: loss_final: 0.878820, loss_mean: 0.833393, loss_mean_cls: 0.045427, grad_norm: 0.318744
[[34m2025-10-04 12:23:04[0m] Step: 5028, Training Logs: loss_final: 0.892001, loss_mean: 0.846124, loss_mean_cls: 0.045878, grad_norm: 0.341150
[[34m2025-10-04 12:23:04[0m] Step: 5029, Training Logs: loss_final: 0.906221, loss_mean: 0.862610, loss_mean_cls: 0.043611, grad_norm: 0.328906
[[34m2025-10-04 12:23:05[0m] Step: 5030, Training Logs: loss_final: 0.884340, loss_mean: 0.838773, loss_mean_cls: 0.045567, grad_norm: 0.337488
[[34m2025-10-04 12:23:05[0m] Step: 5031, Training Logs: loss_final: 0.887704, loss_mean: 0.841539, loss_mean_cls: 0.046165, grad_norm: 0.345466
[[34m2025-10-04 12:23:05[0m] Step: 5032, Training Logs: loss_final: 0.896867, loss_mean: 0.852198, loss_mean_cls: 0.044669, grad_norm: 0.308814
[[34m2025-10-04 12:23:06[0m] Step: 5033, Training Logs: loss_final: 0.913934, loss_mean: 0.870877, loss_mean_cls: 0.043057, grad_norm: 0.449741
[[34m2025-10-04 12:23:06[0m] Step: 5034, Training Logs: loss_final: 0.898273, loss_mean: 0.852429, loss_mean_cls: 0.045844, grad_norm: 0.459183
[[34m2025-10-04 12:23:06[0m] Step: 5035, Training Logs: loss_final: 0.902091, loss_mean: 0.856826, loss_mean_cls: 0.045265, grad_norm: 0.361427
[[34m2025-10-04 12:23:07[0m] Step: 5036, Training Logs: loss_final: 0.891533, loss_mean: 0.847328, loss_mean_cls: 0.044205, grad_norm: 0.444146
[[34m2025-10-04 12:23:07[0m] Step: 5037, Training Logs: loss_final: 0.902073, loss_mean: 0.856998, loss_mean_cls: 0.045075, grad_norm: 0.322676
[[34m2025-10-04 12:23:07[0m] Step: 5038, Training Logs: loss_final: 0.886852, loss_mean: 0.841186, loss_mean_cls: 0.045666, grad_norm: 0.330370
[[34m2025-10-04 12:23:07[0m] Step: 5039, Training Logs: loss_final: 0.905137, loss_mean: 0.860145, loss_mean_cls: 0.044992, grad_norm: 0.425072
[[34m2025-10-04 12:23:08[0m] Step: 5040, Training Logs: loss_final: 0.904554, loss_mean: 0.859578, loss_mean_cls: 0.044976, grad_norm: 0.295184
[[34m2025-10-04 12:23:08[0m] Step: 5041, Training Logs: loss_final: 0.884011, loss_mean: 0.837661, loss_mean_cls: 0.046350, grad_norm: 0.292924
[[34m2025-10-04 12:23:08[0m] Step: 5042, Training Logs: loss_final: 0.892036, loss_mean: 0.847481, loss_mean_cls: 0.044555, grad_norm: 0.377181
[[34m2025-10-04 12:23:09[0m] Step: 5043, Training Logs: loss_final: 0.913434, loss_mean: 0.866847, loss_mean_cls: 0.046588, grad_norm: 0.507833
[[34m2025-10-04 12:23:09[0m] Step: 5044, Training Logs: loss_final: 0.908606, loss_mean: 0.865200, loss_mean_cls: 0.043406, grad_norm: 0.419438
[[34m2025-10-04 12:23:09[0m] Step: 5045, Training Logs: loss_final: 0.869246, loss_mean: 0.823172, loss_mean_cls: 0.046074, grad_norm: 0.348505
[[34m2025-10-04 12:23:09[0m] Step: 5046, Training Logs: loss_final: 0.878430, loss_mean: 0.832356, loss_mean_cls: 0.046075, grad_norm: 0.365034
[[34m2025-10-04 12:23:10[0m] Step: 5047, Training Logs: loss_final: 0.873240, loss_mean: 0.827716, loss_mean_cls: 0.045523, grad_norm: 0.376252
[[34m2025-10-04 12:23:10[0m] Step: 5048, Training Logs: loss_final: 0.887395, loss_mean: 0.841843, loss_mean_cls: 0.045553, grad_norm: 0.296563
[[34m2025-10-04 12:23:10[0m] Step: 5049, Training Logs: loss_final: 0.885579, loss_mean: 0.841263, loss_mean_cls: 0.044316, grad_norm: 0.277993
[[34m2025-10-04 12:23:11[0m] Step: 5050, Training Logs: loss_final: 0.881785, loss_mean: 0.836104, loss_mean_cls: 0.045681, grad_norm: 0.377924
[[34m2025-10-04 12:23:11[0m] Step: 5051, Training Logs: loss_final: 0.890214, loss_mean: 0.844872, loss_mean_cls: 0.045343, grad_norm: 0.261643
[[34m2025-10-04 12:23:11[0m] Step: 5052, Training Logs: loss_final: 0.883472, loss_mean: 0.838438, loss_mean_cls: 0.045033, grad_norm: 0.333099
[[34m2025-10-04 12:23:11[0m] Step: 5053, Training Logs: loss_final: 0.904641, loss_mean: 0.860744, loss_mean_cls: 0.043896, grad_norm: 0.339902
[[34m2025-10-04 12:23:12[0m] Step: 5054, Training Logs: loss_final: 0.896784, loss_mean: 0.852137, loss_mean_cls: 0.044647, grad_norm: 0.360380
[[34m2025-10-04 12:23:12[0m] Step: 5055, Training Logs: loss_final: 0.873751, loss_mean: 0.830227, loss_mean_cls: 0.043524, grad_norm: 0.248312
[[34m2025-10-04 12:23:12[0m] Step: 5056, Training Logs: loss_final: 0.885605, loss_mean: 0.840460, loss_mean_cls: 0.045145, grad_norm: 0.367709
[[34m2025-10-04 12:23:13[0m] Step: 5057, Training Logs: loss_final: 0.872736, loss_mean: 0.826463, loss_mean_cls: 0.046273, grad_norm: 0.366289
[[34m2025-10-04 12:23:13[0m] Step: 5058, Training Logs: loss_final: 0.894521, loss_mean: 0.849164, loss_mean_cls: 0.045357, grad_norm: 0.240726
[[34m2025-10-04 12:23:13[0m] Step: 5059, Training Logs: loss_final: 0.893107, loss_mean: 0.847860, loss_mean_cls: 0.045247, grad_norm: 0.420520
[[34m2025-10-04 12:23:14[0m] Step: 5060, Training Logs: loss_final: 0.893247, loss_mean: 0.849376, loss_mean_cls: 0.043871, grad_norm: 0.296750
[[34m2025-10-04 12:23:14[0m] Step: 5061, Training Logs: loss_final: 0.907611, loss_mean: 0.863888, loss_mean_cls: 0.043723, grad_norm: 0.287277
[[34m2025-10-04 12:23:14[0m] Step: 5062, Training Logs: loss_final: 0.882456, loss_mean: 0.836191, loss_mean_cls: 0.046265, grad_norm: 0.331452
[[34m2025-10-04 12:23:14[0m] Step: 5063, Training Logs: loss_final: 0.876994, loss_mean: 0.833065, loss_mean_cls: 0.043929, grad_norm: 0.286023
[[34m2025-10-04 12:23:15[0m] Step: 5064, Training Logs: loss_final: 0.888956, loss_mean: 0.843253, loss_mean_cls: 0.045703, grad_norm: 0.266272
[[34m2025-10-04 12:23:16[0m] Step: 5065, Training Logs: loss_final: 0.906311, loss_mean: 0.861926, loss_mean_cls: 0.044385, grad_norm: 0.334589
[[34m2025-10-04 12:23:16[0m] Step: 5066, Training Logs: loss_final: 0.883876, loss_mean: 0.839501, loss_mean_cls: 0.044376, grad_norm: 0.330351
[[34m2025-10-04 12:23:16[0m] Step: 5067, Training Logs: loss_final: 0.884554, loss_mean: 0.840668, loss_mean_cls: 0.043886, grad_norm: 0.265865
[[34m2025-10-04 12:23:17[0m] Step: 5068, Training Logs: loss_final: 0.875359, loss_mean: 0.829952, loss_mean_cls: 0.045407, grad_norm: 0.403476
[[34m2025-10-04 12:23:17[0m] Step: 5069, Training Logs: loss_final: 0.895659, loss_mean: 0.850935, loss_mean_cls: 0.044723, grad_norm: 0.297746
[[34m2025-10-04 12:23:17[0m] Step: 5070, Training Logs: loss_final: 0.892699, loss_mean: 0.848583, loss_mean_cls: 0.044116, grad_norm: 0.383745
[[34m2025-10-04 12:23:18[0m] Step: 5071, Training Logs: loss_final: 0.887560, loss_mean: 0.842687, loss_mean_cls: 0.044873, grad_norm: 0.419757
[[34m2025-10-04 12:23:18[0m] Step: 5072, Training Logs: loss_final: 0.890320, loss_mean: 0.845841, loss_mean_cls: 0.044479, grad_norm: 0.377018
[[34m2025-10-04 12:23:18[0m] Step: 5073, Training Logs: loss_final: 0.900331, loss_mean: 0.855464, loss_mean_cls: 0.044867, grad_norm: 0.409831
[[34m2025-10-04 12:23:18[0m] Step: 5074, Training Logs: loss_final: 0.902692, loss_mean: 0.857296, loss_mean_cls: 0.045395, grad_norm: 0.553192
[[34m2025-10-04 12:23:19[0m] Step: 5075, Training Logs: loss_final: 0.896608, loss_mean: 0.852161, loss_mean_cls: 0.044447, grad_norm: 0.298747
[[34m2025-10-04 12:23:19[0m] Step: 5076, Training Logs: loss_final: 0.878596, loss_mean: 0.833673, loss_mean_cls: 0.044923, grad_norm: 0.304954
[[34m2025-10-04 12:23:19[0m] Step: 5077, Training Logs: loss_final: 0.883377, loss_mean: 0.837961, loss_mean_cls: 0.045416, grad_norm: 0.430318
[[34m2025-10-04 12:23:20[0m] Step: 5078, Training Logs: loss_final: 0.894156, loss_mean: 0.848877, loss_mean_cls: 0.045279, grad_norm: 0.325463
[[34m2025-10-04 12:23:20[0m] Step: 5079, Training Logs: loss_final: 0.903322, loss_mean: 0.859864, loss_mean_cls: 0.043458, grad_norm: 0.395473
[[34m2025-10-04 12:23:20[0m] Step: 5080, Training Logs: loss_final: 0.896330, loss_mean: 0.851405, loss_mean_cls: 0.044925, grad_norm: 0.275483
[[34m2025-10-04 12:23:20[0m] Step: 5081, Training Logs: loss_final: 0.888239, loss_mean: 0.843347, loss_mean_cls: 0.044893, grad_norm: 0.289975
[[34m2025-10-04 12:23:21[0m] Step: 5082, Training Logs: loss_final: 0.874011, loss_mean: 0.828315, loss_mean_cls: 0.045695, grad_norm: 0.333902
[[34m2025-10-04 12:23:21[0m] Step: 5083, Training Logs: loss_final: 0.895197, loss_mean: 0.850075, loss_mean_cls: 0.045122, grad_norm: 0.400592
[[34m2025-10-04 12:23:21[0m] Step: 5084, Training Logs: loss_final: 0.872015, loss_mean: 0.827327, loss_mean_cls: 0.044688, grad_norm: 0.267169
[[34m2025-10-04 12:23:22[0m] Step: 5085, Training Logs: loss_final: 0.899640, loss_mean: 0.855655, loss_mean_cls: 0.043984, grad_norm: 0.420266
[[34m2025-10-04 12:23:22[0m] Step: 5086, Training Logs: loss_final: 0.895559, loss_mean: 0.851526, loss_mean_cls: 0.044033, grad_norm: 0.536121
[[34m2025-10-04 12:23:22[0m] Step: 5087, Training Logs: loss_final: 0.898349, loss_mean: 0.853954, loss_mean_cls: 0.044395, grad_norm: 0.311616
[[34m2025-10-04 12:23:22[0m] Step: 5088, Training Logs: loss_final: 0.900410, loss_mean: 0.855476, loss_mean_cls: 0.044934, grad_norm: 0.426235
[[34m2025-10-04 12:23:23[0m] Step: 5089, Training Logs: loss_final: 0.901115, loss_mean: 0.856901, loss_mean_cls: 0.044215, grad_norm: 0.422307
[[34m2025-10-04 12:23:23[0m] Step: 5090, Training Logs: loss_final: 0.891458, loss_mean: 0.846675, loss_mean_cls: 0.044783, grad_norm: 0.329576
[[34m2025-10-04 12:23:23[0m] Step: 5091, Training Logs: loss_final: 0.890582, loss_mean: 0.845791, loss_mean_cls: 0.044791, grad_norm: 0.474792
[[34m2025-10-04 12:23:24[0m] Step: 5092, Training Logs: loss_final: 0.892032, loss_mean: 0.847387, loss_mean_cls: 0.044645, grad_norm: 0.283214
[[34m2025-10-04 12:23:24[0m] Step: 5093, Training Logs: loss_final: 0.917359, loss_mean: 0.873488, loss_mean_cls: 0.043871, grad_norm: 0.366083
[[34m2025-10-04 12:23:24[0m] Step: 5094, Training Logs: loss_final: 0.904919, loss_mean: 0.860963, loss_mean_cls: 0.043956, grad_norm: 0.446169
[[34m2025-10-04 12:23:25[0m] Step: 5095, Training Logs: loss_final: 0.887223, loss_mean: 0.843045, loss_mean_cls: 0.044178, grad_norm: 0.281588
[[34m2025-10-04 12:23:25[0m] Step: 5096, Training Logs: loss_final: 0.892986, loss_mean: 0.846726, loss_mean_cls: 0.046260, grad_norm: 0.313039
[[34m2025-10-04 12:23:25[0m] Step: 5097, Training Logs: loss_final: 0.889522, loss_mean: 0.844271, loss_mean_cls: 0.045251, grad_norm: 0.415243
[[34m2025-10-04 12:23:25[0m] Step: 5098, Training Logs: loss_final: 0.900066, loss_mean: 0.856212, loss_mean_cls: 0.043854, grad_norm: 0.354643
[[34m2025-10-04 12:23:26[0m] Step: 5099, Training Logs: loss_final: 0.889047, loss_mean: 0.844575, loss_mean_cls: 0.044473, grad_norm: 0.280160
[[34m2025-10-04 12:23:26[0m] Step: 5100, Training Logs: loss_final: 0.883902, loss_mean: 0.837787, loss_mean_cls: 0.046115, grad_norm: 0.334025
[[34m2025-10-04 12:23:26[0m] Step: 5101, Training Logs: loss_final: 0.889244, loss_mean: 0.844312, loss_mean_cls: 0.044932, grad_norm: 0.336056
[[34m2025-10-04 12:23:27[0m] Step: 5102, Training Logs: loss_final: 0.884310, loss_mean: 0.839810, loss_mean_cls: 0.044501, grad_norm: 0.362907
[[34m2025-10-04 12:23:27[0m] Step: 5103, Training Logs: loss_final: 0.897994, loss_mean: 0.853280, loss_mean_cls: 0.044713, grad_norm: 0.443984
[[34m2025-10-04 12:23:27[0m] Step: 5104, Training Logs: loss_final: 0.870092, loss_mean: 0.825388, loss_mean_cls: 0.044704, grad_norm: 0.585740
[[34m2025-10-04 12:23:27[0m] Step: 5105, Training Logs: loss_final: 0.893575, loss_mean: 0.847486, loss_mean_cls: 0.046089, grad_norm: 0.295970
[[34m2025-10-04 12:23:28[0m] Step: 5106, Training Logs: loss_final: 0.896388, loss_mean: 0.851159, loss_mean_cls: 0.045229, grad_norm: 0.243934
[[34m2025-10-04 12:23:28[0m] Step: 5107, Training Logs: loss_final: 0.894279, loss_mean: 0.850301, loss_mean_cls: 0.043978, grad_norm: 0.443479
[[34m2025-10-04 12:23:28[0m] Step: 5108, Training Logs: loss_final: 0.873730, loss_mean: 0.829786, loss_mean_cls: 0.043945, grad_norm: 0.369183
[[34m2025-10-04 12:23:29[0m] Step: 5109, Training Logs: loss_final: 0.887099, loss_mean: 0.841238, loss_mean_cls: 0.045861, grad_norm: 0.309270
[[34m2025-10-04 12:23:29[0m] Step: 5110, Training Logs: loss_final: 0.881099, loss_mean: 0.835729, loss_mean_cls: 0.045370, grad_norm: 0.383044
[[34m2025-10-04 12:23:29[0m] Step: 5111, Training Logs: loss_final: 0.898114, loss_mean: 0.853542, loss_mean_cls: 0.044572, grad_norm: 0.472457
[[34m2025-10-04 12:23:29[0m] Step: 5112, Training Logs: loss_final: 0.896612, loss_mean: 0.852170, loss_mean_cls: 0.044443, grad_norm: 0.296320
[[34m2025-10-04 12:23:30[0m] Step: 5113, Training Logs: loss_final: 0.885110, loss_mean: 0.841117, loss_mean_cls: 0.043992, grad_norm: 0.349849
[[34m2025-10-04 12:23:30[0m] Step: 5114, Training Logs: loss_final: 0.893887, loss_mean: 0.849529, loss_mean_cls: 0.044358, grad_norm: 0.372960
[[34m2025-10-04 12:23:30[0m] Step: 5115, Training Logs: loss_final: 0.871169, loss_mean: 0.826285, loss_mean_cls: 0.044884, grad_norm: 0.302667
[[34m2025-10-04 12:23:31[0m] Step: 5116, Training Logs: loss_final: 0.903513, loss_mean: 0.859826, loss_mean_cls: 0.043687, grad_norm: 0.326774
[[34m2025-10-04 12:23:31[0m] Step: 5117, Training Logs: loss_final: 0.897249, loss_mean: 0.852041, loss_mean_cls: 0.045208, grad_norm: 0.345328
[[34m2025-10-04 12:23:31[0m] Step: 5118, Training Logs: loss_final: 0.903796, loss_mean: 0.858289, loss_mean_cls: 0.045508, grad_norm: 0.397459
[[34m2025-10-04 12:23:32[0m] Step: 5119, Training Logs: loss_final: 0.894363, loss_mean: 0.849436, loss_mean_cls: 0.044927, grad_norm: 0.377220
[[34m2025-10-04 12:23:32[0m] Step: 5120, Training Logs: loss_final: 0.891166, loss_mean: 0.847133, loss_mean_cls: 0.044033, grad_norm: 0.459118
[[34m2025-10-04 12:23:32[0m] Step: 5121, Training Logs: loss_final: 0.889098, loss_mean: 0.842164, loss_mean_cls: 0.046934, grad_norm: 0.472579
[[34m2025-10-04 12:23:32[0m] Step: 5122, Training Logs: loss_final: 0.891009, loss_mean: 0.846442, loss_mean_cls: 0.044567, grad_norm: 0.321122
[[34m2025-10-04 12:23:33[0m] Step: 5123, Training Logs: loss_final: 0.884656, loss_mean: 0.839481, loss_mean_cls: 0.045175, grad_norm: 0.441271
[[34m2025-10-04 12:23:33[0m] Step: 5124, Training Logs: loss_final: 0.887468, loss_mean: 0.841632, loss_mean_cls: 0.045836, grad_norm: 0.379806
[[34m2025-10-04 12:23:33[0m] Step: 5125, Training Logs: loss_final: 0.891431, loss_mean: 0.846970, loss_mean_cls: 0.044460, grad_norm: 0.347363
[[34m2025-10-04 12:23:34[0m] Step: 5126, Training Logs: loss_final: 0.877963, loss_mean: 0.831971, loss_mean_cls: 0.045992, grad_norm: 0.477583
[[34m2025-10-04 12:23:34[0m] Step: 5127, Training Logs: loss_final: 0.879038, loss_mean: 0.833944, loss_mean_cls: 0.045094, grad_norm: 0.330135
[[34m2025-10-04 12:23:34[0m] Step: 5128, Training Logs: loss_final: 0.885679, loss_mean: 0.841060, loss_mean_cls: 0.044620, grad_norm: 0.562335
[[34m2025-10-04 12:23:34[0m] Step: 5129, Training Logs: loss_final: 0.897085, loss_mean: 0.852530, loss_mean_cls: 0.044555, grad_norm: 0.484741
[[34m2025-10-04 12:23:35[0m] Step: 5130, Training Logs: loss_final: 0.871273, loss_mean: 0.827919, loss_mean_cls: 0.043355, grad_norm: 0.498215
[[34m2025-10-04 12:23:35[0m] Step: 5131, Training Logs: loss_final: 0.891391, loss_mean: 0.845981, loss_mean_cls: 0.045410, grad_norm: 0.445807
[[34m2025-10-04 12:23:35[0m] Step: 5132, Training Logs: loss_final: 0.870602, loss_mean: 0.825304, loss_mean_cls: 0.045298, grad_norm: 0.375469
[[34m2025-10-04 12:23:36[0m] Step: 5133, Training Logs: loss_final: 0.897015, loss_mean: 0.852179, loss_mean_cls: 0.044836, grad_norm: 0.550703
[[34m2025-10-04 12:23:36[0m] Step: 5134, Training Logs: loss_final: 0.890389, loss_mean: 0.845479, loss_mean_cls: 0.044909, grad_norm: 0.427564
[[34m2025-10-04 12:23:36[0m] Step: 5135, Training Logs: loss_final: 0.890672, loss_mean: 0.845808, loss_mean_cls: 0.044864, grad_norm: 0.462498
[[34m2025-10-04 12:23:36[0m] Step: 5136, Training Logs: loss_final: 0.888260, loss_mean: 0.843410, loss_mean_cls: 0.044850, grad_norm: 0.602749
[[34m2025-10-04 12:23:37[0m] Step: 5137, Training Logs: loss_final: 0.864632, loss_mean: 0.819075, loss_mean_cls: 0.045557, grad_norm: 0.483928
[[34m2025-10-04 12:23:37[0m] Step: 5138, Training Logs: loss_final: 0.881862, loss_mean: 0.835886, loss_mean_cls: 0.045976, grad_norm: 0.364453
[[34m2025-10-04 12:23:37[0m] Step: 5139, Training Logs: loss_final: 0.877134, loss_mean: 0.833037, loss_mean_cls: 0.044097, grad_norm: 0.396035
[[34m2025-10-04 12:23:38[0m] Step: 5140, Training Logs: loss_final: 0.895811, loss_mean: 0.851957, loss_mean_cls: 0.043854, grad_norm: 0.521892
[[34m2025-10-04 12:23:38[0m] Step: 5141, Training Logs: loss_final: 0.894579, loss_mean: 0.849401, loss_mean_cls: 0.045178, grad_norm: 0.356140
[[34m2025-10-04 12:23:38[0m] Step: 5142, Training Logs: loss_final: 0.879807, loss_mean: 0.834942, loss_mean_cls: 0.044865, grad_norm: 0.440530
[[34m2025-10-04 12:23:38[0m] Step: 5143, Training Logs: loss_final: 0.900116, loss_mean: 0.855565, loss_mean_cls: 0.044551, grad_norm: 0.524801
[[34m2025-10-04 12:23:39[0m] Step: 5144, Training Logs: loss_final: 0.894886, loss_mean: 0.849819, loss_mean_cls: 0.045066, grad_norm: 0.431484
[[34m2025-10-04 12:23:39[0m] Step: 5145, Training Logs: loss_final: 0.893361, loss_mean: 0.848723, loss_mean_cls: 0.044638, grad_norm: 0.444352
[[34m2025-10-04 12:23:39[0m] Step: 5146, Training Logs: loss_final: 0.910267, loss_mean: 0.866261, loss_mean_cls: 0.044006, grad_norm: 0.455822
[[34m2025-10-04 12:23:40[0m] Step: 5147, Training Logs: loss_final: 0.888220, loss_mean: 0.842148, loss_mean_cls: 0.046072, grad_norm: 0.365230
[[34m2025-10-04 12:23:40[0m] Step: 5148, Training Logs: loss_final: 0.887150, loss_mean: 0.841347, loss_mean_cls: 0.045804, grad_norm: 0.555034
[[34m2025-10-04 12:23:40[0m] Step: 5149, Training Logs: loss_final: 0.880090, loss_mean: 0.834463, loss_mean_cls: 0.045627, grad_norm: 0.516643
[[34m2025-10-04 12:23:40[0m] Step: 5150, Training Logs: loss_final: 0.883086, loss_mean: 0.839337, loss_mean_cls: 0.043749, grad_norm: 0.283872
[[34m2025-10-04 12:23:41[0m] Step: 5151, Training Logs: loss_final: 0.898753, loss_mean: 0.854315, loss_mean_cls: 0.044438, grad_norm: 0.653094
[[34m2025-10-04 12:23:41[0m] Step: 5152, Training Logs: loss_final: 0.875530, loss_mean: 0.831502, loss_mean_cls: 0.044028, grad_norm: 0.390451
[[34m2025-10-04 12:23:41[0m] Step: 5153, Training Logs: loss_final: 0.896915, loss_mean: 0.852687, loss_mean_cls: 0.044228, grad_norm: 0.386301
[[34m2025-10-04 12:23:42[0m] Step: 5154, Training Logs: loss_final: 0.881062, loss_mean: 0.835180, loss_mean_cls: 0.045882, grad_norm: 0.445855
[[34m2025-10-04 12:23:42[0m] Step: 5155, Training Logs: loss_final: 0.884326, loss_mean: 0.839683, loss_mean_cls: 0.044642, grad_norm: 0.332282
[[34m2025-10-04 12:23:42[0m] Step: 5156, Training Logs: loss_final: 0.892395, loss_mean: 0.849172, loss_mean_cls: 0.043224, grad_norm: 0.436918
[[34m2025-10-04 12:23:43[0m] Step: 5157, Training Logs: loss_final: 0.901252, loss_mean: 0.856261, loss_mean_cls: 0.044991, grad_norm: 0.359242
[[34m2025-10-04 12:23:43[0m] Step: 5158, Training Logs: loss_final: 0.909441, loss_mean: 0.865289, loss_mean_cls: 0.044153, grad_norm: 0.379325
[[34m2025-10-04 12:23:43[0m] Step: 5159, Training Logs: loss_final: 0.897460, loss_mean: 0.853939, loss_mean_cls: 0.043521, grad_norm: 0.487674
[[34m2025-10-04 12:23:43[0m] Step: 5160, Training Logs: loss_final: 0.876616, loss_mean: 0.831073, loss_mean_cls: 0.045544, grad_norm: 0.325294
[[34m2025-10-04 12:23:44[0m] Step: 5161, Training Logs: loss_final: 0.894175, loss_mean: 0.849354, loss_mean_cls: 0.044822, grad_norm: 0.398991
[[34m2025-10-04 12:23:44[0m] Step: 5162, Training Logs: loss_final: 0.885352, loss_mean: 0.840929, loss_mean_cls: 0.044423, grad_norm: 0.430129
[[34m2025-10-04 12:23:44[0m] Step: 5163, Training Logs: loss_final: 0.884539, loss_mean: 0.840225, loss_mean_cls: 0.044314, grad_norm: 0.317502
[[34m2025-10-04 12:23:45[0m] Step: 5164, Training Logs: loss_final: 0.877560, loss_mean: 0.833410, loss_mean_cls: 0.044150, grad_norm: 0.413841
[[34m2025-10-04 12:23:45[0m] Step: 5165, Training Logs: loss_final: 0.872271, loss_mean: 0.827422, loss_mean_cls: 0.044849, grad_norm: 0.338559
[[34m2025-10-04 12:23:45[0m] Step: 5166, Training Logs: loss_final: 0.895325, loss_mean: 0.849864, loss_mean_cls: 0.045461, grad_norm: 0.353514
[[34m2025-10-04 12:23:45[0m] Step: 5167, Training Logs: loss_final: 0.883659, loss_mean: 0.838164, loss_mean_cls: 0.045494, grad_norm: 0.329210
[[34m2025-10-04 12:23:46[0m] Step: 5168, Training Logs: loss_final: 0.888088, loss_mean: 0.843635, loss_mean_cls: 0.044452, grad_norm: 0.456313
[[34m2025-10-04 12:23:46[0m] Step: 5169, Training Logs: loss_final: 0.888566, loss_mean: 0.844009, loss_mean_cls: 0.044557, grad_norm: 0.299213
[[34m2025-10-04 12:23:46[0m] Step: 5170, Training Logs: loss_final: 0.896499, loss_mean: 0.850861, loss_mean_cls: 0.045638, grad_norm: 0.345706
[[34m2025-10-04 12:23:47[0m] Step: 5171, Training Logs: loss_final: 0.902261, loss_mean: 0.856768, loss_mean_cls: 0.045493, grad_norm: 0.420516
[[34m2025-10-04 12:23:47[0m] Step: 5172, Training Logs: loss_final: 0.885957, loss_mean: 0.841679, loss_mean_cls: 0.044278, grad_norm: 0.317406
[[34m2025-10-04 12:23:47[0m] Step: 5173, Training Logs: loss_final: 0.865167, loss_mean: 0.819421, loss_mean_cls: 0.045747, grad_norm: 0.338734
[[34m2025-10-04 12:23:47[0m] Step: 5174, Training Logs: loss_final: 0.891345, loss_mean: 0.846515, loss_mean_cls: 0.044830, grad_norm: 0.235278
[[34m2025-10-04 12:23:48[0m] Step: 5175, Training Logs: loss_final: 0.892629, loss_mean: 0.847187, loss_mean_cls: 0.045442, grad_norm: 0.322241
[[34m2025-10-04 12:23:48[0m] Step: 5176, Training Logs: loss_final: 0.876180, loss_mean: 0.832029, loss_mean_cls: 0.044151, grad_norm: 0.237786
[[34m2025-10-04 12:23:48[0m] Step: 5177, Training Logs: loss_final: 0.878513, loss_mean: 0.833424, loss_mean_cls: 0.045089, grad_norm: 0.271004
[[34m2025-10-04 12:23:49[0m] Step: 5178, Training Logs: loss_final: 0.888363, loss_mean: 0.844453, loss_mean_cls: 0.043910, grad_norm: 0.413212
[[34m2025-10-04 12:23:49[0m] Step: 5179, Training Logs: loss_final: 0.887340, loss_mean: 0.842702, loss_mean_cls: 0.044638, grad_norm: 0.269660
[[34m2025-10-04 12:23:49[0m] Step: 5180, Training Logs: loss_final: 0.906908, loss_mean: 0.862065, loss_mean_cls: 0.044842, grad_norm: 0.311348
[[34m2025-10-04 12:23:49[0m] Step: 5181, Training Logs: loss_final: 0.898045, loss_mean: 0.852674, loss_mean_cls: 0.045371, grad_norm: 0.344698
[[34m2025-10-04 12:23:50[0m] Step: 5182, Training Logs: loss_final: 0.883473, loss_mean: 0.837693, loss_mean_cls: 0.045780, grad_norm: 0.352639
[[34m2025-10-04 12:23:50[0m] Step: 5183, Training Logs: loss_final: 0.890073, loss_mean: 0.846259, loss_mean_cls: 0.043815, grad_norm: 0.345954
[[34m2025-10-04 12:23:50[0m] Step: 5184, Training Logs: loss_final: 0.877335, loss_mean: 0.831936, loss_mean_cls: 0.045399, grad_norm: 0.351806
[[34m2025-10-04 12:23:51[0m] Step: 5185, Training Logs: loss_final: 0.891718, loss_mean: 0.846572, loss_mean_cls: 0.045146, grad_norm: 0.269317
[[34m2025-10-04 12:23:51[0m] Step: 5186, Training Logs: loss_final: 0.895431, loss_mean: 0.852037, loss_mean_cls: 0.043394, grad_norm: 0.338066
[[34m2025-10-04 12:23:51[0m] Step: 5187, Training Logs: loss_final: 0.871611, loss_mean: 0.825838, loss_mean_cls: 0.045773, grad_norm: 0.331683
[[34m2025-10-04 12:23:52[0m] Step: 5188, Training Logs: loss_final: 0.866509, loss_mean: 0.820359, loss_mean_cls: 0.046150, grad_norm: 0.408765
[[34m2025-10-04 12:23:52[0m] Step: 5189, Training Logs: loss_final: 0.874215, loss_mean: 0.828330, loss_mean_cls: 0.045885, grad_norm: 0.404708
[[34m2025-10-04 12:23:52[0m] Step: 5190, Training Logs: loss_final: 0.907553, loss_mean: 0.863894, loss_mean_cls: 0.043659, grad_norm: 0.357838
[[34m2025-10-04 12:23:52[0m] Step: 5191, Training Logs: loss_final: 0.874726, loss_mean: 0.829506, loss_mean_cls: 0.045220, grad_norm: 0.465476
[[34m2025-10-04 12:23:53[0m] Step: 5192, Training Logs: loss_final: 0.873721, loss_mean: 0.829055, loss_mean_cls: 0.044667, grad_norm: 0.348917
[[34m2025-10-04 12:23:53[0m] Step: 5193, Training Logs: loss_final: 0.878220, loss_mean: 0.833522, loss_mean_cls: 0.044698, grad_norm: 0.259647
[[34m2025-10-04 12:23:53[0m] Step: 5194, Training Logs: loss_final: 0.877931, loss_mean: 0.833034, loss_mean_cls: 0.044897, grad_norm: 0.344884
[[34m2025-10-04 12:23:54[0m] Step: 5195, Training Logs: loss_final: 0.897056, loss_mean: 0.852888, loss_mean_cls: 0.044169, grad_norm: 0.464500
[[34m2025-10-04 12:23:54[0m] Step: 5196, Training Logs: loss_final: 0.893832, loss_mean: 0.849551, loss_mean_cls: 0.044281, grad_norm: 0.240864
[[34m2025-10-04 12:23:54[0m] Step: 5197, Training Logs: loss_final: 0.872081, loss_mean: 0.827388, loss_mean_cls: 0.044693, grad_norm: 0.356504
[[34m2025-10-04 12:23:55[0m] Step: 5198, Training Logs: loss_final: 0.904972, loss_mean: 0.859949, loss_mean_cls: 0.045023, grad_norm: 0.348445
[[34m2025-10-04 12:23:55[0m] Step: 5199, Training Logs: loss_final: 0.873685, loss_mean: 0.828395, loss_mean_cls: 0.045290, grad_norm: 0.351332
[[34m2025-10-04 12:23:55[0m] Step: 5200, Training Logs: loss_final: 0.894927, loss_mean: 0.850492, loss_mean_cls: 0.044434, grad_norm: 0.287508
[[34m2025-10-04 12:23:55[0m] Step: 5201, Training Logs: loss_final: 0.864750, loss_mean: 0.819628, loss_mean_cls: 0.045122, grad_norm: 0.497685
[[34m2025-10-04 12:23:56[0m] Step: 5202, Training Logs: loss_final: 0.899255, loss_mean: 0.854288, loss_mean_cls: 0.044967, grad_norm: 0.378975
[[34m2025-10-04 12:23:56[0m] Step: 5203, Training Logs: loss_final: 0.902641, loss_mean: 0.858607, loss_mean_cls: 0.044034, grad_norm: 0.321377
[[34m2025-10-04 12:23:56[0m] Step: 5204, Training Logs: loss_final: 0.900332, loss_mean: 0.856315, loss_mean_cls: 0.044016, grad_norm: 0.331035
[[34m2025-10-04 12:23:57[0m] Step: 5205, Training Logs: loss_final: 0.890058, loss_mean: 0.845225, loss_mean_cls: 0.044833, grad_norm: 0.416833
[[34m2025-10-04 12:23:57[0m] Step: 5206, Training Logs: loss_final: 0.904317, loss_mean: 0.860638, loss_mean_cls: 0.043679, grad_norm: 0.312004
[[34m2025-10-04 12:23:57[0m] Step: 5207, Training Logs: loss_final: 0.901102, loss_mean: 0.856435, loss_mean_cls: 0.044667, grad_norm: 0.345304
[[34m2025-10-04 12:23:58[0m] Step: 5208, Training Logs: loss_final: 0.886603, loss_mean: 0.841713, loss_mean_cls: 0.044890, grad_norm: 0.400381
[[34m2025-10-04 12:23:58[0m] Step: 5209, Training Logs: loss_final: 0.894135, loss_mean: 0.849979, loss_mean_cls: 0.044156, grad_norm: 0.287278
[[34m2025-10-04 12:23:58[0m] Step: 5210, Training Logs: loss_final: 0.882601, loss_mean: 0.837494, loss_mean_cls: 0.045106, grad_norm: 0.299866
[[34m2025-10-04 12:23:58[0m] Step: 5211, Training Logs: loss_final: 0.876670, loss_mean: 0.831837, loss_mean_cls: 0.044834, grad_norm: 0.413370
[[34m2025-10-04 12:23:59[0m] Step: 5212, Training Logs: loss_final: 0.913801, loss_mean: 0.869190, loss_mean_cls: 0.044611, grad_norm: 0.376924
[[34m2025-10-04 12:23:59[0m] Step: 5213, Training Logs: loss_final: 0.888512, loss_mean: 0.844354, loss_mean_cls: 0.044158, grad_norm: 0.405001
[[34m2025-10-04 12:23:59[0m] Step: 5214, Training Logs: loss_final: 0.897488, loss_mean: 0.851604, loss_mean_cls: 0.045884, grad_norm: 0.385286
[[34m2025-10-04 12:24:00[0m] Step: 5215, Training Logs: loss_final: 0.896574, loss_mean: 0.852209, loss_mean_cls: 0.044365, grad_norm: 0.365857
[[34m2025-10-04 12:24:00[0m] Step: 5216, Training Logs: loss_final: 0.899398, loss_mean: 0.853770, loss_mean_cls: 0.045628, grad_norm: 0.392546
[[34m2025-10-04 12:24:00[0m] Step: 5217, Training Logs: loss_final: 0.880272, loss_mean: 0.836017, loss_mean_cls: 0.044255, grad_norm: 0.322341
[[34m2025-10-04 12:24:00[0m] Step: 5218, Training Logs: loss_final: 0.890322, loss_mean: 0.845633, loss_mean_cls: 0.044690, grad_norm: 0.434074
[[34m2025-10-04 12:24:01[0m] Step: 5219, Training Logs: loss_final: 0.894267, loss_mean: 0.850461, loss_mean_cls: 0.043805, grad_norm: 0.461149
[[34m2025-10-04 12:24:01[0m] Step: 5220, Training Logs: loss_final: 0.883889, loss_mean: 0.840667, loss_mean_cls: 0.043222, grad_norm: 0.422412
[[34m2025-10-04 12:24:01[0m] Step: 5221, Training Logs: loss_final: 0.895360, loss_mean: 0.850172, loss_mean_cls: 0.045189, grad_norm: 0.375277
[[34m2025-10-04 12:24:02[0m] Step: 5222, Training Logs: loss_final: 0.879061, loss_mean: 0.835298, loss_mean_cls: 0.043763, grad_norm: 0.453401
[[34m2025-10-04 12:24:02[0m] Step: 5223, Training Logs: loss_final: 0.893436, loss_mean: 0.848482, loss_mean_cls: 0.044954, grad_norm: 0.415774
[[34m2025-10-04 12:24:02[0m] Step: 5224, Training Logs: loss_final: 0.890216, loss_mean: 0.845415, loss_mean_cls: 0.044801, grad_norm: 0.378514
[[34m2025-10-04 12:24:03[0m] Step: 5225, Training Logs: loss_final: 0.881275, loss_mean: 0.836793, loss_mean_cls: 0.044482, grad_norm: 0.313475
[[34m2025-10-04 12:24:03[0m] Step: 5226, Training Logs: loss_final: 0.860776, loss_mean: 0.815926, loss_mean_cls: 0.044850, grad_norm: 0.471039
[[34m2025-10-04 12:24:03[0m] Step: 5227, Training Logs: loss_final: 0.896789, loss_mean: 0.852646, loss_mean_cls: 0.044143, grad_norm: 0.350734
[[34m2025-10-04 12:24:03[0m] Step: 5228, Training Logs: loss_final: 0.902092, loss_mean: 0.857744, loss_mean_cls: 0.044348, grad_norm: 0.359496
[[34m2025-10-04 12:24:04[0m] Step: 5229, Training Logs: loss_final: 0.892510, loss_mean: 0.848466, loss_mean_cls: 0.044044, grad_norm: 0.351065
[[34m2025-10-04 12:24:04[0m] Step: 5230, Training Logs: loss_final: 0.880668, loss_mean: 0.835752, loss_mean_cls: 0.044916, grad_norm: 0.377551
[[34m2025-10-04 12:24:04[0m] Step: 5231, Training Logs: loss_final: 0.887816, loss_mean: 0.842504, loss_mean_cls: 0.045311, grad_norm: 0.354969
[[34m2025-10-04 12:24:05[0m] Step: 5232, Training Logs: loss_final: 0.908468, loss_mean: 0.864758, loss_mean_cls: 0.043709, grad_norm: 0.337520
[[34m2025-10-04 12:24:05[0m] Step: 5233, Training Logs: loss_final: 0.894957, loss_mean: 0.849415, loss_mean_cls: 0.045542, grad_norm: 0.408003
[[34m2025-10-04 12:24:05[0m] Step: 5234, Training Logs: loss_final: 0.877896, loss_mean: 0.832904, loss_mean_cls: 0.044992, grad_norm: 0.443067
[[34m2025-10-04 12:24:06[0m] Step: 5235, Training Logs: loss_final: 0.913706, loss_mean: 0.868680, loss_mean_cls: 0.045025, grad_norm: 0.406054
[[34m2025-10-04 12:24:06[0m] Step: 5236, Training Logs: loss_final: 0.890234, loss_mean: 0.845299, loss_mean_cls: 0.044934, grad_norm: 0.316677
[[34m2025-10-04 12:24:06[0m] Step: 5237, Training Logs: loss_final: 0.885075, loss_mean: 0.840528, loss_mean_cls: 0.044547, grad_norm: 0.348390
[[34m2025-10-04 12:24:06[0m] Step: 5238, Training Logs: loss_final: 0.885483, loss_mean: 0.841276, loss_mean_cls: 0.044207, grad_norm: 0.499972
[[34m2025-10-04 12:24:07[0m] Step: 5239, Training Logs: loss_final: 0.886540, loss_mean: 0.841778, loss_mean_cls: 0.044762, grad_norm: 0.257067
[[34m2025-10-04 12:24:07[0m] Step: 5240, Training Logs: loss_final: 0.879093, loss_mean: 0.833825, loss_mean_cls: 0.045268, grad_norm: 0.412828
[[34m2025-10-04 12:24:07[0m] Step: 5241, Training Logs: loss_final: 0.884654, loss_mean: 0.839378, loss_mean_cls: 0.045276, grad_norm: 0.363772
[[34m2025-10-04 12:24:08[0m] Step: 5242, Training Logs: loss_final: 0.888139, loss_mean: 0.843411, loss_mean_cls: 0.044729, grad_norm: 0.305171
[[34m2025-10-04 12:24:08[0m] Step: 5243, Training Logs: loss_final: 0.889030, loss_mean: 0.842958, loss_mean_cls: 0.046072, grad_norm: 0.334131
[[34m2025-10-04 12:24:08[0m] Step: 5244, Training Logs: loss_final: 0.869693, loss_mean: 0.824157, loss_mean_cls: 0.045535, grad_norm: 0.489422
[[34m2025-10-04 12:24:08[0m] Step: 5245, Training Logs: loss_final: 0.888693, loss_mean: 0.845203, loss_mean_cls: 0.043490, grad_norm: 0.272881
[[34m2025-10-04 12:24:09[0m] Step: 5246, Training Logs: loss_final: 0.886356, loss_mean: 0.842013, loss_mean_cls: 0.044343, grad_norm: 0.363621
[[34m2025-10-04 12:24:09[0m] Step: 5247, Training Logs: loss_final: 0.867967, loss_mean: 0.823568, loss_mean_cls: 0.044399, grad_norm: 0.402310
[[34m2025-10-04 12:24:09[0m] Step: 5248, Training Logs: loss_final: 0.898083, loss_mean: 0.854131, loss_mean_cls: 0.043952, grad_norm: 0.363395
[[34m2025-10-04 12:24:10[0m] Step: 5249, Training Logs: loss_final: 0.909318, loss_mean: 0.863729, loss_mean_cls: 0.045589, grad_norm: 0.339517
[[34m2025-10-04 12:24:10[0m] Step: 5250, Training Logs: loss_final: 0.882015, loss_mean: 0.837100, loss_mean_cls: 0.044915, grad_norm: 0.500607
[[34m2025-10-04 12:24:10[0m] Step: 5251, Training Logs: loss_final: 0.909717, loss_mean: 0.865483, loss_mean_cls: 0.044234, grad_norm: 0.315994
[[34m2025-10-04 12:24:11[0m] Step: 5252, Training Logs: loss_final: 0.879974, loss_mean: 0.835914, loss_mean_cls: 0.044060, grad_norm: 0.451158
[[34m2025-10-04 12:24:11[0m] Step: 5253, Training Logs: loss_final: 0.886575, loss_mean: 0.841429, loss_mean_cls: 0.045145, grad_norm: 0.473824
[[34m2025-10-04 12:24:11[0m] Step: 5254, Training Logs: loss_final: 0.901260, loss_mean: 0.856748, loss_mean_cls: 0.044513, grad_norm: 0.379043
[[34m2025-10-04 12:24:11[0m] Step: 5255, Training Logs: loss_final: 0.892997, loss_mean: 0.847425, loss_mean_cls: 0.045572, grad_norm: 0.500339
[[34m2025-10-04 12:24:12[0m] Step: 5256, Training Logs: loss_final: 0.904674, loss_mean: 0.860011, loss_mean_cls: 0.044663, grad_norm: 0.315493
[[34m2025-10-04 12:24:12[0m] Step: 5257, Training Logs: loss_final: 0.886950, loss_mean: 0.842704, loss_mean_cls: 0.044246, grad_norm: 0.371280
[[34m2025-10-04 12:24:12[0m] Step: 5258, Training Logs: loss_final: 0.885429, loss_mean: 0.840761, loss_mean_cls: 0.044668, grad_norm: 0.354376
[[34m2025-10-04 12:24:13[0m] Step: 5259, Training Logs: loss_final: 0.900074, loss_mean: 0.855698, loss_mean_cls: 0.044376, grad_norm: 0.397137
[[34m2025-10-04 12:24:13[0m] Step: 5260, Training Logs: loss_final: 0.860462, loss_mean: 0.815829, loss_mean_cls: 0.044633, grad_norm: 0.302534
[[34m2025-10-04 12:24:13[0m] Step: 5261, Training Logs: loss_final: 0.878783, loss_mean: 0.834925, loss_mean_cls: 0.043858, grad_norm: 0.374493
[[34m2025-10-04 12:24:14[0m] Step: 5262, Training Logs: loss_final: 0.894612, loss_mean: 0.851282, loss_mean_cls: 0.043331, grad_norm: 0.373820
[[34m2025-10-04 12:24:14[0m] Step: 5263, Training Logs: loss_final: 0.880595, loss_mean: 0.835678, loss_mean_cls: 0.044917, grad_norm: 0.321899
[[34m2025-10-04 12:24:14[0m] Step: 5264, Training Logs: loss_final: 0.880990, loss_mean: 0.836089, loss_mean_cls: 0.044901, grad_norm: 0.334093
[[34m2025-10-04 12:24:14[0m] Step: 5265, Training Logs: loss_final: 0.887061, loss_mean: 0.840634, loss_mean_cls: 0.046427, grad_norm: 0.331244
[[34m2025-10-04 12:24:15[0m] Step: 5266, Training Logs: loss_final: 0.888562, loss_mean: 0.844008, loss_mean_cls: 0.044554, grad_norm: 0.284118
[[34m2025-10-04 12:24:15[0m] Step: 5267, Training Logs: loss_final: 0.896816, loss_mean: 0.851633, loss_mean_cls: 0.045182, grad_norm: 0.281554
[[34m2025-10-04 12:24:15[0m] Step: 5268, Training Logs: loss_final: 0.871673, loss_mean: 0.826929, loss_mean_cls: 0.044744, grad_norm: 0.386398
[[34m2025-10-04 12:24:16[0m] Step: 5269, Training Logs: loss_final: 0.891438, loss_mean: 0.846450, loss_mean_cls: 0.044988, grad_norm: 0.229982
[[34m2025-10-04 12:24:16[0m] Step: 5270, Training Logs: loss_final: 0.879381, loss_mean: 0.834851, loss_mean_cls: 0.044530, grad_norm: 0.275833
[[34m2025-10-04 12:24:16[0m] Step: 5271, Training Logs: loss_final: 0.882864, loss_mean: 0.837872, loss_mean_cls: 0.044992, grad_norm: 0.289209
[[34m2025-10-04 12:24:17[0m] Step: 5272, Training Logs: loss_final: 0.876141, loss_mean: 0.830550, loss_mean_cls: 0.045591, grad_norm: 0.257027
[[34m2025-10-04 12:24:17[0m] Step: 5273, Training Logs: loss_final: 0.886425, loss_mean: 0.841895, loss_mean_cls: 0.044531, grad_norm: 0.263246
[[34m2025-10-04 12:24:17[0m] Step: 5274, Training Logs: loss_final: 0.898232, loss_mean: 0.854792, loss_mean_cls: 0.043439, grad_norm: 0.269278
[[34m2025-10-04 12:24:17[0m] Step: 5275, Training Logs: loss_final: 0.911561, loss_mean: 0.867583, loss_mean_cls: 0.043978, grad_norm: 0.256378
[[34m2025-10-04 12:24:18[0m] Step: 5276, Training Logs: loss_final: 0.883948, loss_mean: 0.838046, loss_mean_cls: 0.045903, grad_norm: 0.284838
[[34m2025-10-04 12:24:18[0m] Step: 5277, Training Logs: loss_final: 0.885028, loss_mean: 0.840968, loss_mean_cls: 0.044060, grad_norm: 0.377959
[[34m2025-10-04 12:24:18[0m] Step: 5278, Training Logs: loss_final: 0.887319, loss_mean: 0.842532, loss_mean_cls: 0.044788, grad_norm: 0.428962
[[34m2025-10-04 12:24:19[0m] Step: 5279, Training Logs: loss_final: 0.923157, loss_mean: 0.878555, loss_mean_cls: 0.044602, grad_norm: 0.367804
[[34m2025-10-04 12:24:19[0m] Step: 5280, Training Logs: loss_final: 0.904745, loss_mean: 0.860293, loss_mean_cls: 0.044453, grad_norm: 0.264757
[[34m2025-10-04 12:24:19[0m] Step: 5281, Training Logs: loss_final: 0.881823, loss_mean: 0.836856, loss_mean_cls: 0.044967, grad_norm: 0.521649
[[34m2025-10-04 12:24:19[0m] Step: 5282, Training Logs: loss_final: 0.878893, loss_mean: 0.833656, loss_mean_cls: 0.045237, grad_norm: 0.372781
[[34m2025-10-04 12:24:20[0m] Step: 5283, Training Logs: loss_final: 0.905204, loss_mean: 0.861344, loss_mean_cls: 0.043860, grad_norm: 0.427488
[[34m2025-10-04 12:24:20[0m] Step: 5284, Training Logs: loss_final: 0.892374, loss_mean: 0.847535, loss_mean_cls: 0.044839, grad_norm: 0.385082
[[34m2025-10-04 12:24:20[0m] Step: 5285, Training Logs: loss_final: 0.876840, loss_mean: 0.832540, loss_mean_cls: 0.044300, grad_norm: 0.373004
[[34m2025-10-04 12:24:21[0m] Step: 5286, Training Logs: loss_final: 0.884333, loss_mean: 0.839738, loss_mean_cls: 0.044594, grad_norm: 0.375112
[[34m2025-10-04 12:24:21[0m] Step: 5287, Training Logs: loss_final: 0.887677, loss_mean: 0.842891, loss_mean_cls: 0.044786, grad_norm: 0.322679
[[34m2025-10-04 12:24:21[0m] Step: 5288, Training Logs: loss_final: 0.894732, loss_mean: 0.850025, loss_mean_cls: 0.044707, grad_norm: 0.326228
[[34m2025-10-04 12:24:22[0m] Step: 5289, Training Logs: loss_final: 0.895902, loss_mean: 0.852362, loss_mean_cls: 0.043540, grad_norm: 0.299798
[[34m2025-10-04 12:24:22[0m] Step: 5290, Training Logs: loss_final: 0.912590, loss_mean: 0.867174, loss_mean_cls: 0.045416, grad_norm: 0.398299
[[34m2025-10-04 12:24:22[0m] Step: 5291, Training Logs: loss_final: 0.869406, loss_mean: 0.824802, loss_mean_cls: 0.044605, grad_norm: 0.492625
[[34m2025-10-04 12:24:22[0m] Step: 5292, Training Logs: loss_final: 0.889367, loss_mean: 0.843769, loss_mean_cls: 0.045598, grad_norm: 0.276191
[[34m2025-10-04 12:24:23[0m] Step: 5293, Training Logs: loss_final: 0.894813, loss_mean: 0.850184, loss_mean_cls: 0.044629, grad_norm: 0.466994
[[34m2025-10-04 12:24:23[0m] Step: 5294, Training Logs: loss_final: 0.881512, loss_mean: 0.836788, loss_mean_cls: 0.044724, grad_norm: 0.337532
[[34m2025-10-04 12:24:23[0m] Step: 5295, Training Logs: loss_final: 0.884935, loss_mean: 0.840435, loss_mean_cls: 0.044499, grad_norm: 0.340490
[[34m2025-10-04 12:24:24[0m] Step: 5296, Training Logs: loss_final: 0.891602, loss_mean: 0.847440, loss_mean_cls: 0.044162, grad_norm: 0.333653
[[34m2025-10-04 12:24:24[0m] Step: 5297, Training Logs: loss_final: 0.892243, loss_mean: 0.848240, loss_mean_cls: 0.044003, grad_norm: 0.375862
[[34m2025-10-04 12:24:24[0m] Step: 5298, Training Logs: loss_final: 0.870173, loss_mean: 0.825164, loss_mean_cls: 0.045009, grad_norm: 0.403847
[[34m2025-10-04 12:24:24[0m] Step: 5299, Training Logs: loss_final: 0.890926, loss_mean: 0.845482, loss_mean_cls: 0.045444, grad_norm: 0.312231
[[34m2025-10-04 12:24:25[0m] Step: 5300, Training Logs: loss_final: 0.883170, loss_mean: 0.838694, loss_mean_cls: 0.044476, grad_norm: 0.352291
[[34m2025-10-04 12:24:25[0m] Step: 5301, Training Logs: loss_final: 0.883919, loss_mean: 0.838385, loss_mean_cls: 0.045534, grad_norm: 0.328245
[[34m2025-10-04 12:24:25[0m] Step: 5302, Training Logs: loss_final: 0.873850, loss_mean: 0.828155, loss_mean_cls: 0.045695, grad_norm: 0.443307
[[34m2025-10-04 12:24:26[0m] Step: 5303, Training Logs: loss_final: 0.877472, loss_mean: 0.832391, loss_mean_cls: 0.045081, grad_norm: 0.430115
[[34m2025-10-04 12:24:26[0m] Step: 5304, Training Logs: loss_final: 0.887792, loss_mean: 0.843132, loss_mean_cls: 0.044661, grad_norm: 0.387050
[[34m2025-10-04 12:24:26[0m] Step: 5305, Training Logs: loss_final: 0.909679, loss_mean: 0.866179, loss_mean_cls: 0.043500, grad_norm: 0.388040
[[34m2025-10-04 12:24:26[0m] Step: 5306, Training Logs: loss_final: 0.905580, loss_mean: 0.861689, loss_mean_cls: 0.043891, grad_norm: 0.338367
[[34m2025-10-04 12:24:27[0m] Step: 5307, Training Logs: loss_final: 0.894608, loss_mean: 0.850133, loss_mean_cls: 0.044474, grad_norm: 0.522136
[[34m2025-10-04 12:24:27[0m] Step: 5308, Training Logs: loss_final: 0.879017, loss_mean: 0.832892, loss_mean_cls: 0.046126, grad_norm: 0.336876
[[34m2025-10-04 12:24:27[0m] Step: 5309, Training Logs: loss_final: 0.894374, loss_mean: 0.849724, loss_mean_cls: 0.044650, grad_norm: 0.375142
[[34m2025-10-04 12:24:28[0m] Step: 5310, Training Logs: loss_final: 0.891344, loss_mean: 0.846520, loss_mean_cls: 0.044824, grad_norm: 0.561005
[[34m2025-10-04 12:24:28[0m] Step: 5311, Training Logs: loss_final: 0.869507, loss_mean: 0.823903, loss_mean_cls: 0.045604, grad_norm: 0.332087
[[34m2025-10-04 12:24:28[0m] Step: 5312, Training Logs: loss_final: 0.886715, loss_mean: 0.842335, loss_mean_cls: 0.044380, grad_norm: 0.400402
[[34m2025-10-04 12:24:29[0m] Step: 5313, Training Logs: loss_final: 0.868464, loss_mean: 0.823512, loss_mean_cls: 0.044952, grad_norm: 0.426360
[[34m2025-10-04 12:24:29[0m] Step: 5314, Training Logs: loss_final: 0.881170, loss_mean: 0.836903, loss_mean_cls: 0.044267, grad_norm: 0.364100
[[34m2025-10-04 12:24:29[0m] Step: 5315, Training Logs: loss_final: 0.915577, loss_mean: 0.872026, loss_mean_cls: 0.043551, grad_norm: 0.384327
[[34m2025-10-04 12:24:29[0m] Step: 5316, Training Logs: loss_final: 0.877260, loss_mean: 0.833185, loss_mean_cls: 0.044075, grad_norm: 0.494497
[[34m2025-10-04 12:24:30[0m] Step: 5317, Training Logs: loss_final: 0.886217, loss_mean: 0.841597, loss_mean_cls: 0.044620, grad_norm: 0.422066
[[34m2025-10-04 12:24:30[0m] Step: 5318, Training Logs: loss_final: 0.890809, loss_mean: 0.847359, loss_mean_cls: 0.043450, grad_norm: 0.412958
[[34m2025-10-04 12:24:30[0m] Step: 5319, Training Logs: loss_final: 0.904732, loss_mean: 0.861019, loss_mean_cls: 0.043713, grad_norm: 0.474958
[[34m2025-10-04 12:24:31[0m] Step: 5320, Training Logs: loss_final: 0.889122, loss_mean: 0.843044, loss_mean_cls: 0.046077, grad_norm: 0.352303
[[34m2025-10-04 12:24:31[0m] Step: 5321, Training Logs: loss_final: 0.901757, loss_mean: 0.858126, loss_mean_cls: 0.043631, grad_norm: 0.299422
[[34m2025-10-04 12:24:31[0m] Step: 5322, Training Logs: loss_final: 0.916413, loss_mean: 0.873175, loss_mean_cls: 0.043238, grad_norm: 0.315492
[[34m2025-10-04 12:24:31[0m] Step: 5323, Training Logs: loss_final: 0.882871, loss_mean: 0.837353, loss_mean_cls: 0.045518, grad_norm: 0.324670
[[34m2025-10-04 12:24:32[0m] Step: 5324, Training Logs: loss_final: 0.907229, loss_mean: 0.863398, loss_mean_cls: 0.043831, grad_norm: 0.322306
[[34m2025-10-04 12:24:32[0m] Step: 5325, Training Logs: loss_final: 0.910785, loss_mean: 0.866767, loss_mean_cls: 0.044018, grad_norm: 0.268907
[[34m2025-10-04 12:24:32[0m] Step: 5326, Training Logs: loss_final: 0.895975, loss_mean: 0.852649, loss_mean_cls: 0.043325, grad_norm: 0.331198
[[34m2025-10-04 12:24:33[0m] Step: 5327, Training Logs: loss_final: 0.877710, loss_mean: 0.831834, loss_mean_cls: 0.045877, grad_norm: 0.413029
[[34m2025-10-04 12:24:33[0m] Step: 5328, Training Logs: loss_final: 0.908428, loss_mean: 0.862667, loss_mean_cls: 0.045761, grad_norm: 0.509416
[[34m2025-10-04 12:24:33[0m] Step: 5329, Training Logs: loss_final: 0.885196, loss_mean: 0.840391, loss_mean_cls: 0.044805, grad_norm: 0.354077
[[34m2025-10-04 12:24:33[0m] Step: 5330, Training Logs: loss_final: 0.896274, loss_mean: 0.852076, loss_mean_cls: 0.044197, grad_norm: 0.506323
[[34m2025-10-04 12:24:34[0m] Step: 5331, Training Logs: loss_final: 0.893666, loss_mean: 0.848169, loss_mean_cls: 0.045497, grad_norm: 0.334033
[[34m2025-10-04 12:24:34[0m] Step: 5332, Training Logs: loss_final: 0.886922, loss_mean: 0.841871, loss_mean_cls: 0.045051, grad_norm: 0.330611
[[34m2025-10-04 12:24:34[0m] Step: 5333, Training Logs: loss_final: 0.882068, loss_mean: 0.837517, loss_mean_cls: 0.044550, grad_norm: 0.338066
[[34m2025-10-04 12:24:35[0m] Step: 5334, Training Logs: loss_final: 0.872747, loss_mean: 0.829026, loss_mean_cls: 0.043720, grad_norm: 0.455180
[[34m2025-10-04 12:24:35[0m] Step: 5335, Training Logs: loss_final: 0.875858, loss_mean: 0.830588, loss_mean_cls: 0.045270, grad_norm: 0.309897
[[34m2025-10-04 12:24:35[0m] Step: 5336, Training Logs: loss_final: 0.902285, loss_mean: 0.856697, loss_mean_cls: 0.045588, grad_norm: 0.322469
[[34m2025-10-04 12:24:35[0m] Step: 5337, Training Logs: loss_final: 0.873347, loss_mean: 0.829133, loss_mean_cls: 0.044214, grad_norm: 0.345567
[[34m2025-10-04 12:24:36[0m] Step: 5338, Training Logs: loss_final: 0.917198, loss_mean: 0.872652, loss_mean_cls: 0.044546, grad_norm: 0.320643
[[34m2025-10-04 12:24:36[0m] Step: 5339, Training Logs: loss_final: 0.876171, loss_mean: 0.832592, loss_mean_cls: 0.043579, grad_norm: 0.392749
[[34m2025-10-04 12:24:36[0m] Step: 5340, Training Logs: loss_final: 0.888295, loss_mean: 0.843773, loss_mean_cls: 0.044522, grad_norm: 0.356687
[[34m2025-10-04 12:24:37[0m] Step: 5341, Training Logs: loss_final: 0.872626, loss_mean: 0.827312, loss_mean_cls: 0.045314, grad_norm: 0.402369
[[34m2025-10-04 12:24:37[0m] Step: 5342, Training Logs: loss_final: 0.888411, loss_mean: 0.842416, loss_mean_cls: 0.045995, grad_norm: 0.319745
[[34m2025-10-04 12:24:37[0m] Step: 5343, Training Logs: loss_final: 0.903717, loss_mean: 0.860571, loss_mean_cls: 0.043146, grad_norm: 0.318121
[[34m2025-10-04 12:24:37[0m] Step: 5344, Training Logs: loss_final: 0.890229, loss_mean: 0.845050, loss_mean_cls: 0.045179, grad_norm: 0.478852
[[34m2025-10-04 12:24:38[0m] Step: 5345, Training Logs: loss_final: 0.873526, loss_mean: 0.827660, loss_mean_cls: 0.045866, grad_norm: 0.276079
[[34m2025-10-04 12:24:38[0m] Step: 5346, Training Logs: loss_final: 0.883403, loss_mean: 0.839130, loss_mean_cls: 0.044272, grad_norm: 0.390097
[[34m2025-10-04 12:24:38[0m] Step: 5347, Training Logs: loss_final: 0.884305, loss_mean: 0.840693, loss_mean_cls: 0.043612, grad_norm: 0.343876
[[34m2025-10-04 12:24:39[0m] Step: 5348, Training Logs: loss_final: 0.880755, loss_mean: 0.835240, loss_mean_cls: 0.045514, grad_norm: 0.461636
[[34m2025-10-04 12:24:39[0m] Step: 5349, Training Logs: loss_final: 0.900724, loss_mean: 0.857031, loss_mean_cls: 0.043693, grad_norm: 0.360629
[[34m2025-10-04 12:24:39[0m] Step: 5350, Training Logs: loss_final: 0.876358, loss_mean: 0.832350, loss_mean_cls: 0.044009, grad_norm: 0.483756
[[34m2025-10-04 12:24:40[0m] Step: 5351, Training Logs: loss_final: 0.882305, loss_mean: 0.836853, loss_mean_cls: 0.045453, grad_norm: 0.503073
[[34m2025-10-04 12:24:40[0m] Step: 5352, Training Logs: loss_final: 0.886172, loss_mean: 0.841542, loss_mean_cls: 0.044630, grad_norm: 0.282562
[[34m2025-10-04 12:24:40[0m] Step: 5353, Training Logs: loss_final: 0.891274, loss_mean: 0.846283, loss_mean_cls: 0.044991, grad_norm: 0.392589
[[34m2025-10-04 12:24:40[0m] Step: 5354, Training Logs: loss_final: 0.893489, loss_mean: 0.849290, loss_mean_cls: 0.044198, grad_norm: 0.279541
[[34m2025-10-04 12:24:41[0m] Step: 5355, Training Logs: loss_final: 0.889328, loss_mean: 0.844494, loss_mean_cls: 0.044834, grad_norm: 0.589831
[[34m2025-10-04 12:24:41[0m] Step: 5356, Training Logs: loss_final: 0.885287, loss_mean: 0.839427, loss_mean_cls: 0.045860, grad_norm: 0.425167
[[34m2025-10-04 12:24:41[0m] Step: 5357, Training Logs: loss_final: 0.891149, loss_mean: 0.846319, loss_mean_cls: 0.044829, grad_norm: 0.452303
[[34m2025-10-04 12:24:42[0m] Step: 5358, Training Logs: loss_final: 0.868808, loss_mean: 0.822364, loss_mean_cls: 0.046444, grad_norm: 0.625321
[[34m2025-10-04 12:24:42[0m] Step: 5359, Training Logs: loss_final: 0.901579, loss_mean: 0.856626, loss_mean_cls: 0.044953, grad_norm: 0.445198
[[34m2025-10-04 12:24:42[0m] Step: 5360, Training Logs: loss_final: 0.866810, loss_mean: 0.822222, loss_mean_cls: 0.044588, grad_norm: 0.450070
[[34m2025-10-04 12:24:43[0m] Step: 5361, Training Logs: loss_final: 0.894669, loss_mean: 0.849818, loss_mean_cls: 0.044851, grad_norm: 0.381953
[[34m2025-10-04 12:24:43[0m] Step: 5362, Training Logs: loss_final: 0.893369, loss_mean: 0.848870, loss_mean_cls: 0.044498, grad_norm: 0.384008
[[34m2025-10-04 12:24:43[0m] Step: 5363, Training Logs: loss_final: 0.882539, loss_mean: 0.837304, loss_mean_cls: 0.045235, grad_norm: 0.260481
[[34m2025-10-04 12:24:43[0m] Step: 5364, Training Logs: loss_final: 0.894153, loss_mean: 0.849547, loss_mean_cls: 0.044605, grad_norm: 0.520315
[[34m2025-10-04 12:24:44[0m] Step: 5365, Training Logs: loss_final: 0.888239, loss_mean: 0.844608, loss_mean_cls: 0.043632, grad_norm: 0.321184
[[34m2025-10-04 12:24:44[0m] Step: 5366, Training Logs: loss_final: 0.884873, loss_mean: 0.839866, loss_mean_cls: 0.045007, grad_norm: 0.597230
[[34m2025-10-04 12:24:44[0m] Step: 5367, Training Logs: loss_final: 0.884104, loss_mean: 0.839098, loss_mean_cls: 0.045006, grad_norm: 0.664868
[[34m2025-10-04 12:24:45[0m] Step: 5368, Training Logs: loss_final: 0.901013, loss_mean: 0.856802, loss_mean_cls: 0.044212, grad_norm: 0.345581
[[34m2025-10-04 12:24:45[0m] Step: 5369, Training Logs: loss_final: 0.889109, loss_mean: 0.844111, loss_mean_cls: 0.044998, grad_norm: 0.680153
[[34m2025-10-04 12:24:45[0m] Step: 5370, Training Logs: loss_final: 0.902145, loss_mean: 0.857881, loss_mean_cls: 0.044264, grad_norm: 0.449917
[[34m2025-10-04 12:24:45[0m] Step: 5371, Training Logs: loss_final: 0.878319, loss_mean: 0.832833, loss_mean_cls: 0.045486, grad_norm: 0.585213
[[34m2025-10-04 12:24:46[0m] Step: 5372, Training Logs: loss_final: 0.878801, loss_mean: 0.833468, loss_mean_cls: 0.045334, grad_norm: 0.554037
[[34m2025-10-04 12:24:46[0m] Step: 5373, Training Logs: loss_final: 0.891410, loss_mean: 0.847280, loss_mean_cls: 0.044130, grad_norm: 0.337854
[[34m2025-10-04 12:24:46[0m] Step: 5374, Training Logs: loss_final: 0.891483, loss_mean: 0.845665, loss_mean_cls: 0.045818, grad_norm: 0.424191
[[34m2025-10-04 12:24:47[0m] Step: 5375, Training Logs: loss_final: 0.898630, loss_mean: 0.854503, loss_mean_cls: 0.044128, grad_norm: 0.306631
[[34m2025-10-04 12:24:47[0m] Step: 5376, Training Logs: loss_final: 0.888907, loss_mean: 0.843820, loss_mean_cls: 0.045087, grad_norm: 0.387317
[[34m2025-10-04 12:24:47[0m] Step: 5377, Training Logs: loss_final: 0.916466, loss_mean: 0.871960, loss_mean_cls: 0.044506, grad_norm: 0.363271
[[34m2025-10-04 12:24:47[0m] Step: 5378, Training Logs: loss_final: 0.902143, loss_mean: 0.857293, loss_mean_cls: 0.044850, grad_norm: 0.472015
[[34m2025-10-04 12:24:48[0m] Step: 5379, Training Logs: loss_final: 0.891305, loss_mean: 0.846323, loss_mean_cls: 0.044982, grad_norm: 0.277853
[[34m2025-10-04 12:24:48[0m] Step: 5380, Training Logs: loss_final: 0.897253, loss_mean: 0.853035, loss_mean_cls: 0.044217, grad_norm: 0.424894
[[34m2025-10-04 12:24:48[0m] Step: 5381, Training Logs: loss_final: 0.881999, loss_mean: 0.837397, loss_mean_cls: 0.044601, grad_norm: 0.338009
[[34m2025-10-04 12:24:49[0m] Step: 5382, Training Logs: loss_final: 0.888210, loss_mean: 0.843448, loss_mean_cls: 0.044762, grad_norm: 0.266477
[[34m2025-10-04 12:24:49[0m] Step: 5383, Training Logs: loss_final: 0.896312, loss_mean: 0.852147, loss_mean_cls: 0.044164, grad_norm: 0.305878
[[34m2025-10-04 12:24:49[0m] Step: 5384, Training Logs: loss_final: 0.911633, loss_mean: 0.867954, loss_mean_cls: 0.043679, grad_norm: 0.346368
[[34m2025-10-04 12:24:50[0m] Step: 5385, Training Logs: loss_final: 0.904133, loss_mean: 0.859230, loss_mean_cls: 0.044903, grad_norm: 0.324627
[[34m2025-10-04 12:24:50[0m] Step: 5386, Training Logs: loss_final: 0.896482, loss_mean: 0.852878, loss_mean_cls: 0.043604, grad_norm: 0.387927
[[34m2025-10-04 12:24:50[0m] Step: 5387, Training Logs: loss_final: 0.886560, loss_mean: 0.841699, loss_mean_cls: 0.044861, grad_norm: 0.394983
[[34m2025-10-04 12:24:50[0m] Step: 5388, Training Logs: loss_final: 0.881389, loss_mean: 0.835187, loss_mean_cls: 0.046202, grad_norm: 0.253357
[[34m2025-10-04 12:24:51[0m] Step: 5389, Training Logs: loss_final: 0.888044, loss_mean: 0.842491, loss_mean_cls: 0.045553, grad_norm: 0.261731
[[34m2025-10-04 12:24:51[0m] Step: 5390, Training Logs: loss_final: 0.866029, loss_mean: 0.821696, loss_mean_cls: 0.044332, grad_norm: 0.359185
[[34m2025-10-04 12:24:51[0m] Step: 5391, Training Logs: loss_final: 0.868477, loss_mean: 0.822251, loss_mean_cls: 0.046226, grad_norm: 0.483046
[[34m2025-10-04 12:24:52[0m] Step: 5392, Training Logs: loss_final: 0.869842, loss_mean: 0.824468, loss_mean_cls: 0.045374, grad_norm: 0.263495
[[34m2025-10-04 12:24:52[0m] Step: 5393, Training Logs: loss_final: 0.883980, loss_mean: 0.838510, loss_mean_cls: 0.045470, grad_norm: 0.343962
[[34m2025-10-04 12:24:52[0m] Step: 5394, Training Logs: loss_final: 0.893529, loss_mean: 0.849141, loss_mean_cls: 0.044388, grad_norm: 0.440724
[[34m2025-10-04 12:24:52[0m] Step: 5395, Training Logs: loss_final: 0.882687, loss_mean: 0.838351, loss_mean_cls: 0.044336, grad_norm: 0.361753
[[34m2025-10-04 12:24:53[0m] Step: 5396, Training Logs: loss_final: 0.883430, loss_mean: 0.839464, loss_mean_cls: 0.043966, grad_norm: 0.295981
[[34m2025-10-04 12:24:53[0m] Step: 5397, Training Logs: loss_final: 0.863004, loss_mean: 0.817377, loss_mean_cls: 0.045627, grad_norm: 0.418703
[[34m2025-10-04 12:24:53[0m] Step: 5398, Training Logs: loss_final: 0.901200, loss_mean: 0.856825, loss_mean_cls: 0.044375, grad_norm: 0.295264
[[34m2025-10-04 12:24:54[0m] Step: 5399, Training Logs: loss_final: 0.885916, loss_mean: 0.840824, loss_mean_cls: 0.045093, grad_norm: 0.432256
[[34m2025-10-04 12:24:54[0m] Step: 5400, Training Logs: loss_final: 0.910971, loss_mean: 0.867247, loss_mean_cls: 0.043723, grad_norm: 0.333966
[[34m2025-10-04 12:24:54[0m] Step: 5401, Training Logs: loss_final: 0.880885, loss_mean: 0.836412, loss_mean_cls: 0.044473, grad_norm: 0.411196
[[34m2025-10-04 12:24:54[0m] Step: 5402, Training Logs: loss_final: 0.899848, loss_mean: 0.855236, loss_mean_cls: 0.044612, grad_norm: 0.409782
[[34m2025-10-04 12:24:55[0m] Step: 5403, Training Logs: loss_final: 0.894617, loss_mean: 0.849483, loss_mean_cls: 0.045134, grad_norm: 0.287560
[[34m2025-10-04 12:24:55[0m] Step: 5404, Training Logs: loss_final: 0.892359, loss_mean: 0.846963, loss_mean_cls: 0.045395, grad_norm: 0.443198
[[34m2025-10-04 12:24:55[0m] Step: 5405, Training Logs: loss_final: 0.874821, loss_mean: 0.829097, loss_mean_cls: 0.045725, grad_norm: 0.419609
[[34m2025-10-04 12:24:56[0m] Step: 5406, Training Logs: loss_final: 0.897116, loss_mean: 0.852844, loss_mean_cls: 0.044272, grad_norm: 0.373626
[[34m2025-10-04 12:24:56[0m] Step: 5407, Training Logs: loss_final: 0.888679, loss_mean: 0.844997, loss_mean_cls: 0.043681, grad_norm: 0.341566
[[34m2025-10-04 12:24:56[0m] Step: 5408, Training Logs: loss_final: 0.890891, loss_mean: 0.845754, loss_mean_cls: 0.045137, grad_norm: 0.371760
[[34m2025-10-04 12:24:56[0m] Step: 5409, Training Logs: loss_final: 0.887200, loss_mean: 0.842639, loss_mean_cls: 0.044561, grad_norm: 0.464399
[[34m2025-10-04 12:24:57[0m] Step: 5410, Training Logs: loss_final: 0.879616, loss_mean: 0.835511, loss_mean_cls: 0.044105, grad_norm: 0.458605
[[34m2025-10-04 12:24:57[0m] Step: 5411, Training Logs: loss_final: 0.897202, loss_mean: 0.853837, loss_mean_cls: 0.043365, grad_norm: 0.282896
[[34m2025-10-04 12:24:57[0m] Step: 5412, Training Logs: loss_final: 0.892911, loss_mean: 0.849219, loss_mean_cls: 0.043692, grad_norm: 0.383146
[[34m2025-10-04 12:24:58[0m] Step: 5413, Training Logs: loss_final: 0.916822, loss_mean: 0.872201, loss_mean_cls: 0.044622, grad_norm: 0.347602
[[34m2025-10-04 12:24:58[0m] Step: 5414, Training Logs: loss_final: 0.891943, loss_mean: 0.848021, loss_mean_cls: 0.043922, grad_norm: 0.312456
[[34m2025-10-04 12:24:58[0m] Step: 5415, Training Logs: loss_final: 0.894876, loss_mean: 0.848772, loss_mean_cls: 0.046104, grad_norm: 0.332777
[[34m2025-10-04 12:24:58[0m] Step: 5416, Training Logs: loss_final: 0.908218, loss_mean: 0.863627, loss_mean_cls: 0.044591, grad_norm: 0.421813
[[34m2025-10-04 12:24:59[0m] Step: 5417, Training Logs: loss_final: 0.886741, loss_mean: 0.841418, loss_mean_cls: 0.045324, grad_norm: 0.330569
[[34m2025-10-04 12:24:59[0m] Step: 5418, Training Logs: loss_final: 0.888198, loss_mean: 0.843484, loss_mean_cls: 0.044714, grad_norm: 0.276325
[[34m2025-10-04 12:24:59[0m] Step: 5419, Training Logs: loss_final: 0.874011, loss_mean: 0.829419, loss_mean_cls: 0.044592, grad_norm: 0.370720
[[34m2025-10-04 12:25:00[0m] Step: 5420, Training Logs: loss_final: 0.883745, loss_mean: 0.838294, loss_mean_cls: 0.045451, grad_norm: 0.265090
[[34m2025-10-04 12:25:00[0m] Step: 5421, Training Logs: loss_final: 0.905086, loss_mean: 0.860632, loss_mean_cls: 0.044454, grad_norm: 0.384194
[[34m2025-10-04 12:25:00[0m] Step: 5422, Training Logs: loss_final: 0.881265, loss_mean: 0.836481, loss_mean_cls: 0.044784, grad_norm: 0.323172
[[34m2025-10-04 12:25:01[0m] Step: 5423, Training Logs: loss_final: 0.895302, loss_mean: 0.850181, loss_mean_cls: 0.045121, grad_norm: 0.290401
[[34m2025-10-04 12:25:01[0m] Step: 5424, Training Logs: loss_final: 0.896844, loss_mean: 0.851395, loss_mean_cls: 0.045449, grad_norm: 0.352180
[[34m2025-10-04 12:25:01[0m] Step: 5425, Training Logs: loss_final: 0.894594, loss_mean: 0.849123, loss_mean_cls: 0.045472, grad_norm: 0.340597
[[34m2025-10-04 12:25:01[0m] Step: 5426, Training Logs: loss_final: 0.889447, loss_mean: 0.844130, loss_mean_cls: 0.045318, grad_norm: 0.399559
[[34m2025-10-04 12:25:02[0m] Step: 5427, Training Logs: loss_final: 0.883614, loss_mean: 0.838882, loss_mean_cls: 0.044732, grad_norm: 0.298594
[[34m2025-10-04 12:25:02[0m] Step: 5428, Training Logs: loss_final: 0.875368, loss_mean: 0.831305, loss_mean_cls: 0.044064, grad_norm: 0.348509
[[34m2025-10-04 12:25:02[0m] Step: 5429, Training Logs: loss_final: 0.900743, loss_mean: 0.856605, loss_mean_cls: 0.044138, grad_norm: 0.405179
[[34m2025-10-04 12:25:03[0m] Step: 5430, Training Logs: loss_final: 0.891079, loss_mean: 0.846748, loss_mean_cls: 0.044331, grad_norm: 0.283545
[[34m2025-10-04 12:25:03[0m] Step: 5431, Training Logs: loss_final: 0.873496, loss_mean: 0.828507, loss_mean_cls: 0.044989, grad_norm: 0.314731
[[34m2025-10-04 12:25:03[0m] Step: 5432, Training Logs: loss_final: 0.869718, loss_mean: 0.824551, loss_mean_cls: 0.045167, grad_norm: 0.324832
[[34m2025-10-04 12:25:03[0m] Step: 5433, Training Logs: loss_final: 0.872171, loss_mean: 0.827825, loss_mean_cls: 0.044346, grad_norm: 0.334033
[[34m2025-10-04 12:25:04[0m] Step: 5434, Training Logs: loss_final: 0.883802, loss_mean: 0.839636, loss_mean_cls: 0.044166, grad_norm: 0.358654
[[34m2025-10-04 12:25:04[0m] Step: 5435, Training Logs: loss_final: 0.896556, loss_mean: 0.852200, loss_mean_cls: 0.044356, grad_norm: 0.405242
[[34m2025-10-04 12:25:04[0m] Step: 5436, Training Logs: loss_final: 0.872647, loss_mean: 0.829451, loss_mean_cls: 0.043196, grad_norm: 0.336839
[[34m2025-10-04 12:25:05[0m] Step: 5437, Training Logs: loss_final: 0.863963, loss_mean: 0.818452, loss_mean_cls: 0.045511, grad_norm: 0.305258
[[34m2025-10-04 12:25:05[0m] Step: 5438, Training Logs: loss_final: 0.879774, loss_mean: 0.834537, loss_mean_cls: 0.045237, grad_norm: 0.403329
[[34m2025-10-04 12:25:05[0m] Step: 5439, Training Logs: loss_final: 0.881044, loss_mean: 0.835345, loss_mean_cls: 0.045699, grad_norm: 0.300238
[[34m2025-10-04 12:25:05[0m] Step: 5440, Training Logs: loss_final: 0.898544, loss_mean: 0.853877, loss_mean_cls: 0.044667, grad_norm: 0.303968
[[34m2025-10-04 12:25:06[0m] Step: 5441, Training Logs: loss_final: 0.895266, loss_mean: 0.851032, loss_mean_cls: 0.044235, grad_norm: 0.325905
[[34m2025-10-04 12:25:06[0m] Step: 5442, Training Logs: loss_final: 0.885616, loss_mean: 0.840762, loss_mean_cls: 0.044853, grad_norm: 0.304247
[[34m2025-10-04 12:25:06[0m] Step: 5443, Training Logs: loss_final: 0.879369, loss_mean: 0.834796, loss_mean_cls: 0.044573, grad_norm: 0.388252
[[34m2025-10-04 12:25:07[0m] Step: 5444, Training Logs: loss_final: 0.884416, loss_mean: 0.839268, loss_mean_cls: 0.045148, grad_norm: 0.361671
[[34m2025-10-04 12:25:07[0m] Step: 5445, Training Logs: loss_final: 0.896594, loss_mean: 0.852156, loss_mean_cls: 0.044439, grad_norm: 0.292560
[[34m2025-10-04 12:25:07[0m] Step: 5446, Training Logs: loss_final: 0.857840, loss_mean: 0.812558, loss_mean_cls: 0.045282, grad_norm: 0.360127
[[34m2025-10-04 12:25:08[0m] Step: 5447, Training Logs: loss_final: 0.886706, loss_mean: 0.841563, loss_mean_cls: 0.045143, grad_norm: 0.343254
[[34m2025-10-04 12:25:08[0m] Step: 5448, Training Logs: loss_final: 0.879123, loss_mean: 0.835009, loss_mean_cls: 0.044115, grad_norm: 0.282138
[[34m2025-10-04 12:25:08[0m] Step: 5449, Training Logs: loss_final: 0.885140, loss_mean: 0.839831, loss_mean_cls: 0.045309, grad_norm: 0.349379
[[34m2025-10-04 12:25:08[0m] Step: 5450, Training Logs: loss_final: 0.893611, loss_mean: 0.847810, loss_mean_cls: 0.045801, grad_norm: 0.311111
[[34m2025-10-04 12:25:09[0m] Step: 5451, Training Logs: loss_final: 0.876161, loss_mean: 0.830773, loss_mean_cls: 0.045388, grad_norm: 0.271387
[[34m2025-10-04 12:25:09[0m] Step: 5452, Training Logs: loss_final: 0.866802, loss_mean: 0.821535, loss_mean_cls: 0.045267, grad_norm: 0.298235
[[34m2025-10-04 12:25:09[0m] Step: 5453, Training Logs: loss_final: 0.899472, loss_mean: 0.855611, loss_mean_cls: 0.043862, grad_norm: 0.294480
[[34m2025-10-04 12:25:10[0m] Step: 5454, Training Logs: loss_final: 0.880891, loss_mean: 0.837420, loss_mean_cls: 0.043471, grad_norm: 0.359223
[[34m2025-10-04 12:25:10[0m] Step: 5455, Training Logs: loss_final: 0.897173, loss_mean: 0.852545, loss_mean_cls: 0.044628, grad_norm: 0.321027
[[34m2025-10-04 12:25:10[0m] Step: 5456, Training Logs: loss_final: 0.909184, loss_mean: 0.865067, loss_mean_cls: 0.044117, grad_norm: 0.239458
[[34m2025-10-04 12:25:10[0m] Step: 5457, Training Logs: loss_final: 0.890738, loss_mean: 0.847105, loss_mean_cls: 0.043633, grad_norm: 0.354274
[[34m2025-10-04 12:25:11[0m] Step: 5458, Training Logs: loss_final: 0.881866, loss_mean: 0.838571, loss_mean_cls: 0.043295, grad_norm: 0.444469
[[34m2025-10-04 12:25:11[0m] Step: 5459, Training Logs: loss_final: 0.892599, loss_mean: 0.849601, loss_mean_cls: 0.042999, grad_norm: 0.309506
[[34m2025-10-04 12:25:11[0m] Step: 5460, Training Logs: loss_final: 0.874692, loss_mean: 0.830936, loss_mean_cls: 0.043757, grad_norm: 0.315710
[[34m2025-10-04 12:25:12[0m] Step: 5461, Training Logs: loss_final: 0.909958, loss_mean: 0.866140, loss_mean_cls: 0.043818, grad_norm: 0.393232
[[34m2025-10-04 12:25:12[0m] Step: 5462, Training Logs: loss_final: 0.889045, loss_mean: 0.843151, loss_mean_cls: 0.045894, grad_norm: 0.361046
[[34m2025-10-04 12:25:12[0m] Step: 5463, Training Logs: loss_final: 0.872115, loss_mean: 0.827768, loss_mean_cls: 0.044346, grad_norm: 0.345853
[[34m2025-10-04 12:25:12[0m] Step: 5464, Training Logs: loss_final: 0.901712, loss_mean: 0.857232, loss_mean_cls: 0.044480, grad_norm: 0.415072
[[34m2025-10-04 12:25:13[0m] Step: 5465, Training Logs: loss_final: 0.876474, loss_mean: 0.831059, loss_mean_cls: 0.045416, grad_norm: 0.249873
[[34m2025-10-04 12:25:13[0m] Step: 5466, Training Logs: loss_final: 0.875449, loss_mean: 0.830035, loss_mean_cls: 0.045414, grad_norm: 0.420489
[[34m2025-10-04 12:25:13[0m] Step: 5467, Training Logs: loss_final: 0.895473, loss_mean: 0.851833, loss_mean_cls: 0.043640, grad_norm: 0.404325
[[34m2025-10-04 12:25:14[0m] Step: 5468, Training Logs: loss_final: 0.888930, loss_mean: 0.845639, loss_mean_cls: 0.043291, grad_norm: 0.275134
[[34m2025-10-04 12:25:14[0m] Step: 5469, Training Logs: loss_final: 0.895882, loss_mean: 0.851114, loss_mean_cls: 0.044768, grad_norm: 0.412186
[[34m2025-10-04 12:25:14[0m] Step: 5470, Training Logs: loss_final: 0.894009, loss_mean: 0.849877, loss_mean_cls: 0.044132, grad_norm: 0.403134
[[34m2025-10-04 12:25:15[0m] Step: 5471, Training Logs: loss_final: 0.905833, loss_mean: 0.861337, loss_mean_cls: 0.044496, grad_norm: 0.467600
[[34m2025-10-04 12:25:15[0m] Step: 5472, Training Logs: loss_final: 0.876008, loss_mean: 0.830144, loss_mean_cls: 0.045864, grad_norm: 0.424334
[[34m2025-10-04 12:25:15[0m] Step: 5473, Training Logs: loss_final: 0.905465, loss_mean: 0.861425, loss_mean_cls: 0.044039, grad_norm: 0.422369
[[34m2025-10-04 12:25:15[0m] Step: 5474, Training Logs: loss_final: 0.879469, loss_mean: 0.834448, loss_mean_cls: 0.045021, grad_norm: 0.537590
[[34m2025-10-04 12:25:16[0m] Step: 5475, Training Logs: loss_final: 0.865881, loss_mean: 0.822360, loss_mean_cls: 0.043521, grad_norm: 0.391294
[[34m2025-10-04 12:25:16[0m] Step: 5476, Training Logs: loss_final: 0.895223, loss_mean: 0.850295, loss_mean_cls: 0.044928, grad_norm: 0.626930
[[34m2025-10-04 12:25:16[0m] Step: 5477, Training Logs: loss_final: 0.884818, loss_mean: 0.840100, loss_mean_cls: 0.044718, grad_norm: 0.400236
[[34m2025-10-04 12:25:17[0m] Step: 5478, Training Logs: loss_final: 0.893209, loss_mean: 0.849148, loss_mean_cls: 0.044061, grad_norm: 0.475298
[[34m2025-10-04 12:25:17[0m] Step: 5479, Training Logs: loss_final: 0.856913, loss_mean: 0.811257, loss_mean_cls: 0.045656, grad_norm: 0.380869
[[34m2025-10-04 12:25:17[0m] Step: 5480, Training Logs: loss_final: 0.867986, loss_mean: 0.823137, loss_mean_cls: 0.044849, grad_norm: 0.390817
[[34m2025-10-04 12:25:17[0m] Step: 5481, Training Logs: loss_final: 0.878307, loss_mean: 0.832190, loss_mean_cls: 0.046117, grad_norm: 0.343108
[[34m2025-10-04 12:25:18[0m] Step: 5482, Training Logs: loss_final: 0.876504, loss_mean: 0.832069, loss_mean_cls: 0.044435, grad_norm: 0.505651
[[34m2025-10-04 12:25:18[0m] Step: 5483, Training Logs: loss_final: 0.894397, loss_mean: 0.849425, loss_mean_cls: 0.044972, grad_norm: 0.316780
[[34m2025-10-04 12:25:18[0m] Step: 5484, Training Logs: loss_final: 0.868196, loss_mean: 0.823104, loss_mean_cls: 0.045092, grad_norm: 0.440789
[[34m2025-10-04 12:25:19[0m] Step: 5485, Training Logs: loss_final: 0.863859, loss_mean: 0.817637, loss_mean_cls: 0.046221, grad_norm: 0.298556
[[34m2025-10-04 12:25:19[0m] Step: 5486, Training Logs: loss_final: 0.889903, loss_mean: 0.845415, loss_mean_cls: 0.044488, grad_norm: 0.359773
[[34m2025-10-04 12:25:19[0m] Step: 5487, Training Logs: loss_final: 0.869350, loss_mean: 0.824638, loss_mean_cls: 0.044712, grad_norm: 0.320400
[[34m2025-10-04 12:25:20[0m] Step: 5488, Training Logs: loss_final: 0.881363, loss_mean: 0.836088, loss_mean_cls: 0.045276, grad_norm: 0.292015
[[34m2025-10-04 12:25:20[0m] Step: 5489, Training Logs: loss_final: 0.892279, loss_mean: 0.847953, loss_mean_cls: 0.044326, grad_norm: 0.357803
[[34m2025-10-04 12:25:20[0m] Step: 5490, Training Logs: loss_final: 0.877288, loss_mean: 0.832660, loss_mean_cls: 0.044629, grad_norm: 0.295949
[[34m2025-10-04 12:25:20[0m] Step: 5491, Training Logs: loss_final: 0.880571, loss_mean: 0.837073, loss_mean_cls: 0.043498, grad_norm: 0.299079
[[34m2025-10-04 12:25:21[0m] Step: 5492, Training Logs: loss_final: 0.882858, loss_mean: 0.838525, loss_mean_cls: 0.044333, grad_norm: 0.593944
[[34m2025-10-04 12:25:21[0m] Step: 5493, Training Logs: loss_final: 0.874861, loss_mean: 0.830425, loss_mean_cls: 0.044436, grad_norm: 0.373396
[[34m2025-10-04 12:25:21[0m] Step: 5494, Training Logs: loss_final: 0.897160, loss_mean: 0.852445, loss_mean_cls: 0.044715, grad_norm: 0.354600
[[34m2025-10-04 12:25:22[0m] Step: 5495, Training Logs: loss_final: 0.905155, loss_mean: 0.861337, loss_mean_cls: 0.043818, grad_norm: 0.543111
[[34m2025-10-04 12:25:22[0m] Step: 5496, Training Logs: loss_final: 0.878154, loss_mean: 0.832954, loss_mean_cls: 0.045201, grad_norm: 0.432196
[[34m2025-10-04 12:25:22[0m] Step: 5497, Training Logs: loss_final: 0.890171, loss_mean: 0.844621, loss_mean_cls: 0.045550, grad_norm: 0.414095
[[34m2025-10-04 12:25:22[0m] Step: 5498, Training Logs: loss_final: 0.893004, loss_mean: 0.848943, loss_mean_cls: 0.044060, grad_norm: 0.473285
[[34m2025-10-04 12:25:23[0m] Step: 5499, Training Logs: loss_final: 0.893667, loss_mean: 0.848911, loss_mean_cls: 0.044756, grad_norm: 0.624822
[[34m2025-10-04 12:25:23[0m] Step: 5500, Training Logs: loss_final: 0.897288, loss_mean: 0.853142, loss_mean_cls: 0.044146, grad_norm: 0.447775
[[34m2025-10-04 12:25:23[0m] Step: 5501, Training Logs: loss_final: 0.873544, loss_mean: 0.828235, loss_mean_cls: 0.045309, grad_norm: 0.362062
[[34m2025-10-04 12:25:24[0m] Step: 5502, Training Logs: loss_final: 0.893921, loss_mean: 0.849390, loss_mean_cls: 0.044531, grad_norm: 0.550155
[[34m2025-10-04 12:25:24[0m] Step: 5503, Training Logs: loss_final: 0.900149, loss_mean: 0.855271, loss_mean_cls: 0.044879, grad_norm: 0.375784
[[34m2025-10-04 12:25:24[0m] Step: 5504, Training Logs: loss_final: 0.914045, loss_mean: 0.871138, loss_mean_cls: 0.042907, grad_norm: 0.581495
[[34m2025-10-04 12:25:25[0m] Step: 5505, Training Logs: loss_final: 0.876555, loss_mean: 0.832118, loss_mean_cls: 0.044437, grad_norm: 0.520586
[[34m2025-10-04 12:25:25[0m] Step: 5506, Training Logs: loss_final: 0.901269, loss_mean: 0.857226, loss_mean_cls: 0.044043, grad_norm: 0.410362
[[34m2025-10-04 12:25:25[0m] Step: 5507, Training Logs: loss_final: 0.882725, loss_mean: 0.837769, loss_mean_cls: 0.044955, grad_norm: 0.331987
[[34m2025-10-04 12:25:25[0m] Step: 5508, Training Logs: loss_final: 0.892326, loss_mean: 0.848428, loss_mean_cls: 0.043898, grad_norm: 0.469647
[[34m2025-10-04 12:25:26[0m] Step: 5509, Training Logs: loss_final: 0.894629, loss_mean: 0.848882, loss_mean_cls: 0.045747, grad_norm: 0.416092
[[34m2025-10-04 12:25:26[0m] Step: 5510, Training Logs: loss_final: 0.859860, loss_mean: 0.815057, loss_mean_cls: 0.044803, grad_norm: 0.351143
[[34m2025-10-04 12:25:26[0m] Step: 5511, Training Logs: loss_final: 0.902535, loss_mean: 0.858597, loss_mean_cls: 0.043938, grad_norm: 0.349059
[[34m2025-10-04 12:25:27[0m] Step: 5512, Training Logs: loss_final: 0.882163, loss_mean: 0.837613, loss_mean_cls: 0.044550, grad_norm: 0.395823
[[34m2025-10-04 12:25:27[0m] Step: 5513, Training Logs: loss_final: 0.871943, loss_mean: 0.827741, loss_mean_cls: 0.044202, grad_norm: 0.431046
[[34m2025-10-04 12:25:27[0m] Step: 5514, Training Logs: loss_final: 0.882138, loss_mean: 0.837695, loss_mean_cls: 0.044444, grad_norm: 0.454445
[[34m2025-10-04 12:25:28[0m] Step: 5515, Training Logs: loss_final: 0.891166, loss_mean: 0.847642, loss_mean_cls: 0.043524, grad_norm: 0.419730
[[34m2025-10-04 12:25:28[0m] Step: 5516, Training Logs: loss_final: 0.870959, loss_mean: 0.827453, loss_mean_cls: 0.043506, grad_norm: 0.424968
[[34m2025-10-04 12:25:28[0m] Step: 5517, Training Logs: loss_final: 0.890534, loss_mean: 0.846876, loss_mean_cls: 0.043658, grad_norm: 0.454830
[[34m2025-10-04 12:25:28[0m] Step: 5518, Training Logs: loss_final: 0.899011, loss_mean: 0.854339, loss_mean_cls: 0.044672, grad_norm: 0.264198
[[34m2025-10-04 12:25:29[0m] Step: 5519, Training Logs: loss_final: 0.899622, loss_mean: 0.855239, loss_mean_cls: 0.044383, grad_norm: 0.620973
[[34m2025-10-04 12:25:29[0m] Step: 5520, Training Logs: loss_final: 0.887622, loss_mean: 0.843536, loss_mean_cls: 0.044087, grad_norm: 0.574175
[[34m2025-10-04 12:25:29[0m] Step: 5521, Training Logs: loss_final: 0.887739, loss_mean: 0.842227, loss_mean_cls: 0.045512, grad_norm: 0.527960
[[34m2025-10-04 12:25:30[0m] Step: 5522, Training Logs: loss_final: 0.898876, loss_mean: 0.854408, loss_mean_cls: 0.044469, grad_norm: 0.517996
[[34m2025-10-04 12:25:30[0m] Step: 5523, Training Logs: loss_final: 0.892491, loss_mean: 0.847605, loss_mean_cls: 0.044887, grad_norm: 0.465232
[[34m2025-10-04 12:25:30[0m] Step: 5524, Training Logs: loss_final: 0.909970, loss_mean: 0.866251, loss_mean_cls: 0.043719, grad_norm: 0.708151
[[34m2025-10-04 12:25:31[0m] Step: 5525, Training Logs: loss_final: 0.912561, loss_mean: 0.868445, loss_mean_cls: 0.044116, grad_norm: 0.401098
[[34m2025-10-04 12:25:31[0m] Step: 5526, Training Logs: loss_final: 0.866984, loss_mean: 0.822208, loss_mean_cls: 0.044776, grad_norm: 0.503160
[[34m2025-10-04 12:25:31[0m] Step: 5527, Training Logs: loss_final: 0.884740, loss_mean: 0.840615, loss_mean_cls: 0.044125, grad_norm: 0.445617
[[34m2025-10-04 12:25:32[0m] Step: 5528, Training Logs: loss_final: 0.876917, loss_mean: 0.831562, loss_mean_cls: 0.045355, grad_norm: 0.680148
[[34m2025-10-04 12:25:32[0m] Step: 5529, Training Logs: loss_final: 0.879062, loss_mean: 0.835016, loss_mean_cls: 0.044045, grad_norm: 0.480338
[[34m2025-10-04 12:25:32[0m] Step: 5530, Training Logs: loss_final: 0.891268, loss_mean: 0.846738, loss_mean_cls: 0.044529, grad_norm: 0.570595
[[34m2025-10-04 12:25:32[0m] Step: 5531, Training Logs: loss_final: 0.906152, loss_mean: 0.862830, loss_mean_cls: 0.043322, grad_norm: 0.529878
[[34m2025-10-04 12:25:33[0m] Step: 5532, Training Logs: loss_final: 0.873836, loss_mean: 0.829983, loss_mean_cls: 0.043853, grad_norm: 0.414981
[[34m2025-10-04 12:25:33[0m] Step: 5533, Training Logs: loss_final: 0.868028, loss_mean: 0.823432, loss_mean_cls: 0.044597, grad_norm: 0.447511
[[34m2025-10-04 12:25:33[0m] Step: 5534, Training Logs: loss_final: 0.890925, loss_mean: 0.846864, loss_mean_cls: 0.044061, grad_norm: 0.350157
[[34m2025-10-04 12:25:34[0m] Step: 5535, Training Logs: loss_final: 0.874553, loss_mean: 0.829679, loss_mean_cls: 0.044874, grad_norm: 0.506845
[[34m2025-10-04 12:25:34[0m] Step: 5536, Training Logs: loss_final: 0.887607, loss_mean: 0.842476, loss_mean_cls: 0.045131, grad_norm: 0.329843
[[34m2025-10-04 12:25:34[0m] Step: 5537, Training Logs: loss_final: 0.903956, loss_mean: 0.860908, loss_mean_cls: 0.043048, grad_norm: 0.486103
[[34m2025-10-04 12:25:35[0m] Step: 5538, Training Logs: loss_final: 0.889405, loss_mean: 0.845174, loss_mean_cls: 0.044231, grad_norm: 0.356560
[[34m2025-10-04 12:25:35[0m] Step: 5539, Training Logs: loss_final: 0.888662, loss_mean: 0.843162, loss_mean_cls: 0.045500, grad_norm: 0.454360
[[34m2025-10-04 12:25:35[0m] Step: 5540, Training Logs: loss_final: 0.883885, loss_mean: 0.838715, loss_mean_cls: 0.045170, grad_norm: 0.323314
[[34m2025-10-04 12:25:35[0m] Step: 5541, Training Logs: loss_final: 0.882900, loss_mean: 0.836664, loss_mean_cls: 0.046236, grad_norm: 0.407519
[[34m2025-10-04 12:25:36[0m] Step: 5542, Training Logs: loss_final: 0.878165, loss_mean: 0.833603, loss_mean_cls: 0.044562, grad_norm: 0.301798
[[34m2025-10-04 12:25:36[0m] Step: 5543, Training Logs: loss_final: 0.890629, loss_mean: 0.846166, loss_mean_cls: 0.044463, grad_norm: 0.316142
[[34m2025-10-04 12:25:36[0m] Step: 5544, Training Logs: loss_final: 0.904884, loss_mean: 0.860208, loss_mean_cls: 0.044676, grad_norm: 0.366091
[[34m2025-10-04 12:25:37[0m] Step: 5545, Training Logs: loss_final: 0.893203, loss_mean: 0.848483, loss_mean_cls: 0.044720, grad_norm: 0.268645
[[34m2025-10-04 12:25:37[0m] Step: 5546, Training Logs: loss_final: 0.898151, loss_mean: 0.852120, loss_mean_cls: 0.046031, grad_norm: 0.264229
[[34m2025-10-04 12:25:37[0m] Step: 5547, Training Logs: loss_final: 0.881301, loss_mean: 0.836330, loss_mean_cls: 0.044971, grad_norm: 0.284714
[[34m2025-10-04 12:25:37[0m] Step: 5548, Training Logs: loss_final: 0.876763, loss_mean: 0.832352, loss_mean_cls: 0.044412, grad_norm: 0.375595
[[34m2025-10-04 12:25:38[0m] Step: 5549, Training Logs: loss_final: 0.865045, loss_mean: 0.819194, loss_mean_cls: 0.045851, grad_norm: 0.411731
[[34m2025-10-04 12:25:38[0m] Step: 5550, Training Logs: loss_final: 0.859507, loss_mean: 0.814743, loss_mean_cls: 0.044764, grad_norm: 0.380193
[[34m2025-10-04 12:25:38[0m] Step: 5551, Training Logs: loss_final: 0.889704, loss_mean: 0.844997, loss_mean_cls: 0.044707, grad_norm: 0.430984
[[34m2025-10-04 12:25:39[0m] Step: 5552, Training Logs: loss_final: 0.886678, loss_mean: 0.841732, loss_mean_cls: 0.044947, grad_norm: 0.416643
[[34m2025-10-04 12:25:39[0m] Step: 5553, Training Logs: loss_final: 0.885906, loss_mean: 0.840880, loss_mean_cls: 0.045026, grad_norm: 0.315884
[[34m2025-10-04 12:25:39[0m] Step: 5554, Training Logs: loss_final: 0.866401, loss_mean: 0.821465, loss_mean_cls: 0.044936, grad_norm: 0.380132
[[34m2025-10-04 12:25:40[0m] Step: 5555, Training Logs: loss_final: 0.887705, loss_mean: 0.843124, loss_mean_cls: 0.044580, grad_norm: 0.385781
[[34m2025-10-04 12:25:40[0m] Step: 5556, Training Logs: loss_final: 0.885334, loss_mean: 0.841383, loss_mean_cls: 0.043952, grad_norm: 0.337871
[[34m2025-10-04 12:25:40[0m] Step: 5557, Training Logs: loss_final: 0.890153, loss_mean: 0.845836, loss_mean_cls: 0.044317, grad_norm: 0.411623
[[34m2025-10-04 12:25:40[0m] Step: 5558, Training Logs: loss_final: 0.889925, loss_mean: 0.845807, loss_mean_cls: 0.044118, grad_norm: 0.504266
[[34m2025-10-04 12:25:41[0m] Step: 5559, Training Logs: loss_final: 0.906746, loss_mean: 0.862080, loss_mean_cls: 0.044666, grad_norm: 0.314897
[[34m2025-10-04 12:25:41[0m] Step: 5560, Training Logs: loss_final: 0.872730, loss_mean: 0.828385, loss_mean_cls: 0.044344, grad_norm: 0.411278
[[34m2025-10-04 12:25:41[0m] Step: 5561, Training Logs: loss_final: 0.891474, loss_mean: 0.846382, loss_mean_cls: 0.045092, grad_norm: 0.285527
[[34m2025-10-04 12:25:42[0m] Step: 5562, Training Logs: loss_final: 0.895470, loss_mean: 0.850458, loss_mean_cls: 0.045012, grad_norm: 0.414382
[[34m2025-10-04 12:25:42[0m] Step: 5563, Training Logs: loss_final: 0.899181, loss_mean: 0.854365, loss_mean_cls: 0.044816, grad_norm: 0.306818
[[34m2025-10-04 12:25:42[0m] Step: 5564, Training Logs: loss_final: 0.899363, loss_mean: 0.856101, loss_mean_cls: 0.043262, grad_norm: 0.374403
[[34m2025-10-04 12:25:42[0m] Step: 5565, Training Logs: loss_final: 0.873038, loss_mean: 0.826573, loss_mean_cls: 0.046464, grad_norm: 0.287862
[[34m2025-10-04 12:25:43[0m] Step: 5566, Training Logs: loss_final: 0.890888, loss_mean: 0.846737, loss_mean_cls: 0.044151, grad_norm: 0.462474
[[34m2025-10-04 12:25:43[0m] Step: 5567, Training Logs: loss_final: 0.884945, loss_mean: 0.840368, loss_mean_cls: 0.044577, grad_norm: 0.310527
[[34m2025-10-04 12:25:43[0m] Step: 5568, Training Logs: loss_final: 0.888601, loss_mean: 0.845230, loss_mean_cls: 0.043372, grad_norm: 0.419541
[[34m2025-10-04 12:25:44[0m] Step: 5569, Training Logs: loss_final: 0.904172, loss_mean: 0.859579, loss_mean_cls: 0.044594, grad_norm: 0.343503
[[34m2025-10-04 12:25:44[0m] Step: 5570, Training Logs: loss_final: 0.871877, loss_mean: 0.825817, loss_mean_cls: 0.046060, grad_norm: 0.596738
[[34m2025-10-04 12:25:44[0m] Step: 5571, Training Logs: loss_final: 0.891845, loss_mean: 0.848290, loss_mean_cls: 0.043555, grad_norm: 0.524010
[[34m2025-10-04 12:25:44[0m] Step: 5572, Training Logs: loss_final: 0.900137, loss_mean: 0.855555, loss_mean_cls: 0.044582, grad_norm: 0.440208
[[34m2025-10-04 12:25:45[0m] Step: 5573, Training Logs: loss_final: 0.892064, loss_mean: 0.846848, loss_mean_cls: 0.045216, grad_norm: 0.507856
[[34m2025-10-04 12:25:45[0m] Step: 5574, Training Logs: loss_final: 0.896820, loss_mean: 0.852845, loss_mean_cls: 0.043975, grad_norm: 0.245361
[[34m2025-10-04 12:25:45[0m] Step: 5575, Training Logs: loss_final: 0.882273, loss_mean: 0.838161, loss_mean_cls: 0.044112, grad_norm: 0.483417
[[34m2025-10-04 12:25:46[0m] Step: 5576, Training Logs: loss_final: 0.879573, loss_mean: 0.835543, loss_mean_cls: 0.044030, grad_norm: 0.259140
[[34m2025-10-04 12:25:46[0m] Step: 5577, Training Logs: loss_final: 0.884503, loss_mean: 0.838945, loss_mean_cls: 0.045558, grad_norm: 0.446520
[[34m2025-10-04 12:25:46[0m] Step: 5578, Training Logs: loss_final: 0.919704, loss_mean: 0.875744, loss_mean_cls: 0.043960, grad_norm: 0.327563
[[34m2025-10-04 12:25:47[0m] Step: 5579, Training Logs: loss_final: 0.881050, loss_mean: 0.836735, loss_mean_cls: 0.044315, grad_norm: 0.429953
[[34m2025-10-04 12:25:47[0m] Step: 5580, Training Logs: loss_final: 0.856977, loss_mean: 0.812842, loss_mean_cls: 0.044134, grad_norm: 0.376712
[[34m2025-10-04 12:25:47[0m] Step: 5581, Training Logs: loss_final: 0.909510, loss_mean: 0.865455, loss_mean_cls: 0.044055, grad_norm: 0.280846
[[34m2025-10-04 12:25:47[0m] Step: 5582, Training Logs: loss_final: 0.888708, loss_mean: 0.844217, loss_mean_cls: 0.044491, grad_norm: 0.359247
[[34m2025-10-04 12:25:48[0m] Step: 5583, Training Logs: loss_final: 0.903593, loss_mean: 0.858454, loss_mean_cls: 0.045139, grad_norm: 0.296355
[[34m2025-10-04 12:25:48[0m] Step: 5584, Training Logs: loss_final: 0.892063, loss_mean: 0.848551, loss_mean_cls: 0.043511, grad_norm: 0.273353
[[34m2025-10-04 12:25:48[0m] Step: 5585, Training Logs: loss_final: 0.896808, loss_mean: 0.852732, loss_mean_cls: 0.044077, grad_norm: 0.449150
[[34m2025-10-04 12:25:49[0m] Step: 5586, Training Logs: loss_final: 0.890479, loss_mean: 0.846064, loss_mean_cls: 0.044415, grad_norm: 0.360772
[[34m2025-10-04 12:25:49[0m] Step: 5587, Training Logs: loss_final: 0.881942, loss_mean: 0.836506, loss_mean_cls: 0.045436, grad_norm: 0.371362
[[34m2025-10-04 12:25:49[0m] Step: 5588, Training Logs: loss_final: 0.906804, loss_mean: 0.862232, loss_mean_cls: 0.044573, grad_norm: 0.242468
[[34m2025-10-04 12:25:49[0m] Step: 5589, Training Logs: loss_final: 0.876558, loss_mean: 0.832264, loss_mean_cls: 0.044295, grad_norm: 0.395062
[[34m2025-10-04 12:25:50[0m] Step: 5590, Training Logs: loss_final: 0.894588, loss_mean: 0.849276, loss_mean_cls: 0.045312, grad_norm: 0.400750
[[34m2025-10-04 12:25:50[0m] Step: 5591, Training Logs: loss_final: 0.880194, loss_mean: 0.835165, loss_mean_cls: 0.045029, grad_norm: 0.299291
[[34m2025-10-04 12:25:50[0m] Step: 5592, Training Logs: loss_final: 0.886155, loss_mean: 0.840762, loss_mean_cls: 0.045393, grad_norm: 0.290290
[[34m2025-10-04 12:25:51[0m] Step: 5593, Training Logs: loss_final: 0.898656, loss_mean: 0.853427, loss_mean_cls: 0.045229, grad_norm: 0.412235
[[34m2025-10-04 12:25:51[0m] Step: 5594, Training Logs: loss_final: 0.883337, loss_mean: 0.838806, loss_mean_cls: 0.044531, grad_norm: 0.267564
[[34m2025-10-04 12:25:51[0m] Step: 5595, Training Logs: loss_final: 0.867549, loss_mean: 0.822627, loss_mean_cls: 0.044922, grad_norm: 0.344777
[[34m2025-10-04 12:25:52[0m] Step: 5596, Training Logs: loss_final: 0.884020, loss_mean: 0.839097, loss_mean_cls: 0.044923, grad_norm: 0.313294
[[34m2025-10-04 12:25:52[0m] Step: 5597, Training Logs: loss_final: 0.843755, loss_mean: 0.798419, loss_mean_cls: 0.045337, grad_norm: 0.278856
[[34m2025-10-04 12:25:52[0m] Step: 5598, Training Logs: loss_final: 0.880775, loss_mean: 0.834623, loss_mean_cls: 0.046152, grad_norm: 0.348359
[[34m2025-10-04 12:25:52[0m] Step: 5599, Training Logs: loss_final: 0.890162, loss_mean: 0.846068, loss_mean_cls: 0.044095, grad_norm: 0.297089
[[34m2025-10-04 12:25:53[0m] Step: 5600, Training Logs: loss_final: 0.886565, loss_mean: 0.841881, loss_mean_cls: 0.044684, grad_norm: 0.498798
[[34m2025-10-04 12:25:53[0m] Step: 5601, Training Logs: loss_final: 0.867392, loss_mean: 0.822488, loss_mean_cls: 0.044903, grad_norm: 0.304419
[[34m2025-10-04 12:25:53[0m] Step: 5602, Training Logs: loss_final: 0.886925, loss_mean: 0.841764, loss_mean_cls: 0.045161, grad_norm: 0.364428
[[34m2025-10-04 12:25:54[0m] Step: 5603, Training Logs: loss_final: 0.878208, loss_mean: 0.832362, loss_mean_cls: 0.045845, grad_norm: 0.245341
[[34m2025-10-04 12:25:54[0m] Step: 5604, Training Logs: loss_final: 0.877546, loss_mean: 0.833381, loss_mean_cls: 0.044165, grad_norm: 0.472688
[[34m2025-10-04 12:25:54[0m] Step: 5605, Training Logs: loss_final: 0.901299, loss_mean: 0.856810, loss_mean_cls: 0.044488, grad_norm: 0.314315
[[34m2025-10-04 12:25:54[0m] Step: 5606, Training Logs: loss_final: 0.867269, loss_mean: 0.821989, loss_mean_cls: 0.045280, grad_norm: 0.308303
[[34m2025-10-04 12:25:55[0m] Step: 5607, Training Logs: loss_final: 0.865933, loss_mean: 0.820373, loss_mean_cls: 0.045560, grad_norm: 0.313711
[[34m2025-10-04 12:25:55[0m] Step: 5608, Training Logs: loss_final: 0.863319, loss_mean: 0.817824, loss_mean_cls: 0.045495, grad_norm: 0.360775
[[34m2025-10-04 12:25:55[0m] Step: 5609, Training Logs: loss_final: 0.882342, loss_mean: 0.837156, loss_mean_cls: 0.045186, grad_norm: 0.336788
[[34m2025-10-04 12:25:56[0m] Step: 5610, Training Logs: loss_final: 0.893711, loss_mean: 0.849831, loss_mean_cls: 0.043880, grad_norm: 0.391638
[[34m2025-10-04 12:25:56[0m] Step: 5611, Training Logs: loss_final: 0.896221, loss_mean: 0.851675, loss_mean_cls: 0.044546, grad_norm: 0.420374
[[34m2025-10-04 12:25:56[0m] Step: 5612, Training Logs: loss_final: 0.873640, loss_mean: 0.828669, loss_mean_cls: 0.044971, grad_norm: 0.324414
[[34m2025-10-04 12:25:56[0m] Step: 5613, Training Logs: loss_final: 0.899536, loss_mean: 0.855940, loss_mean_cls: 0.043596, grad_norm: 0.416673
[[34m2025-10-04 12:25:57[0m] Step: 5614, Training Logs: loss_final: 0.896146, loss_mean: 0.852524, loss_mean_cls: 0.043621, grad_norm: 0.337283
[[34m2025-10-04 12:25:57[0m] Step: 5615, Training Logs: loss_final: 0.885182, loss_mean: 0.841052, loss_mean_cls: 0.044131, grad_norm: 0.437181
[[34m2025-10-04 12:25:57[0m] Step: 5616, Training Logs: loss_final: 0.912757, loss_mean: 0.867698, loss_mean_cls: 0.045059, grad_norm: 0.339970
[[34m2025-10-04 12:25:58[0m] Step: 5617, Training Logs: loss_final: 0.896481, loss_mean: 0.852626, loss_mean_cls: 0.043855, grad_norm: 0.396249
[[34m2025-10-04 12:25:58[0m] Step: 5618, Training Logs: loss_final: 0.899027, loss_mean: 0.852629, loss_mean_cls: 0.046398, grad_norm: 0.398704
[[34m2025-10-04 12:25:58[0m] Step: 5619, Training Logs: loss_final: 0.903730, loss_mean: 0.860284, loss_mean_cls: 0.043445, grad_norm: 0.379942
[[34m2025-10-04 12:25:58[0m] Step: 5620, Training Logs: loss_final: 0.886780, loss_mean: 0.841650, loss_mean_cls: 0.045131, grad_norm: 0.302778
[[34m2025-10-04 12:25:59[0m] Step: 5621, Training Logs: loss_final: 0.898759, loss_mean: 0.853607, loss_mean_cls: 0.045152, grad_norm: 0.546212
[[34m2025-10-04 12:25:59[0m] Step: 5622, Training Logs: loss_final: 0.907882, loss_mean: 0.864601, loss_mean_cls: 0.043281, grad_norm: 0.345898
[[34m2025-10-04 12:25:59[0m] Step: 5623, Training Logs: loss_final: 0.901074, loss_mean: 0.856700, loss_mean_cls: 0.044374, grad_norm: 0.412304
[[34m2025-10-04 12:26:00[0m] Step: 5624, Training Logs: loss_final: 0.868522, loss_mean: 0.823600, loss_mean_cls: 0.044922, grad_norm: 0.366223
[[34m2025-10-04 12:26:00[0m] Step: 5625, Training Logs: loss_final: 0.860845, loss_mean: 0.815931, loss_mean_cls: 0.044915, grad_norm: 0.303560
[[34m2025-10-04 12:26:00[0m] Step: 5626, Training Logs: loss_final: 0.901137, loss_mean: 0.857653, loss_mean_cls: 0.043485, grad_norm: 0.506046
[[34m2025-10-04 12:26:01[0m] Step: 5627, Training Logs: loss_final: 0.884324, loss_mean: 0.838721, loss_mean_cls: 0.045602, grad_norm: 0.353421
[[34m2025-10-04 12:26:01[0m] Step: 5628, Training Logs: loss_final: 0.897346, loss_mean: 0.852485, loss_mean_cls: 0.044861, grad_norm: 0.317751
[[34m2025-10-04 12:26:01[0m] Step: 5629, Training Logs: loss_final: 0.878112, loss_mean: 0.833982, loss_mean_cls: 0.044130, grad_norm: 0.346571
[[34m2025-10-04 12:26:01[0m] Step: 5630, Training Logs: loss_final: 0.870324, loss_mean: 0.827025, loss_mean_cls: 0.043299, grad_norm: 0.288334
[[34m2025-10-04 12:26:02[0m] Step: 5631, Training Logs: loss_final: 0.881222, loss_mean: 0.836825, loss_mean_cls: 0.044396, grad_norm: 0.321750
[[34m2025-10-04 12:26:02[0m] Step: 5632, Training Logs: loss_final: 0.892015, loss_mean: 0.847098, loss_mean_cls: 0.044916, grad_norm: 0.321439
[[34m2025-10-04 12:26:02[0m] Step: 5633, Training Logs: loss_final: 0.896994, loss_mean: 0.851556, loss_mean_cls: 0.045438, grad_norm: 0.252731
[[34m2025-10-04 12:26:03[0m] Step: 5634, Training Logs: loss_final: 0.878407, loss_mean: 0.833651, loss_mean_cls: 0.044755, grad_norm: 0.251044
[[34m2025-10-04 12:26:03[0m] Step: 5635, Training Logs: loss_final: 0.906611, loss_mean: 0.863318, loss_mean_cls: 0.043293, grad_norm: 0.378293
[[34m2025-10-04 12:26:03[0m] Step: 5636, Training Logs: loss_final: 0.891977, loss_mean: 0.847634, loss_mean_cls: 0.044343, grad_norm: 0.277429
[[34m2025-10-04 12:26:03[0m] Step: 5637, Training Logs: loss_final: 0.881369, loss_mean: 0.836357, loss_mean_cls: 0.045012, grad_norm: 0.228612
[[34m2025-10-04 12:26:04[0m] Step: 5638, Training Logs: loss_final: 0.888446, loss_mean: 0.843608, loss_mean_cls: 0.044839, grad_norm: 0.369592
[[34m2025-10-04 12:26:04[0m] Step: 5639, Training Logs: loss_final: 0.878990, loss_mean: 0.834863, loss_mean_cls: 0.044127, grad_norm: 0.434766
[[34m2025-10-04 12:26:04[0m] Step: 5640, Training Logs: loss_final: 0.903532, loss_mean: 0.858693, loss_mean_cls: 0.044839, grad_norm: 0.411921
[[34m2025-10-04 12:26:05[0m] Step: 5641, Training Logs: loss_final: 0.883031, loss_mean: 0.839073, loss_mean_cls: 0.043958, grad_norm: 0.273832
[[34m2025-10-04 12:26:05[0m] Step: 5642, Training Logs: loss_final: 0.886200, loss_mean: 0.841651, loss_mean_cls: 0.044549, grad_norm: 0.310566
[[34m2025-10-04 12:26:05[0m] Step: 5643, Training Logs: loss_final: 0.871387, loss_mean: 0.825970, loss_mean_cls: 0.045417, grad_norm: 0.405095
[[34m2025-10-04 12:26:06[0m] Step: 5644, Training Logs: loss_final: 0.882303, loss_mean: 0.837721, loss_mean_cls: 0.044581, grad_norm: 0.307805
[[34m2025-10-04 12:26:06[0m] Step: 5645, Training Logs: loss_final: 0.901196, loss_mean: 0.858518, loss_mean_cls: 0.042679, grad_norm: 0.419669
[[34m2025-10-04 12:26:06[0m] Step: 5646, Training Logs: loss_final: 0.895057, loss_mean: 0.850332, loss_mean_cls: 0.044725, grad_norm: 0.324724
[[34m2025-10-04 12:26:06[0m] Step: 5647, Training Logs: loss_final: 0.890714, loss_mean: 0.844382, loss_mean_cls: 0.046332, grad_norm: 0.350945
[[34m2025-10-04 12:26:07[0m] Step: 5648, Training Logs: loss_final: 0.877221, loss_mean: 0.831520, loss_mean_cls: 0.045701, grad_norm: 0.373669
[[34m2025-10-04 12:26:07[0m] Step: 5649, Training Logs: loss_final: 0.893261, loss_mean: 0.847748, loss_mean_cls: 0.045514, grad_norm: 0.355233
[[34m2025-10-04 12:26:07[0m] Step: 5650, Training Logs: loss_final: 0.887345, loss_mean: 0.843389, loss_mean_cls: 0.043957, grad_norm: 0.466202
[[34m2025-10-04 12:26:08[0m] Step: 5651, Training Logs: loss_final: 0.891143, loss_mean: 0.847069, loss_mean_cls: 0.044074, grad_norm: 0.421774
[[34m2025-10-04 12:26:08[0m] Step: 5652, Training Logs: loss_final: 0.895530, loss_mean: 0.850424, loss_mean_cls: 0.045106, grad_norm: 0.301965
[[34m2025-10-04 12:26:08[0m] Step: 5653, Training Logs: loss_final: 0.900769, loss_mean: 0.856362, loss_mean_cls: 0.044406, grad_norm: 0.401880
[[34m2025-10-04 12:26:08[0m] Step: 5654, Training Logs: loss_final: 0.899764, loss_mean: 0.855905, loss_mean_cls: 0.043858, grad_norm: 0.434479
[[34m2025-10-04 12:26:09[0m] Step: 5655, Training Logs: loss_final: 0.896967, loss_mean: 0.853001, loss_mean_cls: 0.043965, grad_norm: 0.258270
[[34m2025-10-04 12:26:09[0m] Step: 5656, Training Logs: loss_final: 0.889686, loss_mean: 0.845116, loss_mean_cls: 0.044569, grad_norm: 0.281739
[[34m2025-10-04 12:26:09[0m] Step: 5657, Training Logs: loss_final: 0.863804, loss_mean: 0.818390, loss_mean_cls: 0.045414, grad_norm: 0.363430
[[34m2025-10-04 12:26:10[0m] Step: 5658, Training Logs: loss_final: 0.906354, loss_mean: 0.863617, loss_mean_cls: 0.042737, grad_norm: 0.397549
[[34m2025-10-04 12:26:10[0m] Step: 5659, Training Logs: loss_final: 0.880075, loss_mean: 0.836599, loss_mean_cls: 0.043476, grad_norm: 0.318456
[[34m2025-10-04 12:26:10[0m] Step: 5660, Training Logs: loss_final: 0.877460, loss_mean: 0.832874, loss_mean_cls: 0.044586, grad_norm: 0.328225
[[34m2025-10-04 12:26:11[0m] Step: 5661, Training Logs: loss_final: 0.909972, loss_mean: 0.865877, loss_mean_cls: 0.044095, grad_norm: 0.373638
[[34m2025-10-04 12:26:11[0m] Step: 5662, Training Logs: loss_final: 0.895773, loss_mean: 0.851852, loss_mean_cls: 0.043921, grad_norm: 0.261593
[[34m2025-10-04 12:26:11[0m] Step: 5663, Training Logs: loss_final: 0.896282, loss_mean: 0.851835, loss_mean_cls: 0.044448, grad_norm: 0.438721
[[34m2025-10-04 12:26:11[0m] Step: 5664, Training Logs: loss_final: 0.883651, loss_mean: 0.839715, loss_mean_cls: 0.043936, grad_norm: 0.300669
[[34m2025-10-04 12:26:12[0m] Step: 5665, Training Logs: loss_final: 0.877703, loss_mean: 0.833547, loss_mean_cls: 0.044156, grad_norm: 0.373540
[[34m2025-10-04 12:26:12[0m] Step: 5666, Training Logs: loss_final: 0.882073, loss_mean: 0.838180, loss_mean_cls: 0.043893, grad_norm: 0.310726
[[34m2025-10-04 12:26:12[0m] Step: 5667, Training Logs: loss_final: 0.865297, loss_mean: 0.820916, loss_mean_cls: 0.044381, grad_norm: 0.344290
[[34m2025-10-04 12:26:13[0m] Step: 5668, Training Logs: loss_final: 0.888741, loss_mean: 0.843768, loss_mean_cls: 0.044972, grad_norm: 0.417700
[[34m2025-10-04 12:26:13[0m] Step: 5669, Training Logs: loss_final: 0.895144, loss_mean: 0.851028, loss_mean_cls: 0.044116, grad_norm: 0.459939
[[34m2025-10-04 12:26:13[0m] Step: 5670, Training Logs: loss_final: 0.869186, loss_mean: 0.824528, loss_mean_cls: 0.044658, grad_norm: 0.425432
[[34m2025-10-04 12:26:13[0m] Step: 5671, Training Logs: loss_final: 0.896779, loss_mean: 0.851951, loss_mean_cls: 0.044828, grad_norm: 0.402627
[[34m2025-10-04 12:26:14[0m] Step: 5672, Training Logs: loss_final: 0.863627, loss_mean: 0.818587, loss_mean_cls: 0.045040, grad_norm: 0.396091
[[34m2025-10-04 12:26:14[0m] Step: 5673, Training Logs: loss_final: 0.885145, loss_mean: 0.840851, loss_mean_cls: 0.044293, grad_norm: 0.330568
[[34m2025-10-04 12:26:14[0m] Step: 5674, Training Logs: loss_final: 0.885690, loss_mean: 0.842620, loss_mean_cls: 0.043069, grad_norm: 0.452125
[[34m2025-10-04 12:26:15[0m] Step: 5675, Training Logs: loss_final: 0.888699, loss_mean: 0.845008, loss_mean_cls: 0.043691, grad_norm: 0.353135
[[34m2025-10-04 12:26:15[0m] Step: 5676, Training Logs: loss_final: 0.892543, loss_mean: 0.847779, loss_mean_cls: 0.044764, grad_norm: 0.410309
[[34m2025-10-04 12:26:15[0m] Step: 5677, Training Logs: loss_final: 0.880281, loss_mean: 0.835261, loss_mean_cls: 0.045020, grad_norm: 0.255933
[[34m2025-10-04 12:26:15[0m] Step: 5678, Training Logs: loss_final: 0.869922, loss_mean: 0.825751, loss_mean_cls: 0.044172, grad_norm: 0.333750
[[34m2025-10-04 12:26:16[0m] Step: 5679, Training Logs: loss_final: 0.889535, loss_mean: 0.845025, loss_mean_cls: 0.044510, grad_norm: 0.405468
[[34m2025-10-04 12:26:16[0m] Step: 5680, Training Logs: loss_final: 0.884443, loss_mean: 0.839296, loss_mean_cls: 0.045147, grad_norm: 0.265319
[[34m2025-10-04 12:26:16[0m] Step: 5681, Training Logs: loss_final: 0.867885, loss_mean: 0.822104, loss_mean_cls: 0.045781, grad_norm: 0.282257
[[34m2025-10-04 12:26:17[0m] Step: 5682, Training Logs: loss_final: 0.900387, loss_mean: 0.856335, loss_mean_cls: 0.044052, grad_norm: 0.372092
[[34m2025-10-04 12:26:17[0m] Step: 5683, Training Logs: loss_final: 0.891198, loss_mean: 0.846554, loss_mean_cls: 0.044644, grad_norm: 0.293296
[[34m2025-10-04 12:26:17[0m] Step: 5684, Training Logs: loss_final: 0.889597, loss_mean: 0.844704, loss_mean_cls: 0.044893, grad_norm: 0.350954
[[34m2025-10-04 12:26:18[0m] Step: 5685, Training Logs: loss_final: 0.887848, loss_mean: 0.843112, loss_mean_cls: 0.044736, grad_norm: 0.369117
[[34m2025-10-04 12:26:18[0m] Step: 5686, Training Logs: loss_final: 0.908036, loss_mean: 0.863879, loss_mean_cls: 0.044157, grad_norm: 0.230975
[[34m2025-10-04 12:26:18[0m] Step: 5687, Training Logs: loss_final: 0.888539, loss_mean: 0.843919, loss_mean_cls: 0.044620, grad_norm: 0.285955
[[34m2025-10-04 12:26:18[0m] Step: 5688, Training Logs: loss_final: 0.880200, loss_mean: 0.835135, loss_mean_cls: 0.045065, grad_norm: 0.268169
[[34m2025-10-04 12:26:19[0m] Step: 5689, Training Logs: loss_final: 0.854400, loss_mean: 0.808446, loss_mean_cls: 0.045955, grad_norm: 0.397183
[[34m2025-10-04 12:26:19[0m] Step: 5690, Training Logs: loss_final: 0.860285, loss_mean: 0.815870, loss_mean_cls: 0.044414, grad_norm: 0.249913
[[34m2025-10-04 12:26:19[0m] Step: 5691, Training Logs: loss_final: 0.893081, loss_mean: 0.848803, loss_mean_cls: 0.044279, grad_norm: 0.364720
[[34m2025-10-04 12:26:20[0m] Step: 5692, Training Logs: loss_final: 0.901379, loss_mean: 0.857179, loss_mean_cls: 0.044200, grad_norm: 0.333177
[[34m2025-10-04 12:26:20[0m] Step: 5693, Training Logs: loss_final: 0.886412, loss_mean: 0.841816, loss_mean_cls: 0.044596, grad_norm: 0.310829
[[34m2025-10-04 12:26:20[0m] Step: 5694, Training Logs: loss_final: 0.894237, loss_mean: 0.849531, loss_mean_cls: 0.044706, grad_norm: 0.417396
[[34m2025-10-04 12:26:20[0m] Step: 5695, Training Logs: loss_final: 0.860311, loss_mean: 0.814630, loss_mean_cls: 0.045680, grad_norm: 0.599581
[[34m2025-10-04 12:26:21[0m] Step: 5696, Training Logs: loss_final: 0.880047, loss_mean: 0.835690, loss_mean_cls: 0.044357, grad_norm: 0.362311
[[34m2025-10-04 12:26:21[0m] Step: 5697, Training Logs: loss_final: 0.892873, loss_mean: 0.849172, loss_mean_cls: 0.043702, grad_norm: 0.485873
[[34m2025-10-04 12:26:21[0m] Step: 5698, Training Logs: loss_final: 0.861866, loss_mean: 0.815798, loss_mean_cls: 0.046068, grad_norm: 0.656913
[[34m2025-10-04 12:26:22[0m] Step: 5699, Training Logs: loss_final: 0.901090, loss_mean: 0.857340, loss_mean_cls: 0.043750, grad_norm: 0.369816
[[34m2025-10-04 12:26:22[0m] Step: 5700, Training Logs: loss_final: 0.888287, loss_mean: 0.842663, loss_mean_cls: 0.045624, grad_norm: 0.516002
[[34m2025-10-04 12:26:22[0m] Step: 5701, Training Logs: loss_final: 0.892339, loss_mean: 0.847822, loss_mean_cls: 0.044518, grad_norm: 0.391771
[[34m2025-10-04 12:26:22[0m] Step: 5702, Training Logs: loss_final: 0.866941, loss_mean: 0.822555, loss_mean_cls: 0.044386, grad_norm: 0.462356
[[34m2025-10-04 12:26:23[0m] Step: 5703, Training Logs: loss_final: 0.903866, loss_mean: 0.859868, loss_mean_cls: 0.043998, grad_norm: 0.482553
[[34m2025-10-04 12:26:23[0m] Step: 5704, Training Logs: loss_final: 0.879878, loss_mean: 0.834482, loss_mean_cls: 0.045396, grad_norm: 0.401110
[[34m2025-10-04 12:26:23[0m] Step: 5705, Training Logs: loss_final: 0.868068, loss_mean: 0.822732, loss_mean_cls: 0.045336, grad_norm: 0.416857
[[34m2025-10-04 12:26:24[0m] Step: 5706, Training Logs: loss_final: 0.883308, loss_mean: 0.839694, loss_mean_cls: 0.043614, grad_norm: 0.304461
[[34m2025-10-04 12:26:24[0m] Step: 5707, Training Logs: loss_final: 0.881577, loss_mean: 0.837747, loss_mean_cls: 0.043830, grad_norm: 0.284784
[[34m2025-10-04 12:26:24[0m] Step: 5708, Training Logs: loss_final: 0.906843, loss_mean: 0.863470, loss_mean_cls: 0.043373, grad_norm: 0.329438
[[34m2025-10-04 12:26:24[0m] Step: 5709, Training Logs: loss_final: 0.889059, loss_mean: 0.843604, loss_mean_cls: 0.045455, grad_norm: 0.279931
[[34m2025-10-04 12:26:25[0m] Step: 5710, Training Logs: loss_final: 0.886452, loss_mean: 0.841862, loss_mean_cls: 0.044590, grad_norm: 0.330796
[[34m2025-10-04 12:26:25[0m] Step: 5711, Training Logs: loss_final: 0.867302, loss_mean: 0.821623, loss_mean_cls: 0.045679, grad_norm: 0.321748
[[34m2025-10-04 12:26:25[0m] Step: 5712, Training Logs: loss_final: 0.882803, loss_mean: 0.838220, loss_mean_cls: 0.044583, grad_norm: 0.386722
[[34m2025-10-04 12:26:26[0m] Step: 5713, Training Logs: loss_final: 0.906047, loss_mean: 0.862629, loss_mean_cls: 0.043418, grad_norm: 0.436576
[[34m2025-10-04 12:26:26[0m] Step: 5714, Training Logs: loss_final: 0.899377, loss_mean: 0.854434, loss_mean_cls: 0.044943, grad_norm: 0.415258
[[34m2025-10-04 12:26:26[0m] Step: 5715, Training Logs: loss_final: 0.880004, loss_mean: 0.834818, loss_mean_cls: 0.045186, grad_norm: 0.396972
[[34m2025-10-04 12:26:26[0m] Step: 5716, Training Logs: loss_final: 0.877444, loss_mean: 0.831819, loss_mean_cls: 0.045625, grad_norm: 0.469713
[[34m2025-10-04 12:26:27[0m] Step: 5717, Training Logs: loss_final: 0.882637, loss_mean: 0.839370, loss_mean_cls: 0.043267, grad_norm: 0.369131
[[34m2025-10-04 12:26:27[0m] Step: 5718, Training Logs: loss_final: 0.880929, loss_mean: 0.837030, loss_mean_cls: 0.043898, grad_norm: 0.339008
[[34m2025-10-04 12:26:27[0m] Step: 5719, Training Logs: loss_final: 0.880289, loss_mean: 0.834480, loss_mean_cls: 0.045809, grad_norm: 0.364614
[[34m2025-10-04 12:26:28[0m] Step: 5720, Training Logs: loss_final: 0.881077, loss_mean: 0.836639, loss_mean_cls: 0.044438, grad_norm: 0.298188
[[34m2025-10-04 12:26:28[0m] Step: 5721, Training Logs: loss_final: 0.893998, loss_mean: 0.848608, loss_mean_cls: 0.045389, grad_norm: 0.391578
[[34m2025-10-04 12:26:28[0m] Step: 5722, Training Logs: loss_final: 0.875994, loss_mean: 0.829792, loss_mean_cls: 0.046203, grad_norm: 0.307250
[[34m2025-10-04 12:26:29[0m] Step: 5723, Training Logs: loss_final: 0.885049, loss_mean: 0.840680, loss_mean_cls: 0.044369, grad_norm: 0.472093
[[34m2025-10-04 12:26:29[0m] Step: 5724, Training Logs: loss_final: 0.872919, loss_mean: 0.828690, loss_mean_cls: 0.044229, grad_norm: 0.379966
[[34m2025-10-04 12:26:29[0m] Step: 5725, Training Logs: loss_final: 0.882822, loss_mean: 0.839141, loss_mean_cls: 0.043681, grad_norm: 0.377652
[[34m2025-10-04 12:26:29[0m] Step: 5726, Training Logs: loss_final: 0.884557, loss_mean: 0.840466, loss_mean_cls: 0.044091, grad_norm: 0.377173
[[34m2025-10-04 12:26:30[0m] Step: 5727, Training Logs: loss_final: 0.880858, loss_mean: 0.835287, loss_mean_cls: 0.045571, grad_norm: 0.281637
[[34m2025-10-04 12:26:30[0m] Step: 5728, Training Logs: loss_final: 0.899121, loss_mean: 0.853491, loss_mean_cls: 0.045630, grad_norm: 0.372567
[[34m2025-10-04 12:26:30[0m] Step: 5729, Training Logs: loss_final: 0.892366, loss_mean: 0.848514, loss_mean_cls: 0.043851, grad_norm: 0.294164
[[34m2025-10-04 12:26:31[0m] Step: 5730, Training Logs: loss_final: 0.896006, loss_mean: 0.851798, loss_mean_cls: 0.044209, grad_norm: 0.331366
[[34m2025-10-04 12:26:31[0m] Step: 5731, Training Logs: loss_final: 0.878492, loss_mean: 0.833899, loss_mean_cls: 0.044593, grad_norm: 0.270545
[[34m2025-10-04 12:26:31[0m] Step: 5732, Training Logs: loss_final: 0.882267, loss_mean: 0.838465, loss_mean_cls: 0.043802, grad_norm: 0.340019
[[34m2025-10-04 12:26:31[0m] Step: 5733, Training Logs: loss_final: 0.855763, loss_mean: 0.811368, loss_mean_cls: 0.044395, grad_norm: 0.242212
[[34m2025-10-04 12:26:32[0m] Step: 5734, Training Logs: loss_final: 0.903740, loss_mean: 0.859165, loss_mean_cls: 0.044575, grad_norm: 0.402268
[[34m2025-10-04 12:26:32[0m] Step: 5735, Training Logs: loss_final: 0.889593, loss_mean: 0.845087, loss_mean_cls: 0.044507, grad_norm: 0.282473
[[34m2025-10-04 12:26:32[0m] Step: 5736, Training Logs: loss_final: 0.878708, loss_mean: 0.834805, loss_mean_cls: 0.043903, grad_norm: 0.356356
[[34m2025-10-04 12:26:33[0m] Step: 5737, Training Logs: loss_final: 0.891836, loss_mean: 0.848167, loss_mean_cls: 0.043669, grad_norm: 0.287328
[[34m2025-10-04 12:26:33[0m] Step: 5738, Training Logs: loss_final: 0.904166, loss_mean: 0.859412, loss_mean_cls: 0.044754, grad_norm: 0.277017
[[34m2025-10-04 12:26:33[0m] Step: 5739, Training Logs: loss_final: 0.890533, loss_mean: 0.845101, loss_mean_cls: 0.045432, grad_norm: 0.400073
[[34m2025-10-04 12:26:34[0m] Step: 5740, Training Logs: loss_final: 0.864060, loss_mean: 0.819358, loss_mean_cls: 0.044701, grad_norm: 0.368539
[[34m2025-10-04 12:26:34[0m] Step: 5741, Training Logs: loss_final: 0.886377, loss_mean: 0.842087, loss_mean_cls: 0.044290, grad_norm: 0.366306
[[34m2025-10-04 12:26:34[0m] Step: 5742, Training Logs: loss_final: 0.896605, loss_mean: 0.851359, loss_mean_cls: 0.045246, grad_norm: 0.331830
[[34m2025-10-04 12:26:34[0m] Step: 5743, Training Logs: loss_final: 0.906323, loss_mean: 0.861717, loss_mean_cls: 0.044606, grad_norm: 0.375311
[[34m2025-10-04 12:26:35[0m] Step: 5744, Training Logs: loss_final: 0.887274, loss_mean: 0.842045, loss_mean_cls: 0.045229, grad_norm: 0.362793
[[34m2025-10-04 12:26:35[0m] Step: 5745, Training Logs: loss_final: 0.884269, loss_mean: 0.838955, loss_mean_cls: 0.045314, grad_norm: 0.331970
[[34m2025-10-04 12:26:35[0m] Step: 5746, Training Logs: loss_final: 0.883481, loss_mean: 0.839067, loss_mean_cls: 0.044413, grad_norm: 0.437631
[[34m2025-10-04 12:26:36[0m] Step: 5747, Training Logs: loss_final: 0.901065, loss_mean: 0.857880, loss_mean_cls: 0.043185, grad_norm: 0.377672
[[34m2025-10-04 12:26:36[0m] Step: 5748, Training Logs: loss_final: 0.891956, loss_mean: 0.848436, loss_mean_cls: 0.043520, grad_norm: 0.322507
[[34m2025-10-04 12:26:36[0m] Step: 5749, Training Logs: loss_final: 0.874001, loss_mean: 0.829255, loss_mean_cls: 0.044747, grad_norm: 0.399177
[[34m2025-10-04 12:26:37[0m] Step: 5750, Training Logs: loss_final: 0.866702, loss_mean: 0.822187, loss_mean_cls: 0.044515, grad_norm: 0.476476
[[34m2025-10-04 12:26:37[0m] Step: 5751, Training Logs: loss_final: 0.880586, loss_mean: 0.836843, loss_mean_cls: 0.043743, grad_norm: 0.266169
[[34m2025-10-04 12:26:37[0m] Step: 5752, Training Logs: loss_final: 0.877888, loss_mean: 0.832827, loss_mean_cls: 0.045060, grad_norm: 0.427365
[[34m2025-10-04 12:26:37[0m] Step: 5753, Training Logs: loss_final: 0.891304, loss_mean: 0.847469, loss_mean_cls: 0.043835, grad_norm: 0.383955
[[34m2025-10-04 12:26:38[0m] Step: 5754, Training Logs: loss_final: 0.891404, loss_mean: 0.845760, loss_mean_cls: 0.045645, grad_norm: 0.295063
[[34m2025-10-04 12:26:38[0m] Step: 5755, Training Logs: loss_final: 0.871722, loss_mean: 0.827016, loss_mean_cls: 0.044705, grad_norm: 0.388642
[[34m2025-10-04 12:26:38[0m] Step: 5756, Training Logs: loss_final: 0.890695, loss_mean: 0.847244, loss_mean_cls: 0.043451, grad_norm: 0.374328
[[34m2025-10-04 12:26:39[0m] Step: 5757, Training Logs: loss_final: 0.869755, loss_mean: 0.824652, loss_mean_cls: 0.045104, grad_norm: 0.451334
[[34m2025-10-04 12:26:39[0m] Step: 5758, Training Logs: loss_final: 0.892380, loss_mean: 0.848047, loss_mean_cls: 0.044333, grad_norm: 0.404175
[[34m2025-10-04 12:26:39[0m] Step: 5759, Training Logs: loss_final: 0.897948, loss_mean: 0.853763, loss_mean_cls: 0.044186, grad_norm: 0.435680
[[34m2025-10-04 12:26:40[0m] Step: 5760, Training Logs: loss_final: 0.890209, loss_mean: 0.845047, loss_mean_cls: 0.045162, grad_norm: 0.491659
[[34m2025-10-04 12:26:40[0m] Step: 5761, Training Logs: loss_final: 0.873838, loss_mean: 0.829086, loss_mean_cls: 0.044751, grad_norm: 0.390647
[[34m2025-10-04 12:26:40[0m] Step: 5762, Training Logs: loss_final: 0.881476, loss_mean: 0.837435, loss_mean_cls: 0.044041, grad_norm: 0.403202
[[34m2025-10-04 12:26:40[0m] Step: 5763, Training Logs: loss_final: 0.884223, loss_mean: 0.839575, loss_mean_cls: 0.044648, grad_norm: 0.440190
[[34m2025-10-04 12:26:41[0m] Step: 5764, Training Logs: loss_final: 0.873536, loss_mean: 0.828754, loss_mean_cls: 0.044782, grad_norm: 0.434448
[[34m2025-10-04 12:26:41[0m] Step: 5765, Training Logs: loss_final: 0.890161, loss_mean: 0.846047, loss_mean_cls: 0.044114, grad_norm: 0.345916
[[34m2025-10-04 12:26:41[0m] Step: 5766, Training Logs: loss_final: 0.875930, loss_mean: 0.831095, loss_mean_cls: 0.044835, grad_norm: 0.405627
[[34m2025-10-04 12:26:42[0m] Step: 5767, Training Logs: loss_final: 0.888706, loss_mean: 0.845694, loss_mean_cls: 0.043012, grad_norm: 0.411799
[[34m2025-10-04 12:26:42[0m] Step: 5768, Training Logs: loss_final: 0.894064, loss_mean: 0.850132, loss_mean_cls: 0.043933, grad_norm: 0.347225
[[34m2025-10-04 12:26:42[0m] Step: 5769, Training Logs: loss_final: 0.896823, loss_mean: 0.851658, loss_mean_cls: 0.045165, grad_norm: 0.410566
[[34m2025-10-04 12:26:42[0m] Step: 5770, Training Logs: loss_final: 0.883815, loss_mean: 0.839800, loss_mean_cls: 0.044015, grad_norm: 0.330842
[[34m2025-10-04 12:26:43[0m] Step: 5771, Training Logs: loss_final: 0.908488, loss_mean: 0.865260, loss_mean_cls: 0.043228, grad_norm: 0.285739
[[34m2025-10-04 12:26:43[0m] Step: 5772, Training Logs: loss_final: 0.889747, loss_mean: 0.845990, loss_mean_cls: 0.043757, grad_norm: 0.309660
[[34m2025-10-04 12:26:43[0m] Step: 5773, Training Logs: loss_final: 0.870412, loss_mean: 0.826180, loss_mean_cls: 0.044232, grad_norm: 0.399201
[[34m2025-10-04 12:26:44[0m] Step: 5774, Training Logs: loss_final: 0.884644, loss_mean: 0.839351, loss_mean_cls: 0.045293, grad_norm: 0.346589
[[34m2025-10-04 12:26:44[0m] Step: 5775, Training Logs: loss_final: 0.880564, loss_mean: 0.833814, loss_mean_cls: 0.046750, grad_norm: 0.360387
[[34m2025-10-04 12:26:44[0m] Step: 5776, Training Logs: loss_final: 0.887540, loss_mean: 0.841914, loss_mean_cls: 0.045625, grad_norm: 0.360681
[[34m2025-10-04 12:26:44[0m] Step: 5777, Training Logs: loss_final: 0.864431, loss_mean: 0.820334, loss_mean_cls: 0.044097, grad_norm: 0.251225
[[34m2025-10-04 12:26:45[0m] Step: 5778, Training Logs: loss_final: 0.897043, loss_mean: 0.852376, loss_mean_cls: 0.044666, grad_norm: 0.458654
[[34m2025-10-04 12:26:45[0m] Step: 5779, Training Logs: loss_final: 0.862903, loss_mean: 0.818048, loss_mean_cls: 0.044856, grad_norm: 0.336554
[[34m2025-10-04 12:26:45[0m] Step: 5780, Training Logs: loss_final: 0.865320, loss_mean: 0.819554, loss_mean_cls: 0.045767, grad_norm: 0.341498
[[34m2025-10-04 12:26:46[0m] Step: 5781, Training Logs: loss_final: 0.873952, loss_mean: 0.829291, loss_mean_cls: 0.044661, grad_norm: 0.264762
[[34m2025-10-04 12:26:46[0m] Step: 5782, Training Logs: loss_final: 0.867272, loss_mean: 0.822500, loss_mean_cls: 0.044772, grad_norm: 0.448306
[[34m2025-10-04 12:26:46[0m] Step: 5783, Training Logs: loss_final: 0.884794, loss_mean: 0.840290, loss_mean_cls: 0.044504, grad_norm: 0.311627
[[34m2025-10-04 12:26:47[0m] Step: 5784, Training Logs: loss_final: 0.902676, loss_mean: 0.858894, loss_mean_cls: 0.043782, grad_norm: 0.279079
[[34m2025-10-04 12:26:47[0m] Step: 5785, Training Logs: loss_final: 0.890953, loss_mean: 0.845535, loss_mean_cls: 0.045418, grad_norm: 0.359650
[[34m2025-10-04 12:26:47[0m] Step: 5786, Training Logs: loss_final: 0.879285, loss_mean: 0.834086, loss_mean_cls: 0.045198, grad_norm: 0.225951
[[34m2025-10-04 12:26:47[0m] Step: 5787, Training Logs: loss_final: 0.886809, loss_mean: 0.842166, loss_mean_cls: 0.044642, grad_norm: 0.282064
[[34m2025-10-04 12:26:48[0m] Step: 5788, Training Logs: loss_final: 0.866144, loss_mean: 0.820527, loss_mean_cls: 0.045617, grad_norm: 0.264350
[[34m2025-10-04 12:26:48[0m] Step: 5789, Training Logs: loss_final: 0.888496, loss_mean: 0.844828, loss_mean_cls: 0.043669, grad_norm: 0.303069
[[34m2025-10-04 12:26:48[0m] Step: 5790, Training Logs: loss_final: 0.867846, loss_mean: 0.823486, loss_mean_cls: 0.044361, grad_norm: 0.299262
[[34m2025-10-04 12:26:49[0m] Step: 5791, Training Logs: loss_final: 0.877100, loss_mean: 0.833569, loss_mean_cls: 0.043530, grad_norm: 0.167684
[[34m2025-10-04 12:26:49[0m] Step: 5792, Training Logs: loss_final: 0.864360, loss_mean: 0.818552, loss_mean_cls: 0.045808, grad_norm: 0.339949
[[34m2025-10-04 12:26:49[0m] Step: 5793, Training Logs: loss_final: 0.873962, loss_mean: 0.829666, loss_mean_cls: 0.044297, grad_norm: 0.240204
[[34m2025-10-04 12:26:49[0m] Step: 5794, Training Logs: loss_final: 0.872589, loss_mean: 0.827065, loss_mean_cls: 0.045525, grad_norm: 0.383301
[[34m2025-10-04 12:26:50[0m] Step: 5795, Training Logs: loss_final: 0.915649, loss_mean: 0.871491, loss_mean_cls: 0.044158, grad_norm: 0.373145
[[34m2025-10-04 12:26:50[0m] Step: 5796, Training Logs: loss_final: 0.892388, loss_mean: 0.847906, loss_mean_cls: 0.044482, grad_norm: 0.301642
[[34m2025-10-04 12:26:50[0m] Step: 5797, Training Logs: loss_final: 0.878757, loss_mean: 0.833940, loss_mean_cls: 0.044816, grad_norm: 0.322270
[[34m2025-10-04 12:26:51[0m] Step: 5798, Training Logs: loss_final: 0.912948, loss_mean: 0.870385, loss_mean_cls: 0.042563, grad_norm: 0.332523
[[34m2025-10-04 12:26:51[0m] Step: 5799, Training Logs: loss_final: 0.907016, loss_mean: 0.863438, loss_mean_cls: 0.043578, grad_norm: 0.350462
[[34m2025-10-04 12:26:51[0m] Step: 5800, Training Logs: loss_final: 0.883523, loss_mean: 0.839214, loss_mean_cls: 0.044308, grad_norm: 0.277855
[[34m2025-10-04 12:26:52[0m] Step: 5801, Training Logs: loss_final: 0.892292, loss_mean: 0.847778, loss_mean_cls: 0.044514, grad_norm: 0.288241
[[34m2025-10-04 12:26:52[0m] Step: 5802, Training Logs: loss_final: 0.905703, loss_mean: 0.861005, loss_mean_cls: 0.044698, grad_norm: 0.238254
[[34m2025-10-04 12:26:52[0m] Step: 5803, Training Logs: loss_final: 0.904159, loss_mean: 0.859831, loss_mean_cls: 0.044327, grad_norm: 0.301701
[[34m2025-10-04 12:26:52[0m] Step: 5804, Training Logs: loss_final: 0.887633, loss_mean: 0.843930, loss_mean_cls: 0.043702, grad_norm: 0.379947
[[34m2025-10-04 12:26:53[0m] Step: 5805, Training Logs: loss_final: 0.894362, loss_mean: 0.851441, loss_mean_cls: 0.042922, grad_norm: 0.369792
[[34m2025-10-04 12:26:53[0m] Step: 5806, Training Logs: loss_final: 0.887205, loss_mean: 0.842926, loss_mean_cls: 0.044280, grad_norm: 0.290581
[[34m2025-10-04 12:26:53[0m] Step: 5807, Training Logs: loss_final: 0.879226, loss_mean: 0.834015, loss_mean_cls: 0.045212, grad_norm: 0.360402
[[34m2025-10-04 12:26:54[0m] Step: 5808, Training Logs: loss_final: 0.904909, loss_mean: 0.861158, loss_mean_cls: 0.043752, grad_norm: 0.453364
[[34m2025-10-04 12:26:54[0m] Step: 5809, Training Logs: loss_final: 0.909864, loss_mean: 0.865952, loss_mean_cls: 0.043912, grad_norm: 0.341072
[[34m2025-10-04 12:26:54[0m] Step: 5810, Training Logs: loss_final: 0.881478, loss_mean: 0.837544, loss_mean_cls: 0.043934, grad_norm: 0.437191
[[34m2025-10-04 12:26:55[0m] Step: 5811, Training Logs: loss_final: 0.882035, loss_mean: 0.838359, loss_mean_cls: 0.043676, grad_norm: 0.357922
[[34m2025-10-04 12:26:55[0m] Step: 5812, Training Logs: loss_final: 0.878577, loss_mean: 0.833920, loss_mean_cls: 0.044656, grad_norm: 0.341329
[[34m2025-10-04 12:26:55[0m] Step: 5813, Training Logs: loss_final: 0.871046, loss_mean: 0.825771, loss_mean_cls: 0.045275, grad_norm: 0.390283
[[34m2025-10-04 12:26:55[0m] Step: 5814, Training Logs: loss_final: 0.899715, loss_mean: 0.855140, loss_mean_cls: 0.044575, grad_norm: 0.246839
[[34m2025-10-04 12:26:56[0m] Step: 5815, Training Logs: loss_final: 0.889047, loss_mean: 0.843854, loss_mean_cls: 0.045193, grad_norm: 0.349052
[[34m2025-10-04 12:26:56[0m] Step: 5816, Training Logs: loss_final: 0.900761, loss_mean: 0.856862, loss_mean_cls: 0.043899, grad_norm: 0.346752
[[34m2025-10-04 12:26:56[0m] Step: 5817, Training Logs: loss_final: 0.885432, loss_mean: 0.840896, loss_mean_cls: 0.044536, grad_norm: 0.276282
[[34m2025-10-04 12:26:57[0m] Step: 5818, Training Logs: loss_final: 0.874892, loss_mean: 0.830304, loss_mean_cls: 0.044588, grad_norm: 0.451504
[[34m2025-10-04 12:26:57[0m] Step: 5819, Training Logs: loss_final: 0.894769, loss_mean: 0.851458, loss_mean_cls: 0.043311, grad_norm: 0.405139
[[34m2025-10-04 12:26:57[0m] Step: 5820, Training Logs: loss_final: 0.882608, loss_mean: 0.838407, loss_mean_cls: 0.044201, grad_norm: 0.281036
[[34m2025-10-04 12:26:57[0m] Step: 5821, Training Logs: loss_final: 0.872631, loss_mean: 0.828172, loss_mean_cls: 0.044459, grad_norm: 0.434142
[[34m2025-10-04 12:26:58[0m] Step: 5822, Training Logs: loss_final: 0.898056, loss_mean: 0.853590, loss_mean_cls: 0.044467, grad_norm: 0.471504
[[34m2025-10-04 12:26:58[0m] Step: 5823, Training Logs: loss_final: 0.897681, loss_mean: 0.853570, loss_mean_cls: 0.044112, grad_norm: 0.299921
[[34m2025-10-04 12:26:58[0m] Step: 5824, Training Logs: loss_final: 0.877698, loss_mean: 0.833690, loss_mean_cls: 0.044008, grad_norm: 0.253855
[[34m2025-10-04 12:26:59[0m] Step: 5825, Training Logs: loss_final: 0.884059, loss_mean: 0.839547, loss_mean_cls: 0.044512, grad_norm: 0.436381
[[34m2025-10-04 12:26:59[0m] Step: 5826, Training Logs: loss_final: 0.878507, loss_mean: 0.833802, loss_mean_cls: 0.044705, grad_norm: 0.428583
[[34m2025-10-04 12:26:59[0m] Step: 5827, Training Logs: loss_final: 0.877834, loss_mean: 0.833220, loss_mean_cls: 0.044615, grad_norm: 0.409031
[[34m2025-10-04 12:26:59[0m] Step: 5828, Training Logs: loss_final: 0.866310, loss_mean: 0.821619, loss_mean_cls: 0.044692, grad_norm: 0.330503
[[34m2025-10-04 12:27:00[0m] Step: 5829, Training Logs: loss_final: 0.901656, loss_mean: 0.857329, loss_mean_cls: 0.044327, grad_norm: 0.555560
[[34m2025-10-04 12:27:00[0m] Step: 5830, Training Logs: loss_final: 0.896782, loss_mean: 0.853058, loss_mean_cls: 0.043724, grad_norm: 0.554859
[[34m2025-10-04 12:27:00[0m] Step: 5831, Training Logs: loss_final: 0.885840, loss_mean: 0.841061, loss_mean_cls: 0.044779, grad_norm: 0.306462
[[34m2025-10-04 12:27:01[0m] Step: 5832, Training Logs: loss_final: 0.880044, loss_mean: 0.836342, loss_mean_cls: 0.043702, grad_norm: 0.322349
[[34m2025-10-04 12:27:01[0m] Step: 5833, Training Logs: loss_final: 0.888531, loss_mean: 0.843784, loss_mean_cls: 0.044747, grad_norm: 0.539867
[[34m2025-10-04 12:27:01[0m] Step: 5834, Training Logs: loss_final: 0.878573, loss_mean: 0.833980, loss_mean_cls: 0.044593, grad_norm: 0.407534
[[34m2025-10-04 12:27:02[0m] Step: 5835, Training Logs: loss_final: 0.886801, loss_mean: 0.841922, loss_mean_cls: 0.044879, grad_norm: 0.303297
[[34m2025-10-04 12:27:02[0m] Step: 5836, Training Logs: loss_final: 0.880552, loss_mean: 0.836695, loss_mean_cls: 0.043857, grad_norm: 0.294727
[[34m2025-10-04 12:27:02[0m] Step: 5837, Training Logs: loss_final: 0.897639, loss_mean: 0.854594, loss_mean_cls: 0.043045, grad_norm: 0.367194
[[34m2025-10-04 12:27:02[0m] Step: 5838, Training Logs: loss_final: 0.877166, loss_mean: 0.833298, loss_mean_cls: 0.043868, grad_norm: 0.284659
[[34m2025-10-04 12:27:03[0m] Step: 5839, Training Logs: loss_final: 0.890802, loss_mean: 0.846225, loss_mean_cls: 0.044577, grad_norm: 0.347635
[[34m2025-10-04 12:27:03[0m] Step: 5840, Training Logs: loss_final: 0.888456, loss_mean: 0.842941, loss_mean_cls: 0.045515, grad_norm: 0.303533
[[34m2025-10-04 12:27:03[0m] Step: 5841, Training Logs: loss_final: 0.872686, loss_mean: 0.828314, loss_mean_cls: 0.044372, grad_norm: 0.379035
[[34m2025-10-04 12:27:04[0m] Step: 5842, Training Logs: loss_final: 0.891770, loss_mean: 0.846904, loss_mean_cls: 0.044866, grad_norm: 0.342269
[[34m2025-10-04 12:27:04[0m] Step: 5843, Training Logs: loss_final: 0.890092, loss_mean: 0.844764, loss_mean_cls: 0.045328, grad_norm: 0.320796
[[34m2025-10-04 12:27:04[0m] Step: 5844, Training Logs: loss_final: 0.897655, loss_mean: 0.854748, loss_mean_cls: 0.042907, grad_norm: 0.339964
[[34m2025-10-04 12:27:04[0m] Step: 5845, Training Logs: loss_final: 0.863688, loss_mean: 0.819020, loss_mean_cls: 0.044668, grad_norm: 0.314826
[[34m2025-10-04 12:27:05[0m] Step: 5846, Training Logs: loss_final: 0.871682, loss_mean: 0.826075, loss_mean_cls: 0.045607, grad_norm: 0.280469
[[34m2025-10-04 12:27:05[0m] Step: 5847, Training Logs: loss_final: 0.888738, loss_mean: 0.844405, loss_mean_cls: 0.044334, grad_norm: 0.236215
[[34m2025-10-04 12:27:05[0m] Step: 5848, Training Logs: loss_final: 0.856366, loss_mean: 0.812633, loss_mean_cls: 0.043733, grad_norm: 0.314368
[[34m2025-10-04 12:27:06[0m] Step: 5849, Training Logs: loss_final: 0.897074, loss_mean: 0.853303, loss_mean_cls: 0.043771, grad_norm: 0.289155
[[34m2025-10-04 12:27:06[0m] Step: 5850, Training Logs: loss_final: 0.891662, loss_mean: 0.848754, loss_mean_cls: 0.042908, grad_norm: 0.281972
[[34m2025-10-04 12:27:06[0m] Step: 5851, Training Logs: loss_final: 0.878554, loss_mean: 0.835076, loss_mean_cls: 0.043478, grad_norm: 0.376395
[[34m2025-10-04 12:27:06[0m] Step: 5852, Training Logs: loss_final: 0.853734, loss_mean: 0.809401, loss_mean_cls: 0.044333, grad_norm: 0.261732
[[34m2025-10-04 12:27:07[0m] Step: 5853, Training Logs: loss_final: 0.893591, loss_mean: 0.850393, loss_mean_cls: 0.043198, grad_norm: 0.297394
[[34m2025-10-04 12:27:07[0m] Step: 5854, Training Logs: loss_final: 0.889819, loss_mean: 0.845697, loss_mean_cls: 0.044122, grad_norm: 0.347553
[[34m2025-10-04 12:27:07[0m] Step: 5855, Training Logs: loss_final: 0.895534, loss_mean: 0.850707, loss_mean_cls: 0.044827, grad_norm: 0.364203
[[34m2025-10-04 12:27:08[0m] Step: 5856, Training Logs: loss_final: 0.884083, loss_mean: 0.839812, loss_mean_cls: 0.044271, grad_norm: 0.343506
[[34m2025-10-04 12:27:08[0m] Step: 5857, Training Logs: loss_final: 0.881786, loss_mean: 0.838324, loss_mean_cls: 0.043462, grad_norm: 0.370039
[[34m2025-10-04 12:27:08[0m] Step: 5858, Training Logs: loss_final: 0.865289, loss_mean: 0.820765, loss_mean_cls: 0.044524, grad_norm: 0.302181
[[34m2025-10-04 12:27:09[0m] Step: 5859, Training Logs: loss_final: 0.883999, loss_mean: 0.837458, loss_mean_cls: 0.046541, grad_norm: 0.349821
[[34m2025-10-04 12:27:09[0m] Step: 5860, Training Logs: loss_final: 0.902963, loss_mean: 0.858745, loss_mean_cls: 0.044218, grad_norm: 0.239833
[[34m2025-10-04 12:27:09[0m] Step: 5861, Training Logs: loss_final: 0.900755, loss_mean: 0.857206, loss_mean_cls: 0.043549, grad_norm: 0.363190
[[34m2025-10-04 12:27:09[0m] Step: 5862, Training Logs: loss_final: 0.884287, loss_mean: 0.838680, loss_mean_cls: 0.045607, grad_norm: 0.325578
[[34m2025-10-04 12:27:10[0m] Step: 5863, Training Logs: loss_final: 0.887640, loss_mean: 0.842980, loss_mean_cls: 0.044660, grad_norm: 0.318133
[[34m2025-10-04 12:27:10[0m] Step: 5864, Training Logs: loss_final: 0.904474, loss_mean: 0.859016, loss_mean_cls: 0.045457, grad_norm: 0.343999
[[34m2025-10-04 12:27:10[0m] Step: 5865, Training Logs: loss_final: 0.856109, loss_mean: 0.811493, loss_mean_cls: 0.044616, grad_norm: 0.264388
[[34m2025-10-04 12:27:11[0m] Step: 5866, Training Logs: loss_final: 0.880973, loss_mean: 0.836343, loss_mean_cls: 0.044629, grad_norm: 0.369811
[[34m2025-10-04 12:27:11[0m] Step: 5867, Training Logs: loss_final: 0.877482, loss_mean: 0.832780, loss_mean_cls: 0.044702, grad_norm: 0.338355
[[34m2025-10-04 12:27:11[0m] Step: 5868, Training Logs: loss_final: 0.884964, loss_mean: 0.841053, loss_mean_cls: 0.043911, grad_norm: 0.273927
[[34m2025-10-04 12:27:11[0m] Step: 5869, Training Logs: loss_final: 0.894730, loss_mean: 0.851550, loss_mean_cls: 0.043180, grad_norm: 0.264591
[[34m2025-10-04 12:27:12[0m] Step: 5870, Training Logs: loss_final: 0.894592, loss_mean: 0.852499, loss_mean_cls: 0.042092, grad_norm: 0.295168
[[34m2025-10-04 12:27:12[0m] Step: 5871, Training Logs: loss_final: 0.873915, loss_mean: 0.829790, loss_mean_cls: 0.044125, grad_norm: 0.302239
[[34m2025-10-04 12:27:12[0m] Step: 5872, Training Logs: loss_final: 0.914757, loss_mean: 0.871431, loss_mean_cls: 0.043326, grad_norm: 0.341176
[[34m2025-10-04 12:27:13[0m] Step: 5873, Training Logs: loss_final: 0.880949, loss_mean: 0.836338, loss_mean_cls: 0.044611, grad_norm: 0.218878
[[34m2025-10-04 12:27:13[0m] Step: 5874, Training Logs: loss_final: 0.884176, loss_mean: 0.839378, loss_mean_cls: 0.044798, grad_norm: 0.337291
[[34m2025-10-04 12:27:13[0m] Step: 5875, Training Logs: loss_final: 0.881512, loss_mean: 0.836830, loss_mean_cls: 0.044682, grad_norm: 0.300686
[[34m2025-10-04 12:27:14[0m] Step: 5876, Training Logs: loss_final: 0.891159, loss_mean: 0.846344, loss_mean_cls: 0.044815, grad_norm: 0.308881
[[34m2025-10-04 12:27:14[0m] Step: 5877, Training Logs: loss_final: 0.879778, loss_mean: 0.834973, loss_mean_cls: 0.044805, grad_norm: 0.220268
[[34m2025-10-04 12:27:14[0m] Step: 5878, Training Logs: loss_final: 0.892491, loss_mean: 0.847336, loss_mean_cls: 0.045155, grad_norm: 0.292333
[[34m2025-10-04 12:27:14[0m] Step: 5879, Training Logs: loss_final: 0.878544, loss_mean: 0.834203, loss_mean_cls: 0.044342, grad_norm: 0.304741
[[34m2025-10-04 12:27:15[0m] Step: 5880, Training Logs: loss_final: 0.861737, loss_mean: 0.817466, loss_mean_cls: 0.044271, grad_norm: 0.418056
[[34m2025-10-04 12:27:15[0m] Step: 5881, Training Logs: loss_final: 0.877809, loss_mean: 0.832379, loss_mean_cls: 0.045430, grad_norm: 0.328291
[[34m2025-10-04 12:27:15[0m] Step: 5882, Training Logs: loss_final: 0.875208, loss_mean: 0.830689, loss_mean_cls: 0.044519, grad_norm: 0.339665
[[34m2025-10-04 12:27:16[0m] Step: 5883, Training Logs: loss_final: 0.871765, loss_mean: 0.827090, loss_mean_cls: 0.044675, grad_norm: 0.529927
[[34m2025-10-04 12:27:16[0m] Step: 5884, Training Logs: loss_final: 0.897611, loss_mean: 0.853804, loss_mean_cls: 0.043807, grad_norm: 0.309828
[[34m2025-10-04 12:27:16[0m] Step: 5885, Training Logs: loss_final: 0.895496, loss_mean: 0.850072, loss_mean_cls: 0.045423, grad_norm: 0.348040
[[34m2025-10-04 12:27:17[0m] Step: 5886, Training Logs: loss_final: 0.883058, loss_mean: 0.837908, loss_mean_cls: 0.045150, grad_norm: 0.347650
[[34m2025-10-04 12:27:17[0m] Step: 5887, Training Logs: loss_final: 0.911554, loss_mean: 0.867931, loss_mean_cls: 0.043624, grad_norm: 0.324170
[[34m2025-10-04 12:27:17[0m] Step: 5888, Training Logs: loss_final: 0.890492, loss_mean: 0.846211, loss_mean_cls: 0.044281, grad_norm: 0.354766
[[34m2025-10-04 12:27:17[0m] Step: 5889, Training Logs: loss_final: 0.877466, loss_mean: 0.833417, loss_mean_cls: 0.044049, grad_norm: 0.257613
[[34m2025-10-04 12:27:18[0m] Step: 5890, Training Logs: loss_final: 0.876629, loss_mean: 0.832041, loss_mean_cls: 0.044588, grad_norm: 0.388552
[[34m2025-10-04 12:27:18[0m] Step: 5891, Training Logs: loss_final: 0.870860, loss_mean: 0.825758, loss_mean_cls: 0.045103, grad_norm: 0.224543
[[34m2025-10-04 12:27:18[0m] Step: 5892, Training Logs: loss_final: 0.874962, loss_mean: 0.830470, loss_mean_cls: 0.044491, grad_norm: 0.312734
[[34m2025-10-04 12:27:19[0m] Step: 5893, Training Logs: loss_final: 0.880728, loss_mean: 0.837070, loss_mean_cls: 0.043658, grad_norm: 0.330081
[[34m2025-10-04 12:27:19[0m] Step: 5894, Training Logs: loss_final: 0.877045, loss_mean: 0.832268, loss_mean_cls: 0.044777, grad_norm: 0.228009
[[34m2025-10-04 12:27:19[0m] Step: 5895, Training Logs: loss_final: 0.871125, loss_mean: 0.826175, loss_mean_cls: 0.044951, grad_norm: 0.432428
[[34m2025-10-04 12:27:19[0m] Step: 5896, Training Logs: loss_final: 0.869190, loss_mean: 0.824699, loss_mean_cls: 0.044492, grad_norm: 0.422767
[[34m2025-10-04 12:27:20[0m] Step: 5897, Training Logs: loss_final: 0.884767, loss_mean: 0.840873, loss_mean_cls: 0.043894, grad_norm: 0.236970
[[34m2025-10-04 12:27:20[0m] Step: 5898, Training Logs: loss_final: 0.898898, loss_mean: 0.855134, loss_mean_cls: 0.043764, grad_norm: 0.306337
[[34m2025-10-04 12:27:20[0m] Step: 5899, Training Logs: loss_final: 0.906879, loss_mean: 0.863360, loss_mean_cls: 0.043519, grad_norm: 0.372394
[[34m2025-10-04 12:27:21[0m] Step: 5900, Training Logs: loss_final: 0.906663, loss_mean: 0.862705, loss_mean_cls: 0.043958, grad_norm: 0.283628
[[34m2025-10-04 12:27:21[0m] Step: 5901, Training Logs: loss_final: 0.884052, loss_mean: 0.839927, loss_mean_cls: 0.044125, grad_norm: 0.301485
[[34m2025-10-04 12:27:21[0m] Step: 5902, Training Logs: loss_final: 0.890037, loss_mean: 0.845363, loss_mean_cls: 0.044674, grad_norm: 0.348864
[[34m2025-10-04 12:27:21[0m] Step: 5903, Training Logs: loss_final: 0.885618, loss_mean: 0.841511, loss_mean_cls: 0.044107, grad_norm: 0.277684
[[34m2025-10-04 12:27:22[0m] Step: 5904, Training Logs: loss_final: 0.896130, loss_mean: 0.851676, loss_mean_cls: 0.044454, grad_norm: 0.428676
[[34m2025-10-04 12:27:22[0m] Step: 5905, Training Logs: loss_final: 0.881950, loss_mean: 0.837706, loss_mean_cls: 0.044244, grad_norm: 0.275204
[[34m2025-10-04 12:27:22[0m] Step: 5906, Training Logs: loss_final: 0.865705, loss_mean: 0.820830, loss_mean_cls: 0.044874, grad_norm: 0.347863
[[34m2025-10-04 12:27:23[0m] Step: 5907, Training Logs: loss_final: 0.887969, loss_mean: 0.843143, loss_mean_cls: 0.044825, grad_norm: 0.291839
[[34m2025-10-04 12:27:23[0m] Step: 5908, Training Logs: loss_final: 0.894377, loss_mean: 0.849818, loss_mean_cls: 0.044559, grad_norm: 0.236711
[[34m2025-10-04 12:27:23[0m] Step: 5909, Training Logs: loss_final: 0.883435, loss_mean: 0.838445, loss_mean_cls: 0.044990, grad_norm: 0.284584
[[34m2025-10-04 12:27:24[0m] Step: 5910, Training Logs: loss_final: 0.880490, loss_mean: 0.835708, loss_mean_cls: 0.044782, grad_norm: 0.321958
[[34m2025-10-04 12:27:24[0m] Step: 5911, Training Logs: loss_final: 0.862266, loss_mean: 0.818104, loss_mean_cls: 0.044163, grad_norm: 0.225812
[[34m2025-10-04 12:27:24[0m] Step: 5912, Training Logs: loss_final: 0.887855, loss_mean: 0.843094, loss_mean_cls: 0.044761, grad_norm: 0.279081
[[34m2025-10-04 12:27:24[0m] Step: 5913, Training Logs: loss_final: 0.890942, loss_mean: 0.846715, loss_mean_cls: 0.044227, grad_norm: 0.291112
[[34m2025-10-04 12:27:25[0m] Step: 5914, Training Logs: loss_final: 0.916130, loss_mean: 0.871547, loss_mean_cls: 0.044583, grad_norm: 0.209050
[[34m2025-10-04 12:27:25[0m] Step: 5915, Training Logs: loss_final: 0.909700, loss_mean: 0.865388, loss_mean_cls: 0.044312, grad_norm: 0.262439
[[34m2025-10-04 12:27:25[0m] Step: 5916, Training Logs: loss_final: 0.903761, loss_mean: 0.859996, loss_mean_cls: 0.043765, grad_norm: 0.299002
[[34m2025-10-04 12:27:26[0m] Step: 5917, Training Logs: loss_final: 0.878398, loss_mean: 0.834960, loss_mean_cls: 0.043437, grad_norm: 0.324740
[[34m2025-10-04 12:27:26[0m] Step: 5918, Training Logs: loss_final: 0.895186, loss_mean: 0.851574, loss_mean_cls: 0.043612, grad_norm: 0.286863
[[34m2025-10-04 12:27:26[0m] Step: 5919, Training Logs: loss_final: 0.873955, loss_mean: 0.827991, loss_mean_cls: 0.045963, grad_norm: 0.293467
[[34m2025-10-04 12:27:26[0m] Step: 5920, Training Logs: loss_final: 0.881389, loss_mean: 0.836842, loss_mean_cls: 0.044547, grad_norm: 0.374463
[[34m2025-10-04 12:27:27[0m] Step: 5921, Training Logs: loss_final: 0.887526, loss_mean: 0.843421, loss_mean_cls: 0.044106, grad_norm: 0.245827
[[34m2025-10-04 12:27:27[0m] Step: 5922, Training Logs: loss_final: 0.881282, loss_mean: 0.836866, loss_mean_cls: 0.044416, grad_norm: 0.266674
[[34m2025-10-04 12:27:27[0m] Step: 5923, Training Logs: loss_final: 0.877360, loss_mean: 0.832484, loss_mean_cls: 0.044876, grad_norm: 0.267156
[[34m2025-10-04 12:27:28[0m] Step: 5924, Training Logs: loss_final: 0.892058, loss_mean: 0.847922, loss_mean_cls: 0.044136, grad_norm: 0.407074
[[34m2025-10-04 12:27:28[0m] Step: 5925, Training Logs: loss_final: 0.869576, loss_mean: 0.826113, loss_mean_cls: 0.043463, grad_norm: 0.370382
[[34m2025-10-04 12:27:28[0m] Step: 5926, Training Logs: loss_final: 0.893927, loss_mean: 0.849875, loss_mean_cls: 0.044052, grad_norm: 0.372273
[[34m2025-10-04 12:27:29[0m] Step: 5927, Training Logs: loss_final: 0.886287, loss_mean: 0.842150, loss_mean_cls: 0.044137, grad_norm: 0.422744
[[34m2025-10-04 12:27:29[0m] Step: 5928, Training Logs: loss_final: 0.880731, loss_mean: 0.833972, loss_mean_cls: 0.046758, grad_norm: 0.284669
[[34m2025-10-04 12:27:29[0m] Step: 5929, Training Logs: loss_final: 0.885157, loss_mean: 0.841269, loss_mean_cls: 0.043888, grad_norm: 0.484504
[[34m2025-10-04 12:27:29[0m] Step: 5930, Training Logs: loss_final: 0.876577, loss_mean: 0.832137, loss_mean_cls: 0.044440, grad_norm: 0.222024
[[34m2025-10-04 12:27:30[0m] Step: 5931, Training Logs: loss_final: 0.885546, loss_mean: 0.840820, loss_mean_cls: 0.044725, grad_norm: 0.390346
[[34m2025-10-04 12:27:30[0m] Step: 5932, Training Logs: loss_final: 0.860568, loss_mean: 0.815157, loss_mean_cls: 0.045411, grad_norm: 0.394361
[[34m2025-10-04 12:27:30[0m] Step: 5933, Training Logs: loss_final: 0.903773, loss_mean: 0.860442, loss_mean_cls: 0.043330, grad_norm: 0.334752
[[34m2025-10-04 12:27:31[0m] Step: 5934, Training Logs: loss_final: 0.896665, loss_mean: 0.853115, loss_mean_cls: 0.043550, grad_norm: 0.246081
[[34m2025-10-04 12:27:31[0m] Step: 5935, Training Logs: loss_final: 0.879090, loss_mean: 0.836078, loss_mean_cls: 0.043011, grad_norm: 0.292288
[[34m2025-10-04 12:27:31[0m] Step: 5936, Training Logs: loss_final: 0.880025, loss_mean: 0.836244, loss_mean_cls: 0.043781, grad_norm: 0.353738
[[34m2025-10-04 12:27:32[0m] Step: 5937, Training Logs: loss_final: 0.877755, loss_mean: 0.833396, loss_mean_cls: 0.044358, grad_norm: 0.342004
[[34m2025-10-04 12:27:32[0m] Step: 5938, Training Logs: loss_final: 0.910771, loss_mean: 0.866835, loss_mean_cls: 0.043936, grad_norm: 0.379886
[[34m2025-10-04 12:27:32[0m] Step: 5939, Training Logs: loss_final: 0.878395, loss_mean: 0.834774, loss_mean_cls: 0.043621, grad_norm: 0.294573
[[34m2025-10-04 12:27:32[0m] Step: 5940, Training Logs: loss_final: 0.861122, loss_mean: 0.816322, loss_mean_cls: 0.044800, grad_norm: 0.392853
[[34m2025-10-04 12:27:33[0m] Step: 5941, Training Logs: loss_final: 0.861070, loss_mean: 0.816563, loss_mean_cls: 0.044506, grad_norm: 0.334733
[[34m2025-10-04 12:27:33[0m] Step: 5942, Training Logs: loss_final: 0.883458, loss_mean: 0.839051, loss_mean_cls: 0.044407, grad_norm: 0.361928
[[34m2025-10-04 12:27:33[0m] Step: 5943, Training Logs: loss_final: 0.873515, loss_mean: 0.828443, loss_mean_cls: 0.045072, grad_norm: 0.243269
[[34m2025-10-04 12:27:34[0m] Step: 5944, Training Logs: loss_final: 0.889793, loss_mean: 0.845371, loss_mean_cls: 0.044422, grad_norm: 0.471025
[[34m2025-10-04 12:27:34[0m] Step: 5945, Training Logs: loss_final: 0.884646, loss_mean: 0.839254, loss_mean_cls: 0.045392, grad_norm: 0.375810
[[34m2025-10-04 12:27:34[0m] Step: 5946, Training Logs: loss_final: 0.875585, loss_mean: 0.831687, loss_mean_cls: 0.043898, grad_norm: 0.319611
[[34m2025-10-04 12:27:34[0m] Step: 5947, Training Logs: loss_final: 0.887153, loss_mean: 0.842629, loss_mean_cls: 0.044525, grad_norm: 0.398419
[[34m2025-10-04 12:27:35[0m] Step: 5948, Training Logs: loss_final: 0.887465, loss_mean: 0.842297, loss_mean_cls: 0.045168, grad_norm: 0.316887
[[34m2025-10-04 12:27:35[0m] Step: 5949, Training Logs: loss_final: 0.905041, loss_mean: 0.861914, loss_mean_cls: 0.043127, grad_norm: 0.336336
[[34m2025-10-04 12:27:35[0m] Step: 5950, Training Logs: loss_final: 0.881955, loss_mean: 0.838102, loss_mean_cls: 0.043853, grad_norm: 0.245861
[[34m2025-10-04 12:27:36[0m] Step: 5951, Training Logs: loss_final: 0.890934, loss_mean: 0.847162, loss_mean_cls: 0.043772, grad_norm: 0.409823
[[34m2025-10-04 12:27:36[0m] Step: 5952, Training Logs: loss_final: 0.883973, loss_mean: 0.839975, loss_mean_cls: 0.043999, grad_norm: 0.241346
[[34m2025-10-04 12:27:36[0m] Step: 5953, Training Logs: loss_final: 0.886723, loss_mean: 0.842970, loss_mean_cls: 0.043754, grad_norm: 0.307046
[[34m2025-10-04 12:27:37[0m] Step: 5954, Training Logs: loss_final: 0.882521, loss_mean: 0.837940, loss_mean_cls: 0.044582, grad_norm: 0.307999
[[34m2025-10-04 12:27:37[0m] Step: 5955, Training Logs: loss_final: 0.882536, loss_mean: 0.838814, loss_mean_cls: 0.043722, grad_norm: 0.292156
[[34m2025-10-04 12:27:37[0m] Step: 5956, Training Logs: loss_final: 0.884900, loss_mean: 0.841897, loss_mean_cls: 0.043003, grad_norm: 0.289357
[[34m2025-10-04 12:27:37[0m] Step: 5957, Training Logs: loss_final: 0.881112, loss_mean: 0.836888, loss_mean_cls: 0.044224, grad_norm: 0.268236
[[34m2025-10-04 12:27:38[0m] Step: 5958, Training Logs: loss_final: 0.893060, loss_mean: 0.849343, loss_mean_cls: 0.043717, grad_norm: 0.207625
[[34m2025-10-04 12:27:38[0m] Step: 5959, Training Logs: loss_final: 0.902593, loss_mean: 0.858893, loss_mean_cls: 0.043700, grad_norm: 0.235217
[[34m2025-10-04 12:27:38[0m] Step: 5960, Training Logs: loss_final: 0.892458, loss_mean: 0.848599, loss_mean_cls: 0.043859, grad_norm: 0.347583
[[34m2025-10-04 12:27:39[0m] Step: 5961, Training Logs: loss_final: 0.877995, loss_mean: 0.834238, loss_mean_cls: 0.043757, grad_norm: 0.246821
[[34m2025-10-04 12:27:39[0m] Step: 5962, Training Logs: loss_final: 0.879660, loss_mean: 0.834298, loss_mean_cls: 0.045362, grad_norm: 0.389861
[[34m2025-10-04 12:27:39[0m] Step: 5963, Training Logs: loss_final: 0.899198, loss_mean: 0.853907, loss_mean_cls: 0.045291, grad_norm: 0.489327
[[34m2025-10-04 12:27:40[0m] Step: 5964, Training Logs: loss_final: 0.871210, loss_mean: 0.826671, loss_mean_cls: 0.044540, grad_norm: 0.344722
[[34m2025-10-04 12:27:40[0m] Step: 5965, Training Logs: loss_final: 0.880480, loss_mean: 0.835270, loss_mean_cls: 0.045209, grad_norm: 0.509443
[[34m2025-10-04 12:27:40[0m] Step: 5966, Training Logs: loss_final: 0.862255, loss_mean: 0.818227, loss_mean_cls: 0.044028, grad_norm: 0.314908
[[34m2025-10-04 12:27:40[0m] Step: 5967, Training Logs: loss_final: 0.893149, loss_mean: 0.850199, loss_mean_cls: 0.042950, grad_norm: 0.400219
[[34m2025-10-04 12:27:41[0m] Step: 5968, Training Logs: loss_final: 0.891153, loss_mean: 0.845761, loss_mean_cls: 0.045392, grad_norm: 0.347061
[[34m2025-10-04 12:27:41[0m] Step: 5969, Training Logs: loss_final: 0.873848, loss_mean: 0.830387, loss_mean_cls: 0.043461, grad_norm: 0.429567
[[34m2025-10-04 12:27:41[0m] Step: 5970, Training Logs: loss_final: 0.880465, loss_mean: 0.836402, loss_mean_cls: 0.044063, grad_norm: 0.467966
[[34m2025-10-04 12:27:42[0m] Step: 5971, Training Logs: loss_final: 0.896075, loss_mean: 0.852088, loss_mean_cls: 0.043987, grad_norm: 0.371129
[[34m2025-10-04 12:27:42[0m] Step: 5972, Training Logs: loss_final: 0.907154, loss_mean: 0.863115, loss_mean_cls: 0.044039, grad_norm: 0.522462
[[34m2025-10-04 12:27:42[0m] Step: 5973, Training Logs: loss_final: 0.884372, loss_mean: 0.840397, loss_mean_cls: 0.043975, grad_norm: 0.337446
[[34m2025-10-04 12:27:42[0m] Step: 5974, Training Logs: loss_final: 0.862287, loss_mean: 0.817095, loss_mean_cls: 0.045192, grad_norm: 0.470049
[[34m2025-10-04 12:27:43[0m] Step: 5975, Training Logs: loss_final: 0.899258, loss_mean: 0.855604, loss_mean_cls: 0.043654, grad_norm: 0.511406
[[34m2025-10-04 12:27:43[0m] Step: 5976, Training Logs: loss_final: 0.888005, loss_mean: 0.843066, loss_mean_cls: 0.044939, grad_norm: 0.504078
[[34m2025-10-04 12:27:43[0m] Step: 5977, Training Logs: loss_final: 0.887065, loss_mean: 0.843883, loss_mean_cls: 0.043183, grad_norm: 0.366762
[[34m2025-10-04 12:27:44[0m] Step: 5978, Training Logs: loss_final: 0.877688, loss_mean: 0.833604, loss_mean_cls: 0.044084, grad_norm: 0.415049
[[34m2025-10-04 12:27:44[0m] Step: 5979, Training Logs: loss_final: 0.903103, loss_mean: 0.859041, loss_mean_cls: 0.044062, grad_norm: 0.402987
[[34m2025-10-04 12:27:44[0m] Step: 5980, Training Logs: loss_final: 0.875715, loss_mean: 0.832024, loss_mean_cls: 0.043690, grad_norm: 0.350436
[[34m2025-10-04 12:27:44[0m] Step: 5981, Training Logs: loss_final: 0.887715, loss_mean: 0.844327, loss_mean_cls: 0.043388, grad_norm: 0.403946
[[34m2025-10-04 12:27:45[0m] Step: 5982, Training Logs: loss_final: 0.877640, loss_mean: 0.832670, loss_mean_cls: 0.044970, grad_norm: 0.319317
[[34m2025-10-04 12:27:45[0m] Step: 5983, Training Logs: loss_final: 0.894311, loss_mean: 0.847827, loss_mean_cls: 0.046484, grad_norm: 0.311145
[[34m2025-10-04 12:27:45[0m] Step: 5984, Training Logs: loss_final: 0.885139, loss_mean: 0.840412, loss_mean_cls: 0.044727, grad_norm: 0.228463
[[34m2025-10-04 12:27:46[0m] Step: 5985, Training Logs: loss_final: 0.881133, loss_mean: 0.836079, loss_mean_cls: 0.045054, grad_norm: 0.451735
[[34m2025-10-04 12:27:46[0m] Step: 5986, Training Logs: loss_final: 0.890649, loss_mean: 0.846109, loss_mean_cls: 0.044540, grad_norm: 0.454144
[[34m2025-10-04 12:27:46[0m] Step: 5987, Training Logs: loss_final: 0.875719, loss_mean: 0.831293, loss_mean_cls: 0.044426, grad_norm: 0.294860
[[34m2025-10-04 12:27:47[0m] Step: 5988, Training Logs: loss_final: 0.884879, loss_mean: 0.840664, loss_mean_cls: 0.044215, grad_norm: 0.567514
[[34m2025-10-04 12:27:47[0m] Step: 5989, Training Logs: loss_final: 0.866251, loss_mean: 0.822665, loss_mean_cls: 0.043586, grad_norm: 0.385332
[[34m2025-10-04 12:27:47[0m] Step: 5990, Training Logs: loss_final: 0.889591, loss_mean: 0.845676, loss_mean_cls: 0.043914, grad_norm: 0.394437
[[34m2025-10-04 12:27:47[0m] Step: 5991, Training Logs: loss_final: 0.882503, loss_mean: 0.838203, loss_mean_cls: 0.044301, grad_norm: 0.424217
[[34m2025-10-04 12:27:48[0m] Step: 5992, Training Logs: loss_final: 0.899253, loss_mean: 0.854869, loss_mean_cls: 0.044384, grad_norm: 0.288307
[[34m2025-10-04 12:27:48[0m] Step: 5993, Training Logs: loss_final: 0.895554, loss_mean: 0.852026, loss_mean_cls: 0.043528, grad_norm: 0.432399
[[34m2025-10-04 12:27:48[0m] Step: 5994, Training Logs: loss_final: 0.899920, loss_mean: 0.856128, loss_mean_cls: 0.043793, grad_norm: 0.275737
[[34m2025-10-04 12:27:49[0m] Step: 5995, Training Logs: loss_final: 0.891734, loss_mean: 0.846743, loss_mean_cls: 0.044991, grad_norm: 0.362990
[[34m2025-10-04 12:27:49[0m] Step: 5996, Training Logs: loss_final: 0.864885, loss_mean: 0.820330, loss_mean_cls: 0.044556, grad_norm: 0.332943
[[34m2025-10-04 12:27:49[0m] Step: 5997, Training Logs: loss_final: 0.881555, loss_mean: 0.837293, loss_mean_cls: 0.044263, grad_norm: 0.352896
[[34m2025-10-04 12:27:49[0m] Step: 5998, Training Logs: loss_final: 0.893179, loss_mean: 0.848161, loss_mean_cls: 0.045019, grad_norm: 0.453463
[[34m2025-10-04 12:27:50[0m] Step: 5999, Training Logs: loss_final: 0.889205, loss_mean: 0.845846, loss_mean_cls: 0.043358, grad_norm: 0.317871
[[34m2025-10-04 12:27:50[0m] Step: 6000, Training Logs: loss_final: 0.905977, loss_mean: 0.862322, loss_mean_cls: 0.043655, grad_norm: 0.437061
[[34m2025-10-04 12:27:50[0m] Step: 6001, Training Logs: loss_final: 0.895379, loss_mean: 0.851305, loss_mean_cls: 0.044074, grad_norm: 0.212537
[[34m2025-10-04 12:27:51[0m] Step: 6002, Training Logs: loss_final: 0.906023, loss_mean: 0.861935, loss_mean_cls: 0.044088, grad_norm: 0.287943
[[34m2025-10-04 12:27:51[0m] Step: 6003, Training Logs: loss_final: 0.881277, loss_mean: 0.836519, loss_mean_cls: 0.044758, grad_norm: 0.285250
[[34m2025-10-04 12:27:51[0m] Step: 6004, Training Logs: loss_final: 0.896607, loss_mean: 0.853370, loss_mean_cls: 0.043237, grad_norm: 0.502486
[[34m2025-10-04 12:27:51[0m] Step: 6005, Training Logs: loss_final: 0.894745, loss_mean: 0.850356, loss_mean_cls: 0.044389, grad_norm: 0.325159
[[34m2025-10-04 12:27:52[0m] Step: 6006, Training Logs: loss_final: 0.889823, loss_mean: 0.845691, loss_mean_cls: 0.044132, grad_norm: 0.389082
[[34m2025-10-04 12:27:52[0m] Step: 6007, Training Logs: loss_final: 0.882239, loss_mean: 0.837544, loss_mean_cls: 0.044695, grad_norm: 0.353412
[[34m2025-10-04 12:27:52[0m] Step: 6008, Training Logs: loss_final: 0.886909, loss_mean: 0.842121, loss_mean_cls: 0.044788, grad_norm: 0.299403
[[34m2025-10-04 12:27:53[0m] Step: 6009, Training Logs: loss_final: 0.883121, loss_mean: 0.837772, loss_mean_cls: 0.045349, grad_norm: 0.354140
[[34m2025-10-04 12:27:53[0m] Step: 6010, Training Logs: loss_final: 0.889044, loss_mean: 0.845574, loss_mean_cls: 0.043470, grad_norm: 0.349061
[[34m2025-10-04 12:27:53[0m] Step: 6011, Training Logs: loss_final: 0.897283, loss_mean: 0.852890, loss_mean_cls: 0.044393, grad_norm: 0.396932
[[34m2025-10-04 12:27:53[0m] Step: 6012, Training Logs: loss_final: 0.888445, loss_mean: 0.843722, loss_mean_cls: 0.044723, grad_norm: 0.294553
[[34m2025-10-04 12:27:54[0m] Step: 6013, Training Logs: loss_final: 0.885849, loss_mean: 0.841510, loss_mean_cls: 0.044339, grad_norm: 0.369195
[[34m2025-10-04 12:27:54[0m] Step: 6014, Training Logs: loss_final: 0.881481, loss_mean: 0.837734, loss_mean_cls: 0.043747, grad_norm: 0.423706
[[34m2025-10-04 12:27:54[0m] Step: 6015, Training Logs: loss_final: 0.896769, loss_mean: 0.851375, loss_mean_cls: 0.045394, grad_norm: 0.332989
[[34m2025-10-04 12:27:55[0m] Step: 6016, Training Logs: loss_final: 0.882934, loss_mean: 0.838144, loss_mean_cls: 0.044789, grad_norm: 0.413249
[[34m2025-10-04 12:27:55[0m] Step: 6017, Training Logs: loss_final: 0.881196, loss_mean: 0.836042, loss_mean_cls: 0.045155, grad_norm: 0.374530
[[34m2025-10-04 12:27:55[0m] Step: 6018, Training Logs: loss_final: 0.922449, loss_mean: 0.878338, loss_mean_cls: 0.044111, grad_norm: 0.409034
[[34m2025-10-04 12:27:55[0m] Step: 6019, Training Logs: loss_final: 0.875054, loss_mean: 0.831598, loss_mean_cls: 0.043456, grad_norm: inf
[[34m2025-10-04 12:27:56[0m] Step: 6020, Training Logs: loss_final: 0.902982, loss_mean: 0.858547, loss_mean_cls: 0.044435, grad_norm: 0.570793
[[34m2025-10-04 12:27:56[0m] Step: 6021, Training Logs: loss_final: 0.896098, loss_mean: 0.851367, loss_mean_cls: 0.044731, grad_norm: 0.345871
[[34m2025-10-04 12:27:56[0m] Step: 6022, Training Logs: loss_final: 0.896976, loss_mean: 0.851627, loss_mean_cls: 0.045349, grad_norm: 0.441645
[[34m2025-10-04 12:27:57[0m] Step: 6023, Training Logs: loss_final: 0.870091, loss_mean: 0.826879, loss_mean_cls: 0.043212, grad_norm: 0.481160
[[34m2025-10-04 12:27:57[0m] Step: 6024, Training Logs: loss_final: 0.884701, loss_mean: 0.839990, loss_mean_cls: 0.044712, grad_norm: 0.364037
[[34m2025-10-04 12:27:57[0m] Step: 6025, Training Logs: loss_final: 0.877999, loss_mean: 0.834193, loss_mean_cls: 0.043806, grad_norm: 0.480441
[[34m2025-10-04 12:27:58[0m] Step: 6026, Training Logs: loss_final: 0.862828, loss_mean: 0.818002, loss_mean_cls: 0.044826, grad_norm: 0.384352
[[34m2025-10-04 12:27:58[0m] Step: 6027, Training Logs: loss_final: 0.870207, loss_mean: 0.826681, loss_mean_cls: 0.043526, grad_norm: 0.359897
[[34m2025-10-04 12:27:58[0m] Step: 6028, Training Logs: loss_final: 0.894368, loss_mean: 0.849896, loss_mean_cls: 0.044472, grad_norm: 0.413432
[[34m2025-10-04 12:27:58[0m] Step: 6029, Training Logs: loss_final: 0.892339, loss_mean: 0.848855, loss_mean_cls: 0.043484, grad_norm: 0.371624
[[34m2025-10-04 12:27:59[0m] Step: 6030, Training Logs: loss_final: 0.870325, loss_mean: 0.824982, loss_mean_cls: 0.045343, grad_norm: 0.350397
[[34m2025-10-04 12:27:59[0m] Step: 6031, Training Logs: loss_final: 0.872443, loss_mean: 0.827584, loss_mean_cls: 0.044860, grad_norm: 0.338109
[[34m2025-10-04 12:27:59[0m] Step: 6032, Training Logs: loss_final: 0.871543, loss_mean: 0.827228, loss_mean_cls: 0.044314, grad_norm: 0.419418
[[34m2025-10-04 12:28:00[0m] Step: 6033, Training Logs: loss_final: 0.889944, loss_mean: 0.844498, loss_mean_cls: 0.045446, grad_norm: 0.324177
[[34m2025-10-04 12:28:00[0m] Step: 6034, Training Logs: loss_final: 0.889183, loss_mean: 0.845213, loss_mean_cls: 0.043970, grad_norm: 0.237572
[[34m2025-10-04 12:28:00[0m] Step: 6035, Training Logs: loss_final: 0.895254, loss_mean: 0.850244, loss_mean_cls: 0.045010, grad_norm: 0.335983
[[34m2025-10-04 12:28:00[0m] Step: 6036, Training Logs: loss_final: 0.890438, loss_mean: 0.846096, loss_mean_cls: 0.044342, grad_norm: 0.316757
[[34m2025-10-04 12:28:01[0m] Step: 6037, Training Logs: loss_final: 0.897653, loss_mean: 0.853705, loss_mean_cls: 0.043948, grad_norm: 0.375466
[[34m2025-10-04 12:28:01[0m] Step: 6038, Training Logs: loss_final: 0.886594, loss_mean: 0.842562, loss_mean_cls: 0.044031, grad_norm: 0.220718
[[34m2025-10-04 12:28:01[0m] Step: 6039, Training Logs: loss_final: 0.869339, loss_mean: 0.825247, loss_mean_cls: 0.044092, grad_norm: 0.351471
[[34m2025-10-04 12:28:02[0m] Step: 6040, Training Logs: loss_final: 0.875963, loss_mean: 0.830949, loss_mean_cls: 0.045014, grad_norm: 0.499413
[[34m2025-10-04 12:28:02[0m] Step: 6041, Training Logs: loss_final: 0.878843, loss_mean: 0.834054, loss_mean_cls: 0.044789, grad_norm: 0.253808
[[34m2025-10-04 12:28:02[0m] Step: 6042, Training Logs: loss_final: 0.882832, loss_mean: 0.837786, loss_mean_cls: 0.045046, grad_norm: 0.415306
[[34m2025-10-04 12:28:02[0m] Step: 6043, Training Logs: loss_final: 0.876606, loss_mean: 0.832717, loss_mean_cls: 0.043889, grad_norm: 0.313575
[[34m2025-10-04 12:28:03[0m] Step: 6044, Training Logs: loss_final: 0.883876, loss_mean: 0.838611, loss_mean_cls: 0.045265, grad_norm: 0.281767
[[34m2025-10-04 12:28:03[0m] Step: 6045, Training Logs: loss_final: 0.880121, loss_mean: 0.834306, loss_mean_cls: 0.045815, grad_norm: 0.317814
[[34m2025-10-04 12:28:03[0m] Step: 6046, Training Logs: loss_final: 0.883053, loss_mean: 0.839271, loss_mean_cls: 0.043782, grad_norm: 0.280335
[[34m2025-10-04 12:28:04[0m] Step: 6047, Training Logs: loss_final: 0.875217, loss_mean: 0.831165, loss_mean_cls: 0.044052, grad_norm: 0.330657
[[34m2025-10-04 12:28:04[0m] Step: 6048, Training Logs: loss_final: 0.886543, loss_mean: 0.842024, loss_mean_cls: 0.044518, grad_norm: 0.344577
[[34m2025-10-04 12:28:04[0m] Step: 6049, Training Logs: loss_final: 0.864191, loss_mean: 0.819180, loss_mean_cls: 0.045011, grad_norm: 0.271283
[[34m2025-10-04 12:28:05[0m] Step: 6050, Training Logs: loss_final: 0.882535, loss_mean: 0.838586, loss_mean_cls: 0.043948, grad_norm: 0.420095
[[34m2025-10-04 12:28:05[0m] Step: 6051, Training Logs: loss_final: 0.899724, loss_mean: 0.855984, loss_mean_cls: 0.043741, grad_norm: 0.323069
[[34m2025-10-04 12:28:05[0m] Step: 6052, Training Logs: loss_final: 0.887938, loss_mean: 0.844271, loss_mean_cls: 0.043667, grad_norm: 0.309057
[[34m2025-10-04 12:28:05[0m] Step: 6053, Training Logs: loss_final: 0.874438, loss_mean: 0.829856, loss_mean_cls: 0.044582, grad_norm: 0.385939
[[34m2025-10-04 12:28:06[0m] Step: 6054, Training Logs: loss_final: 0.896159, loss_mean: 0.851426, loss_mean_cls: 0.044733, grad_norm: 0.258493
[[34m2025-10-04 12:28:06[0m] Step: 6055, Training Logs: loss_final: 0.900474, loss_mean: 0.856243, loss_mean_cls: 0.044231, grad_norm: 0.421374
[[34m2025-10-04 12:28:06[0m] Step: 6056, Training Logs: loss_final: 0.883429, loss_mean: 0.837920, loss_mean_cls: 0.045508, grad_norm: 0.326385
[[34m2025-10-04 12:28:07[0m] Step: 6057, Training Logs: loss_final: 0.871868, loss_mean: 0.828473, loss_mean_cls: 0.043395, grad_norm: 0.230774
[[34m2025-10-04 12:28:07[0m] Step: 6058, Training Logs: loss_final: 0.874615, loss_mean: 0.829641, loss_mean_cls: 0.044974, grad_norm: 0.318416
[[34m2025-10-04 12:28:07[0m] Step: 6059, Training Logs: loss_final: 0.897545, loss_mean: 0.851595, loss_mean_cls: 0.045950, grad_norm: 0.465695
[[34m2025-10-04 12:28:08[0m] Step: 6060, Training Logs: loss_final: 0.871155, loss_mean: 0.827678, loss_mean_cls: 0.043476, grad_norm: 0.242060
[[34m2025-10-04 12:28:08[0m] Step: 6061, Training Logs: loss_final: 0.894855, loss_mean: 0.851061, loss_mean_cls: 0.043794, grad_norm: 0.474931
[[34m2025-10-04 12:28:08[0m] Step: 6062, Training Logs: loss_final: 0.869051, loss_mean: 0.824340, loss_mean_cls: 0.044711, grad_norm: 0.397427
[[34m2025-10-04 12:28:08[0m] Step: 6063, Training Logs: loss_final: 0.883872, loss_mean: 0.840186, loss_mean_cls: 0.043686, grad_norm: 0.283176
[[34m2025-10-04 12:28:09[0m] Step: 6064, Training Logs: loss_final: 0.896285, loss_mean: 0.851198, loss_mean_cls: 0.045086, grad_norm: 0.392114
[[34m2025-10-04 12:28:09[0m] Step: 6065, Training Logs: loss_final: 0.880422, loss_mean: 0.835102, loss_mean_cls: 0.045320, grad_norm: 0.357408
[[34m2025-10-04 12:28:09[0m] Step: 6066, Training Logs: loss_final: 0.890105, loss_mean: 0.845897, loss_mean_cls: 0.044208, grad_norm: 0.323482
[[34m2025-10-04 12:28:10[0m] Step: 6067, Training Logs: loss_final: 0.871749, loss_mean: 0.827798, loss_mean_cls: 0.043951, grad_norm: 0.371985
[[34m2025-10-04 12:28:10[0m] Step: 6068, Training Logs: loss_final: 0.897404, loss_mean: 0.852421, loss_mean_cls: 0.044983, grad_norm: 0.304260
[[34m2025-10-04 12:28:10[0m] Step: 6069, Training Logs: loss_final: 0.878602, loss_mean: 0.833701, loss_mean_cls: 0.044901, grad_norm: 0.360334
[[34m2025-10-04 12:28:11[0m] Step: 6070, Training Logs: loss_final: 0.887326, loss_mean: 0.841664, loss_mean_cls: 0.045661, grad_norm: 0.409496
[[34m2025-10-04 12:28:11[0m] Step: 6071, Training Logs: loss_final: 0.891485, loss_mean: 0.846950, loss_mean_cls: 0.044535, grad_norm: 0.284286
[[34m2025-10-04 12:28:11[0m] Step: 6072, Training Logs: loss_final: 0.868510, loss_mean: 0.824153, loss_mean_cls: 0.044357, grad_norm: 0.259451
[[34m2025-10-04 12:28:11[0m] Step: 6073, Training Logs: loss_final: 0.873465, loss_mean: 0.828593, loss_mean_cls: 0.044872, grad_norm: 0.370433
[[34m2025-10-04 12:28:12[0m] Step: 6074, Training Logs: loss_final: 0.878683, loss_mean: 0.834504, loss_mean_cls: 0.044179, grad_norm: 0.342946
[[34m2025-10-04 12:28:12[0m] Step: 6075, Training Logs: loss_final: 0.871581, loss_mean: 0.827006, loss_mean_cls: 0.044576, grad_norm: 0.296014
[[34m2025-10-04 12:28:12[0m] Step: 6076, Training Logs: loss_final: 0.880369, loss_mean: 0.836374, loss_mean_cls: 0.043995, grad_norm: 0.409688
[[34m2025-10-04 12:28:13[0m] Step: 6077, Training Logs: loss_final: 0.888730, loss_mean: 0.845784, loss_mean_cls: 0.042947, grad_norm: 0.277792
[[34m2025-10-04 12:28:13[0m] Step: 6078, Training Logs: loss_final: 0.891850, loss_mean: 0.847110, loss_mean_cls: 0.044740, grad_norm: 0.273926
[[34m2025-10-04 12:28:13[0m] Step: 6079, Training Logs: loss_final: 0.872153, loss_mean: 0.827339, loss_mean_cls: 0.044814, grad_norm: 0.255869
[[34m2025-10-04 12:28:13[0m] Step: 6080, Training Logs: loss_final: 0.891242, loss_mean: 0.846630, loss_mean_cls: 0.044612, grad_norm: 0.397567
[[34m2025-10-04 12:28:14[0m] Step: 6081, Training Logs: loss_final: 0.878690, loss_mean: 0.835619, loss_mean_cls: 0.043071, grad_norm: 0.352393
[[34m2025-10-04 12:28:14[0m] Step: 6082, Training Logs: loss_final: 0.886663, loss_mean: 0.843282, loss_mean_cls: 0.043380, grad_norm: 0.284618
[[34m2025-10-04 12:28:14[0m] Step: 6083, Training Logs: loss_final: 0.863545, loss_mean: 0.818210, loss_mean_cls: 0.045335, grad_norm: 0.349228
[[34m2025-10-04 12:28:15[0m] Step: 6084, Training Logs: loss_final: 0.884903, loss_mean: 0.841356, loss_mean_cls: 0.043547, grad_norm: 0.327739
[[34m2025-10-04 12:28:15[0m] Step: 6085, Training Logs: loss_final: 0.899306, loss_mean: 0.856073, loss_mean_cls: 0.043232, grad_norm: 0.358808
[[34m2025-10-04 12:28:15[0m] Step: 6086, Training Logs: loss_final: 0.875278, loss_mean: 0.829479, loss_mean_cls: 0.045799, grad_norm: 0.430569
[[34m2025-10-04 12:28:16[0m] Step: 6087, Training Logs: loss_final: 0.895098, loss_mean: 0.850391, loss_mean_cls: 0.044707, grad_norm: 0.309368
[[34m2025-10-04 12:28:16[0m] Step: 6088, Training Logs: loss_final: 0.887700, loss_mean: 0.844286, loss_mean_cls: 0.043414, grad_norm: 0.426613
[[34m2025-10-04 12:28:16[0m] Step: 6089, Training Logs: loss_final: 0.895060, loss_mean: 0.850919, loss_mean_cls: 0.044141, grad_norm: 0.402750
[[34m2025-10-04 12:28:16[0m] Step: 6090, Training Logs: loss_final: 0.874113, loss_mean: 0.828415, loss_mean_cls: 0.045697, grad_norm: 0.419042
[[34m2025-10-04 12:28:17[0m] Step: 6091, Training Logs: loss_final: 0.873909, loss_mean: 0.829916, loss_mean_cls: 0.043993, grad_norm: 0.266699
[[34m2025-10-04 12:28:17[0m] Step: 6092, Training Logs: loss_final: 0.865641, loss_mean: 0.821050, loss_mean_cls: 0.044592, grad_norm: 0.405539
[[34m2025-10-04 12:28:17[0m] Step: 6093, Training Logs: loss_final: 0.906563, loss_mean: 0.860709, loss_mean_cls: 0.045854, grad_norm: 0.385497
[[34m2025-10-04 12:28:18[0m] Step: 6094, Training Logs: loss_final: 0.880461, loss_mean: 0.837639, loss_mean_cls: 0.042822, grad_norm: 0.221734
[[34m2025-10-04 12:28:18[0m] Step: 6095, Training Logs: loss_final: 0.887727, loss_mean: 0.843293, loss_mean_cls: 0.044435, grad_norm: 0.345281
[[34m2025-10-04 12:28:18[0m] Step: 6096, Training Logs: loss_final: 0.891114, loss_mean: 0.846041, loss_mean_cls: 0.045072, grad_norm: 0.291001
[[34m2025-10-04 12:28:19[0m] Step: 6097, Training Logs: loss_final: 0.886653, loss_mean: 0.842095, loss_mean_cls: 0.044557, grad_norm: 0.279222
[[34m2025-10-04 12:28:19[0m] Step: 6098, Training Logs: loss_final: 0.873450, loss_mean: 0.829608, loss_mean_cls: 0.043842, grad_norm: 0.243565
[[34m2025-10-04 12:28:19[0m] Step: 6099, Training Logs: loss_final: 0.880461, loss_mean: 0.835888, loss_mean_cls: 0.044573, grad_norm: 0.260634
[[34m2025-10-04 12:28:19[0m] Step: 6100, Training Logs: loss_final: 0.902914, loss_mean: 0.859683, loss_mean_cls: 0.043231, grad_norm: 0.356717
[[34m2025-10-04 12:28:20[0m] Step: 6101, Training Logs: loss_final: 0.893607, loss_mean: 0.849767, loss_mean_cls: 0.043840, grad_norm: 0.286266
[[34m2025-10-04 12:28:20[0m] Step: 6102, Training Logs: loss_final: 0.880654, loss_mean: 0.836496, loss_mean_cls: 0.044158, grad_norm: 0.199276
[[34m2025-10-04 12:28:20[0m] Step: 6103, Training Logs: loss_final: 0.886103, loss_mean: 0.841264, loss_mean_cls: 0.044838, grad_norm: 0.331668
[[34m2025-10-04 12:28:21[0m] Step: 6104, Training Logs: loss_final: 0.883883, loss_mean: 0.840612, loss_mean_cls: 0.043271, grad_norm: 0.363167
[[34m2025-10-04 12:28:21[0m] Step: 6105, Training Logs: loss_final: 0.899948, loss_mean: 0.855402, loss_mean_cls: 0.044546, grad_norm: 0.363778
[[34m2025-10-04 12:28:21[0m] Step: 6106, Training Logs: loss_final: 0.893895, loss_mean: 0.850827, loss_mean_cls: 0.043068, grad_norm: 0.300212
[[34m2025-10-04 12:28:22[0m] Step: 6107, Training Logs: loss_final: 0.877897, loss_mean: 0.833661, loss_mean_cls: 0.044236, grad_norm: 0.329548
[[34m2025-10-04 12:28:22[0m] Step: 6108, Training Logs: loss_final: 0.870895, loss_mean: 0.826860, loss_mean_cls: 0.044035, grad_norm: 0.319586
[[34m2025-10-04 12:28:22[0m] Step: 6109, Training Logs: loss_final: 0.869557, loss_mean: 0.825356, loss_mean_cls: 0.044200, grad_norm: 0.278644
[[34m2025-10-04 12:28:22[0m] Step: 6110, Training Logs: loss_final: 0.874130, loss_mean: 0.829881, loss_mean_cls: 0.044248, grad_norm: 0.350909
[[34m2025-10-04 12:28:23[0m] Step: 6111, Training Logs: loss_final: 0.868167, loss_mean: 0.822211, loss_mean_cls: 0.045956, grad_norm: 0.297750
[[34m2025-10-04 12:28:23[0m] Step: 6112, Training Logs: loss_final: 0.885615, loss_mean: 0.840078, loss_mean_cls: 0.045536, grad_norm: 0.283123
[[34m2025-10-04 12:28:23[0m] Step: 6113, Training Logs: loss_final: 0.865546, loss_mean: 0.821186, loss_mean_cls: 0.044360, grad_norm: 0.326672
[[34m2025-10-04 12:28:24[0m] Step: 6114, Training Logs: loss_final: 0.897580, loss_mean: 0.854023, loss_mean_cls: 0.043557, grad_norm: 0.277814
[[34m2025-10-04 12:28:24[0m] Step: 6115, Training Logs: loss_final: 0.868932, loss_mean: 0.824545, loss_mean_cls: 0.044387, grad_norm: 0.241293
[[34m2025-10-04 12:28:24[0m] Step: 6116, Training Logs: loss_final: 0.899488, loss_mean: 0.855614, loss_mean_cls: 0.043874, grad_norm: 0.244556
[[34m2025-10-04 12:28:25[0m] Step: 6117, Training Logs: loss_final: 0.870584, loss_mean: 0.825866, loss_mean_cls: 0.044718, grad_norm: 0.372335
[[34m2025-10-04 12:28:25[0m] Step: 6118, Training Logs: loss_final: 0.877635, loss_mean: 0.832588, loss_mean_cls: 0.045047, grad_norm: 0.270434
[[34m2025-10-04 12:28:25[0m] Step: 6119, Training Logs: loss_final: 0.875999, loss_mean: 0.830974, loss_mean_cls: 0.045025, grad_norm: 0.278281
[[34m2025-10-04 12:28:25[0m] Step: 6120, Training Logs: loss_final: 0.889962, loss_mean: 0.844827, loss_mean_cls: 0.045135, grad_norm: 0.420889
[[34m2025-10-04 12:28:26[0m] Step: 6121, Training Logs: loss_final: 0.879530, loss_mean: 0.835070, loss_mean_cls: 0.044459, grad_norm: 0.468244
[[34m2025-10-04 12:28:26[0m] Step: 6122, Training Logs: loss_final: 0.885539, loss_mean: 0.841825, loss_mean_cls: 0.043714, grad_norm: 0.261016
[[34m2025-10-04 12:28:26[0m] Step: 6123, Training Logs: loss_final: 0.882802, loss_mean: 0.839128, loss_mean_cls: 0.043674, grad_norm: 0.341644
[[34m2025-10-04 12:28:27[0m] Step: 6124, Training Logs: loss_final: 0.883580, loss_mean: 0.838173, loss_mean_cls: 0.045407, grad_norm: 0.379030
[[34m2025-10-04 12:28:27[0m] Step: 6125, Training Logs: loss_final: 0.898458, loss_mean: 0.854419, loss_mean_cls: 0.044040, grad_norm: 0.221894
[[34m2025-10-04 12:28:27[0m] Step: 6126, Training Logs: loss_final: 0.877022, loss_mean: 0.832747, loss_mean_cls: 0.044276, grad_norm: 0.235225
[[34m2025-10-04 12:28:27[0m] Step: 6127, Training Logs: loss_final: 0.891291, loss_mean: 0.847305, loss_mean_cls: 0.043986, grad_norm: 0.320466
[[34m2025-10-04 12:28:28[0m] Step: 6128, Training Logs: loss_final: 0.878865, loss_mean: 0.834683, loss_mean_cls: 0.044182, grad_norm: 0.271729
[[34m2025-10-04 12:28:28[0m] Step: 6129, Training Logs: loss_final: 0.876835, loss_mean: 0.833200, loss_mean_cls: 0.043635, grad_norm: 0.367574
[[34m2025-10-04 12:28:28[0m] Step: 6130, Training Logs: loss_final: 0.892536, loss_mean: 0.847091, loss_mean_cls: 0.045445, grad_norm: 0.341272
[[34m2025-10-04 12:28:29[0m] Step: 6131, Training Logs: loss_final: 0.900253, loss_mean: 0.856875, loss_mean_cls: 0.043378, grad_norm: 0.221145
[[34m2025-10-04 12:28:29[0m] Step: 6132, Training Logs: loss_final: 0.880123, loss_mean: 0.835330, loss_mean_cls: 0.044792, grad_norm: 0.324521
[[34m2025-10-04 12:28:29[0m] Step: 6133, Training Logs: loss_final: 0.905884, loss_mean: 0.861862, loss_mean_cls: 0.044022, grad_norm: 0.368524
[[34m2025-10-04 12:28:29[0m] Step: 6134, Training Logs: loss_final: 0.891630, loss_mean: 0.847611, loss_mean_cls: 0.044019, grad_norm: 0.301357
[[34m2025-10-04 12:28:30[0m] Step: 6135, Training Logs: loss_final: 0.878281, loss_mean: 0.833402, loss_mean_cls: 0.044880, grad_norm: 0.290525
[[34m2025-10-04 12:28:30[0m] Step: 6136, Training Logs: loss_final: 0.902354, loss_mean: 0.858319, loss_mean_cls: 0.044035, grad_norm: 0.344753
[[34m2025-10-04 12:28:30[0m] Step: 6137, Training Logs: loss_final: 0.884694, loss_mean: 0.840300, loss_mean_cls: 0.044394, grad_norm: 0.341526
[[34m2025-10-04 12:28:31[0m] Step: 6138, Training Logs: loss_final: 0.890610, loss_mean: 0.847141, loss_mean_cls: 0.043469, grad_norm: 0.325628
[[34m2025-10-04 12:28:31[0m] Step: 6139, Training Logs: loss_final: 0.889768, loss_mean: 0.845333, loss_mean_cls: 0.044435, grad_norm: 0.280813
[[34m2025-10-04 12:28:31[0m] Step: 6140, Training Logs: loss_final: 0.887282, loss_mean: 0.844226, loss_mean_cls: 0.043057, grad_norm: 0.364371
[[34m2025-10-04 12:28:31[0m] Step: 6141, Training Logs: loss_final: 0.881031, loss_mean: 0.835856, loss_mean_cls: 0.045175, grad_norm: 0.310315
[[34m2025-10-04 12:28:32[0m] Step: 6142, Training Logs: loss_final: 0.879469, loss_mean: 0.834141, loss_mean_cls: 0.045328, grad_norm: 0.275153
[[34m2025-10-04 12:28:32[0m] Step: 6143, Training Logs: loss_final: 0.880927, loss_mean: 0.835887, loss_mean_cls: 0.045040, grad_norm: 0.354875
[[34m2025-10-04 12:28:32[0m] Step: 6144, Training Logs: loss_final: 0.890609, loss_mean: 0.846736, loss_mean_cls: 0.043873, grad_norm: 0.285905
[[34m2025-10-04 12:28:33[0m] Step: 6145, Training Logs: loss_final: 0.881865, loss_mean: 0.837723, loss_mean_cls: 0.044142, grad_norm: 0.342632
[[34m2025-10-04 12:28:33[0m] Step: 6146, Training Logs: loss_final: 0.877682, loss_mean: 0.833519, loss_mean_cls: 0.044163, grad_norm: 0.441949
[[34m2025-10-04 12:28:33[0m] Step: 6147, Training Logs: loss_final: 0.876927, loss_mean: 0.830989, loss_mean_cls: 0.045938, grad_norm: 0.253630
[[34m2025-10-04 12:28:34[0m] Step: 6148, Training Logs: loss_final: 0.900720, loss_mean: 0.857314, loss_mean_cls: 0.043406, grad_norm: 0.381704
[[34m2025-10-04 12:28:34[0m] Step: 6149, Training Logs: loss_final: 0.883204, loss_mean: 0.838594, loss_mean_cls: 0.044610, grad_norm: 0.445006
[[34m2025-10-04 12:28:34[0m] Step: 6150, Training Logs: loss_final: 0.885499, loss_mean: 0.841651, loss_mean_cls: 0.043848, grad_norm: 0.252118
[[34m2025-10-04 12:28:34[0m] Step: 6151, Training Logs: loss_final: 0.890720, loss_mean: 0.846890, loss_mean_cls: 0.043829, grad_norm: 0.394789
[[34m2025-10-04 12:28:35[0m] Step: 6152, Training Logs: loss_final: 0.886155, loss_mean: 0.842272, loss_mean_cls: 0.043883, grad_norm: 0.402397
[[34m2025-10-04 12:28:35[0m] Step: 6153, Training Logs: loss_final: 0.864980, loss_mean: 0.819807, loss_mean_cls: 0.045173, grad_norm: 0.242330
[[34m2025-10-04 12:28:35[0m] Step: 6154, Training Logs: loss_final: 0.893251, loss_mean: 0.849450, loss_mean_cls: 0.043801, grad_norm: 0.316044
[[34m2025-10-04 12:28:36[0m] Step: 6155, Training Logs: loss_final: 0.877048, loss_mean: 0.832613, loss_mean_cls: 0.044434, grad_norm: 0.406336
[[34m2025-10-04 12:28:36[0m] Step: 6156, Training Logs: loss_final: 0.857404, loss_mean: 0.813364, loss_mean_cls: 0.044041, grad_norm: 0.313429
[[34m2025-10-04 12:28:36[0m] Step: 6157, Training Logs: loss_final: 0.899638, loss_mean: 0.855227, loss_mean_cls: 0.044412, grad_norm: 0.339984
[[34m2025-10-04 12:28:36[0m] Step: 6158, Training Logs: loss_final: 0.900491, loss_mean: 0.856044, loss_mean_cls: 0.044447, grad_norm: 0.409299
[[34m2025-10-04 12:28:37[0m] Step: 6159, Training Logs: loss_final: 0.896363, loss_mean: 0.853065, loss_mean_cls: 0.043297, grad_norm: 0.394758
[[34m2025-10-04 12:28:37[0m] Step: 6160, Training Logs: loss_final: 0.890489, loss_mean: 0.845380, loss_mean_cls: 0.045109, grad_norm: 0.319647
[[34m2025-10-04 12:28:37[0m] Step: 6161, Training Logs: loss_final: 0.877707, loss_mean: 0.834847, loss_mean_cls: 0.042860, grad_norm: 0.394033
[[34m2025-10-04 12:28:38[0m] Step: 6162, Training Logs: loss_final: 0.854911, loss_mean: 0.809366, loss_mean_cls: 0.045544, grad_norm: 0.344007
[[34m2025-10-04 12:28:38[0m] Step: 6163, Training Logs: loss_final: 0.878956, loss_mean: 0.833654, loss_mean_cls: 0.045302, grad_norm: 0.335614
[[34m2025-10-04 12:28:38[0m] Step: 6164, Training Logs: loss_final: 0.886209, loss_mean: 0.841902, loss_mean_cls: 0.044307, grad_norm: 0.351532
[[34m2025-10-04 12:28:38[0m] Step: 6165, Training Logs: loss_final: 0.876941, loss_mean: 0.832349, loss_mean_cls: 0.044592, grad_norm: 0.316557
[[34m2025-10-04 12:28:39[0m] Step: 6166, Training Logs: loss_final: 0.911099, loss_mean: 0.867401, loss_mean_cls: 0.043699, grad_norm: 0.395717
[[34m2025-10-04 12:28:39[0m] Step: 6167, Training Logs: loss_final: 0.892573, loss_mean: 0.848345, loss_mean_cls: 0.044229, grad_norm: 0.267723
[[34m2025-10-04 12:28:39[0m] Step: 6168, Training Logs: loss_final: 0.891249, loss_mean: 0.847051, loss_mean_cls: 0.044198, grad_norm: 0.358065
[[34m2025-10-04 12:28:40[0m] Step: 6169, Training Logs: loss_final: 0.888169, loss_mean: 0.844748, loss_mean_cls: 0.043421, grad_norm: 0.358607
[[34m2025-10-04 12:28:40[0m] Step: 6170, Training Logs: loss_final: 0.904726, loss_mean: 0.861891, loss_mean_cls: 0.042835, grad_norm: 0.311486
[[34m2025-10-04 12:28:40[0m] Step: 6171, Training Logs: loss_final: 0.883518, loss_mean: 0.839583, loss_mean_cls: 0.043934, grad_norm: 0.438122
[[34m2025-10-04 12:28:41[0m] Step: 6172, Training Logs: loss_final: 0.873332, loss_mean: 0.830299, loss_mean_cls: 0.043033, grad_norm: 0.303200
[[34m2025-10-04 12:28:41[0m] Step: 6173, Training Logs: loss_final: 0.898504, loss_mean: 0.854746, loss_mean_cls: 0.043758, grad_norm: 0.440637
[[34m2025-10-04 12:28:41[0m] Step: 6174, Training Logs: loss_final: 0.891278, loss_mean: 0.847152, loss_mean_cls: 0.044126, grad_norm: 0.352950
[[34m2025-10-04 12:28:41[0m] Step: 6175, Training Logs: loss_final: 0.893045, loss_mean: 0.849781, loss_mean_cls: 0.043265, grad_norm: 0.373243
[[34m2025-10-04 12:28:42[0m] Step: 6176, Training Logs: loss_final: 0.894477, loss_mean: 0.850438, loss_mean_cls: 0.044039, grad_norm: 0.360971
[[34m2025-10-04 12:28:42[0m] Step: 6177, Training Logs: loss_final: 0.864535, loss_mean: 0.820608, loss_mean_cls: 0.043927, grad_norm: 0.453597
[[34m2025-10-04 12:28:42[0m] Step: 6178, Training Logs: loss_final: 0.873800, loss_mean: 0.829251, loss_mean_cls: 0.044549, grad_norm: 0.545489
[[34m2025-10-04 12:28:43[0m] Step: 6179, Training Logs: loss_final: 0.886446, loss_mean: 0.842411, loss_mean_cls: 0.044035, grad_norm: 0.386030
[[34m2025-10-04 12:28:43[0m] Step: 6180, Training Logs: loss_final: 0.880134, loss_mean: 0.835103, loss_mean_cls: 0.045031, grad_norm: 0.802412
[[34m2025-10-04 12:28:43[0m] Step: 6181, Training Logs: loss_final: 0.875972, loss_mean: 0.830838, loss_mean_cls: 0.045135, grad_norm: 0.498092
[[34m2025-10-04 12:28:43[0m] Step: 6182, Training Logs: loss_final: 0.882050, loss_mean: 0.838655, loss_mean_cls: 0.043395, grad_norm: 0.494045
[[34m2025-10-04 12:28:44[0m] Step: 6183, Training Logs: loss_final: 0.895143, loss_mean: 0.852177, loss_mean_cls: 0.042966, grad_norm: 0.393809
[[34m2025-10-04 12:28:44[0m] Step: 6184, Training Logs: loss_final: 0.858014, loss_mean: 0.813414, loss_mean_cls: 0.044600, grad_norm: 0.411565
[[34m2025-10-04 12:28:44[0m] Step: 6185, Training Logs: loss_final: 0.876746, loss_mean: 0.832233, loss_mean_cls: 0.044514, grad_norm: 0.331019
[[34m2025-10-04 12:28:45[0m] Step: 6186, Training Logs: loss_final: 0.893561, loss_mean: 0.849172, loss_mean_cls: 0.044388, grad_norm: 0.314383
[[34m2025-10-04 12:28:45[0m] Step: 6187, Training Logs: loss_final: 0.868320, loss_mean: 0.823546, loss_mean_cls: 0.044774, grad_norm: 0.377500
[[34m2025-10-04 12:28:45[0m] Step: 6188, Training Logs: loss_final: 0.888669, loss_mean: 0.844904, loss_mean_cls: 0.043765, grad_norm: 0.281628
[[34m2025-10-04 12:28:45[0m] Step: 6189, Training Logs: loss_final: 0.885327, loss_mean: 0.840220, loss_mean_cls: 0.045107, grad_norm: 0.411480
[[34m2025-10-04 12:28:46[0m] Step: 6190, Training Logs: loss_final: 0.883097, loss_mean: 0.838800, loss_mean_cls: 0.044297, grad_norm: 0.320687
[[34m2025-10-04 12:28:46[0m] Step: 6191, Training Logs: loss_final: 0.885011, loss_mean: 0.841583, loss_mean_cls: 0.043428, grad_norm: 0.346574
[[34m2025-10-04 12:28:46[0m] Step: 6192, Training Logs: loss_final: 0.893669, loss_mean: 0.850724, loss_mean_cls: 0.042945, grad_norm: 0.316951
[[34m2025-10-04 12:28:47[0m] Step: 6193, Training Logs: loss_final: 0.901646, loss_mean: 0.858939, loss_mean_cls: 0.042707, grad_norm: 0.424276
[[34m2025-10-04 12:28:47[0m] Step: 6194, Training Logs: loss_final: 0.895795, loss_mean: 0.852238, loss_mean_cls: 0.043557, grad_norm: 0.333109
[[34m2025-10-04 12:28:47[0m] Step: 6195, Training Logs: loss_final: 0.914256, loss_mean: 0.870828, loss_mean_cls: 0.043428, grad_norm: 0.480242
[[34m2025-10-04 12:28:47[0m] Step: 6196, Training Logs: loss_final: 0.882354, loss_mean: 0.837793, loss_mean_cls: 0.044561, grad_norm: 0.416960
[[34m2025-10-04 12:28:48[0m] Step: 6197, Training Logs: loss_final: 0.875954, loss_mean: 0.831391, loss_mean_cls: 0.044563, grad_norm: 0.320396
[[34m2025-10-04 12:28:48[0m] Step: 6198, Training Logs: loss_final: 0.873331, loss_mean: 0.827713, loss_mean_cls: 0.045618, grad_norm: 0.285121
[[34m2025-10-04 12:28:48[0m] Step: 6199, Training Logs: loss_final: 0.885605, loss_mean: 0.841405, loss_mean_cls: 0.044199, grad_norm: 0.316974
[[34m2025-10-04 12:28:49[0m] Step: 6200, Training Logs: loss_final: 0.892717, loss_mean: 0.848634, loss_mean_cls: 0.044084, grad_norm: 0.432048
[[34m2025-10-04 12:28:49[0m] Step: 6201, Training Logs: loss_final: 0.861523, loss_mean: 0.817068, loss_mean_cls: 0.044455, grad_norm: 0.231617
[[34m2025-10-04 12:28:49[0m] Step: 6202, Training Logs: loss_final: 0.869727, loss_mean: 0.824838, loss_mean_cls: 0.044888, grad_norm: 0.401245
[[34m2025-10-04 12:28:50[0m] Step: 6203, Training Logs: loss_final: 0.879637, loss_mean: 0.835803, loss_mean_cls: 0.043834, grad_norm: 0.300862
[[34m2025-10-04 12:28:50[0m] Step: 6204, Training Logs: loss_final: 0.881948, loss_mean: 0.837308, loss_mean_cls: 0.044640, grad_norm: 0.360528
[[34m2025-10-04 12:28:50[0m] Step: 6205, Training Logs: loss_final: 0.884579, loss_mean: 0.841383, loss_mean_cls: 0.043196, grad_norm: 0.258142
[[34m2025-10-04 12:28:50[0m] Step: 6206, Training Logs: loss_final: 0.875034, loss_mean: 0.830631, loss_mean_cls: 0.044403, grad_norm: 0.363172
[[34m2025-10-04 12:28:51[0m] Step: 6207, Training Logs: loss_final: 0.871280, loss_mean: 0.825626, loss_mean_cls: 0.045654, grad_norm: 0.378949
[[34m2025-10-04 12:28:51[0m] Step: 6208, Training Logs: loss_final: 0.886052, loss_mean: 0.841615, loss_mean_cls: 0.044436, grad_norm: 0.353203
[[34m2025-10-04 12:28:51[0m] Step: 6209, Training Logs: loss_final: 0.885259, loss_mean: 0.841098, loss_mean_cls: 0.044161, grad_norm: 0.318724
[[34m2025-10-04 12:28:52[0m] Step: 6210, Training Logs: loss_final: 0.880734, loss_mean: 0.836084, loss_mean_cls: 0.044650, grad_norm: 0.385864
[[34m2025-10-04 12:28:52[0m] Step: 6211, Training Logs: loss_final: 0.901765, loss_mean: 0.857926, loss_mean_cls: 0.043839, grad_norm: 0.355337
[[34m2025-10-04 12:28:52[0m] Step: 6212, Training Logs: loss_final: 0.883551, loss_mean: 0.839179, loss_mean_cls: 0.044372, grad_norm: 0.302067
[[34m2025-10-04 12:28:52[0m] Step: 6213, Training Logs: loss_final: 0.879555, loss_mean: 0.834978, loss_mean_cls: 0.044577, grad_norm: 0.276445
[[34m2025-10-04 12:28:53[0m] Step: 6214, Training Logs: loss_final: 0.882884, loss_mean: 0.838470, loss_mean_cls: 0.044414, grad_norm: 0.306860
[[34m2025-10-04 12:28:53[0m] Step: 6215, Training Logs: loss_final: 0.885293, loss_mean: 0.840061, loss_mean_cls: 0.045232, grad_norm: 0.250878
[[34m2025-10-04 12:28:53[0m] Step: 6216, Training Logs: loss_final: 0.886520, loss_mean: 0.843410, loss_mean_cls: 0.043111, grad_norm: 0.239477
[[34m2025-10-04 12:28:54[0m] Step: 6217, Training Logs: loss_final: 0.906234, loss_mean: 0.862496, loss_mean_cls: 0.043737, grad_norm: 0.266233
[[34m2025-10-04 12:28:54[0m] Step: 6218, Training Logs: loss_final: 0.878203, loss_mean: 0.834823, loss_mean_cls: 0.043381, grad_norm: 0.270360
[[34m2025-10-04 12:28:54[0m] Step: 6219, Training Logs: loss_final: 0.902145, loss_mean: 0.857334, loss_mean_cls: 0.044811, grad_norm: 0.197566
[[34m2025-10-04 12:28:54[0m] Step: 6220, Training Logs: loss_final: 0.879599, loss_mean: 0.836247, loss_mean_cls: 0.043353, grad_norm: 0.277155
[[34m2025-10-04 12:28:55[0m] Step: 6221, Training Logs: loss_final: 0.914003, loss_mean: 0.869595, loss_mean_cls: 0.044408, grad_norm: 0.267549
[[34m2025-10-04 12:28:55[0m] Step: 6222, Training Logs: loss_final: 0.890177, loss_mean: 0.845650, loss_mean_cls: 0.044527, grad_norm: 0.300735
[[34m2025-10-04 12:28:55[0m] Step: 6223, Training Logs: loss_final: 0.896208, loss_mean: 0.852480, loss_mean_cls: 0.043728, grad_norm: 0.288989
[[34m2025-10-04 12:28:56[0m] Step: 6224, Training Logs: loss_final: 0.905154, loss_mean: 0.861676, loss_mean_cls: 0.043479, grad_norm: 0.206571
[[34m2025-10-04 12:28:56[0m] Step: 6225, Training Logs: loss_final: 0.876624, loss_mean: 0.832089, loss_mean_cls: 0.044535, grad_norm: 0.305931
[[34m2025-10-04 12:28:56[0m] Step: 6226, Training Logs: loss_final: 0.896931, loss_mean: 0.853786, loss_mean_cls: 0.043145, grad_norm: 0.249298
[[34m2025-10-04 12:28:57[0m] Step: 6227, Training Logs: loss_final: 0.875168, loss_mean: 0.830652, loss_mean_cls: 0.044516, grad_norm: 0.351977
[[34m2025-10-04 12:28:57[0m] Step: 6228, Training Logs: loss_final: 0.902377, loss_mean: 0.858193, loss_mean_cls: 0.044184, grad_norm: 0.336050
[[34m2025-10-04 12:28:57[0m] Step: 6229, Training Logs: loss_final: 0.880418, loss_mean: 0.836420, loss_mean_cls: 0.043998, grad_norm: 0.279825
[[34m2025-10-04 12:28:57[0m] Step: 6230, Training Logs: loss_final: 0.892578, loss_mean: 0.848727, loss_mean_cls: 0.043851, grad_norm: 0.320898
[[34m2025-10-04 12:28:58[0m] Step: 6231, Training Logs: loss_final: 0.892964, loss_mean: 0.848382, loss_mean_cls: 0.044582, grad_norm: 0.299192
[[34m2025-10-04 12:28:58[0m] Step: 6232, Training Logs: loss_final: 0.883599, loss_mean: 0.839675, loss_mean_cls: 0.043924, grad_norm: 0.266293
[[34m2025-10-04 12:28:58[0m] Step: 6233, Training Logs: loss_final: 0.890737, loss_mean: 0.846388, loss_mean_cls: 0.044349, grad_norm: 0.296521
[[34m2025-10-04 12:28:59[0m] Step: 6234, Training Logs: loss_final: 0.882666, loss_mean: 0.839292, loss_mean_cls: 0.043374, grad_norm: 0.218711
[[34m2025-10-04 12:28:59[0m] Step: 6235, Training Logs: loss_final: 0.891409, loss_mean: 0.847745, loss_mean_cls: 0.043664, grad_norm: 0.304794
[[34m2025-10-04 12:28:59[0m] Step: 6236, Training Logs: loss_final: 0.888992, loss_mean: 0.845108, loss_mean_cls: 0.043884, grad_norm: 0.379993
[[34m2025-10-04 12:29:00[0m] Step: 6237, Training Logs: loss_final: 0.875989, loss_mean: 0.831859, loss_mean_cls: 0.044129, grad_norm: 0.226832
[[34m2025-10-04 12:29:00[0m] Step: 6238, Training Logs: loss_final: 0.905861, loss_mean: 0.862651, loss_mean_cls: 0.043211, grad_norm: 0.285410
[[34m2025-10-04 12:29:00[0m] Step: 6239, Training Logs: loss_final: 0.902489, loss_mean: 0.858099, loss_mean_cls: 0.044390, grad_norm: 0.244243
[[34m2025-10-04 12:29:00[0m] Step: 6240, Training Logs: loss_final: 0.862974, loss_mean: 0.818626, loss_mean_cls: 0.044348, grad_norm: 0.243707
[[34m2025-10-04 12:29:01[0m] Step: 6241, Training Logs: loss_final: 0.869644, loss_mean: 0.824317, loss_mean_cls: 0.045327, grad_norm: 0.338993
[[34m2025-10-04 12:29:01[0m] Step: 6242, Training Logs: loss_final: 0.903406, loss_mean: 0.858925, loss_mean_cls: 0.044481, grad_norm: 0.292319
[[34m2025-10-04 12:29:01[0m] Step: 6243, Training Logs: loss_final: 0.894086, loss_mean: 0.851331, loss_mean_cls: 0.042755, grad_norm: 0.372786
[[34m2025-10-04 12:29:02[0m] Step: 6244, Training Logs: loss_final: 0.904052, loss_mean: 0.860953, loss_mean_cls: 0.043099, grad_norm: 0.281302
[[34m2025-10-04 12:29:02[0m] Step: 6245, Training Logs: loss_final: 0.893983, loss_mean: 0.850431, loss_mean_cls: 0.043552, grad_norm: 0.292955
[[34m2025-10-04 12:29:02[0m] Step: 6246, Training Logs: loss_final: 0.898384, loss_mean: 0.855568, loss_mean_cls: 0.042816, grad_norm: 0.340571
[[34m2025-10-04 12:29:03[0m] Step: 6247, Training Logs: loss_final: 0.884719, loss_mean: 0.841331, loss_mean_cls: 0.043388, grad_norm: 0.416817
[[34m2025-10-04 12:29:03[0m] Step: 6248, Training Logs: loss_final: 0.885068, loss_mean: 0.840374, loss_mean_cls: 0.044694, grad_norm: 0.371446
[[34m2025-10-04 12:29:03[0m] Step: 6249, Training Logs: loss_final: 0.881042, loss_mean: 0.837617, loss_mean_cls: 0.043425, grad_norm: 0.300550
[[34m2025-10-04 12:29:03[0m] Step: 6250, Training Logs: loss_final: 0.892589, loss_mean: 0.849318, loss_mean_cls: 0.043271, grad_norm: 0.295375
[[34m2025-10-04 12:29:04[0m] Step: 6251, Training Logs: loss_final: 0.869528, loss_mean: 0.824733, loss_mean_cls: 0.044795, grad_norm: 0.412453
[[34m2025-10-04 12:29:04[0m] Step: 6252, Training Logs: loss_final: 0.890089, loss_mean: 0.845595, loss_mean_cls: 0.044495, grad_norm: 0.321095
[[34m2025-10-04 12:29:04[0m] Step: 6253, Training Logs: loss_final: 0.888046, loss_mean: 0.842954, loss_mean_cls: 0.045092, grad_norm: 0.288210
[[34m2025-10-04 12:29:05[0m] Step: 6254, Training Logs: loss_final: 0.897923, loss_mean: 0.853080, loss_mean_cls: 0.044843, grad_norm: 0.304833
[[34m2025-10-04 12:29:05[0m] Step: 6255, Training Logs: loss_final: 0.866694, loss_mean: 0.821884, loss_mean_cls: 0.044810, grad_norm: 0.398888
[[34m2025-10-04 12:29:05[0m] Step: 6256, Training Logs: loss_final: 0.880251, loss_mean: 0.834593, loss_mean_cls: 0.045658, grad_norm: 0.279554
[[34m2025-10-04 12:29:05[0m] Step: 6257, Training Logs: loss_final: 0.916294, loss_mean: 0.872416, loss_mean_cls: 0.043878, grad_norm: 0.313271
[[34m2025-10-04 12:29:06[0m] Step: 6258, Training Logs: loss_final: 0.875347, loss_mean: 0.830176, loss_mean_cls: 0.045171, grad_norm: 0.573498
[[34m2025-10-04 12:29:06[0m] Step: 6259, Training Logs: loss_final: 0.880634, loss_mean: 0.835592, loss_mean_cls: 0.045042, grad_norm: 0.296385
[[34m2025-10-04 12:29:06[0m] Step: 6260, Training Logs: loss_final: 0.885656, loss_mean: 0.840814, loss_mean_cls: 0.044842, grad_norm: 0.385346
[[34m2025-10-04 12:29:07[0m] Step: 6261, Training Logs: loss_final: 0.870495, loss_mean: 0.825943, loss_mean_cls: 0.044553, grad_norm: 0.435576
[[34m2025-10-04 12:29:07[0m] Step: 6262, Training Logs: loss_final: 0.885935, loss_mean: 0.841829, loss_mean_cls: 0.044106, grad_norm: 0.424366
[[34m2025-10-04 12:29:07[0m] Step: 6263, Training Logs: loss_final: 0.866540, loss_mean: 0.822442, loss_mean_cls: 0.044098, grad_norm: 0.323757
[[34m2025-10-04 12:29:07[0m] Step: 6264, Training Logs: loss_final: 0.885255, loss_mean: 0.842222, loss_mean_cls: 0.043034, grad_norm: 0.220376
[[34m2025-10-04 12:29:08[0m] Step: 6265, Training Logs: loss_final: 0.895344, loss_mean: 0.851279, loss_mean_cls: 0.044066, grad_norm: 0.341999
[[34m2025-10-04 12:29:08[0m] Step: 6266, Training Logs: loss_final: 0.889514, loss_mean: 0.844915, loss_mean_cls: 0.044599, grad_norm: 0.374599
[[34m2025-10-04 12:29:08[0m] Step: 6267, Training Logs: loss_final: 0.883636, loss_mean: 0.839913, loss_mean_cls: 0.043722, grad_norm: 0.206867
[[34m2025-10-04 12:29:09[0m] Step: 6268, Training Logs: loss_final: 0.891951, loss_mean: 0.848242, loss_mean_cls: 0.043709, grad_norm: 0.254185
[[34m2025-10-04 12:29:09[0m] Step: 6269, Training Logs: loss_final: 0.895725, loss_mean: 0.851248, loss_mean_cls: 0.044477, grad_norm: 0.399366
[[34m2025-10-04 12:29:09[0m] Step: 6270, Training Logs: loss_final: 0.896011, loss_mean: 0.851661, loss_mean_cls: 0.044350, grad_norm: 0.329268
[[34m2025-10-04 12:29:09[0m] Step: 6271, Training Logs: loss_final: 0.871371, loss_mean: 0.827310, loss_mean_cls: 0.044061, grad_norm: 0.245416
[[34m2025-10-04 12:29:10[0m] Step: 6272, Training Logs: loss_final: 0.881269, loss_mean: 0.837088, loss_mean_cls: 0.044181, grad_norm: 0.285441
[[34m2025-10-04 12:29:10[0m] Step: 6273, Training Logs: loss_final: 0.884775, loss_mean: 0.840102, loss_mean_cls: 0.044674, grad_norm: 0.294257
[[34m2025-10-04 12:29:10[0m] Step: 6274, Training Logs: loss_final: 0.888343, loss_mean: 0.842593, loss_mean_cls: 0.045750, grad_norm: 0.292149
[[34m2025-10-04 12:29:11[0m] Step: 6275, Training Logs: loss_final: 0.869559, loss_mean: 0.825588, loss_mean_cls: 0.043971, grad_norm: 0.359883
[[34m2025-10-04 12:29:11[0m] Step: 6276, Training Logs: loss_final: 0.893187, loss_mean: 0.849415, loss_mean_cls: 0.043772, grad_norm: 0.319209
[[34m2025-10-04 12:29:11[0m] Step: 6277, Training Logs: loss_final: 0.896683, loss_mean: 0.852747, loss_mean_cls: 0.043936, grad_norm: 0.364345
[[34m2025-10-04 12:29:12[0m] Step: 6278, Training Logs: loss_final: 0.893788, loss_mean: 0.850220, loss_mean_cls: 0.043568, grad_norm: 0.394572
[[34m2025-10-04 12:29:12[0m] Step: 6279, Training Logs: loss_final: 0.887838, loss_mean: 0.843737, loss_mean_cls: 0.044102, grad_norm: 0.366979
[[34m2025-10-04 12:29:12[0m] Step: 6280, Training Logs: loss_final: 0.879638, loss_mean: 0.835813, loss_mean_cls: 0.043824, grad_norm: 0.386075
[[34m2025-10-04 12:29:12[0m] Step: 6281, Training Logs: loss_final: 0.866414, loss_mean: 0.821238, loss_mean_cls: 0.045176, grad_norm: 0.333795
[[34m2025-10-04 12:29:13[0m] Step: 6282, Training Logs: loss_final: 0.906698, loss_mean: 0.862941, loss_mean_cls: 0.043757, grad_norm: 0.361199
[[34m2025-10-04 12:29:13[0m] Step: 6283, Training Logs: loss_final: 0.882015, loss_mean: 0.837869, loss_mean_cls: 0.044146, grad_norm: 0.432728
[[34m2025-10-04 12:29:13[0m] Step: 6284, Training Logs: loss_final: 0.893597, loss_mean: 0.849873, loss_mean_cls: 0.043723, grad_norm: 0.279588
[[34m2025-10-04 12:29:14[0m] Step: 6285, Training Logs: loss_final: 0.870683, loss_mean: 0.825924, loss_mean_cls: 0.044759, grad_norm: 0.377043
[[34m2025-10-04 12:29:14[0m] Step: 6286, Training Logs: loss_final: 0.891986, loss_mean: 0.847133, loss_mean_cls: 0.044854, grad_norm: 0.247237
[[34m2025-10-04 12:29:14[0m] Step: 6287, Training Logs: loss_final: 0.858174, loss_mean: 0.813646, loss_mean_cls: 0.044528, grad_norm: 0.363728
[[34m2025-10-04 12:29:14[0m] Step: 6288, Training Logs: loss_final: 0.878652, loss_mean: 0.835380, loss_mean_cls: 0.043273, grad_norm: 0.334886
[[34m2025-10-04 12:29:15[0m] Step: 6289, Training Logs: loss_final: 0.874146, loss_mean: 0.829659, loss_mean_cls: 0.044486, grad_norm: 0.344814
[[34m2025-10-04 12:29:15[0m] Step: 6290, Training Logs: loss_final: 0.891243, loss_mean: 0.847260, loss_mean_cls: 0.043982, grad_norm: 0.276098
[[34m2025-10-04 12:29:15[0m] Step: 6291, Training Logs: loss_final: 0.880596, loss_mean: 0.835647, loss_mean_cls: 0.044949, grad_norm: 0.331177
[[34m2025-10-04 12:29:16[0m] Step: 6292, Training Logs: loss_final: 0.893222, loss_mean: 0.848672, loss_mean_cls: 0.044551, grad_norm: 0.312525
[[34m2025-10-04 12:29:16[0m] Step: 6293, Training Logs: loss_final: 0.894437, loss_mean: 0.850551, loss_mean_cls: 0.043886, grad_norm: 0.312509
[[34m2025-10-04 12:29:16[0m] Step: 6294, Training Logs: loss_final: 0.868607, loss_mean: 0.824903, loss_mean_cls: 0.043704, grad_norm: 0.366955
[[34m2025-10-04 12:29:16[0m] Step: 6295, Training Logs: loss_final: 0.875002, loss_mean: 0.830504, loss_mean_cls: 0.044498, grad_norm: 0.309333
[[34m2025-10-04 12:29:17[0m] Step: 6296, Training Logs: loss_final: 0.879714, loss_mean: 0.835481, loss_mean_cls: 0.044233, grad_norm: 0.281455
[[34m2025-10-04 12:29:17[0m] Step: 6297, Training Logs: loss_final: 0.860431, loss_mean: 0.816140, loss_mean_cls: 0.044291, grad_norm: 0.424231
[[34m2025-10-04 12:29:17[0m] Step: 6298, Training Logs: loss_final: 0.879430, loss_mean: 0.835934, loss_mean_cls: 0.043496, grad_norm: 0.250022
[[34m2025-10-04 12:29:18[0m] Step: 6299, Training Logs: loss_final: 0.896350, loss_mean: 0.852371, loss_mean_cls: 0.043978, grad_norm: 0.298111
[[34m2025-10-04 12:29:18[0m] Step: 6300, Training Logs: loss_final: 0.873964, loss_mean: 0.830183, loss_mean_cls: 0.043782, grad_norm: 0.276361
[[34m2025-10-04 12:29:18[0m] Step: 6301, Training Logs: loss_final: 0.914998, loss_mean: 0.871922, loss_mean_cls: 0.043076, grad_norm: 0.310637
[[34m2025-10-04 12:29:18[0m] Step: 6302, Training Logs: loss_final: 0.872231, loss_mean: 0.828145, loss_mean_cls: 0.044085, grad_norm: 0.293626
[[34m2025-10-04 12:29:19[0m] Step: 6303, Training Logs: loss_final: 0.878537, loss_mean: 0.834483, loss_mean_cls: 0.044054, grad_norm: 0.256142
[[34m2025-10-04 12:29:19[0m] Step: 6304, Training Logs: loss_final: 0.893905, loss_mean: 0.850281, loss_mean_cls: 0.043624, grad_norm: 0.250771
[[34m2025-10-04 12:29:19[0m] Step: 6305, Training Logs: loss_final: 0.859113, loss_mean: 0.815319, loss_mean_cls: 0.043794, grad_norm: 0.245441
[[34m2025-10-04 12:29:20[0m] Step: 6306, Training Logs: loss_final: 0.892057, loss_mean: 0.847957, loss_mean_cls: 0.044100, grad_norm: 0.252590
[[34m2025-10-04 12:29:20[0m] Step: 6307, Training Logs: loss_final: 0.881968, loss_mean: 0.836458, loss_mean_cls: 0.045510, grad_norm: 0.292686
[[34m2025-10-04 12:29:20[0m] Step: 6308, Training Logs: loss_final: 0.882904, loss_mean: 0.838742, loss_mean_cls: 0.044162, grad_norm: 0.279891
[[34m2025-10-04 12:29:20[0m] Step: 6309, Training Logs: loss_final: 0.892359, loss_mean: 0.848310, loss_mean_cls: 0.044049, grad_norm: 0.329443
[[34m2025-10-04 12:29:21[0m] Step: 6310, Training Logs: loss_final: 0.890385, loss_mean: 0.844913, loss_mean_cls: 0.045473, grad_norm: 0.323405
[[34m2025-10-04 12:29:21[0m] Step: 6311, Training Logs: loss_final: 0.872316, loss_mean: 0.827034, loss_mean_cls: 0.045282, grad_norm: 0.257455
[[34m2025-10-04 12:29:21[0m] Step: 6312, Training Logs: loss_final: 0.851038, loss_mean: 0.805987, loss_mean_cls: 0.045051, grad_norm: 0.294304
[[34m2025-10-04 12:29:22[0m] Step: 6313, Training Logs: loss_final: 0.870579, loss_mean: 0.826840, loss_mean_cls: 0.043739, grad_norm: 0.326967
[[34m2025-10-04 12:29:22[0m] Step: 6314, Training Logs: loss_final: 0.888783, loss_mean: 0.846064, loss_mean_cls: 0.042719, grad_norm: 0.327555
[[34m2025-10-04 12:29:22[0m] Step: 6315, Training Logs: loss_final: 0.882081, loss_mean: 0.836860, loss_mean_cls: 0.045221, grad_norm: 0.259594
[[34m2025-10-04 12:29:23[0m] Step: 6316, Training Logs: loss_final: 0.879422, loss_mean: 0.835099, loss_mean_cls: 0.044323, grad_norm: 0.412817
[[34m2025-10-04 12:29:23[0m] Step: 6317, Training Logs: loss_final: 0.899809, loss_mean: 0.855253, loss_mean_cls: 0.044555, grad_norm: 0.260585
[[34m2025-10-04 12:29:23[0m] Step: 6318, Training Logs: loss_final: 0.909302, loss_mean: 0.865935, loss_mean_cls: 0.043367, grad_norm: 0.315204
[[34m2025-10-04 12:29:23[0m] Step: 6319, Training Logs: loss_final: 0.887907, loss_mean: 0.844046, loss_mean_cls: 0.043862, grad_norm: 0.408623
[[34m2025-10-04 12:29:24[0m] Step: 6320, Training Logs: loss_final: 0.866302, loss_mean: 0.822252, loss_mean_cls: 0.044050, grad_norm: 0.364366
[[34m2025-10-04 12:29:24[0m] Step: 6321, Training Logs: loss_final: 0.874851, loss_mean: 0.829231, loss_mean_cls: 0.045620, grad_norm: 0.407718
[[34m2025-10-04 12:29:24[0m] Step: 6322, Training Logs: loss_final: 0.880184, loss_mean: 0.836718, loss_mean_cls: 0.043466, grad_norm: 0.446622
[[34m2025-10-04 12:29:25[0m] Step: 6323, Training Logs: loss_final: 0.870163, loss_mean: 0.824594, loss_mean_cls: 0.045569, grad_norm: 0.411855
[[34m2025-10-04 12:29:25[0m] Step: 6324, Training Logs: loss_final: 0.870348, loss_mean: 0.826950, loss_mean_cls: 0.043398, grad_norm: 0.263342
[[34m2025-10-04 12:29:25[0m] Step: 6325, Training Logs: loss_final: 0.899551, loss_mean: 0.855483, loss_mean_cls: 0.044068, grad_norm: 0.451707
[[34m2025-10-04 12:29:26[0m] Step: 6326, Training Logs: loss_final: 0.869453, loss_mean: 0.824992, loss_mean_cls: 0.044461, grad_norm: 0.446403
[[34m2025-10-04 12:29:26[0m] Step: 6327, Training Logs: loss_final: 0.882839, loss_mean: 0.839435, loss_mean_cls: 0.043404, grad_norm: 0.419215
[[34m2025-10-04 12:29:26[0m] Step: 6328, Training Logs: loss_final: 0.880137, loss_mean: 0.835663, loss_mean_cls: 0.044474, grad_norm: 0.492630
[[34m2025-10-04 12:29:26[0m] Step: 6329, Training Logs: loss_final: 0.898846, loss_mean: 0.854407, loss_mean_cls: 0.044439, grad_norm: 0.255409
[[34m2025-10-04 12:29:27[0m] Step: 6330, Training Logs: loss_final: 0.883826, loss_mean: 0.839822, loss_mean_cls: 0.044004, grad_norm: 0.443917
[[34m2025-10-04 12:29:27[0m] Step: 6331, Training Logs: loss_final: 0.883095, loss_mean: 0.838643, loss_mean_cls: 0.044453, grad_norm: 0.285930
[[34m2025-10-04 12:29:27[0m] Step: 6332, Training Logs: loss_final: 0.891831, loss_mean: 0.847205, loss_mean_cls: 0.044626, grad_norm: 0.521672
[[34m2025-10-04 12:29:28[0m] Step: 6333, Training Logs: loss_final: 0.875070, loss_mean: 0.830061, loss_mean_cls: 0.045009, grad_norm: 0.356662
[[34m2025-10-04 12:29:28[0m] Step: 6334, Training Logs: loss_final: 0.881844, loss_mean: 0.837896, loss_mean_cls: 0.043948, grad_norm: 0.228105
[[34m2025-10-04 12:29:28[0m] Step: 6335, Training Logs: loss_final: 0.870335, loss_mean: 0.824406, loss_mean_cls: 0.045929, grad_norm: 0.439326
[[34m2025-10-04 12:29:28[0m] Step: 6336, Training Logs: loss_final: 0.880071, loss_mean: 0.835589, loss_mean_cls: 0.044482, grad_norm: 0.359171
[[34m2025-10-04 12:29:29[0m] Step: 6337, Training Logs: loss_final: 0.902294, loss_mean: 0.857574, loss_mean_cls: 0.044720, grad_norm: 0.416869
[[34m2025-10-04 12:29:29[0m] Step: 6338, Training Logs: loss_final: 0.882844, loss_mean: 0.838663, loss_mean_cls: 0.044181, grad_norm: 0.348520
[[34m2025-10-04 12:29:29[0m] Step: 6339, Training Logs: loss_final: 0.862096, loss_mean: 0.816979, loss_mean_cls: 0.045117, grad_norm: 0.309728
[[34m2025-10-04 12:29:30[0m] Step: 6340, Training Logs: loss_final: 0.875236, loss_mean: 0.832898, loss_mean_cls: 0.042339, grad_norm: 0.308455
[[34m2025-10-04 12:29:30[0m] Step: 6341, Training Logs: loss_final: 0.873352, loss_mean: 0.829199, loss_mean_cls: 0.044152, grad_norm: 0.384109
[[34m2025-10-04 12:29:30[0m] Step: 6342, Training Logs: loss_final: 0.884299, loss_mean: 0.840266, loss_mean_cls: 0.044033, grad_norm: 0.285019
[[34m2025-10-04 12:29:30[0m] Step: 6343, Training Logs: loss_final: 0.863642, loss_mean: 0.820346, loss_mean_cls: 0.043296, grad_norm: 0.314309
[[34m2025-10-04 12:29:31[0m] Step: 6344, Training Logs: loss_final: 0.892507, loss_mean: 0.849934, loss_mean_cls: 0.042573, grad_norm: 0.253776
[[34m2025-10-04 12:29:31[0m] Step: 6345, Training Logs: loss_final: 0.885104, loss_mean: 0.841671, loss_mean_cls: 0.043433, grad_norm: 0.267409
[[34m2025-10-04 12:29:31[0m] Step: 6346, Training Logs: loss_final: 0.886321, loss_mean: 0.842245, loss_mean_cls: 0.044076, grad_norm: 0.264784
[[34m2025-10-04 12:29:32[0m] Step: 6347, Training Logs: loss_final: 0.888322, loss_mean: 0.844369, loss_mean_cls: 0.043954, grad_norm: 0.270577
[[34m2025-10-04 12:29:32[0m] Step: 6348, Training Logs: loss_final: 0.877690, loss_mean: 0.833097, loss_mean_cls: 0.044593, grad_norm: 0.270956
[[34m2025-10-04 12:29:32[0m] Step: 6349, Training Logs: loss_final: 0.902505, loss_mean: 0.858521, loss_mean_cls: 0.043983, grad_norm: 0.260829
[[34m2025-10-04 12:29:33[0m] Step: 6350, Training Logs: loss_final: 0.867392, loss_mean: 0.823309, loss_mean_cls: 0.044083, grad_norm: 0.293559
[[34m2025-10-04 12:29:33[0m] Step: 6351, Training Logs: loss_final: 0.882976, loss_mean: 0.839334, loss_mean_cls: 0.043642, grad_norm: 0.200905
[[34m2025-10-04 12:29:33[0m] Step: 6352, Training Logs: loss_final: 0.874186, loss_mean: 0.830654, loss_mean_cls: 0.043531, grad_norm: 0.274284
[[34m2025-10-04 12:29:33[0m] Step: 6353, Training Logs: loss_final: 0.886771, loss_mean: 0.842559, loss_mean_cls: 0.044212, grad_norm: 0.284977
[[34m2025-10-04 12:29:34[0m] Step: 6354, Training Logs: loss_final: 0.896034, loss_mean: 0.852247, loss_mean_cls: 0.043787, grad_norm: 0.299319
[[34m2025-10-04 12:29:34[0m] Step: 6355, Training Logs: loss_final: 0.886714, loss_mean: 0.842943, loss_mean_cls: 0.043771, grad_norm: 0.330427
[[34m2025-10-04 12:29:34[0m] Step: 6356, Training Logs: loss_final: 0.884799, loss_mean: 0.841703, loss_mean_cls: 0.043096, grad_norm: 0.289727
[[34m2025-10-04 12:29:35[0m] Step: 6357, Training Logs: loss_final: 0.871786, loss_mean: 0.826540, loss_mean_cls: 0.045246, grad_norm: 0.413840
[[34m2025-10-04 12:29:35[0m] Step: 6358, Training Logs: loss_final: 0.881772, loss_mean: 0.837266, loss_mean_cls: 0.044506, grad_norm: 0.528760
[[34m2025-10-04 12:29:35[0m] Step: 6359, Training Logs: loss_final: 0.887045, loss_mean: 0.842972, loss_mean_cls: 0.044073, grad_norm: 0.232421
[[34m2025-10-04 12:29:35[0m] Step: 6360, Training Logs: loss_final: 0.890595, loss_mean: 0.844635, loss_mean_cls: 0.045959, grad_norm: 0.372792
[[34m2025-10-04 12:29:36[0m] Step: 6361, Training Logs: loss_final: 0.890286, loss_mean: 0.846028, loss_mean_cls: 0.044258, grad_norm: 0.341043
[[34m2025-10-04 12:29:36[0m] Step: 6362, Training Logs: loss_final: 0.893424, loss_mean: 0.849949, loss_mean_cls: 0.043475, grad_norm: 0.382337
[[34m2025-10-04 12:29:36[0m] Step: 6363, Training Logs: loss_final: 0.876839, loss_mean: 0.831736, loss_mean_cls: 0.045103, grad_norm: 0.361647
[[34m2025-10-04 12:29:37[0m] Step: 6364, Training Logs: loss_final: 0.890374, loss_mean: 0.846112, loss_mean_cls: 0.044262, grad_norm: 0.333785
[[34m2025-10-04 12:29:37[0m] Step: 6365, Training Logs: loss_final: 0.880829, loss_mean: 0.836821, loss_mean_cls: 0.044008, grad_norm: 0.575785
[[34m2025-10-04 12:29:37[0m] Step: 6366, Training Logs: loss_final: 0.870990, loss_mean: 0.825360, loss_mean_cls: 0.045630, grad_norm: 0.283622
[[34m2025-10-04 12:29:38[0m] Step: 6367, Training Logs: loss_final: 0.891900, loss_mean: 0.847473, loss_mean_cls: 0.044427, grad_norm: 0.422839
[[34m2025-10-04 12:29:38[0m] Step: 6368, Training Logs: loss_final: 0.892947, loss_mean: 0.850018, loss_mean_cls: 0.042929, grad_norm: 0.284412
[[34m2025-10-04 12:29:38[0m] Step: 6369, Training Logs: loss_final: 0.871253, loss_mean: 0.828621, loss_mean_cls: 0.042631, grad_norm: 0.291713
[[34m2025-10-04 12:29:38[0m] Step: 6370, Training Logs: loss_final: 0.875983, loss_mean: 0.831534, loss_mean_cls: 0.044449, grad_norm: 0.323445
[[34m2025-10-04 12:29:39[0m] Step: 6371, Training Logs: loss_final: 0.887679, loss_mean: 0.842688, loss_mean_cls: 0.044991, grad_norm: 0.290151
[[34m2025-10-04 12:29:39[0m] Step: 6372, Training Logs: loss_final: 0.877316, loss_mean: 0.834403, loss_mean_cls: 0.042913, grad_norm: 0.362152
[[34m2025-10-04 12:29:39[0m] Step: 6373, Training Logs: loss_final: 0.863359, loss_mean: 0.818896, loss_mean_cls: 0.044463, grad_norm: 0.309934
[[34m2025-10-04 12:29:40[0m] Step: 6374, Training Logs: loss_final: 0.885650, loss_mean: 0.840228, loss_mean_cls: 0.045422, grad_norm: 0.652006
[[34m2025-10-04 12:29:40[0m] Step: 6375, Training Logs: loss_final: 0.896904, loss_mean: 0.852861, loss_mean_cls: 0.044044, grad_norm: 0.478260
[[34m2025-10-04 12:29:40[0m] Step: 6376, Training Logs: loss_final: 0.879080, loss_mean: 0.834797, loss_mean_cls: 0.044284, grad_norm: 0.326655
[[34m2025-10-04 12:29:40[0m] Step: 6377, Training Logs: loss_final: 0.883743, loss_mean: 0.839765, loss_mean_cls: 0.043978, grad_norm: 0.298665
[[34m2025-10-04 12:29:41[0m] Step: 6378, Training Logs: loss_final: 0.873652, loss_mean: 0.830117, loss_mean_cls: 0.043535, grad_norm: 0.328387
[[34m2025-10-04 12:29:41[0m] Step: 6379, Training Logs: loss_final: 0.878046, loss_mean: 0.834952, loss_mean_cls: 0.043095, grad_norm: 0.284620
[[34m2025-10-04 12:29:41[0m] Step: 6380, Training Logs: loss_final: 0.894996, loss_mean: 0.851247, loss_mean_cls: 0.043749, grad_norm: 0.261832
[[34m2025-10-04 12:29:42[0m] Step: 6381, Training Logs: loss_final: 0.871722, loss_mean: 0.828687, loss_mean_cls: 0.043035, grad_norm: 0.323898
[[34m2025-10-04 12:29:42[0m] Step: 6382, Training Logs: loss_final: 0.867524, loss_mean: 0.822880, loss_mean_cls: 0.044645, grad_norm: 0.343569
[[34m2025-10-04 12:29:42[0m] Step: 6383, Training Logs: loss_final: 0.901791, loss_mean: 0.857932, loss_mean_cls: 0.043859, grad_norm: 0.293986
[[34m2025-10-04 12:29:43[0m] Step: 6384, Training Logs: loss_final: 0.900616, loss_mean: 0.856592, loss_mean_cls: 0.044024, grad_norm: 0.409192
[[34m2025-10-04 12:29:43[0m] Step: 6385, Training Logs: loss_final: 0.878860, loss_mean: 0.833960, loss_mean_cls: 0.044900, grad_norm: 0.494586
[[34m2025-10-04 12:29:43[0m] Step: 6386, Training Logs: loss_final: 0.870391, loss_mean: 0.824423, loss_mean_cls: 0.045968, grad_norm: 0.401254
[[34m2025-10-04 12:29:43[0m] Step: 6387, Training Logs: loss_final: 0.898059, loss_mean: 0.854285, loss_mean_cls: 0.043774, grad_norm: 0.653601
[[34m2025-10-04 12:29:44[0m] Step: 6388, Training Logs: loss_final: 0.892312, loss_mean: 0.849762, loss_mean_cls: 0.042550, grad_norm: 0.440761
[[34m2025-10-04 12:29:44[0m] Step: 6389, Training Logs: loss_final: 0.876086, loss_mean: 0.832035, loss_mean_cls: 0.044051, grad_norm: 0.645470
[[34m2025-10-04 12:29:44[0m] Step: 6390, Training Logs: loss_final: 0.875918, loss_mean: 0.832565, loss_mean_cls: 0.043353, grad_norm: 0.435529
[[34m2025-10-04 12:29:45[0m] Step: 6391, Training Logs: loss_final: 0.883767, loss_mean: 0.840102, loss_mean_cls: 0.043665, grad_norm: 0.446852
[[34m2025-10-04 12:29:45[0m] Step: 6392, Training Logs: loss_final: 0.879646, loss_mean: 0.834925, loss_mean_cls: 0.044721, grad_norm: 0.314366
[[34m2025-10-04 12:29:45[0m] Step: 6393, Training Logs: loss_final: 0.890702, loss_mean: 0.847477, loss_mean_cls: 0.043225, grad_norm: 0.475637
[[34m2025-10-04 12:29:45[0m] Step: 6394, Training Logs: loss_final: 0.872147, loss_mean: 0.828019, loss_mean_cls: 0.044128, grad_norm: 0.450282
[[34m2025-10-04 12:29:46[0m] Step: 6395, Training Logs: loss_final: 0.866916, loss_mean: 0.821539, loss_mean_cls: 0.045377, grad_norm: 0.465091
[[34m2025-10-04 12:29:46[0m] Step: 6396, Training Logs: loss_final: 0.892814, loss_mean: 0.848827, loss_mean_cls: 0.043987, grad_norm: 0.528267
[[34m2025-10-04 12:29:46[0m] Step: 6397, Training Logs: loss_final: 0.898247, loss_mean: 0.854366, loss_mean_cls: 0.043882, grad_norm: 0.286374
[[34m2025-10-04 12:29:47[0m] Step: 6398, Training Logs: loss_final: 0.894518, loss_mean: 0.850381, loss_mean_cls: 0.044137, grad_norm: 0.519780
[[34m2025-10-04 12:29:47[0m] Step: 6399, Training Logs: loss_final: 0.883734, loss_mean: 0.839669, loss_mean_cls: 0.044065, grad_norm: 0.257205
[[34m2025-10-04 12:29:47[0m] Step: 6400, Training Logs: loss_final: 0.872898, loss_mean: 0.827599, loss_mean_cls: 0.045299, grad_norm: 0.519117
[[34m2025-10-04 12:29:47[0m] Step: 6401, Training Logs: loss_final: 0.887811, loss_mean: 0.843471, loss_mean_cls: 0.044340, grad_norm: 0.302643
[[34m2025-10-04 12:29:48[0m] Step: 6402, Training Logs: loss_final: 0.853238, loss_mean: 0.809107, loss_mean_cls: 0.044131, grad_norm: 0.426658
[[34m2025-10-04 12:29:48[0m] Step: 6403, Training Logs: loss_final: 0.879541, loss_mean: 0.835599, loss_mean_cls: 0.043943, grad_norm: 0.382753
[[34m2025-10-04 12:29:48[0m] Step: 6404, Training Logs: loss_final: 0.899380, loss_mean: 0.856623, loss_mean_cls: 0.042757, grad_norm: 0.392821
[[34m2025-10-04 12:29:49[0m] Step: 6405, Training Logs: loss_final: 0.886143, loss_mean: 0.841379, loss_mean_cls: 0.044764, grad_norm: 0.318763
[[34m2025-10-04 12:29:49[0m] Step: 6406, Training Logs: loss_final: 0.887924, loss_mean: 0.842828, loss_mean_cls: 0.045096, grad_norm: 0.277330
[[34m2025-10-04 12:29:49[0m] Step: 6407, Training Logs: loss_final: 0.875569, loss_mean: 0.830995, loss_mean_cls: 0.044575, grad_norm: 0.401239
[[34m2025-10-04 12:29:50[0m] Step: 6408, Training Logs: loss_final: 0.885817, loss_mean: 0.841670, loss_mean_cls: 0.044147, grad_norm: 0.323334
[[34m2025-10-04 12:29:50[0m] Step: 6409, Training Logs: loss_final: 0.876289, loss_mean: 0.832761, loss_mean_cls: 0.043529, grad_norm: 0.288601
[[34m2025-10-04 12:29:50[0m] Step: 6410, Training Logs: loss_final: 0.891764, loss_mean: 0.848379, loss_mean_cls: 0.043386, grad_norm: 0.422334
[[34m2025-10-04 12:29:50[0m] Step: 6411, Training Logs: loss_final: 0.878502, loss_mean: 0.832890, loss_mean_cls: 0.045613, grad_norm: 0.332973
[[34m2025-10-04 12:29:51[0m] Step: 6412, Training Logs: loss_final: 0.875447, loss_mean: 0.830895, loss_mean_cls: 0.044552, grad_norm: 0.244069
[[34m2025-10-04 12:29:51[0m] Step: 6413, Training Logs: loss_final: 0.874712, loss_mean: 0.831111, loss_mean_cls: 0.043601, grad_norm: 0.264899
[[34m2025-10-04 12:29:51[0m] Step: 6414, Training Logs: loss_final: 0.882053, loss_mean: 0.838796, loss_mean_cls: 0.043256, grad_norm: 0.381747
[[34m2025-10-04 12:29:52[0m] Step: 6415, Training Logs: loss_final: 0.878305, loss_mean: 0.834117, loss_mean_cls: 0.044188, grad_norm: 0.293922
[[34m2025-10-04 12:29:52[0m] Step: 6416, Training Logs: loss_final: 0.901758, loss_mean: 0.859426, loss_mean_cls: 0.042332, grad_norm: 0.244124
[[34m2025-10-04 12:29:52[0m] Step: 6417, Training Logs: loss_final: 0.887566, loss_mean: 0.843680, loss_mean_cls: 0.043885, grad_norm: 0.308548
[[34m2025-10-04 12:29:52[0m] Step: 6418, Training Logs: loss_final: 0.882951, loss_mean: 0.839378, loss_mean_cls: 0.043573, grad_norm: 0.295610
[[34m2025-10-04 12:29:53[0m] Step: 6419, Training Logs: loss_final: 0.872090, loss_mean: 0.827967, loss_mean_cls: 0.044123, grad_norm: 0.250414
[[34m2025-10-04 12:29:53[0m] Step: 6420, Training Logs: loss_final: 0.891568, loss_mean: 0.846920, loss_mean_cls: 0.044648, grad_norm: 0.370146
[[34m2025-10-04 12:29:53[0m] Step: 6421, Training Logs: loss_final: 0.872441, loss_mean: 0.827198, loss_mean_cls: 0.045243, grad_norm: 0.309719
[[34m2025-10-04 12:29:54[0m] Step: 6422, Training Logs: loss_final: 0.880639, loss_mean: 0.837838, loss_mean_cls: 0.042801, grad_norm: 0.308408
[[34m2025-10-04 12:29:54[0m] Step: 6423, Training Logs: loss_final: 0.881439, loss_mean: 0.837916, loss_mean_cls: 0.043523, grad_norm: 0.356836
[[34m2025-10-04 12:29:54[0m] Step: 6424, Training Logs: loss_final: 0.878084, loss_mean: 0.834492, loss_mean_cls: 0.043592, grad_norm: 0.418935
[[34m2025-10-04 12:29:54[0m] Step: 6425, Training Logs: loss_final: 0.902604, loss_mean: 0.857499, loss_mean_cls: 0.045105, grad_norm: 0.452471
[[34m2025-10-04 12:29:55[0m] Step: 6426, Training Logs: loss_final: 0.898803, loss_mean: 0.854469, loss_mean_cls: 0.044334, grad_norm: 0.371070
[[34m2025-10-04 12:29:55[0m] Step: 6427, Training Logs: loss_final: 0.882421, loss_mean: 0.838705, loss_mean_cls: 0.043717, grad_norm: 0.462851
[[34m2025-10-04 12:29:55[0m] Step: 6428, Training Logs: loss_final: 0.903776, loss_mean: 0.859946, loss_mean_cls: 0.043831, grad_norm: 0.309757
[[34m2025-10-04 12:29:56[0m] Step: 6429, Training Logs: loss_final: 0.888241, loss_mean: 0.844288, loss_mean_cls: 0.043953, grad_norm: 0.422924
[[34m2025-10-04 12:29:56[0m] Step: 6430, Training Logs: loss_final: 0.897218, loss_mean: 0.853077, loss_mean_cls: 0.044141, grad_norm: 0.350974
[[34m2025-10-04 12:29:56[0m] Step: 6431, Training Logs: loss_final: 0.871176, loss_mean: 0.826694, loss_mean_cls: 0.044482, grad_norm: 0.496532
[[34m2025-10-04 12:29:57[0m] Step: 6432, Training Logs: loss_final: 0.886443, loss_mean: 0.843200, loss_mean_cls: 0.043242, grad_norm: 0.393117
[[34m2025-10-04 12:29:57[0m] Step: 6433, Training Logs: loss_final: 0.887866, loss_mean: 0.844267, loss_mean_cls: 0.043599, grad_norm: 0.360891
[[34m2025-10-04 12:29:57[0m] Step: 6434, Training Logs: loss_final: 0.872727, loss_mean: 0.826789, loss_mean_cls: 0.045938, grad_norm: 0.376172
[[34m2025-10-04 12:29:57[0m] Step: 6435, Training Logs: loss_final: 0.883294, loss_mean: 0.837262, loss_mean_cls: 0.046032, grad_norm: 0.548889
[[34m2025-10-04 12:29:58[0m] Step: 6436, Training Logs: loss_final: 0.886047, loss_mean: 0.842498, loss_mean_cls: 0.043549, grad_norm: 0.581354
[[34m2025-10-04 12:29:58[0m] Step: 6437, Training Logs: loss_final: 0.876688, loss_mean: 0.832005, loss_mean_cls: 0.044683, grad_norm: 0.417913
[[34m2025-10-04 12:29:58[0m] Step: 6438, Training Logs: loss_final: 0.896310, loss_mean: 0.852458, loss_mean_cls: 0.043852, grad_norm: 0.684278
[[34m2025-10-04 12:29:59[0m] Step: 6439, Training Logs: loss_final: 0.870348, loss_mean: 0.825912, loss_mean_cls: 0.044436, grad_norm: 0.316186
[[34m2025-10-04 12:29:59[0m] Step: 6440, Training Logs: loss_final: 0.892248, loss_mean: 0.848863, loss_mean_cls: 0.043386, grad_norm: 0.742120
[[34m2025-10-04 12:29:59[0m] Step: 6441, Training Logs: loss_final: 0.918093, loss_mean: 0.874091, loss_mean_cls: 0.044002, grad_norm: 0.321222
[[34m2025-10-04 12:29:59[0m] Step: 6442, Training Logs: loss_final: 0.898360, loss_mean: 0.854923, loss_mean_cls: 0.043436, grad_norm: 0.459982
[[34m2025-10-04 12:30:00[0m] Step: 6443, Training Logs: loss_final: 0.867489, loss_mean: 0.823317, loss_mean_cls: 0.044172, grad_norm: 0.292999
[[34m2025-10-04 12:30:00[0m] Step: 6444, Training Logs: loss_final: 0.886949, loss_mean: 0.843426, loss_mean_cls: 0.043524, grad_norm: 0.531431
[[34m2025-10-04 12:30:00[0m] Step: 6445, Training Logs: loss_final: 0.873439, loss_mean: 0.828460, loss_mean_cls: 0.044979, grad_norm: 0.351233
[[34m2025-10-04 12:30:01[0m] Step: 6446, Training Logs: loss_final: 0.875766, loss_mean: 0.831007, loss_mean_cls: 0.044759, grad_norm: 0.345293
[[34m2025-10-04 12:30:01[0m] Step: 6447, Training Logs: loss_final: 0.882806, loss_mean: 0.838842, loss_mean_cls: 0.043964, grad_norm: 0.222115
[[34m2025-10-04 12:30:01[0m] Step: 6448, Training Logs: loss_final: 0.903899, loss_mean: 0.860851, loss_mean_cls: 0.043048, grad_norm: 0.323730
[[34m2025-10-04 12:30:02[0m] Step: 6449, Training Logs: loss_final: 0.874205, loss_mean: 0.829155, loss_mean_cls: 0.045050, grad_norm: 0.454682
[[34m2025-10-04 12:30:02[0m] Step: 6450, Training Logs: loss_final: 0.874632, loss_mean: 0.831145, loss_mean_cls: 0.043488, grad_norm: 0.220309
[[34m2025-10-04 12:30:02[0m] Step: 6451, Training Logs: loss_final: 0.888651, loss_mean: 0.843489, loss_mean_cls: 0.045162, grad_norm: 0.338979
[[34m2025-10-04 12:30:02[0m] Step: 6452, Training Logs: loss_final: 0.872256, loss_mean: 0.828599, loss_mean_cls: 0.043657, grad_norm: 0.315754
[[34m2025-10-04 12:30:03[0m] Step: 6453, Training Logs: loss_final: 0.887656, loss_mean: 0.843205, loss_mean_cls: 0.044451, grad_norm: 0.324484
[[34m2025-10-04 12:30:03[0m] Step: 6454, Training Logs: loss_final: 0.876076, loss_mean: 0.831933, loss_mean_cls: 0.044143, grad_norm: 0.213814
[[34m2025-10-04 12:30:03[0m] Step: 6455, Training Logs: loss_final: 0.888717, loss_mean: 0.845494, loss_mean_cls: 0.043223, grad_norm: 0.306259
[[34m2025-10-04 12:30:04[0m] Step: 6456, Training Logs: loss_final: 0.908029, loss_mean: 0.863230, loss_mean_cls: 0.044799, grad_norm: 0.339859
[[34m2025-10-04 12:30:04[0m] Step: 6457, Training Logs: loss_final: 0.871547, loss_mean: 0.827849, loss_mean_cls: 0.043698, grad_norm: 0.281072
[[34m2025-10-04 12:30:04[0m] Step: 6458, Training Logs: loss_final: 0.895194, loss_mean: 0.851185, loss_mean_cls: 0.044009, grad_norm: 0.223418
[[34m2025-10-04 12:30:04[0m] Step: 6459, Training Logs: loss_final: 0.881393, loss_mean: 0.838240, loss_mean_cls: 0.043153, grad_norm: 0.328514
[[34m2025-10-04 12:30:05[0m] Step: 6460, Training Logs: loss_final: 0.866833, loss_mean: 0.822496, loss_mean_cls: 0.044337, grad_norm: 0.271606
[[34m2025-10-04 12:30:05[0m] Step: 6461, Training Logs: loss_final: 0.861868, loss_mean: 0.817164, loss_mean_cls: 0.044704, grad_norm: 0.390754
[[34m2025-10-04 12:30:05[0m] Step: 6462, Training Logs: loss_final: 0.893430, loss_mean: 0.849373, loss_mean_cls: 0.044057, grad_norm: 0.312354
[[34m2025-10-04 12:30:06[0m] Step: 6463, Training Logs: loss_final: 0.874447, loss_mean: 0.829731, loss_mean_cls: 0.044716, grad_norm: 0.371940
[[34m2025-10-04 12:30:06[0m] Step: 6464, Training Logs: loss_final: 0.888188, loss_mean: 0.844692, loss_mean_cls: 0.043496, grad_norm: 0.357061
[[34m2025-10-04 12:30:06[0m] Step: 6465, Training Logs: loss_final: 0.878497, loss_mean: 0.835748, loss_mean_cls: 0.042749, grad_norm: 0.370213
[[34m2025-10-04 12:30:07[0m] Step: 6466, Training Logs: loss_final: 0.856428, loss_mean: 0.812761, loss_mean_cls: 0.043667, grad_norm: 0.242721
[[34m2025-10-04 12:30:07[0m] Step: 6467, Training Logs: loss_final: 0.876132, loss_mean: 0.832787, loss_mean_cls: 0.043345, grad_norm: 0.229209
[[34m2025-10-04 12:30:07[0m] Step: 6468, Training Logs: loss_final: 0.898889, loss_mean: 0.853255, loss_mean_cls: 0.045634, grad_norm: 0.220938
[[34m2025-10-04 12:30:07[0m] Step: 6469, Training Logs: loss_final: 0.921766, loss_mean: 0.878166, loss_mean_cls: 0.043600, grad_norm: 0.323191
[[34m2025-10-04 12:30:08[0m] Step: 6470, Training Logs: loss_final: 0.909465, loss_mean: 0.865401, loss_mean_cls: 0.044064, grad_norm: 0.319453
[[34m2025-10-04 12:30:08[0m] Step: 6471, Training Logs: loss_final: 0.860364, loss_mean: 0.816655, loss_mean_cls: 0.043709, grad_norm: 0.254060
[[34m2025-10-04 12:30:08[0m] Step: 6472, Training Logs: loss_final: 0.891163, loss_mean: 0.846735, loss_mean_cls: 0.044428, grad_norm: 0.318503
[[34m2025-10-04 12:30:09[0m] Step: 6473, Training Logs: loss_final: 0.889049, loss_mean: 0.845225, loss_mean_cls: 0.043825, grad_norm: 0.249136
[[34m2025-10-04 12:30:09[0m] Step: 6474, Training Logs: loss_final: 0.894414, loss_mean: 0.851195, loss_mean_cls: 0.043220, grad_norm: 0.205426
[[34m2025-10-04 12:30:09[0m] Step: 6475, Training Logs: loss_final: 0.877482, loss_mean: 0.834257, loss_mean_cls: 0.043226, grad_norm: 0.302194
[[34m2025-10-04 12:30:09[0m] Step: 6476, Training Logs: loss_final: 0.900385, loss_mean: 0.856402, loss_mean_cls: 0.043983, grad_norm: 0.301263
[[34m2025-10-04 12:30:10[0m] Step: 6477, Training Logs: loss_final: 0.890442, loss_mean: 0.847192, loss_mean_cls: 0.043250, grad_norm: 0.354998
[[34m2025-10-04 12:30:10[0m] Step: 6478, Training Logs: loss_final: 0.860989, loss_mean: 0.815664, loss_mean_cls: 0.045325, grad_norm: 0.350198
[[34m2025-10-04 12:30:10[0m] Step: 6479, Training Logs: loss_final: 0.888905, loss_mean: 0.844830, loss_mean_cls: 0.044075, grad_norm: 0.265426
[[34m2025-10-04 12:30:11[0m] Step: 6480, Training Logs: loss_final: 0.873132, loss_mean: 0.830614, loss_mean_cls: 0.042517, grad_norm: 0.303803
[[34m2025-10-04 12:30:11[0m] Step: 6481, Training Logs: loss_final: 0.886084, loss_mean: 0.841489, loss_mean_cls: 0.044595, grad_norm: 0.235747
[[34m2025-10-04 12:30:11[0m] Step: 6482, Training Logs: loss_final: 0.892839, loss_mean: 0.848841, loss_mean_cls: 0.043998, grad_norm: 0.309567
[[34m2025-10-04 12:30:11[0m] Step: 6483, Training Logs: loss_final: 0.873757, loss_mean: 0.829151, loss_mean_cls: 0.044606, grad_norm: 0.335834
[[34m2025-10-04 12:30:12[0m] Step: 6484, Training Logs: loss_final: 0.885864, loss_mean: 0.842433, loss_mean_cls: 0.043431, grad_norm: 0.245638
[[34m2025-10-04 12:30:12[0m] Step: 6485, Training Logs: loss_final: 0.881450, loss_mean: 0.836138, loss_mean_cls: 0.045312, grad_norm: 0.400021
[[34m2025-10-04 12:30:12[0m] Step: 6486, Training Logs: loss_final: 0.892373, loss_mean: 0.847412, loss_mean_cls: 0.044962, grad_norm: 0.327232
[[34m2025-10-04 12:30:13[0m] Step: 6487, Training Logs: loss_final: 0.888359, loss_mean: 0.844741, loss_mean_cls: 0.043618, grad_norm: 0.254018
[[34m2025-10-04 12:30:13[0m] Step: 6488, Training Logs: loss_final: 0.888719, loss_mean: 0.843774, loss_mean_cls: 0.044945, grad_norm: 0.369505
[[34m2025-10-04 12:30:13[0m] Step: 6489, Training Logs: loss_final: 0.896068, loss_mean: 0.853097, loss_mean_cls: 0.042971, grad_norm: 0.240862
[[34m2025-10-04 12:30:14[0m] Step: 6490, Training Logs: loss_final: 0.888244, loss_mean: 0.844805, loss_mean_cls: 0.043439, grad_norm: 0.426763
[[34m2025-10-04 12:30:14[0m] Step: 6491, Training Logs: loss_final: 0.873025, loss_mean: 0.828668, loss_mean_cls: 0.044357, grad_norm: 0.375356
[[34m2025-10-04 12:30:14[0m] Step: 6492, Training Logs: loss_final: 0.912485, loss_mean: 0.869059, loss_mean_cls: 0.043426, grad_norm: 0.340975
[[34m2025-10-04 12:30:14[0m] Step: 6493, Training Logs: loss_final: 0.891070, loss_mean: 0.845445, loss_mean_cls: 0.045625, grad_norm: 0.347577
[[34m2025-10-04 12:30:15[0m] Step: 6494, Training Logs: loss_final: 0.884336, loss_mean: 0.839102, loss_mean_cls: 0.045234, grad_norm: 0.349547
[[34m2025-10-04 12:30:15[0m] Step: 6495, Training Logs: loss_final: 0.878469, loss_mean: 0.833545, loss_mean_cls: 0.044924, grad_norm: 0.411248
[[34m2025-10-04 12:30:15[0m] Step: 6496, Training Logs: loss_final: 0.885183, loss_mean: 0.841780, loss_mean_cls: 0.043403, grad_norm: 0.331651
[[34m2025-10-04 12:30:16[0m] Step: 6497, Training Logs: loss_final: 0.887477, loss_mean: 0.842955, loss_mean_cls: 0.044523, grad_norm: 0.534970
[[34m2025-10-04 12:30:16[0m] Step: 6498, Training Logs: loss_final: 0.851995, loss_mean: 0.807227, loss_mean_cls: 0.044768, grad_norm: 0.292013
[[34m2025-10-04 12:30:16[0m] Step: 6499, Training Logs: loss_final: 0.892392, loss_mean: 0.848081, loss_mean_cls: 0.044311, grad_norm: 0.367713
[[34m2025-10-04 12:30:16[0m] Step: 6500, Training Logs: loss_final: 0.889977, loss_mean: 0.845802, loss_mean_cls: 0.044175, grad_norm: 0.258017
[[34m2025-10-04 12:30:17[0m] Step: 6501, Training Logs: loss_final: 0.857588, loss_mean: 0.813324, loss_mean_cls: 0.044264, grad_norm: 0.340905
[[34m2025-10-04 12:30:17[0m] Step: 6502, Training Logs: loss_final: 0.898909, loss_mean: 0.854217, loss_mean_cls: 0.044692, grad_norm: 0.396647
[[34m2025-10-04 12:30:17[0m] Step: 6503, Training Logs: loss_final: 0.859098, loss_mean: 0.814032, loss_mean_cls: 0.045065, grad_norm: 0.296318
[[34m2025-10-04 12:30:18[0m] Step: 6504, Training Logs: loss_final: 0.891358, loss_mean: 0.847744, loss_mean_cls: 0.043613, grad_norm: 0.587771
[[34m2025-10-04 12:30:18[0m] Step: 6505, Training Logs: loss_final: 0.884035, loss_mean: 0.838585, loss_mean_cls: 0.045450, grad_norm: 0.232971
[[34m2025-10-04 12:30:18[0m] Step: 6506, Training Logs: loss_final: 0.879202, loss_mean: 0.835867, loss_mean_cls: 0.043334, grad_norm: 0.408624
[[34m2025-10-04 12:30:19[0m] Step: 6507, Training Logs: loss_final: 0.882167, loss_mean: 0.837967, loss_mean_cls: 0.044200, grad_norm: 0.362492
[[34m2025-10-04 12:30:19[0m] Step: 6508, Training Logs: loss_final: 0.884728, loss_mean: 0.840569, loss_mean_cls: 0.044159, grad_norm: 0.205991
[[34m2025-10-04 12:30:19[0m] Step: 6509, Training Logs: loss_final: 0.872436, loss_mean: 0.828426, loss_mean_cls: 0.044010, grad_norm: 0.384773
[[34m2025-10-04 12:30:19[0m] Step: 6510, Training Logs: loss_final: 0.891665, loss_mean: 0.848684, loss_mean_cls: 0.042981, grad_norm: 0.290976
[[34m2025-10-04 12:30:20[0m] Step: 6511, Training Logs: loss_final: 0.866272, loss_mean: 0.821914, loss_mean_cls: 0.044359, grad_norm: 0.319634
[[34m2025-10-04 12:30:20[0m] Step: 6512, Training Logs: loss_final: 0.886614, loss_mean: 0.842864, loss_mean_cls: 0.043751, grad_norm: 0.340109
[[34m2025-10-04 12:30:20[0m] Step: 6513, Training Logs: loss_final: 0.880349, loss_mean: 0.835809, loss_mean_cls: 0.044540, grad_norm: 0.319602
[[34m2025-10-04 12:30:21[0m] Step: 6514, Training Logs: loss_final: 0.870472, loss_mean: 0.826147, loss_mean_cls: 0.044325, grad_norm: 0.343149
[[34m2025-10-04 12:30:21[0m] Step: 6515, Training Logs: loss_final: 0.897467, loss_mean: 0.853450, loss_mean_cls: 0.044017, grad_norm: 0.386883
[[34m2025-10-04 12:30:21[0m] Step: 6516, Training Logs: loss_final: 0.861720, loss_mean: 0.818338, loss_mean_cls: 0.043381, grad_norm: 0.284560
[[34m2025-10-04 12:30:22[0m] Step: 6517, Training Logs: loss_final: 0.886407, loss_mean: 0.842053, loss_mean_cls: 0.044354, grad_norm: 0.383793
[[34m2025-10-04 12:30:22[0m] Step: 6518, Training Logs: loss_final: 0.875475, loss_mean: 0.831647, loss_mean_cls: 0.043828, grad_norm: 0.329102
[[34m2025-10-04 12:30:22[0m] Step: 6519, Training Logs: loss_final: 0.880605, loss_mean: 0.836971, loss_mean_cls: 0.043634, grad_norm: 0.269798
[[34m2025-10-04 12:30:22[0m] Step: 6520, Training Logs: loss_final: 0.879173, loss_mean: 0.834623, loss_mean_cls: 0.044551, grad_norm: 0.213243
[[34m2025-10-04 12:30:23[0m] Step: 6521, Training Logs: loss_final: 0.888807, loss_mean: 0.845617, loss_mean_cls: 0.043190, grad_norm: 0.268123
[[34m2025-10-04 12:30:23[0m] Step: 6522, Training Logs: loss_final: 0.889505, loss_mean: 0.845496, loss_mean_cls: 0.044008, grad_norm: 0.334176
[[34m2025-10-04 12:30:23[0m] Step: 6523, Training Logs: loss_final: 0.887258, loss_mean: 0.843443, loss_mean_cls: 0.043815, grad_norm: 0.255750
[[34m2025-10-04 12:30:24[0m] Step: 6524, Training Logs: loss_final: 0.871402, loss_mean: 0.827960, loss_mean_cls: 0.043442, grad_norm: 0.289497
[[34m2025-10-04 12:30:24[0m] Step: 6525, Training Logs: loss_final: 0.887034, loss_mean: 0.841372, loss_mean_cls: 0.045663, grad_norm: 0.329477
[[34m2025-10-04 12:30:24[0m] Step: 6526, Training Logs: loss_final: 0.890562, loss_mean: 0.848022, loss_mean_cls: 0.042540, grad_norm: 0.405334
[[34m2025-10-04 12:30:24[0m] Step: 6527, Training Logs: loss_final: 0.874593, loss_mean: 0.829962, loss_mean_cls: 0.044631, grad_norm: 0.248997
[[34m2025-10-04 12:30:25[0m] Step: 6528, Training Logs: loss_final: 0.870834, loss_mean: 0.827307, loss_mean_cls: 0.043527, grad_norm: 0.340860
[[34m2025-10-04 12:30:25[0m] Step: 6529, Training Logs: loss_final: 0.871899, loss_mean: 0.828150, loss_mean_cls: 0.043750, grad_norm: 0.193493
[[34m2025-10-04 12:30:25[0m] Step: 6530, Training Logs: loss_final: 0.899495, loss_mean: 0.856040, loss_mean_cls: 0.043455, grad_norm: 0.474545
[[34m2025-10-04 12:30:26[0m] Step: 6531, Training Logs: loss_final: 0.886143, loss_mean: 0.841482, loss_mean_cls: 0.044662, grad_norm: 0.325344
[[34m2025-10-04 12:30:26[0m] Step: 6532, Training Logs: loss_final: 0.882010, loss_mean: 0.838483, loss_mean_cls: 0.043527, grad_norm: 0.339049
[[34m2025-10-04 12:30:26[0m] Step: 6533, Training Logs: loss_final: 0.869257, loss_mean: 0.823408, loss_mean_cls: 0.045849, grad_norm: 0.366351
[[34m2025-10-04 12:30:26[0m] Step: 6534, Training Logs: loss_final: 0.876954, loss_mean: 0.832250, loss_mean_cls: 0.044704, grad_norm: 0.314384
[[34m2025-10-04 12:30:27[0m] Step: 6535, Training Logs: loss_final: 0.890948, loss_mean: 0.847684, loss_mean_cls: 0.043264, grad_norm: 0.402750
[[34m2025-10-04 12:30:27[0m] Step: 6536, Training Logs: loss_final: 0.884898, loss_mean: 0.840637, loss_mean_cls: 0.044261, grad_norm: 0.271152
[[34m2025-10-04 12:30:27[0m] Step: 6537, Training Logs: loss_final: 0.878377, loss_mean: 0.833888, loss_mean_cls: 0.044489, grad_norm: 0.285692
[[34m2025-10-04 12:30:28[0m] Step: 6538, Training Logs: loss_final: 0.867114, loss_mean: 0.823297, loss_mean_cls: 0.043818, grad_norm: 0.288194
[[34m2025-10-04 12:30:28[0m] Step: 6539, Training Logs: loss_final: 0.885061, loss_mean: 0.841039, loss_mean_cls: 0.044022, grad_norm: 0.275130
[[34m2025-10-04 12:30:28[0m] Step: 6540, Training Logs: loss_final: 0.875322, loss_mean: 0.831707, loss_mean_cls: 0.043615, grad_norm: 0.431132
[[34m2025-10-04 12:30:29[0m] Step: 6541, Training Logs: loss_final: 0.876559, loss_mean: 0.831492, loss_mean_cls: 0.045067, grad_norm: 0.333758
[[34m2025-10-04 12:30:29[0m] Step: 6542, Training Logs: loss_final: 0.878111, loss_mean: 0.834373, loss_mean_cls: 0.043739, grad_norm: 0.450325
[[34m2025-10-04 12:30:29[0m] Step: 6543, Training Logs: loss_final: 0.894205, loss_mean: 0.850904, loss_mean_cls: 0.043300, grad_norm: 0.491146
[[34m2025-10-04 12:30:29[0m] Step: 6544, Training Logs: loss_final: 0.871676, loss_mean: 0.828138, loss_mean_cls: 0.043538, grad_norm: 0.222942
[[34m2025-10-04 12:30:30[0m] Step: 6545, Training Logs: loss_final: 0.883476, loss_mean: 0.839376, loss_mean_cls: 0.044100, grad_norm: 0.393282
[[34m2025-10-04 12:30:30[0m] Step: 6546, Training Logs: loss_final: 0.883831, loss_mean: 0.840426, loss_mean_cls: 0.043405, grad_norm: 0.273690
[[34m2025-10-04 12:30:30[0m] Step: 6547, Training Logs: loss_final: 0.883584, loss_mean: 0.839928, loss_mean_cls: 0.043656, grad_norm: 0.357707
[[34m2025-10-04 12:30:31[0m] Step: 6548, Training Logs: loss_final: 0.877558, loss_mean: 0.834103, loss_mean_cls: 0.043455, grad_norm: 0.287140
[[34m2025-10-04 12:30:31[0m] Step: 6549, Training Logs: loss_final: 0.873497, loss_mean: 0.828797, loss_mean_cls: 0.044700, grad_norm: 0.360953
[[34m2025-10-04 12:30:31[0m] Step: 6550, Training Logs: loss_final: 0.879983, loss_mean: 0.836158, loss_mean_cls: 0.043825, grad_norm: 0.444265
[[34m2025-10-04 12:30:31[0m] Step: 6551, Training Logs: loss_final: 0.865840, loss_mean: 0.821846, loss_mean_cls: 0.043994, grad_norm: 0.338505
[[34m2025-10-04 12:30:32[0m] Step: 6552, Training Logs: loss_final: 0.882522, loss_mean: 0.839122, loss_mean_cls: 0.043400, grad_norm: 0.400958
[[34m2025-10-04 12:30:32[0m] Step: 6553, Training Logs: loss_final: 0.890383, loss_mean: 0.845637, loss_mean_cls: 0.044746, grad_norm: 0.367569
[[34m2025-10-04 12:30:32[0m] Step: 6554, Training Logs: loss_final: 0.890141, loss_mean: 0.845704, loss_mean_cls: 0.044437, grad_norm: 0.430935
[[34m2025-10-04 12:30:33[0m] Step: 6555, Training Logs: loss_final: 0.893624, loss_mean: 0.850093, loss_mean_cls: 0.043531, grad_norm: 0.342591
[[34m2025-10-04 12:30:33[0m] Step: 6556, Training Logs: loss_final: 0.880163, loss_mean: 0.835612, loss_mean_cls: 0.044551, grad_norm: 0.420524
[[34m2025-10-04 12:30:33[0m] Step: 6557, Training Logs: loss_final: 0.885984, loss_mean: 0.843330, loss_mean_cls: 0.042654, grad_norm: 0.377866
[[34m2025-10-04 12:30:33[0m] Step: 6558, Training Logs: loss_final: 0.886103, loss_mean: 0.842054, loss_mean_cls: 0.044048, grad_norm: 0.298976
[[34m2025-10-04 12:30:34[0m] Step: 6559, Training Logs: loss_final: 0.887202, loss_mean: 0.843369, loss_mean_cls: 0.043832, grad_norm: 0.329565
[[34m2025-10-04 12:30:34[0m] Step: 6560, Training Logs: loss_final: 0.892442, loss_mean: 0.848454, loss_mean_cls: 0.043989, grad_norm: 0.269219
[[34m2025-10-04 12:30:34[0m] Step: 6561, Training Logs: loss_final: 0.871906, loss_mean: 0.827693, loss_mean_cls: 0.044213, grad_norm: 0.362611
[[34m2025-10-04 12:30:35[0m] Step: 6562, Training Logs: loss_final: 0.882430, loss_mean: 0.838702, loss_mean_cls: 0.043728, grad_norm: 0.337613
[[34m2025-10-04 12:30:35[0m] Step: 6563, Training Logs: loss_final: 0.883155, loss_mean: 0.839622, loss_mean_cls: 0.043533, grad_norm: 0.491785
[[34m2025-10-04 12:30:35[0m] Step: 6564, Training Logs: loss_final: 0.872409, loss_mean: 0.828886, loss_mean_cls: 0.043523, grad_norm: 0.416970
[[34m2025-10-04 12:30:36[0m] Step: 6565, Training Logs: loss_final: 0.886442, loss_mean: 0.842287, loss_mean_cls: 0.044154, grad_norm: 0.538770
[[34m2025-10-04 12:30:36[0m] Step: 6566, Training Logs: loss_final: 0.894493, loss_mean: 0.850353, loss_mean_cls: 0.044140, grad_norm: 0.458453
[[34m2025-10-04 12:30:36[0m] Step: 6567, Training Logs: loss_final: 0.879805, loss_mean: 0.834261, loss_mean_cls: 0.045544, grad_norm: 0.588331
[[34m2025-10-04 12:30:36[0m] Step: 6568, Training Logs: loss_final: 0.876410, loss_mean: 0.833510, loss_mean_cls: 0.042901, grad_norm: 0.286128
[[34m2025-10-04 12:30:37[0m] Step: 6569, Training Logs: loss_final: 0.886531, loss_mean: 0.841738, loss_mean_cls: 0.044792, grad_norm: 0.531021
[[34m2025-10-04 12:30:37[0m] Step: 6570, Training Logs: loss_final: 0.874876, loss_mean: 0.828748, loss_mean_cls: 0.046128, grad_norm: 0.610360
[[34m2025-10-04 12:30:37[0m] Step: 6571, Training Logs: loss_final: 0.878646, loss_mean: 0.835336, loss_mean_cls: 0.043311, grad_norm: 0.552749
[[34m2025-10-04 12:30:38[0m] Step: 6572, Training Logs: loss_final: 0.890563, loss_mean: 0.846406, loss_mean_cls: 0.044156, grad_norm: 0.214490
[[34m2025-10-04 12:30:38[0m] Step: 6573, Training Logs: loss_final: 0.865295, loss_mean: 0.820696, loss_mean_cls: 0.044599, grad_norm: 0.503738
[[34m2025-10-04 12:30:38[0m] Step: 6574, Training Logs: loss_final: 0.888790, loss_mean: 0.844984, loss_mean_cls: 0.043806, grad_norm: 0.439762
[[34m2025-10-04 12:30:38[0m] Step: 6575, Training Logs: loss_final: 0.880822, loss_mean: 0.836490, loss_mean_cls: 0.044332, grad_norm: 0.490237
[[34m2025-10-04 12:30:39[0m] Step: 6576, Training Logs: loss_final: 0.887673, loss_mean: 0.843248, loss_mean_cls: 0.044425, grad_norm: 0.250832
[[34m2025-10-04 12:30:39[0m] Step: 6577, Training Logs: loss_final: 0.903061, loss_mean: 0.860997, loss_mean_cls: 0.042064, grad_norm: 0.408996
[[34m2025-10-04 12:30:39[0m] Step: 6578, Training Logs: loss_final: 0.887302, loss_mean: 0.842154, loss_mean_cls: 0.045148, grad_norm: 0.466431
[[34m2025-10-04 12:30:40[0m] Step: 6579, Training Logs: loss_final: 0.902246, loss_mean: 0.858154, loss_mean_cls: 0.044091, grad_norm: 0.314066
[[34m2025-10-04 12:30:40[0m] Step: 6580, Training Logs: loss_final: 0.884607, loss_mean: 0.840651, loss_mean_cls: 0.043956, grad_norm: 0.266803
[[34m2025-10-04 12:30:40[0m] Step: 6581, Training Logs: loss_final: 0.872492, loss_mean: 0.826864, loss_mean_cls: 0.045629, grad_norm: 0.368748
[[34m2025-10-04 12:30:41[0m] Step: 6582, Training Logs: loss_final: 0.887948, loss_mean: 0.845048, loss_mean_cls: 0.042900, grad_norm: 0.415235
[[34m2025-10-04 12:30:41[0m] Step: 6583, Training Logs: loss_final: 0.889894, loss_mean: 0.846282, loss_mean_cls: 0.043612, grad_norm: 0.331842
[[34m2025-10-04 12:30:41[0m] Step: 6584, Training Logs: loss_final: 0.882829, loss_mean: 0.839156, loss_mean_cls: 0.043674, grad_norm: 0.328843
[[34m2025-10-04 12:30:41[0m] Step: 6585, Training Logs: loss_final: 0.886684, loss_mean: 0.842571, loss_mean_cls: 0.044114, grad_norm: 0.330523
[[34m2025-10-04 12:30:42[0m] Step: 6586, Training Logs: loss_final: 0.870005, loss_mean: 0.824641, loss_mean_cls: 0.045364, grad_norm: 0.366122
[[34m2025-10-04 12:30:42[0m] Step: 6587, Training Logs: loss_final: 0.878497, loss_mean: 0.833872, loss_mean_cls: 0.044624, grad_norm: 0.390385
[[34m2025-10-04 12:30:42[0m] Step: 6588, Training Logs: loss_final: 0.871148, loss_mean: 0.825699, loss_mean_cls: 0.045449, grad_norm: 0.334363
[[34m2025-10-04 12:30:43[0m] Step: 6589, Training Logs: loss_final: 0.878781, loss_mean: 0.835629, loss_mean_cls: 0.043152, grad_norm: 0.354900
[[34m2025-10-04 12:30:43[0m] Step: 6590, Training Logs: loss_final: 0.867291, loss_mean: 0.822705, loss_mean_cls: 0.044586, grad_norm: 0.420580
[[34m2025-10-04 12:30:43[0m] Step: 6591, Training Logs: loss_final: 0.861749, loss_mean: 0.817982, loss_mean_cls: 0.043768, grad_norm: 0.686705
[[34m2025-10-04 12:30:43[0m] Step: 6592, Training Logs: loss_final: 0.879019, loss_mean: 0.835374, loss_mean_cls: 0.043645, grad_norm: 0.291535
[[34m2025-10-04 12:30:44[0m] Step: 6593, Training Logs: loss_final: 0.882368, loss_mean: 0.838124, loss_mean_cls: 0.044245, grad_norm: 0.552096
[[34m2025-10-04 12:30:44[0m] Step: 6594, Training Logs: loss_final: 0.894294, loss_mean: 0.849311, loss_mean_cls: 0.044983, grad_norm: 0.333590
[[34m2025-10-04 12:30:44[0m] Step: 6595, Training Logs: loss_final: 0.859905, loss_mean: 0.816672, loss_mean_cls: 0.043233, grad_norm: 0.410101
[[34m2025-10-04 12:30:45[0m] Step: 6596, Training Logs: loss_final: 0.875614, loss_mean: 0.831572, loss_mean_cls: 0.044042, grad_norm: 0.335480
[[34m2025-10-04 12:30:45[0m] Step: 6597, Training Logs: loss_final: 0.880896, loss_mean: 0.837907, loss_mean_cls: 0.042988, grad_norm: 0.554664
[[34m2025-10-04 12:30:45[0m] Step: 6598, Training Logs: loss_final: 0.885680, loss_mean: 0.841193, loss_mean_cls: 0.044487, grad_norm: 0.286321
[[34m2025-10-04 12:30:46[0m] Step: 6599, Training Logs: loss_final: 0.875994, loss_mean: 0.832841, loss_mean_cls: 0.043154, grad_norm: 0.442163
[[34m2025-10-04 12:30:46[0m] Step: 6600, Training Logs: loss_final: 0.874312, loss_mean: 0.830309, loss_mean_cls: 0.044003, grad_norm: 0.333928
[[34m2025-10-04 12:30:46[0m] Step: 6601, Training Logs: loss_final: 0.877860, loss_mean: 0.834092, loss_mean_cls: 0.043769, grad_norm: 0.494382
[[34m2025-10-04 12:30:46[0m] Step: 6602, Training Logs: loss_final: 0.884488, loss_mean: 0.839309, loss_mean_cls: 0.045179, grad_norm: 0.281755
[[34m2025-10-04 12:30:47[0m] Step: 6603, Training Logs: loss_final: 0.890079, loss_mean: 0.845900, loss_mean_cls: 0.044179, grad_norm: 0.385908
[[34m2025-10-04 12:30:47[0m] Step: 6604, Training Logs: loss_final: 0.883799, loss_mean: 0.839269, loss_mean_cls: 0.044530, grad_norm: 0.334273
[[34m2025-10-04 12:30:47[0m] Step: 6605, Training Logs: loss_final: 0.874587, loss_mean: 0.831746, loss_mean_cls: 0.042841, grad_norm: 0.386927
[[34m2025-10-04 12:30:48[0m] Step: 6606, Training Logs: loss_final: 0.869928, loss_mean: 0.825352, loss_mean_cls: 0.044575, grad_norm: 0.349162
[[34m2025-10-04 12:30:48[0m] Step: 6607, Training Logs: loss_final: 0.881846, loss_mean: 0.838485, loss_mean_cls: 0.043361, grad_norm: 0.263917
[[34m2025-10-04 12:30:48[0m] Step: 6608, Training Logs: loss_final: 0.869655, loss_mean: 0.825867, loss_mean_cls: 0.043788, grad_norm: 0.298474
[[34m2025-10-04 12:30:49[0m] Step: 6609, Training Logs: loss_final: 0.871356, loss_mean: 0.826131, loss_mean_cls: 0.045225, grad_norm: 0.301362
[[34m2025-10-04 12:30:49[0m] Step: 6610, Training Logs: loss_final: 0.883495, loss_mean: 0.838917, loss_mean_cls: 0.044578, grad_norm: 0.303662
[[34m2025-10-04 12:30:49[0m] Step: 6611, Training Logs: loss_final: 0.886205, loss_mean: 0.842683, loss_mean_cls: 0.043521, grad_norm: 0.321996
[[34m2025-10-04 12:30:49[0m] Step: 6612, Training Logs: loss_final: 0.910551, loss_mean: 0.867903, loss_mean_cls: 0.042647, grad_norm: 0.420054
[[34m2025-10-04 12:30:50[0m] Step: 6613, Training Logs: loss_final: 0.884455, loss_mean: 0.840340, loss_mean_cls: 0.044115, grad_norm: 0.224990
[[34m2025-10-04 12:30:50[0m] Step: 6614, Training Logs: loss_final: 0.906897, loss_mean: 0.863417, loss_mean_cls: 0.043480, grad_norm: 0.461110
[[34m2025-10-04 12:30:50[0m] Step: 6615, Training Logs: loss_final: 0.888283, loss_mean: 0.844892, loss_mean_cls: 0.043391, grad_norm: 0.245101
[[34m2025-10-04 12:30:51[0m] Step: 6616, Training Logs: loss_final: 0.909032, loss_mean: 0.866268, loss_mean_cls: 0.042764, grad_norm: 0.416377
[[34m2025-10-04 12:30:51[0m] Step: 6617, Training Logs: loss_final: 0.902693, loss_mean: 0.858538, loss_mean_cls: 0.044155, grad_norm: 0.299080
[[34m2025-10-04 12:30:51[0m] Step: 6618, Training Logs: loss_final: 0.876365, loss_mean: 0.832837, loss_mean_cls: 0.043527, grad_norm: 0.328346
[[34m2025-10-04 12:30:51[0m] Step: 6619, Training Logs: loss_final: 0.858060, loss_mean: 0.814398, loss_mean_cls: 0.043662, grad_norm: 0.291698
[[34m2025-10-04 12:30:52[0m] Step: 6620, Training Logs: loss_final: 0.872579, loss_mean: 0.828854, loss_mean_cls: 0.043725, grad_norm: 0.252162
[[34m2025-10-04 12:30:52[0m] Step: 6621, Training Logs: loss_final: 0.873417, loss_mean: 0.829994, loss_mean_cls: 0.043424, grad_norm: 0.398497
[[34m2025-10-04 12:30:52[0m] Step: 6622, Training Logs: loss_final: 0.860615, loss_mean: 0.815869, loss_mean_cls: 0.044745, grad_norm: 0.283088
[[34m2025-10-04 12:30:53[0m] Step: 6623, Training Logs: loss_final: 0.885797, loss_mean: 0.842427, loss_mean_cls: 0.043370, grad_norm: 0.361257
[[34m2025-10-04 12:30:53[0m] Step: 6624, Training Logs: loss_final: 0.874160, loss_mean: 0.830695, loss_mean_cls: 0.043465, grad_norm: 0.320411
[[34m2025-10-04 12:30:53[0m] Step: 6625, Training Logs: loss_final: 0.879070, loss_mean: 0.834798, loss_mean_cls: 0.044272, grad_norm: 0.270571
[[34m2025-10-04 12:30:53[0m] Step: 6626, Training Logs: loss_final: 0.869046, loss_mean: 0.824030, loss_mean_cls: 0.045017, grad_norm: 0.396830
[[34m2025-10-04 12:30:54[0m] Step: 6627, Training Logs: loss_final: 0.900981, loss_mean: 0.857122, loss_mean_cls: 0.043859, grad_norm: 0.275899
[[34m2025-10-04 12:30:54[0m] Step: 6628, Training Logs: loss_final: 0.882963, loss_mean: 0.838381, loss_mean_cls: 0.044581, grad_norm: 0.228686
[[34m2025-10-04 12:30:54[0m] Step: 6629, Training Logs: loss_final: 0.905582, loss_mean: 0.861488, loss_mean_cls: 0.044093, grad_norm: 0.276117
[[34m2025-10-04 12:30:55[0m] Step: 6630, Training Logs: loss_final: 0.861468, loss_mean: 0.817218, loss_mean_cls: 0.044250, grad_norm: 0.279965
[[34m2025-10-04 12:30:55[0m] Step: 6631, Training Logs: loss_final: 0.870947, loss_mean: 0.826155, loss_mean_cls: 0.044792, grad_norm: 0.269328
[[34m2025-10-04 12:30:55[0m] Step: 6632, Training Logs: loss_final: 0.893432, loss_mean: 0.849868, loss_mean_cls: 0.043565, grad_norm: 0.398440
[[34m2025-10-04 12:30:56[0m] Step: 6633, Training Logs: loss_final: 0.885453, loss_mean: 0.842105, loss_mean_cls: 0.043348, grad_norm: 0.252448
[[34m2025-10-04 12:30:56[0m] Step: 6634, Training Logs: loss_final: 0.886540, loss_mean: 0.842839, loss_mean_cls: 0.043701, grad_norm: 0.321876
[[34m2025-10-04 12:30:56[0m] Step: 6635, Training Logs: loss_final: 0.882812, loss_mean: 0.840459, loss_mean_cls: 0.042353, grad_norm: 0.288123
[[34m2025-10-04 12:30:56[0m] Step: 6636, Training Logs: loss_final: 0.882071, loss_mean: 0.837938, loss_mean_cls: 0.044133, grad_norm: 0.293578
[[34m2025-10-04 12:30:57[0m] Step: 6637, Training Logs: loss_final: 0.879602, loss_mean: 0.834802, loss_mean_cls: 0.044801, grad_norm: 0.227194
[[34m2025-10-04 12:30:57[0m] Step: 6638, Training Logs: loss_final: 0.903930, loss_mean: 0.859801, loss_mean_cls: 0.044129, grad_norm: 0.328201
[[34m2025-10-04 12:30:57[0m] Step: 6639, Training Logs: loss_final: 0.884958, loss_mean: 0.840546, loss_mean_cls: 0.044411, grad_norm: 0.306043
[[34m2025-10-04 12:30:58[0m] Step: 6640, Training Logs: loss_final: 0.883323, loss_mean: 0.839355, loss_mean_cls: 0.043968, grad_norm: 0.208057
[[34m2025-10-04 12:30:58[0m] Step: 6641, Training Logs: loss_final: 0.900048, loss_mean: 0.857013, loss_mean_cls: 0.043034, grad_norm: 0.238269
[[34m2025-10-04 12:30:58[0m] Step: 6642, Training Logs: loss_final: 0.897770, loss_mean: 0.853033, loss_mean_cls: 0.044737, grad_norm: 0.345058
[[34m2025-10-04 12:30:58[0m] Step: 6643, Training Logs: loss_final: 0.884289, loss_mean: 0.841158, loss_mean_cls: 0.043131, grad_norm: 0.350835
[[34m2025-10-04 12:30:59[0m] Step: 6644, Training Logs: loss_final: 0.879318, loss_mean: 0.834788, loss_mean_cls: 0.044530, grad_norm: 0.351712
[[34m2025-10-04 12:30:59[0m] Step: 6645, Training Logs: loss_final: 0.879405, loss_mean: 0.834946, loss_mean_cls: 0.044459, grad_norm: 0.278512
[[34m2025-10-04 12:30:59[0m] Step: 6646, Training Logs: loss_final: 0.891278, loss_mean: 0.847514, loss_mean_cls: 0.043764, grad_norm: 0.334466
[[34m2025-10-04 12:31:00[0m] Step: 6647, Training Logs: loss_final: 0.876077, loss_mean: 0.832334, loss_mean_cls: 0.043743, grad_norm: 0.282785
[[34m2025-10-04 12:31:00[0m] Step: 6648, Training Logs: loss_final: 0.866899, loss_mean: 0.822931, loss_mean_cls: 0.043968, grad_norm: 0.346699
[[34m2025-10-04 12:31:00[0m] Step: 6649, Training Logs: loss_final: 0.861730, loss_mean: 0.817033, loss_mean_cls: 0.044697, grad_norm: 0.276112
[[34m2025-10-04 12:31:01[0m] Step: 6650, Training Logs: loss_final: 0.867885, loss_mean: 0.823180, loss_mean_cls: 0.044706, grad_norm: 0.326078
[[34m2025-10-04 12:31:01[0m] Step: 6651, Training Logs: loss_final: 0.876757, loss_mean: 0.832716, loss_mean_cls: 0.044041, grad_norm: 0.312447
[[34m2025-10-04 12:31:01[0m] Step: 6652, Training Logs: loss_final: 0.891111, loss_mean: 0.846859, loss_mean_cls: 0.044252, grad_norm: 0.300463
[[34m2025-10-04 12:31:01[0m] Step: 6653, Training Logs: loss_final: 0.879443, loss_mean: 0.835155, loss_mean_cls: 0.044288, grad_norm: 0.281259
[[34m2025-10-04 12:31:02[0m] Step: 6654, Training Logs: loss_final: 0.874643, loss_mean: 0.830854, loss_mean_cls: 0.043788, grad_norm: 0.224744
[[34m2025-10-04 12:31:02[0m] Step: 6655, Training Logs: loss_final: 0.869389, loss_mean: 0.823910, loss_mean_cls: 0.045479, grad_norm: 0.236974
[[34m2025-10-04 12:31:02[0m] Step: 6656, Training Logs: loss_final: 0.873166, loss_mean: 0.828234, loss_mean_cls: 0.044931, grad_norm: 0.229793
[[34m2025-10-04 12:31:03[0m] Step: 6657, Training Logs: loss_final: 0.883947, loss_mean: 0.839614, loss_mean_cls: 0.044333, grad_norm: 0.227462
[[34m2025-10-04 12:31:03[0m] Step: 6658, Training Logs: loss_final: 0.895336, loss_mean: 0.852486, loss_mean_cls: 0.042851, grad_norm: 0.220510
[[34m2025-10-04 12:31:03[0m] Step: 6659, Training Logs: loss_final: 0.878786, loss_mean: 0.834809, loss_mean_cls: 0.043977, grad_norm: 0.230716
[[34m2025-10-04 12:31:03[0m] Step: 6660, Training Logs: loss_final: 0.881560, loss_mean: 0.837548, loss_mean_cls: 0.044013, grad_norm: 0.284217
[[34m2025-10-04 12:31:04[0m] Step: 6661, Training Logs: loss_final: 0.870587, loss_mean: 0.826897, loss_mean_cls: 0.043690, grad_norm: 0.359644
[[34m2025-10-04 12:31:04[0m] Step: 6662, Training Logs: loss_final: 0.882252, loss_mean: 0.837594, loss_mean_cls: 0.044657, grad_norm: 0.260795
[[34m2025-10-04 12:31:04[0m] Step: 6663, Training Logs: loss_final: 0.861215, loss_mean: 0.817995, loss_mean_cls: 0.043220, grad_norm: 0.288772
[[34m2025-10-04 12:31:05[0m] Step: 6664, Training Logs: loss_final: 0.873127, loss_mean: 0.829413, loss_mean_cls: 0.043714, grad_norm: 0.324163
[[34m2025-10-04 12:31:05[0m] Step: 6665, Training Logs: loss_final: 0.887758, loss_mean: 0.843851, loss_mean_cls: 0.043907, grad_norm: 0.410183
[[34m2025-10-04 12:31:05[0m] Step: 6666, Training Logs: loss_final: 0.881786, loss_mean: 0.838304, loss_mean_cls: 0.043482, grad_norm: 0.234303
[[34m2025-10-04 12:31:05[0m] Step: 6667, Training Logs: loss_final: 0.880216, loss_mean: 0.835724, loss_mean_cls: 0.044492, grad_norm: 0.316117
[[34m2025-10-04 12:31:06[0m] Step: 6668, Training Logs: loss_final: 0.890161, loss_mean: 0.847218, loss_mean_cls: 0.042944, grad_norm: 0.304876
[[34m2025-10-04 12:31:06[0m] Step: 6669, Training Logs: loss_final: 0.856209, loss_mean: 0.810356, loss_mean_cls: 0.045853, grad_norm: 0.351033
[[34m2025-10-04 12:31:06[0m] Step: 6670, Training Logs: loss_final: 0.893993, loss_mean: 0.849427, loss_mean_cls: 0.044566, grad_norm: 0.253688
[[34m2025-10-04 12:31:07[0m] Step: 6671, Training Logs: loss_final: 0.884762, loss_mean: 0.840035, loss_mean_cls: 0.044727, grad_norm: 0.373809
[[34m2025-10-04 12:31:07[0m] Step: 6672, Training Logs: loss_final: 0.889244, loss_mean: 0.845713, loss_mean_cls: 0.043531, grad_norm: 0.282558
[[34m2025-10-04 12:31:07[0m] Step: 6673, Training Logs: loss_final: 0.891944, loss_mean: 0.847515, loss_mean_cls: 0.044430, grad_norm: 0.303919
[[34m2025-10-04 12:31:08[0m] Step: 6674, Training Logs: loss_final: 0.879599, loss_mean: 0.835917, loss_mean_cls: 0.043682, grad_norm: 0.287818
[[34m2025-10-04 12:31:08[0m] Step: 6675, Training Logs: loss_final: 0.888624, loss_mean: 0.843963, loss_mean_cls: 0.044662, grad_norm: 0.395911
[[34m2025-10-04 12:31:08[0m] Step: 6676, Training Logs: loss_final: 0.877951, loss_mean: 0.834246, loss_mean_cls: 0.043705, grad_norm: 0.255858
[[34m2025-10-04 12:31:08[0m] Step: 6677, Training Logs: loss_final: 0.888872, loss_mean: 0.844749, loss_mean_cls: 0.044123, grad_norm: 0.327875
[[34m2025-10-04 12:31:09[0m] Step: 6678, Training Logs: loss_final: 0.893124, loss_mean: 0.848577, loss_mean_cls: 0.044547, grad_norm: 0.332491
[[34m2025-10-04 12:31:09[0m] Step: 6679, Training Logs: loss_final: 0.898906, loss_mean: 0.854592, loss_mean_cls: 0.044313, grad_norm: 0.346666
[[34m2025-10-04 12:31:09[0m] Step: 6680, Training Logs: loss_final: 0.890798, loss_mean: 0.846252, loss_mean_cls: 0.044546, grad_norm: 0.441212
[[34m2025-10-04 12:31:10[0m] Step: 6681, Training Logs: loss_final: 0.875587, loss_mean: 0.832188, loss_mean_cls: 0.043399, grad_norm: 0.325511
[[34m2025-10-04 12:31:10[0m] Step: 6682, Training Logs: loss_final: 0.883971, loss_mean: 0.840004, loss_mean_cls: 0.043967, grad_norm: 0.547157
[[34m2025-10-04 12:31:10[0m] Step: 6683, Training Logs: loss_final: 0.897529, loss_mean: 0.854562, loss_mean_cls: 0.042966, grad_norm: 0.330750
[[34m2025-10-04 12:31:10[0m] Step: 6684, Training Logs: loss_final: 0.903860, loss_mean: 0.860342, loss_mean_cls: 0.043518, grad_norm: 0.427279
[[34m2025-10-04 12:31:11[0m] Step: 6685, Training Logs: loss_final: 0.861331, loss_mean: 0.817329, loss_mean_cls: 0.044002, grad_norm: 0.284572
[[34m2025-10-04 12:31:11[0m] Step: 6686, Training Logs: loss_final: 0.873498, loss_mean: 0.829413, loss_mean_cls: 0.044085, grad_norm: 0.319496
[[34m2025-10-04 12:31:11[0m] Step: 6687, Training Logs: loss_final: 0.883059, loss_mean: 0.839125, loss_mean_cls: 0.043934, grad_norm: 0.257681
[[34m2025-10-04 12:31:12[0m] Step: 6688, Training Logs: loss_final: 0.906574, loss_mean: 0.862290, loss_mean_cls: 0.044284, grad_norm: 0.359423
[[34m2025-10-04 12:31:12[0m] Step: 6689, Training Logs: loss_final: 0.878555, loss_mean: 0.834688, loss_mean_cls: 0.043867, grad_norm: 0.304319
[[34m2025-10-04 12:31:12[0m] Step: 6690, Training Logs: loss_final: 0.886336, loss_mean: 0.843010, loss_mean_cls: 0.043326, grad_norm: 0.305701
[[34m2025-10-04 12:31:12[0m] Step: 6691, Training Logs: loss_final: 0.887617, loss_mean: 0.843717, loss_mean_cls: 0.043900, grad_norm: 0.344992
[[34m2025-10-04 12:31:13[0m] Step: 6692, Training Logs: loss_final: 0.863206, loss_mean: 0.819752, loss_mean_cls: 0.043454, grad_norm: 0.358818
[[34m2025-10-04 12:31:13[0m] Step: 6693, Training Logs: loss_final: 0.889295, loss_mean: 0.845083, loss_mean_cls: 0.044212, grad_norm: 0.377654
[[34m2025-10-04 12:31:13[0m] Step: 6694, Training Logs: loss_final: 0.861810, loss_mean: 0.817840, loss_mean_cls: 0.043970, grad_norm: 0.290188
[[34m2025-10-04 12:31:14[0m] Step: 6695, Training Logs: loss_final: 0.878392, loss_mean: 0.834748, loss_mean_cls: 0.043645, grad_norm: 0.417268
[[34m2025-10-04 12:31:14[0m] Step: 6696, Training Logs: loss_final: 0.881330, loss_mean: 0.837490, loss_mean_cls: 0.043840, grad_norm: 0.242729
[[34m2025-10-04 12:31:14[0m] Step: 6697, Training Logs: loss_final: 0.905151, loss_mean: 0.862185, loss_mean_cls: 0.042966, grad_norm: 0.328280
[[34m2025-10-04 12:31:15[0m] Step: 6698, Training Logs: loss_final: 0.874515, loss_mean: 0.830261, loss_mean_cls: 0.044254, grad_norm: 0.360817
[[34m2025-10-04 12:31:15[0m] Step: 6699, Training Logs: loss_final: 0.906103, loss_mean: 0.862566, loss_mean_cls: 0.043537, grad_norm: 0.400883
[[34m2025-10-04 12:31:15[0m] Step: 6700, Training Logs: loss_final: 0.890199, loss_mean: 0.846597, loss_mean_cls: 0.043602, grad_norm: 0.316210
[[34m2025-10-04 12:31:15[0m] Step: 6701, Training Logs: loss_final: 0.880476, loss_mean: 0.836202, loss_mean_cls: 0.044273, grad_norm: 0.303209
[[34m2025-10-04 12:31:16[0m] Step: 6702, Training Logs: loss_final: 0.877447, loss_mean: 0.832729, loss_mean_cls: 0.044718, grad_norm: 0.331720
[[34m2025-10-04 12:31:16[0m] Step: 6703, Training Logs: loss_final: 0.864632, loss_mean: 0.819419, loss_mean_cls: 0.045212, grad_norm: 0.297334
[[34m2025-10-04 12:31:16[0m] Step: 6704, Training Logs: loss_final: 0.885565, loss_mean: 0.842119, loss_mean_cls: 0.043446, grad_norm: 0.364626
[[34m2025-10-04 12:31:17[0m] Step: 6705, Training Logs: loss_final: 0.895932, loss_mean: 0.853235, loss_mean_cls: 0.042697, grad_norm: 0.374553
[[34m2025-10-04 12:31:17[0m] Step: 6706, Training Logs: loss_final: 0.872203, loss_mean: 0.827953, loss_mean_cls: 0.044249, grad_norm: 0.324783
[[34m2025-10-04 12:31:17[0m] Step: 6707, Training Logs: loss_final: 0.873002, loss_mean: 0.828233, loss_mean_cls: 0.044769, grad_norm: 0.382886
[[34m2025-10-04 12:31:17[0m] Step: 6708, Training Logs: loss_final: 0.888913, loss_mean: 0.846037, loss_mean_cls: 0.042876, grad_norm: 0.357942
[[34m2025-10-04 12:31:18[0m] Step: 6709, Training Logs: loss_final: 0.881818, loss_mean: 0.837695, loss_mean_cls: 0.044123, grad_norm: 0.342720
[[34m2025-10-04 12:31:18[0m] Step: 6710, Training Logs: loss_final: 0.869367, loss_mean: 0.825797, loss_mean_cls: 0.043570, grad_norm: 0.383196
[[34m2025-10-04 12:31:18[0m] Step: 6711, Training Logs: loss_final: 0.865953, loss_mean: 0.823354, loss_mean_cls: 0.042599, grad_norm: 0.245834
[[34m2025-10-04 12:31:19[0m] Step: 6712, Training Logs: loss_final: 0.883581, loss_mean: 0.838888, loss_mean_cls: 0.044693, grad_norm: 0.438742
[[34m2025-10-04 12:31:19[0m] Step: 6713, Training Logs: loss_final: 0.867946, loss_mean: 0.824095, loss_mean_cls: 0.043851, grad_norm: 0.323670
[[34m2025-10-04 12:31:19[0m] Step: 6714, Training Logs: loss_final: 0.890647, loss_mean: 0.847560, loss_mean_cls: 0.043086, grad_norm: 0.266269
[[34m2025-10-04 12:31:20[0m] Step: 6715, Training Logs: loss_final: 0.875726, loss_mean: 0.831376, loss_mean_cls: 0.044349, grad_norm: 0.331942
[[34m2025-10-04 12:31:20[0m] Step: 6716, Training Logs: loss_final: 0.886117, loss_mean: 0.843700, loss_mean_cls: 0.042417, grad_norm: 0.327842
[[34m2025-10-04 12:31:20[0m] Step: 6717, Training Logs: loss_final: 0.881773, loss_mean: 0.838411, loss_mean_cls: 0.043361, grad_norm: 0.284822
[[34m2025-10-04 12:31:20[0m] Step: 6718, Training Logs: loss_final: 0.866588, loss_mean: 0.822003, loss_mean_cls: 0.044586, grad_norm: 0.266492
[[34m2025-10-04 12:31:21[0m] Step: 6719, Training Logs: loss_final: 0.874134, loss_mean: 0.829854, loss_mean_cls: 0.044280, grad_norm: 0.277280
[[34m2025-10-04 12:31:21[0m] Step: 6720, Training Logs: loss_final: 0.869827, loss_mean: 0.825548, loss_mean_cls: 0.044279, grad_norm: 0.298179
[[34m2025-10-04 12:31:21[0m] Step: 6721, Training Logs: loss_final: 0.900317, loss_mean: 0.855762, loss_mean_cls: 0.044556, grad_norm: 0.242689
[[34m2025-10-04 12:31:22[0m] Step: 6722, Training Logs: loss_final: 0.886346, loss_mean: 0.842431, loss_mean_cls: 0.043915, grad_norm: 0.258270
[[34m2025-10-04 12:31:22[0m] Step: 6723, Training Logs: loss_final: 0.859408, loss_mean: 0.814443, loss_mean_cls: 0.044965, grad_norm: 0.230593
[[34m2025-10-04 12:31:22[0m] Step: 6724, Training Logs: loss_final: 0.857979, loss_mean: 0.813652, loss_mean_cls: 0.044326, grad_norm: 0.288236
[[34m2025-10-04 12:31:22[0m] Step: 6725, Training Logs: loss_final: 0.887447, loss_mean: 0.843879, loss_mean_cls: 0.043567, grad_norm: 0.217312
[[34m2025-10-04 12:31:23[0m] Step: 6726, Training Logs: loss_final: 0.864285, loss_mean: 0.818255, loss_mean_cls: 0.046030, grad_norm: 0.409353
[[34m2025-10-04 12:31:23[0m] Step: 6727, Training Logs: loss_final: 0.881846, loss_mean: 0.838305, loss_mean_cls: 0.043540, grad_norm: 0.237729
[[34m2025-10-04 12:31:23[0m] Step: 6728, Training Logs: loss_final: 0.877620, loss_mean: 0.833477, loss_mean_cls: 0.044143, grad_norm: 0.330832
[[34m2025-10-04 12:31:24[0m] Step: 6729, Training Logs: loss_final: 0.869855, loss_mean: 0.824983, loss_mean_cls: 0.044872, grad_norm: 0.369717
[[34m2025-10-04 12:31:24[0m] Step: 6730, Training Logs: loss_final: 0.889833, loss_mean: 0.845883, loss_mean_cls: 0.043950, grad_norm: 0.303913
[[34m2025-10-04 12:31:24[0m] Step: 6731, Training Logs: loss_final: 0.899885, loss_mean: 0.856687, loss_mean_cls: 0.043199, grad_norm: 0.329536
[[34m2025-10-04 12:31:25[0m] Step: 6732, Training Logs: loss_final: 0.896202, loss_mean: 0.852047, loss_mean_cls: 0.044155, grad_norm: 0.293301
[[34m2025-10-04 12:31:25[0m] Step: 6733, Training Logs: loss_final: 0.882215, loss_mean: 0.837521, loss_mean_cls: 0.044694, grad_norm: 0.265799
[[34m2025-10-04 12:31:25[0m] Step: 6734, Training Logs: loss_final: 0.910396, loss_mean: 0.867312, loss_mean_cls: 0.043084, grad_norm: 0.315346
[[34m2025-10-04 12:31:26[0m] Step: 6735, Training Logs: loss_final: 0.870238, loss_mean: 0.825967, loss_mean_cls: 0.044271, grad_norm: 0.246621
[[34m2025-10-04 12:31:26[0m] Step: 6736, Training Logs: loss_final: 0.870481, loss_mean: 0.825179, loss_mean_cls: 0.045302, grad_norm: 0.327900
[[34m2025-10-04 12:31:26[0m] Step: 6737, Training Logs: loss_final: 0.874927, loss_mean: 0.830887, loss_mean_cls: 0.044040, grad_norm: 0.259007
[[34m2025-10-04 12:31:26[0m] Step: 6738, Training Logs: loss_final: 0.877372, loss_mean: 0.833214, loss_mean_cls: 0.044159, grad_norm: 0.236365
[[34m2025-10-04 12:31:27[0m] Step: 6739, Training Logs: loss_final: 0.878679, loss_mean: 0.834538, loss_mean_cls: 0.044141, grad_norm: 0.369892
[[34m2025-10-04 12:31:27[0m] Step: 6740, Training Logs: loss_final: 0.860738, loss_mean: 0.816365, loss_mean_cls: 0.044373, grad_norm: 0.242090
[[34m2025-10-04 12:31:27[0m] Step: 6741, Training Logs: loss_final: 0.874966, loss_mean: 0.830938, loss_mean_cls: 0.044028, grad_norm: 0.270251
[[34m2025-10-04 12:31:28[0m] Step: 6742, Training Logs: loss_final: 0.884314, loss_mean: 0.839457, loss_mean_cls: 0.044857, grad_norm: 0.256297
[[34m2025-10-04 12:31:28[0m] Step: 6743, Training Logs: loss_final: 0.869680, loss_mean: 0.825005, loss_mean_cls: 0.044675, grad_norm: 0.386371
[[34m2025-10-04 12:31:28[0m] Step: 6744, Training Logs: loss_final: 0.872526, loss_mean: 0.828025, loss_mean_cls: 0.044502, grad_norm: 0.339033
[[34m2025-10-04 12:31:28[0m] Step: 6745, Training Logs: loss_final: 0.893967, loss_mean: 0.849965, loss_mean_cls: 0.044002, grad_norm: 0.329468
[[34m2025-10-04 12:31:29[0m] Step: 6746, Training Logs: loss_final: 0.887100, loss_mean: 0.842237, loss_mean_cls: 0.044862, grad_norm: 0.344811
[[34m2025-10-04 12:31:29[0m] Step: 6747, Training Logs: loss_final: 0.885219, loss_mean: 0.842937, loss_mean_cls: 0.042282, grad_norm: 0.213202
[[34m2025-10-04 12:31:29[0m] Step: 6748, Training Logs: loss_final: 0.879886, loss_mean: 0.835653, loss_mean_cls: 0.044234, grad_norm: 0.289677
[[34m2025-10-04 12:31:30[0m] Step: 6749, Training Logs: loss_final: 0.852046, loss_mean: 0.807715, loss_mean_cls: 0.044330, grad_norm: 0.288990
[[34m2025-10-04 12:31:30[0m] Step: 6750, Training Logs: loss_final: 0.872094, loss_mean: 0.828468, loss_mean_cls: 0.043627, grad_norm: 0.301009
[[34m2025-10-04 12:31:30[0m] Step: 6751, Training Logs: loss_final: 0.902083, loss_mean: 0.859632, loss_mean_cls: 0.042451, grad_norm: 0.249651
[[34m2025-10-04 12:31:31[0m] Step: 6752, Training Logs: loss_final: 0.883549, loss_mean: 0.839391, loss_mean_cls: 0.044159, grad_norm: 0.397132
[[34m2025-10-04 12:31:31[0m] Step: 6753, Training Logs: loss_final: 0.892683, loss_mean: 0.848236, loss_mean_cls: 0.044448, grad_norm: 0.316742
[[34m2025-10-04 12:31:31[0m] Step: 6754, Training Logs: loss_final: 0.872314, loss_mean: 0.827864, loss_mean_cls: 0.044451, grad_norm: 0.341198
[[34m2025-10-04 12:31:32[0m] Step: 6755, Training Logs: loss_final: 0.901988, loss_mean: 0.858036, loss_mean_cls: 0.043953, grad_norm: 0.298744
[[34m2025-10-04 12:31:32[0m] Step: 6756, Training Logs: loss_final: 0.897301, loss_mean: 0.853508, loss_mean_cls: 0.043793, grad_norm: 0.348514
[[34m2025-10-04 12:31:32[0m] Step: 6757, Training Logs: loss_final: 0.895279, loss_mean: 0.850928, loss_mean_cls: 0.044351, grad_norm: 0.360639
[[34m2025-10-04 12:31:32[0m] Step: 6758, Training Logs: loss_final: 0.870637, loss_mean: 0.826900, loss_mean_cls: 0.043737, grad_norm: 0.275540
[[34m2025-10-04 12:31:33[0m] Step: 6759, Training Logs: loss_final: 0.876675, loss_mean: 0.832572, loss_mean_cls: 0.044104, grad_norm: 0.322142
[[34m2025-10-04 12:31:33[0m] Step: 6760, Training Logs: loss_final: 0.889729, loss_mean: 0.846121, loss_mean_cls: 0.043608, grad_norm: 0.326174
[[34m2025-10-04 12:31:33[0m] Step: 6761, Training Logs: loss_final: 0.889138, loss_mean: 0.845268, loss_mean_cls: 0.043870, grad_norm: 0.335820
[[34m2025-10-04 12:31:34[0m] Step: 6762, Training Logs: loss_final: 0.876564, loss_mean: 0.834175, loss_mean_cls: 0.042389, grad_norm: 0.291505
[[34m2025-10-04 12:31:34[0m] Step: 6763, Training Logs: loss_final: 0.891956, loss_mean: 0.848692, loss_mean_cls: 0.043264, grad_norm: 0.407464
[[34m2025-10-04 12:31:34[0m] Step: 6764, Training Logs: loss_final: 0.884210, loss_mean: 0.840339, loss_mean_cls: 0.043872, grad_norm: 0.267847
[[34m2025-10-04 12:31:34[0m] Step: 6765, Training Logs: loss_final: 0.899012, loss_mean: 0.855625, loss_mean_cls: 0.043387, grad_norm: 0.285563
[[34m2025-10-04 12:31:35[0m] Step: 6766, Training Logs: loss_final: 0.863195, loss_mean: 0.818981, loss_mean_cls: 0.044214, grad_norm: 0.324292
[[34m2025-10-04 12:31:35[0m] Step: 6767, Training Logs: loss_final: 0.863152, loss_mean: 0.819299, loss_mean_cls: 0.043854, grad_norm: 0.335880
[[34m2025-10-04 12:31:35[0m] Step: 6768, Training Logs: loss_final: 0.882580, loss_mean: 0.838861, loss_mean_cls: 0.043719, grad_norm: 0.257573
[[34m2025-10-04 12:31:36[0m] Step: 6769, Training Logs: loss_final: 0.874061, loss_mean: 0.828390, loss_mean_cls: 0.045671, grad_norm: 0.354757
[[34m2025-10-04 12:31:36[0m] Step: 6770, Training Logs: loss_final: 0.899723, loss_mean: 0.856776, loss_mean_cls: 0.042946, grad_norm: 0.310879
[[34m2025-10-04 12:31:36[0m] Step: 6771, Training Logs: loss_final: 0.880927, loss_mean: 0.837455, loss_mean_cls: 0.043471, grad_norm: 0.301916
[[34m2025-10-04 12:31:36[0m] Step: 6772, Training Logs: loss_final: 0.869471, loss_mean: 0.825722, loss_mean_cls: 0.043749, grad_norm: 0.396526
[[34m2025-10-04 12:31:37[0m] Step: 6773, Training Logs: loss_final: 0.872711, loss_mean: 0.827399, loss_mean_cls: 0.045312, grad_norm: 0.282217
[[34m2025-10-04 12:31:37[0m] Step: 6774, Training Logs: loss_final: 0.867029, loss_mean: 0.821452, loss_mean_cls: 0.045577, grad_norm: 0.487179
[[34m2025-10-04 12:31:37[0m] Step: 6775, Training Logs: loss_final: 0.877783, loss_mean: 0.833609, loss_mean_cls: 0.044174, grad_norm: 0.370308
[[34m2025-10-04 12:31:38[0m] Step: 6776, Training Logs: loss_final: 0.868610, loss_mean: 0.824804, loss_mean_cls: 0.043806, grad_norm: 0.391767
[[34m2025-10-04 12:31:38[0m] Step: 6777, Training Logs: loss_final: 0.876607, loss_mean: 0.831942, loss_mean_cls: 0.044665, grad_norm: 0.396412
[[34m2025-10-04 12:31:38[0m] Step: 6778, Training Logs: loss_final: 0.877240, loss_mean: 0.834817, loss_mean_cls: 0.042423, grad_norm: 0.480820
[[34m2025-10-04 12:31:39[0m] Step: 6779, Training Logs: loss_final: 0.883698, loss_mean: 0.838818, loss_mean_cls: 0.044880, grad_norm: 0.322489
[[34m2025-10-04 12:31:39[0m] Step: 6780, Training Logs: loss_final: 0.869871, loss_mean: 0.825646, loss_mean_cls: 0.044224, grad_norm: 0.279636
[[34m2025-10-04 12:31:39[0m] Step: 6781, Training Logs: loss_final: 0.898112, loss_mean: 0.854267, loss_mean_cls: 0.043845, grad_norm: 0.429611
[[34m2025-10-04 12:31:39[0m] Step: 6782, Training Logs: loss_final: 0.876916, loss_mean: 0.832304, loss_mean_cls: 0.044612, grad_norm: 0.348587
[[34m2025-10-04 12:31:40[0m] Step: 6783, Training Logs: loss_final: 0.872443, loss_mean: 0.828462, loss_mean_cls: 0.043981, grad_norm: 0.450742
[[34m2025-10-04 12:31:40[0m] Step: 6784, Training Logs: loss_final: 0.864997, loss_mean: 0.819765, loss_mean_cls: 0.045232, grad_norm: 0.307559
[[34m2025-10-04 12:31:40[0m] Step: 6785, Training Logs: loss_final: 0.878946, loss_mean: 0.834560, loss_mean_cls: 0.044386, grad_norm: 0.315299
[[34m2025-10-04 12:31:41[0m] Step: 6786, Training Logs: loss_final: 0.878378, loss_mean: 0.834991, loss_mean_cls: 0.043387, grad_norm: 0.400183
[[34m2025-10-04 12:31:41[0m] Step: 6787, Training Logs: loss_final: 0.883041, loss_mean: 0.839452, loss_mean_cls: 0.043589, grad_norm: 0.244334
[[34m2025-10-04 12:31:41[0m] Step: 6788, Training Logs: loss_final: 0.886125, loss_mean: 0.840994, loss_mean_cls: 0.045131, grad_norm: 0.279052
[[34m2025-10-04 12:31:41[0m] Step: 6789, Training Logs: loss_final: 0.878722, loss_mean: 0.833185, loss_mean_cls: 0.045537, grad_norm: 0.290581
[[34m2025-10-04 12:31:42[0m] Step: 6790, Training Logs: loss_final: 0.868900, loss_mean: 0.824359, loss_mean_cls: 0.044540, grad_norm: 0.199856
[[34m2025-10-04 12:31:42[0m] Step: 6791, Training Logs: loss_final: 0.893989, loss_mean: 0.850580, loss_mean_cls: 0.043410, grad_norm: 0.234356
[[34m2025-10-04 12:31:42[0m] Step: 6792, Training Logs: loss_final: 0.884485, loss_mean: 0.840008, loss_mean_cls: 0.044477, grad_norm: 0.257541
[[34m2025-10-04 12:31:43[0m] Step: 6793, Training Logs: loss_final: 0.889602, loss_mean: 0.844926, loss_mean_cls: 0.044676, grad_norm: 0.259910
[[34m2025-10-04 12:31:43[0m] Step: 6794, Training Logs: loss_final: 0.887115, loss_mean: 0.842800, loss_mean_cls: 0.044316, grad_norm: 0.312842
[[34m2025-10-04 12:31:43[0m] Step: 6795, Training Logs: loss_final: 0.886594, loss_mean: 0.842364, loss_mean_cls: 0.044230, grad_norm: 0.270670
[[34m2025-10-04 12:31:44[0m] Step: 6796, Training Logs: loss_final: 0.894426, loss_mean: 0.850872, loss_mean_cls: 0.043554, grad_norm: 0.229128
[[34m2025-10-04 12:31:44[0m] Step: 6797, Training Logs: loss_final: 0.871603, loss_mean: 0.827237, loss_mean_cls: 0.044366, grad_norm: 0.345019
[[34m2025-10-04 12:31:44[0m] Step: 6798, Training Logs: loss_final: 0.870070, loss_mean: 0.825752, loss_mean_cls: 0.044318, grad_norm: 0.235418
[[34m2025-10-04 12:31:44[0m] Step: 6799, Training Logs: loss_final: 0.879827, loss_mean: 0.835296, loss_mean_cls: 0.044531, grad_norm: 0.346067
[[34m2025-10-04 12:31:45[0m] Step: 6800, Training Logs: loss_final: 0.886904, loss_mean: 0.842662, loss_mean_cls: 0.044242, grad_norm: 0.408983
[[34m2025-10-04 12:31:45[0m] Step: 6801, Training Logs: loss_final: 0.881881, loss_mean: 0.837951, loss_mean_cls: 0.043929, grad_norm: 0.348369
[[34m2025-10-04 12:31:45[0m] Step: 6802, Training Logs: loss_final: 0.865088, loss_mean: 0.821325, loss_mean_cls: 0.043763, grad_norm: 0.332369
[[34m2025-10-04 12:31:46[0m] Step: 6803, Training Logs: loss_final: 0.884379, loss_mean: 0.840102, loss_mean_cls: 0.044278, grad_norm: 0.448873
[[34m2025-10-04 12:31:46[0m] Step: 6804, Training Logs: loss_final: 0.891414, loss_mean: 0.846944, loss_mean_cls: 0.044471, grad_norm: 0.319171
[[34m2025-10-04 12:31:46[0m] Step: 6805, Training Logs: loss_final: 0.906240, loss_mean: 0.862460, loss_mean_cls: 0.043780, grad_norm: 0.274504
[[34m2025-10-04 12:31:46[0m] Step: 6806, Training Logs: loss_final: 0.879634, loss_mean: 0.835634, loss_mean_cls: 0.044000, grad_norm: 0.346643
[[34m2025-10-04 12:31:47[0m] Step: 6807, Training Logs: loss_final: 0.893245, loss_mean: 0.850886, loss_mean_cls: 0.042360, grad_norm: 0.378051
[[34m2025-10-04 12:31:47[0m] Step: 6808, Training Logs: loss_final: 0.891905, loss_mean: 0.848715, loss_mean_cls: 0.043190, grad_norm: 0.475497
[[34m2025-10-04 12:31:47[0m] Step: 6809, Training Logs: loss_final: 0.879466, loss_mean: 0.834425, loss_mean_cls: 0.045041, grad_norm: 0.224632
[[34m2025-10-04 12:31:48[0m] Step: 6810, Training Logs: loss_final: 0.875003, loss_mean: 0.830298, loss_mean_cls: 0.044705, grad_norm: 0.492606
[[34m2025-10-04 12:31:48[0m] Step: 6811, Training Logs: loss_final: 0.901117, loss_mean: 0.858561, loss_mean_cls: 0.042556, grad_norm: 0.506829
[[34m2025-10-04 12:31:48[0m] Step: 6812, Training Logs: loss_final: 0.894417, loss_mean: 0.851187, loss_mean_cls: 0.043229, grad_norm: 0.185748
[[34m2025-10-04 12:31:48[0m] Step: 6813, Training Logs: loss_final: 0.885734, loss_mean: 0.841752, loss_mean_cls: 0.043982, grad_norm: 0.542113
[[34m2025-10-04 12:31:49[0m] Step: 6814, Training Logs: loss_final: 0.891461, loss_mean: 0.848245, loss_mean_cls: 0.043217, grad_norm: 0.351660
[[34m2025-10-04 12:31:49[0m] Step: 6815, Training Logs: loss_final: 0.877087, loss_mean: 0.832574, loss_mean_cls: 0.044513, grad_norm: 0.424916
[[34m2025-10-04 12:31:49[0m] Step: 6816, Training Logs: loss_final: 0.868262, loss_mean: 0.825238, loss_mean_cls: 0.043025, grad_norm: 0.293276
[[34m2025-10-04 12:31:50[0m] Step: 6817, Training Logs: loss_final: 0.884747, loss_mean: 0.841108, loss_mean_cls: 0.043639, grad_norm: 0.495466
[[34m2025-10-04 12:31:50[0m] Step: 6818, Training Logs: loss_final: 0.869027, loss_mean: 0.825369, loss_mean_cls: 0.043657, grad_norm: 0.271005
[[34m2025-10-04 12:31:50[0m] Step: 6819, Training Logs: loss_final: 0.885531, loss_mean: 0.841036, loss_mean_cls: 0.044495, grad_norm: 0.527016
[[34m2025-10-04 12:31:51[0m] Step: 6820, Training Logs: loss_final: 0.891925, loss_mean: 0.848713, loss_mean_cls: 0.043212, grad_norm: 0.482982
[[34m2025-10-04 12:31:51[0m] Step: 6821, Training Logs: loss_final: 0.882457, loss_mean: 0.838372, loss_mean_cls: 0.044085, grad_norm: 0.261637
[[34m2025-10-04 12:31:51[0m] Step: 6822, Training Logs: loss_final: 0.882805, loss_mean: 0.837806, loss_mean_cls: 0.044999, grad_norm: 0.367401
[[34m2025-10-04 12:31:51[0m] Step: 6823, Training Logs: loss_final: 0.872760, loss_mean: 0.827693, loss_mean_cls: 0.045067, grad_norm: 0.310052
[[34m2025-10-04 12:31:52[0m] Step: 6824, Training Logs: loss_final: 0.880250, loss_mean: 0.836074, loss_mean_cls: 0.044176, grad_norm: 0.436707
[[34m2025-10-04 12:31:52[0m] Step: 6825, Training Logs: loss_final: 0.885417, loss_mean: 0.841608, loss_mean_cls: 0.043809, grad_norm: 0.222469
[[34m2025-10-04 12:31:52[0m] Step: 6826, Training Logs: loss_final: 0.882811, loss_mean: 0.839571, loss_mean_cls: 0.043240, grad_norm: 0.265263
[[34m2025-10-04 12:31:53[0m] Step: 6827, Training Logs: loss_final: 0.882124, loss_mean: 0.838316, loss_mean_cls: 0.043807, grad_norm: 0.264459
[[34m2025-10-04 12:31:53[0m] Step: 6828, Training Logs: loss_final: 0.894955, loss_mean: 0.851662, loss_mean_cls: 0.043293, grad_norm: 0.297419
[[34m2025-10-04 12:31:53[0m] Step: 6829, Training Logs: loss_final: 0.880231, loss_mean: 0.835676, loss_mean_cls: 0.044555, grad_norm: 0.286903
[[34m2025-10-04 12:31:53[0m] Step: 6830, Training Logs: loss_final: 0.886217, loss_mean: 0.842185, loss_mean_cls: 0.044032, grad_norm: 0.353496
[[34m2025-10-04 12:31:54[0m] Step: 6831, Training Logs: loss_final: 0.876793, loss_mean: 0.832095, loss_mean_cls: 0.044698, grad_norm: 0.226990
[[34m2025-10-04 12:31:54[0m] Step: 6832, Training Logs: loss_final: 0.879605, loss_mean: 0.835044, loss_mean_cls: 0.044561, grad_norm: 0.345021
[[34m2025-10-04 12:31:54[0m] Step: 6833, Training Logs: loss_final: 0.881140, loss_mean: 0.836919, loss_mean_cls: 0.044220, grad_norm: 0.213360
[[34m2025-10-04 12:31:55[0m] Step: 6834, Training Logs: loss_final: 0.899826, loss_mean: 0.855185, loss_mean_cls: 0.044640, grad_norm: 0.223605
[[34m2025-10-04 12:31:55[0m] Step: 6835, Training Logs: loss_final: 0.884685, loss_mean: 0.841092, loss_mean_cls: 0.043592, grad_norm: 0.271219
[[34m2025-10-04 12:31:55[0m] Step: 6836, Training Logs: loss_final: 0.898376, loss_mean: 0.854687, loss_mean_cls: 0.043689, grad_norm: 0.290221
[[34m2025-10-04 12:31:55[0m] Step: 6837, Training Logs: loss_final: 0.890718, loss_mean: 0.845855, loss_mean_cls: 0.044863, grad_norm: 0.198826
[[34m2025-10-04 12:31:56[0m] Step: 6838, Training Logs: loss_final: 0.862533, loss_mean: 0.818543, loss_mean_cls: 0.043990, grad_norm: 0.306104
[[34m2025-10-04 12:31:56[0m] Step: 6839, Training Logs: loss_final: 0.869761, loss_mean: 0.825168, loss_mean_cls: 0.044592, grad_norm: 0.225954
[[34m2025-10-04 12:31:56[0m] Step: 6840, Training Logs: loss_final: 0.894216, loss_mean: 0.850740, loss_mean_cls: 0.043476, grad_norm: 0.325325
[[34m2025-10-04 12:31:57[0m] Step: 6841, Training Logs: loss_final: 0.875178, loss_mean: 0.831440, loss_mean_cls: 0.043738, grad_norm: 0.247182
[[34m2025-10-04 12:31:57[0m] Step: 6842, Training Logs: loss_final: 0.878009, loss_mean: 0.833095, loss_mean_cls: 0.044914, grad_norm: 0.284257
[[34m2025-10-04 12:31:57[0m] Step: 6843, Training Logs: loss_final: 0.880572, loss_mean: 0.836760, loss_mean_cls: 0.043812, grad_norm: 0.250226
[[34m2025-10-04 12:31:57[0m] Step: 6844, Training Logs: loss_final: 0.883777, loss_mean: 0.839974, loss_mean_cls: 0.043803, grad_norm: 0.214675
[[34m2025-10-04 12:31:58[0m] Step: 6845, Training Logs: loss_final: 0.886047, loss_mean: 0.843334, loss_mean_cls: 0.042713, grad_norm: 0.231501
[[34m2025-10-04 12:31:58[0m] Step: 6846, Training Logs: loss_final: 0.880456, loss_mean: 0.836109, loss_mean_cls: 0.044347, grad_norm: 0.266469
[[34m2025-10-04 12:31:58[0m] Step: 6847, Training Logs: loss_final: 0.911675, loss_mean: 0.869311, loss_mean_cls: 0.042364, grad_norm: 0.261639
[[34m2025-10-04 12:31:59[0m] Step: 6848, Training Logs: loss_final: 0.882711, loss_mean: 0.838351, loss_mean_cls: 0.044360, grad_norm: 0.235759
[[34m2025-10-04 12:31:59[0m] Step: 6849, Training Logs: loss_final: 0.888733, loss_mean: 0.845786, loss_mean_cls: 0.042947, grad_norm: 0.216646
[[34m2025-10-04 12:31:59[0m] Step: 6850, Training Logs: loss_final: 0.874606, loss_mean: 0.830092, loss_mean_cls: 0.044513, grad_norm: 0.258184
[[34m2025-10-04 12:32:00[0m] Step: 6851, Training Logs: loss_final: 0.891200, loss_mean: 0.846928, loss_mean_cls: 0.044272, grad_norm: 0.230666
[[34m2025-10-04 12:32:00[0m] Step: 6852, Training Logs: loss_final: 0.866371, loss_mean: 0.821158, loss_mean_cls: 0.045214, grad_norm: 0.217025
[[34m2025-10-04 12:32:00[0m] Step: 6853, Training Logs: loss_final: 0.869597, loss_mean: 0.826080, loss_mean_cls: 0.043517, grad_norm: 0.259757
[[34m2025-10-04 12:32:00[0m] Step: 6854, Training Logs: loss_final: 0.877429, loss_mean: 0.834479, loss_mean_cls: 0.042949, grad_norm: 0.292291
[[34m2025-10-04 12:32:01[0m] Step: 6855, Training Logs: loss_final: 0.855689, loss_mean: 0.810792, loss_mean_cls: 0.044897, grad_norm: 0.215192
[[34m2025-10-04 12:32:01[0m] Step: 6856, Training Logs: loss_final: 0.865636, loss_mean: 0.821280, loss_mean_cls: 0.044356, grad_norm: 0.322616
[[34m2025-10-04 12:32:01[0m] Step: 6857, Training Logs: loss_final: 0.885842, loss_mean: 0.841968, loss_mean_cls: 0.043874, grad_norm: 0.375637
[[34m2025-10-04 12:32:02[0m] Step: 6858, Training Logs: loss_final: 0.876457, loss_mean: 0.832819, loss_mean_cls: 0.043638, grad_norm: 0.328231
[[34m2025-10-04 12:32:02[0m] Step: 6859, Training Logs: loss_final: 0.877243, loss_mean: 0.834919, loss_mean_cls: 0.042324, grad_norm: 0.267462
[[34m2025-10-04 12:32:02[0m] Step: 6860, Training Logs: loss_final: 0.878726, loss_mean: 0.835968, loss_mean_cls: 0.042758, grad_norm: 0.388044
[[34m2025-10-04 12:32:02[0m] Step: 6861, Training Logs: loss_final: 0.893602, loss_mean: 0.850045, loss_mean_cls: 0.043557, grad_norm: 0.259801
[[34m2025-10-04 12:32:03[0m] Step: 6862, Training Logs: loss_final: 0.861423, loss_mean: 0.817609, loss_mean_cls: 0.043814, grad_norm: 0.313356
[[34m2025-10-04 12:32:03[0m] Step: 6863, Training Logs: loss_final: 0.868523, loss_mean: 0.825002, loss_mean_cls: 0.043521, grad_norm: 0.269417
[[34m2025-10-04 12:32:03[0m] Step: 6864, Training Logs: loss_final: 0.860825, loss_mean: 0.816769, loss_mean_cls: 0.044056, grad_norm: 0.292084
[[34m2025-10-04 12:32:04[0m] Step: 6865, Training Logs: loss_final: 0.862495, loss_mean: 0.818252, loss_mean_cls: 0.044242, grad_norm: 0.347372
[[34m2025-10-04 12:32:04[0m] Step: 6866, Training Logs: loss_final: 0.875416, loss_mean: 0.830504, loss_mean_cls: 0.044912, grad_norm: 0.272962
[[34m2025-10-04 12:32:04[0m] Step: 6867, Training Logs: loss_final: 0.884689, loss_mean: 0.839877, loss_mean_cls: 0.044812, grad_norm: 0.325281
[[34m2025-10-04 12:32:04[0m] Step: 6868, Training Logs: loss_final: 0.889136, loss_mean: 0.845846, loss_mean_cls: 0.043290, grad_norm: 0.275130
[[34m2025-10-04 12:32:05[0m] Step: 6869, Training Logs: loss_final: 0.881645, loss_mean: 0.836906, loss_mean_cls: 0.044739, grad_norm: 0.353432
[[34m2025-10-04 12:32:05[0m] Step: 6870, Training Logs: loss_final: 0.871271, loss_mean: 0.826623, loss_mean_cls: 0.044648, grad_norm: 0.351068
[[34m2025-10-04 12:32:05[0m] Step: 6871, Training Logs: loss_final: 0.874669, loss_mean: 0.830135, loss_mean_cls: 0.044533, grad_norm: 0.253450
[[34m2025-10-04 12:32:06[0m] Step: 6872, Training Logs: loss_final: 0.888922, loss_mean: 0.844768, loss_mean_cls: 0.044154, grad_norm: 0.400472
[[34m2025-10-04 12:32:06[0m] Step: 6873, Training Logs: loss_final: 0.884930, loss_mean: 0.840420, loss_mean_cls: 0.044510, grad_norm: 0.284168
[[34m2025-10-04 12:32:06[0m] Step: 6874, Training Logs: loss_final: 0.872344, loss_mean: 0.827844, loss_mean_cls: 0.044500, grad_norm: 0.217434
[[34m2025-10-04 12:32:07[0m] Step: 6875, Training Logs: loss_final: 0.882215, loss_mean: 0.838522, loss_mean_cls: 0.043693, grad_norm: 0.303764
[[34m2025-10-04 12:32:07[0m] Step: 6876, Training Logs: loss_final: 0.853999, loss_mean: 0.810038, loss_mean_cls: 0.043961, grad_norm: 0.306244
[[34m2025-10-04 12:32:07[0m] Step: 6877, Training Logs: loss_final: 0.876818, loss_mean: 0.833651, loss_mean_cls: 0.043167, grad_norm: 0.277168
[[34m2025-10-04 12:32:07[0m] Step: 6878, Training Logs: loss_final: 0.893663, loss_mean: 0.849423, loss_mean_cls: 0.044240, grad_norm: 0.265635
[[34m2025-10-04 12:32:08[0m] Step: 6879, Training Logs: loss_final: 0.882607, loss_mean: 0.838757, loss_mean_cls: 0.043850, grad_norm: 0.255143
[[34m2025-10-04 12:32:08[0m] Step: 6880, Training Logs: loss_final: 0.885365, loss_mean: 0.841982, loss_mean_cls: 0.043382, grad_norm: 0.289053
[[34m2025-10-04 12:32:08[0m] Step: 6881, Training Logs: loss_final: 0.868311, loss_mean: 0.824813, loss_mean_cls: 0.043497, grad_norm: 0.222977
[[34m2025-10-04 12:32:09[0m] Step: 6882, Training Logs: loss_final: 0.874617, loss_mean: 0.831614, loss_mean_cls: 0.043003, grad_norm: 0.247281
[[34m2025-10-04 12:32:09[0m] Step: 6883, Training Logs: loss_final: 0.882777, loss_mean: 0.839177, loss_mean_cls: 0.043600, grad_norm: 0.241704
[[34m2025-10-04 12:32:09[0m] Step: 6884, Training Logs: loss_final: 0.885319, loss_mean: 0.841692, loss_mean_cls: 0.043627, grad_norm: 0.394629
[[34m2025-10-04 12:32:09[0m] Step: 6885, Training Logs: loss_final: 0.880795, loss_mean: 0.837656, loss_mean_cls: 0.043139, grad_norm: 0.231900
[[34m2025-10-04 12:32:10[0m] Step: 6886, Training Logs: loss_final: 0.872946, loss_mean: 0.828288, loss_mean_cls: 0.044658, grad_norm: 0.406976
[[34m2025-10-04 12:32:10[0m] Step: 6887, Training Logs: loss_final: 0.896239, loss_mean: 0.852267, loss_mean_cls: 0.043972, grad_norm: 0.280676
[[34m2025-10-04 12:32:10[0m] Step: 6888, Training Logs: loss_final: 0.875474, loss_mean: 0.831782, loss_mean_cls: 0.043692, grad_norm: 0.349992
[[34m2025-10-04 12:32:11[0m] Step: 6889, Training Logs: loss_final: 0.884604, loss_mean: 0.840161, loss_mean_cls: 0.044443, grad_norm: 0.293741
[[34m2025-10-04 12:32:11[0m] Step: 6890, Training Logs: loss_final: 0.879446, loss_mean: 0.833713, loss_mean_cls: 0.045733, grad_norm: 0.351200
[[34m2025-10-04 12:32:11[0m] Step: 6891, Training Logs: loss_final: 0.878526, loss_mean: 0.834314, loss_mean_cls: 0.044213, grad_norm: 0.297210
[[34m2025-10-04 12:32:12[0m] Step: 6892, Training Logs: loss_final: 0.870350, loss_mean: 0.826986, loss_mean_cls: 0.043363, grad_norm: 0.257311
[[34m2025-10-04 12:32:12[0m] Step: 6893, Training Logs: loss_final: 0.873808, loss_mean: 0.829061, loss_mean_cls: 0.044747, grad_norm: 0.276575
[[34m2025-10-04 12:32:12[0m] Step: 6894, Training Logs: loss_final: 0.884520, loss_mean: 0.842273, loss_mean_cls: 0.042248, grad_norm: 0.260115
[[34m2025-10-04 12:32:12[0m] Step: 6895, Training Logs: loss_final: 0.870885, loss_mean: 0.827231, loss_mean_cls: 0.043653, grad_norm: 0.265107
[[34m2025-10-04 12:32:13[0m] Step: 6896, Training Logs: loss_final: 0.897128, loss_mean: 0.853865, loss_mean_cls: 0.043263, grad_norm: 0.233612
[[34m2025-10-04 12:32:13[0m] Step: 6897, Training Logs: loss_final: 0.898782, loss_mean: 0.855124, loss_mean_cls: 0.043658, grad_norm: 0.350353
[[34m2025-10-04 12:32:13[0m] Step: 6898, Training Logs: loss_final: 0.893838, loss_mean: 0.849805, loss_mean_cls: 0.044033, grad_norm: 0.259178
[[34m2025-10-04 12:32:14[0m] Step: 6899, Training Logs: loss_final: 0.874801, loss_mean: 0.831157, loss_mean_cls: 0.043644, grad_norm: 0.287875
[[34m2025-10-04 12:32:14[0m] Step: 6900, Training Logs: loss_final: 0.880088, loss_mean: 0.836266, loss_mean_cls: 0.043822, grad_norm: 0.274734
[[34m2025-10-04 12:32:14[0m] Step: 6901, Training Logs: loss_final: 0.869962, loss_mean: 0.825511, loss_mean_cls: 0.044451, grad_norm: 0.230066
[[34m2025-10-04 12:32:14[0m] Step: 6902, Training Logs: loss_final: 0.869640, loss_mean: 0.825912, loss_mean_cls: 0.043729, grad_norm: 0.281020
[[34m2025-10-04 12:32:15[0m] Step: 6903, Training Logs: loss_final: 0.867333, loss_mean: 0.822653, loss_mean_cls: 0.044680, grad_norm: 0.263040
[[34m2025-10-04 12:32:15[0m] Step: 6904, Training Logs: loss_final: 0.901116, loss_mean: 0.858309, loss_mean_cls: 0.042806, grad_norm: 0.387866
[[34m2025-10-04 12:32:15[0m] Step: 6905, Training Logs: loss_final: 0.879080, loss_mean: 0.835814, loss_mean_cls: 0.043266, grad_norm: 0.243453
[[34m2025-10-04 12:32:16[0m] Step: 6906, Training Logs: loss_final: 0.904065, loss_mean: 0.861487, loss_mean_cls: 0.042578, grad_norm: 0.345867
[[34m2025-10-04 12:32:16[0m] Step: 6907, Training Logs: loss_final: 0.872943, loss_mean: 0.828263, loss_mean_cls: 0.044680, grad_norm: 0.395144
[[34m2025-10-04 12:32:16[0m] Step: 6908, Training Logs: loss_final: 0.881052, loss_mean: 0.837230, loss_mean_cls: 0.043822, grad_norm: 0.378638
[[34m2025-10-04 12:32:16[0m] Step: 6909, Training Logs: loss_final: 0.882415, loss_mean: 0.839097, loss_mean_cls: 0.043318, grad_norm: 0.336171
[[34m2025-10-04 12:32:17[0m] Step: 6910, Training Logs: loss_final: 0.883971, loss_mean: 0.839452, loss_mean_cls: 0.044519, grad_norm: 0.345658
[[34m2025-10-04 12:32:17[0m] Step: 6911, Training Logs: loss_final: 0.879794, loss_mean: 0.835515, loss_mean_cls: 0.044279, grad_norm: 0.544520
[[34m2025-10-04 12:32:17[0m] Step: 6912, Training Logs: loss_final: 0.878075, loss_mean: 0.833636, loss_mean_cls: 0.044439, grad_norm: 0.267312
[[34m2025-10-04 12:32:18[0m] Step: 6913, Training Logs: loss_final: 0.881787, loss_mean: 0.837414, loss_mean_cls: 0.044373, grad_norm: 0.408108
[[34m2025-10-04 12:32:18[0m] Step: 6914, Training Logs: loss_final: 0.885458, loss_mean: 0.841031, loss_mean_cls: 0.044427, grad_norm: 0.293808
[[34m2025-10-04 12:32:18[0m] Step: 6915, Training Logs: loss_final: 0.859172, loss_mean: 0.815739, loss_mean_cls: 0.043432, grad_norm: 0.305719
[[34m2025-10-04 12:32:18[0m] Step: 6916, Training Logs: loss_final: 0.860326, loss_mean: 0.816609, loss_mean_cls: 0.043717, grad_norm: 0.272040
[[34m2025-10-04 12:32:19[0m] Step: 6917, Training Logs: loss_final: 0.875562, loss_mean: 0.830918, loss_mean_cls: 0.044644, grad_norm: 0.310238
[[34m2025-10-04 12:32:19[0m] Step: 6918, Training Logs: loss_final: 0.876764, loss_mean: 0.832462, loss_mean_cls: 0.044302, grad_norm: 0.566667
[[34m2025-10-04 12:32:19[0m] Step: 6919, Training Logs: loss_final: 0.871390, loss_mean: 0.826509, loss_mean_cls: 0.044881, grad_norm: 0.231936
[[34m2025-10-04 12:32:20[0m] Step: 6920, Training Logs: loss_final: 0.887514, loss_mean: 0.843207, loss_mean_cls: 0.044307, grad_norm: 0.396331
[[34m2025-10-04 12:32:20[0m] Step: 6921, Training Logs: loss_final: 0.857978, loss_mean: 0.812857, loss_mean_cls: 0.045121, grad_norm: 0.232343
[[34m2025-10-04 12:32:20[0m] Step: 6922, Training Logs: loss_final: 0.876170, loss_mean: 0.832878, loss_mean_cls: 0.043292, grad_norm: 0.296940
[[34m2025-10-04 12:32:21[0m] Step: 6923, Training Logs: loss_final: 0.906106, loss_mean: 0.862087, loss_mean_cls: 0.044020, grad_norm: 0.358372
[[34m2025-10-04 12:32:21[0m] Step: 6924, Training Logs: loss_final: 0.863301, loss_mean: 0.819326, loss_mean_cls: 0.043975, grad_norm: 0.595192
[[34m2025-10-04 12:32:21[0m] Step: 6925, Training Logs: loss_final: 0.890286, loss_mean: 0.845752, loss_mean_cls: 0.044535, grad_norm: 0.295255
[[34m2025-10-04 12:32:21[0m] Step: 6926, Training Logs: loss_final: 0.868959, loss_mean: 0.824848, loss_mean_cls: 0.044111, grad_norm: 0.496085
[[34m2025-10-04 12:32:22[0m] Step: 6927, Training Logs: loss_final: 0.867771, loss_mean: 0.822233, loss_mean_cls: 0.045539, grad_norm: 0.378222
[[34m2025-10-04 12:32:22[0m] Step: 6928, Training Logs: loss_final: 0.888155, loss_mean: 0.845339, loss_mean_cls: 0.042816, grad_norm: 0.368870
[[34m2025-10-04 12:32:22[0m] Step: 6929, Training Logs: loss_final: 0.893399, loss_mean: 0.849068, loss_mean_cls: 0.044330, grad_norm: 0.310345
[[34m2025-10-04 12:32:23[0m] Step: 6930, Training Logs: loss_final: 0.882859, loss_mean: 0.838690, loss_mean_cls: 0.044170, grad_norm: 0.608761
[[34m2025-10-04 12:32:23[0m] Step: 6931, Training Logs: loss_final: 0.872937, loss_mean: 0.829887, loss_mean_cls: 0.043050, grad_norm: 0.523770
[[34m2025-10-04 12:32:23[0m] Step: 6932, Training Logs: loss_final: 0.884998, loss_mean: 0.843021, loss_mean_cls: 0.041977, grad_norm: 0.336375
[[34m2025-10-04 12:32:23[0m] Step: 6933, Training Logs: loss_final: 0.889807, loss_mean: 0.846900, loss_mean_cls: 0.042907, grad_norm: 0.381933
[[34m2025-10-04 12:32:24[0m] Step: 6934, Training Logs: loss_final: 0.875370, loss_mean: 0.831353, loss_mean_cls: 0.044016, grad_norm: 0.410547
[[34m2025-10-04 12:32:24[0m] Step: 6935, Training Logs: loss_final: 0.870707, loss_mean: 0.826423, loss_mean_cls: 0.044284, grad_norm: 0.345403
[[34m2025-10-04 12:32:24[0m] Step: 6936, Training Logs: loss_final: 0.877758, loss_mean: 0.832950, loss_mean_cls: 0.044808, grad_norm: 0.341542
[[34m2025-10-04 12:32:25[0m] Step: 6937, Training Logs: loss_final: 0.866332, loss_mean: 0.822323, loss_mean_cls: 0.044009, grad_norm: 0.501266
[[34m2025-10-04 12:32:25[0m] Step: 6938, Training Logs: loss_final: 0.899718, loss_mean: 0.856004, loss_mean_cls: 0.043714, grad_norm: 0.275146
[[34m2025-10-04 12:32:25[0m] Step: 6939, Training Logs: loss_final: 0.895520, loss_mean: 0.852421, loss_mean_cls: 0.043099, grad_norm: 0.398402
[[34m2025-10-04 12:32:25[0m] Step: 6940, Training Logs: loss_final: 0.882860, loss_mean: 0.839666, loss_mean_cls: 0.043195, grad_norm: 0.336895
[[34m2025-10-04 12:32:26[0m] Step: 6941, Training Logs: loss_final: 0.860335, loss_mean: 0.816494, loss_mean_cls: 0.043841, grad_norm: 0.402199
[[34m2025-10-04 12:32:26[0m] Step: 6942, Training Logs: loss_final: 0.862452, loss_mean: 0.818509, loss_mean_cls: 0.043943, grad_norm: 0.294638
[[34m2025-10-04 12:32:26[0m] Step: 6943, Training Logs: loss_final: 0.885981, loss_mean: 0.842950, loss_mean_cls: 0.043031, grad_norm: 0.299103
[[34m2025-10-04 12:32:27[0m] Step: 6944, Training Logs: loss_final: 0.876854, loss_mean: 0.832352, loss_mean_cls: 0.044502, grad_norm: 0.353700
[[34m2025-10-04 12:32:27[0m] Step: 6945, Training Logs: loss_final: 0.872088, loss_mean: 0.827192, loss_mean_cls: 0.044896, grad_norm: 0.440381
[[34m2025-10-04 12:32:27[0m] Step: 6946, Training Logs: loss_final: 0.880819, loss_mean: 0.836990, loss_mean_cls: 0.043829, grad_norm: 0.209258
[[34m2025-10-04 12:32:27[0m] Step: 6947, Training Logs: loss_final: 0.870408, loss_mean: 0.825694, loss_mean_cls: 0.044714, grad_norm: 0.343543
[[34m2025-10-04 12:32:28[0m] Step: 6948, Training Logs: loss_final: 0.876664, loss_mean: 0.833767, loss_mean_cls: 0.042896, grad_norm: 0.350323
[[34m2025-10-04 12:32:28[0m] Step: 6949, Training Logs: loss_final: 0.869962, loss_mean: 0.825789, loss_mean_cls: 0.044173, grad_norm: 0.223629
[[34m2025-10-04 12:32:28[0m] Step: 6950, Training Logs: loss_final: 0.871236, loss_mean: 0.828163, loss_mean_cls: 0.043073, grad_norm: 0.293087
[[34m2025-10-04 12:32:29[0m] Step: 6951, Training Logs: loss_final: 0.887104, loss_mean: 0.844684, loss_mean_cls: 0.042420, grad_norm: 0.234990
[[34m2025-10-04 12:32:29[0m] Step: 6952, Training Logs: loss_final: 0.879326, loss_mean: 0.836841, loss_mean_cls: 0.042485, grad_norm: 0.266400
[[34m2025-10-04 12:32:29[0m] Step: 6953, Training Logs: loss_final: 0.892564, loss_mean: 0.849338, loss_mean_cls: 0.043226, grad_norm: 0.390881
[[34m2025-10-04 12:32:29[0m] Step: 6954, Training Logs: loss_final: 0.884393, loss_mean: 0.839905, loss_mean_cls: 0.044489, grad_norm: 0.285948
[[34m2025-10-04 12:32:30[0m] Step: 6955, Training Logs: loss_final: 0.871776, loss_mean: 0.826548, loss_mean_cls: 0.045228, grad_norm: 0.385982
[[34m2025-10-04 12:32:30[0m] Step: 6956, Training Logs: loss_final: 0.871960, loss_mean: 0.827342, loss_mean_cls: 0.044618, grad_norm: 0.253427
[[34m2025-10-04 12:32:30[0m] Step: 6957, Training Logs: loss_final: 0.880134, loss_mean: 0.835778, loss_mean_cls: 0.044355, grad_norm: 0.428469
[[34m2025-10-04 12:32:31[0m] Step: 6958, Training Logs: loss_final: 0.889346, loss_mean: 0.846088, loss_mean_cls: 0.043257, grad_norm: 0.243160
[[34m2025-10-04 12:32:31[0m] Step: 6959, Training Logs: loss_final: 0.870170, loss_mean: 0.825040, loss_mean_cls: 0.045129, grad_norm: 0.287925
[[34m2025-10-04 12:32:31[0m] Step: 6960, Training Logs: loss_final: 0.860468, loss_mean: 0.817035, loss_mean_cls: 0.043433, grad_norm: 0.274714
[[34m2025-10-04 12:32:32[0m] Step: 6961, Training Logs: loss_final: 0.896363, loss_mean: 0.853337, loss_mean_cls: 0.043026, grad_norm: 0.353514
[[34m2025-10-04 12:32:32[0m] Step: 6962, Training Logs: loss_final: 0.893243, loss_mean: 0.849642, loss_mean_cls: 0.043601, grad_norm: 0.213530
[[34m2025-10-04 12:32:32[0m] Step: 6963, Training Logs: loss_final: 0.886564, loss_mean: 0.842265, loss_mean_cls: 0.044299, grad_norm: 0.289616
[[34m2025-10-04 12:32:32[0m] Step: 6964, Training Logs: loss_final: 0.891586, loss_mean: 0.847470, loss_mean_cls: 0.044115, grad_norm: 0.314820
[[34m2025-10-04 12:32:33[0m] Step: 6965, Training Logs: loss_final: 0.870385, loss_mean: 0.825945, loss_mean_cls: 0.044441, grad_norm: 0.408978
[[34m2025-10-04 12:32:33[0m] Step: 6966, Training Logs: loss_final: 0.864271, loss_mean: 0.820275, loss_mean_cls: 0.043996, grad_norm: 0.299161
[[34m2025-10-04 12:32:33[0m] Step: 6967, Training Logs: loss_final: 0.885505, loss_mean: 0.841671, loss_mean_cls: 0.043834, grad_norm: 0.249259
[[34m2025-10-04 12:32:34[0m] Step: 6968, Training Logs: loss_final: 0.867745, loss_mean: 0.824257, loss_mean_cls: 0.043488, grad_norm: 0.302651
[[34m2025-10-04 12:32:34[0m] Step: 6969, Training Logs: loss_final: 0.861661, loss_mean: 0.816672, loss_mean_cls: 0.044989, grad_norm: 0.227287
[[34m2025-10-04 12:32:34[0m] Step: 6970, Training Logs: loss_final: 0.879962, loss_mean: 0.835869, loss_mean_cls: 0.044093, grad_norm: 0.218376
[[34m2025-10-04 12:32:34[0m] Step: 6971, Training Logs: loss_final: 0.882244, loss_mean: 0.839147, loss_mean_cls: 0.043097, grad_norm: 0.275214
[[34m2025-10-04 12:32:35[0m] Step: 6972, Training Logs: loss_final: 0.879192, loss_mean: 0.835675, loss_mean_cls: 0.043518, grad_norm: 0.352354
[[34m2025-10-04 12:32:35[0m] Step: 6973, Training Logs: loss_final: 0.865581, loss_mean: 0.821576, loss_mean_cls: 0.044006, grad_norm: 0.233820
[[34m2025-10-04 12:32:35[0m] Step: 6974, Training Logs: loss_final: 0.896702, loss_mean: 0.852073, loss_mean_cls: 0.044629, grad_norm: 0.325810
[[34m2025-10-04 12:32:36[0m] Step: 6975, Training Logs: loss_final: 0.883318, loss_mean: 0.839556, loss_mean_cls: 0.043762, grad_norm: 0.306184
[[34m2025-10-04 12:32:36[0m] Step: 6976, Training Logs: loss_final: 0.862595, loss_mean: 0.817541, loss_mean_cls: 0.045054, grad_norm: 0.324380
[[34m2025-10-04 12:32:36[0m] Step: 6977, Training Logs: loss_final: 0.881027, loss_mean: 0.836408, loss_mean_cls: 0.044618, grad_norm: 0.263802
[[34m2025-10-04 12:32:36[0m] Step: 6978, Training Logs: loss_final: 0.873728, loss_mean: 0.829344, loss_mean_cls: 0.044385, grad_norm: 0.267048
[[34m2025-10-04 12:32:37[0m] Step: 6979, Training Logs: loss_final: 0.889914, loss_mean: 0.846089, loss_mean_cls: 0.043824, grad_norm: 0.296277
[[34m2025-10-04 12:32:37[0m] Step: 6980, Training Logs: loss_final: 0.888078, loss_mean: 0.843607, loss_mean_cls: 0.044471, grad_norm: 0.214087
[[34m2025-10-04 12:32:37[0m] Step: 6981, Training Logs: loss_final: 0.890730, loss_mean: 0.846824, loss_mean_cls: 0.043906, grad_norm: 0.313625
[[34m2025-10-04 12:32:38[0m] Step: 6982, Training Logs: loss_final: 0.894792, loss_mean: 0.850914, loss_mean_cls: 0.043878, grad_norm: 0.323331
[[34m2025-10-04 12:32:38[0m] Step: 6983, Training Logs: loss_final: 0.872890, loss_mean: 0.828468, loss_mean_cls: 0.044422, grad_norm: 0.301146
[[34m2025-10-04 12:32:38[0m] Step: 6984, Training Logs: loss_final: 0.893068, loss_mean: 0.849294, loss_mean_cls: 0.043774, grad_norm: 0.257287
[[34m2025-10-04 12:32:39[0m] Step: 6985, Training Logs: loss_final: 0.871188, loss_mean: 0.826803, loss_mean_cls: 0.044386, grad_norm: 0.355498
[[34m2025-10-04 12:32:39[0m] Step: 6986, Training Logs: loss_final: 0.878900, loss_mean: 0.834721, loss_mean_cls: 0.044179, grad_norm: 0.405144
[[34m2025-10-04 12:32:39[0m] Step: 6987, Training Logs: loss_final: 0.870366, loss_mean: 0.826566, loss_mean_cls: 0.043800, grad_norm: 0.337023
[[34m2025-10-04 12:32:39[0m] Step: 6988, Training Logs: loss_final: 0.869476, loss_mean: 0.825117, loss_mean_cls: 0.044359, grad_norm: 0.471373
[[34m2025-10-04 12:32:40[0m] Step: 6989, Training Logs: loss_final: 0.878643, loss_mean: 0.835945, loss_mean_cls: 0.042698, grad_norm: 0.223095
[[34m2025-10-04 12:32:40[0m] Step: 6990, Training Logs: loss_final: 0.874373, loss_mean: 0.831370, loss_mean_cls: 0.043003, grad_norm: 0.401929
[[34m2025-10-04 12:32:40[0m] Step: 6991, Training Logs: loss_final: 0.882433, loss_mean: 0.839667, loss_mean_cls: 0.042766, grad_norm: 0.348783
[[34m2025-10-04 12:32:41[0m] Step: 6992, Training Logs: loss_final: 0.866865, loss_mean: 0.822663, loss_mean_cls: 0.044202, grad_norm: 0.376127
[[34m2025-10-04 12:32:41[0m] Step: 6993, Training Logs: loss_final: 0.872662, loss_mean: 0.828083, loss_mean_cls: 0.044580, grad_norm: 0.269897
[[34m2025-10-04 12:32:41[0m] Step: 6994, Training Logs: loss_final: 0.899113, loss_mean: 0.856920, loss_mean_cls: 0.042193, grad_norm: 0.586545
[[34m2025-10-04 12:32:41[0m] Step: 6995, Training Logs: loss_final: 0.900180, loss_mean: 0.857470, loss_mean_cls: 0.042711, grad_norm: 0.341335
[[34m2025-10-04 12:32:42[0m] Step: 6996, Training Logs: loss_final: 0.892192, loss_mean: 0.848071, loss_mean_cls: 0.044121, grad_norm: 0.299965
[[34m2025-10-04 12:32:42[0m] Step: 6997, Training Logs: loss_final: 0.875244, loss_mean: 0.830536, loss_mean_cls: 0.044707, grad_norm: 0.410183
[[34m2025-10-04 12:32:42[0m] Step: 6998, Training Logs: loss_final: 0.879434, loss_mean: 0.835010, loss_mean_cls: 0.044424, grad_norm: 0.232603
[[34m2025-10-04 12:32:43[0m] Step: 6999, Training Logs: loss_final: 0.869535, loss_mean: 0.825547, loss_mean_cls: 0.043988, grad_norm: 0.268964
[[34m2025-10-04 12:32:43[0m] Step: 7000, Training Logs: loss_final: 0.904818, loss_mean: 0.862847, loss_mean_cls: 0.041971, grad_norm: 0.267770
[[34m2025-10-04 12:32:43[0m] Step: 7001, Training Logs: loss_final: 0.862480, loss_mean: 0.818864, loss_mean_cls: 0.043616, grad_norm: 0.214908
[[34m2025-10-04 12:32:43[0m] Step: 7002, Training Logs: loss_final: 0.875924, loss_mean: 0.832363, loss_mean_cls: 0.043561, grad_norm: 0.227408
[[34m2025-10-04 12:32:44[0m] Step: 7003, Training Logs: loss_final: 0.876618, loss_mean: 0.832743, loss_mean_cls: 0.043876, grad_norm: 0.303778
[[34m2025-10-04 12:32:44[0m] Step: 7004, Training Logs: loss_final: 0.863402, loss_mean: 0.819085, loss_mean_cls: 0.044317, grad_norm: 0.219960
[[34m2025-10-04 12:32:44[0m] Step: 7005, Training Logs: loss_final: 0.874380, loss_mean: 0.830586, loss_mean_cls: 0.043794, grad_norm: 0.374304
[[34m2025-10-04 12:32:45[0m] Step: 7006, Training Logs: loss_final: 0.875261, loss_mean: 0.831479, loss_mean_cls: 0.043781, grad_norm: 0.241747
[[34m2025-10-04 12:32:45[0m] Step: 7007, Training Logs: loss_final: 0.883388, loss_mean: 0.839629, loss_mean_cls: 0.043759, grad_norm: 0.391269
[[34m2025-10-04 12:32:45[0m] Step: 7008, Training Logs: loss_final: 0.874078, loss_mean: 0.829366, loss_mean_cls: 0.044712, grad_norm: 0.272026
[[34m2025-10-04 12:32:45[0m] Step: 7009, Training Logs: loss_final: 0.890366, loss_mean: 0.846127, loss_mean_cls: 0.044239, grad_norm: 0.446574
[[34m2025-10-04 12:32:46[0m] Step: 7010, Training Logs: loss_final: 0.883386, loss_mean: 0.839249, loss_mean_cls: 0.044137, grad_norm: 0.356073
[[34m2025-10-04 12:32:46[0m] Step: 7011, Training Logs: loss_final: 0.901236, loss_mean: 0.857808, loss_mean_cls: 0.043428, grad_norm: 0.373182
[[34m2025-10-04 12:32:46[0m] Step: 7012, Training Logs: loss_final: 0.889055, loss_mean: 0.846309, loss_mean_cls: 0.042746, grad_norm: 0.405625
[[34m2025-10-04 12:32:47[0m] Step: 7013, Training Logs: loss_final: 0.873207, loss_mean: 0.829435, loss_mean_cls: 0.043772, grad_norm: 0.313155
[[34m2025-10-04 12:32:47[0m] Step: 7014, Training Logs: loss_final: 0.898063, loss_mean: 0.855035, loss_mean_cls: 0.043028, grad_norm: 0.395969
[[34m2025-10-04 12:32:47[0m] Step: 7015, Training Logs: loss_final: 0.878930, loss_mean: 0.835153, loss_mean_cls: 0.043777, grad_norm: 0.230434
[[34m2025-10-04 12:32:47[0m] Step: 7016, Training Logs: loss_final: 0.866845, loss_mean: 0.822514, loss_mean_cls: 0.044331, grad_norm: 0.343050
[[34m2025-10-04 12:32:48[0m] Step: 7017, Training Logs: loss_final: 0.880309, loss_mean: 0.836565, loss_mean_cls: 0.043743, grad_norm: 0.252480
[[34m2025-10-04 12:32:48[0m] Step: 7018, Training Logs: loss_final: 0.884966, loss_mean: 0.842647, loss_mean_cls: 0.042319, grad_norm: 0.422376
[[34m2025-10-04 12:32:48[0m] Step: 7019, Training Logs: loss_final: 0.901779, loss_mean: 0.858328, loss_mean_cls: 0.043452, grad_norm: 0.402009
[[34m2025-10-04 12:32:49[0m] Step: 7020, Training Logs: loss_final: 0.875268, loss_mean: 0.830247, loss_mean_cls: 0.045021, grad_norm: 0.334452
[[34m2025-10-04 12:32:49[0m] Step: 7021, Training Logs: loss_final: 0.873731, loss_mean: 0.829610, loss_mean_cls: 0.044120, grad_norm: 0.397369
[[34m2025-10-04 12:32:49[0m] Step: 7022, Training Logs: loss_final: 0.885651, loss_mean: 0.841895, loss_mean_cls: 0.043755, grad_norm: 0.431839
[[34m2025-10-04 12:32:50[0m] Step: 7023, Training Logs: loss_final: 0.887616, loss_mean: 0.843179, loss_mean_cls: 0.044438, grad_norm: 0.311590
[[34m2025-10-04 12:32:50[0m] Step: 7024, Training Logs: loss_final: 0.866995, loss_mean: 0.823917, loss_mean_cls: 0.043078, grad_norm: 0.245078
[[34m2025-10-04 12:32:50[0m] Step: 7025, Training Logs: loss_final: 0.891336, loss_mean: 0.847094, loss_mean_cls: 0.044242, grad_norm: 0.292742
[[34m2025-10-04 12:32:50[0m] Step: 7026, Training Logs: loss_final: 0.868016, loss_mean: 0.823187, loss_mean_cls: 0.044830, grad_norm: 0.388376
[[34m2025-10-04 12:32:51[0m] Step: 7027, Training Logs: loss_final: 0.898461, loss_mean: 0.854289, loss_mean_cls: 0.044172, grad_norm: 0.363071
[[34m2025-10-04 12:32:51[0m] Step: 7028, Training Logs: loss_final: 0.880912, loss_mean: 0.837561, loss_mean_cls: 0.043351, grad_norm: 0.247042
[[34m2025-10-04 12:32:51[0m] Step: 7029, Training Logs: loss_final: 0.896878, loss_mean: 0.853570, loss_mean_cls: 0.043307, grad_norm: 0.237761
[[34m2025-10-04 12:32:52[0m] Step: 7030, Training Logs: loss_final: 0.852430, loss_mean: 0.807845, loss_mean_cls: 0.044586, grad_norm: 0.402895
[[34m2025-10-04 12:32:52[0m] Step: 7031, Training Logs: loss_final: 0.879142, loss_mean: 0.834635, loss_mean_cls: 0.044506, grad_norm: 0.259898
[[34m2025-10-04 12:32:52[0m] Step: 7032, Training Logs: loss_final: 0.887986, loss_mean: 0.845028, loss_mean_cls: 0.042958, grad_norm: 0.237585
[[34m2025-10-04 12:32:52[0m] Step: 7033, Training Logs: loss_final: 0.882436, loss_mean: 0.838951, loss_mean_cls: 0.043485, grad_norm: 0.220443
[[34m2025-10-04 12:32:53[0m] Step: 7034, Training Logs: loss_final: 0.891208, loss_mean: 0.847123, loss_mean_cls: 0.044085, grad_norm: 0.280174
[[34m2025-10-04 12:32:53[0m] Step: 7035, Training Logs: loss_final: 0.869200, loss_mean: 0.825604, loss_mean_cls: 0.043596, grad_norm: 0.261370
[[34m2025-10-04 12:32:53[0m] Step: 7036, Training Logs: loss_final: 0.889453, loss_mean: 0.845117, loss_mean_cls: 0.044336, grad_norm: 0.334020
[[34m2025-10-04 12:32:54[0m] Step: 7037, Training Logs: loss_final: 0.878116, loss_mean: 0.834120, loss_mean_cls: 0.043996, grad_norm: 0.266853
[[34m2025-10-04 12:32:54[0m] Step: 7038, Training Logs: loss_final: 0.862251, loss_mean: 0.819162, loss_mean_cls: 0.043089, grad_norm: 0.336555
[[34m2025-10-04 12:32:54[0m] Step: 7039, Training Logs: loss_final: 0.883898, loss_mean: 0.839229, loss_mean_cls: 0.044669, grad_norm: 0.244644
[[34m2025-10-04 12:32:54[0m] Step: 7040, Training Logs: loss_final: 0.870288, loss_mean: 0.825723, loss_mean_cls: 0.044565, grad_norm: 0.292560
[[34m2025-10-04 12:32:55[0m] Step: 7041, Training Logs: loss_final: 0.881138, loss_mean: 0.837815, loss_mean_cls: 0.043323, grad_norm: 0.240047
[[34m2025-10-04 12:32:55[0m] Step: 7042, Training Logs: loss_final: 0.857789, loss_mean: 0.811485, loss_mean_cls: 0.046305, grad_norm: 0.406109
[[34m2025-10-04 12:32:55[0m] Step: 7043, Training Logs: loss_final: 0.887770, loss_mean: 0.842450, loss_mean_cls: 0.045320, grad_norm: 0.472494
[[34m2025-10-04 12:32:56[0m] Step: 7044, Training Logs: loss_final: 0.870059, loss_mean: 0.826092, loss_mean_cls: 0.043967, grad_norm: 0.542135
[[34m2025-10-04 12:32:56[0m] Step: 7045, Training Logs: loss_final: 0.896318, loss_mean: 0.852060, loss_mean_cls: 0.044258, grad_norm: 0.327931
[[34m2025-10-04 12:32:56[0m] Step: 7046, Training Logs: loss_final: 0.870063, loss_mean: 0.825633, loss_mean_cls: 0.044429, grad_norm: 0.362267
[[34m2025-10-04 12:32:56[0m] Step: 7047, Training Logs: loss_final: 0.906893, loss_mean: 0.863513, loss_mean_cls: 0.043381, grad_norm: 0.305143
[[34m2025-10-04 12:32:57[0m] Step: 7048, Training Logs: loss_final: 0.869182, loss_mean: 0.825392, loss_mean_cls: 0.043790, grad_norm: 0.455299
[[34m2025-10-04 12:32:57[0m] Step: 7049, Training Logs: loss_final: 0.901937, loss_mean: 0.858855, loss_mean_cls: 0.043082, grad_norm: 0.331916
[[34m2025-10-04 12:32:57[0m] Step: 7050, Training Logs: loss_final: 0.885007, loss_mean: 0.842049, loss_mean_cls: 0.042958, grad_norm: 0.435656
[[34m2025-10-04 12:32:58[0m] Step: 7051, Training Logs: loss_final: 0.873733, loss_mean: 0.828925, loss_mean_cls: 0.044808, grad_norm: 0.285753
[[34m2025-10-04 12:32:58[0m] Step: 7052, Training Logs: loss_final: 0.883770, loss_mean: 0.839033, loss_mean_cls: 0.044736, grad_norm: 0.353579
[[34m2025-10-04 12:32:58[0m] Step: 7053, Training Logs: loss_final: 0.862628, loss_mean: 0.817769, loss_mean_cls: 0.044859, grad_norm: 0.459107
[[34m2025-10-04 12:32:59[0m] Step: 7054, Training Logs: loss_final: 0.863199, loss_mean: 0.818637, loss_mean_cls: 0.044562, grad_norm: 0.431548
[[34m2025-10-04 12:32:59[0m] Step: 7055, Training Logs: loss_final: 0.889513, loss_mean: 0.845339, loss_mean_cls: 0.044174, grad_norm: 0.447776
[[34m2025-10-04 12:32:59[0m] Step: 7056, Training Logs: loss_final: 0.861203, loss_mean: 0.817520, loss_mean_cls: 0.043683, grad_norm: 0.576876
[[34m2025-10-04 12:32:59[0m] Step: 7057, Training Logs: loss_final: 0.884528, loss_mean: 0.840117, loss_mean_cls: 0.044411, grad_norm: 0.366529
[[34m2025-10-04 12:33:00[0m] Step: 7058, Training Logs: loss_final: 0.868246, loss_mean: 0.823842, loss_mean_cls: 0.044404, grad_norm: 0.430128
[[34m2025-10-04 12:33:00[0m] Step: 7059, Training Logs: loss_final: 0.877796, loss_mean: 0.834096, loss_mean_cls: 0.043700, grad_norm: 0.340441
[[34m2025-10-04 12:33:00[0m] Step: 7060, Training Logs: loss_final: 0.879294, loss_mean: 0.836346, loss_mean_cls: 0.042948, grad_norm: 0.440349
[[34m2025-10-04 12:33:01[0m] Step: 7061, Training Logs: loss_final: 0.878865, loss_mean: 0.834657, loss_mean_cls: 0.044208, grad_norm: 0.405038
[[34m2025-10-04 12:33:01[0m] Step: 7062, Training Logs: loss_final: 0.881893, loss_mean: 0.836648, loss_mean_cls: 0.045245, grad_norm: 0.515764
[[34m2025-10-04 12:33:01[0m] Step: 7063, Training Logs: loss_final: 0.878767, loss_mean: 0.834956, loss_mean_cls: 0.043812, grad_norm: 0.256572
[[34m2025-10-04 12:33:02[0m] Step: 7064, Training Logs: loss_final: 0.885983, loss_mean: 0.841583, loss_mean_cls: 0.044400, grad_norm: 0.374813
[[34m2025-10-04 12:33:02[0m] Step: 7065, Training Logs: loss_final: 0.863731, loss_mean: 0.818726, loss_mean_cls: 0.045005, grad_norm: 0.282970
[[34m2025-10-04 12:33:02[0m] Step: 7066, Training Logs: loss_final: 0.885079, loss_mean: 0.840878, loss_mean_cls: 0.044201, grad_norm: 0.281072
[[34m2025-10-04 12:33:02[0m] Step: 7067, Training Logs: loss_final: 0.870806, loss_mean: 0.826383, loss_mean_cls: 0.044423, grad_norm: 0.296622
[[34m2025-10-04 12:33:03[0m] Step: 7068, Training Logs: loss_final: 0.897698, loss_mean: 0.853881, loss_mean_cls: 0.043817, grad_norm: 0.245880
[[34m2025-10-04 12:33:03[0m] Step: 7069, Training Logs: loss_final: 0.874415, loss_mean: 0.830396, loss_mean_cls: 0.044019, grad_norm: 0.373775
[[34m2025-10-04 12:33:03[0m] Step: 7070, Training Logs: loss_final: 0.889845, loss_mean: 0.845720, loss_mean_cls: 0.044126, grad_norm: 0.402336
[[34m2025-10-04 12:33:04[0m] Step: 7071, Training Logs: loss_final: 0.895842, loss_mean: 0.852266, loss_mean_cls: 0.043576, grad_norm: 0.322703
[[34m2025-10-04 12:33:04[0m] Step: 7072, Training Logs: loss_final: 0.863058, loss_mean: 0.817492, loss_mean_cls: 0.045565, grad_norm: 0.286694
[[34m2025-10-04 12:33:04[0m] Step: 7073, Training Logs: loss_final: 0.856940, loss_mean: 0.812770, loss_mean_cls: 0.044170, grad_norm: 0.315702
[[34m2025-10-04 12:33:04[0m] Step: 7074, Training Logs: loss_final: 0.892818, loss_mean: 0.849751, loss_mean_cls: 0.043067, grad_norm: 0.263762
[[34m2025-10-04 12:33:05[0m] Step: 7075, Training Logs: loss_final: 0.883071, loss_mean: 0.839226, loss_mean_cls: 0.043845, grad_norm: 0.390935
[[34m2025-10-04 12:33:05[0m] Step: 7076, Training Logs: loss_final: 0.894661, loss_mean: 0.851027, loss_mean_cls: 0.043634, grad_norm: 0.271198
[[34m2025-10-04 12:33:05[0m] Step: 7077, Training Logs: loss_final: 0.853645, loss_mean: 0.809297, loss_mean_cls: 0.044347, grad_norm: 0.377374
[[34m2025-10-04 12:33:06[0m] Step: 7078, Training Logs: loss_final: 0.874786, loss_mean: 0.830695, loss_mean_cls: 0.044092, grad_norm: 0.275336
[[34m2025-10-04 12:33:06[0m] Step: 7079, Training Logs: loss_final: 0.876865, loss_mean: 0.834006, loss_mean_cls: 0.042859, grad_norm: 0.252168
[[34m2025-10-04 12:33:06[0m] Step: 7080, Training Logs: loss_final: 0.864455, loss_mean: 0.821041, loss_mean_cls: 0.043414, grad_norm: 0.306310
[[34m2025-10-04 12:33:06[0m] Step: 7081, Training Logs: loss_final: 0.878941, loss_mean: 0.834553, loss_mean_cls: 0.044388, grad_norm: 0.253347
[[34m2025-10-04 12:33:07[0m] Step: 7082, Training Logs: loss_final: 0.902844, loss_mean: 0.859750, loss_mean_cls: 0.043093, grad_norm: 0.290736
[[34m2025-10-04 12:33:07[0m] Step: 7083, Training Logs: loss_final: 0.876935, loss_mean: 0.833366, loss_mean_cls: 0.043568, grad_norm: 0.200594
[[34m2025-10-04 12:33:07[0m] Step: 7084, Training Logs: loss_final: 0.883523, loss_mean: 0.839109, loss_mean_cls: 0.044414, grad_norm: 0.230407
[[34m2025-10-04 12:33:08[0m] Step: 7085, Training Logs: loss_final: 0.901638, loss_mean: 0.857907, loss_mean_cls: 0.043731, grad_norm: 0.231668
[[34m2025-10-04 12:33:08[0m] Step: 7086, Training Logs: loss_final: 0.890086, loss_mean: 0.846491, loss_mean_cls: 0.043595, grad_norm: 0.294118
[[34m2025-10-04 12:33:08[0m] Step: 7087, Training Logs: loss_final: 0.867279, loss_mean: 0.822467, loss_mean_cls: 0.044813, grad_norm: 0.233643
[[34m2025-10-04 12:33:08[0m] Step: 7088, Training Logs: loss_final: 0.875450, loss_mean: 0.832253, loss_mean_cls: 0.043197, grad_norm: 0.270519
[[34m2025-10-04 12:33:09[0m] Step: 7089, Training Logs: loss_final: 0.880114, loss_mean: 0.835812, loss_mean_cls: 0.044301, grad_norm: 0.261316
[[34m2025-10-04 12:33:09[0m] Step: 7090, Training Logs: loss_final: 0.887666, loss_mean: 0.844538, loss_mean_cls: 0.043128, grad_norm: 0.286397
[[34m2025-10-04 12:33:09[0m] Step: 7091, Training Logs: loss_final: 0.891316, loss_mean: 0.847799, loss_mean_cls: 0.043518, grad_norm: 0.258744
[[34m2025-10-04 12:33:10[0m] Step: 7092, Training Logs: loss_final: 0.894632, loss_mean: 0.850167, loss_mean_cls: 0.044465, grad_norm: 0.346830
[[34m2025-10-04 12:33:10[0m] Step: 7093, Training Logs: loss_final: 0.873344, loss_mean: 0.829985, loss_mean_cls: 0.043359, grad_norm: 0.230924
[[34m2025-10-04 12:33:10[0m] Step: 7094, Training Logs: loss_final: 0.860039, loss_mean: 0.814109, loss_mean_cls: 0.045930, grad_norm: 0.479309
[[34m2025-10-04 12:33:11[0m] Step: 7095, Training Logs: loss_final: 0.867338, loss_mean: 0.824033, loss_mean_cls: 0.043304, grad_norm: 0.257975
[[34m2025-10-04 12:33:11[0m] Step: 7096, Training Logs: loss_final: 0.903524, loss_mean: 0.859464, loss_mean_cls: 0.044060, grad_norm: 0.451404
[[34m2025-10-04 12:33:11[0m] Step: 7097, Training Logs: loss_final: 0.862780, loss_mean: 0.818920, loss_mean_cls: 0.043861, grad_norm: 0.372264
[[34m2025-10-04 12:33:11[0m] Step: 7098, Training Logs: loss_final: 0.882383, loss_mean: 0.837821, loss_mean_cls: 0.044562, grad_norm: 0.461941
[[34m2025-10-04 12:33:12[0m] Step: 7099, Training Logs: loss_final: 0.867974, loss_mean: 0.823586, loss_mean_cls: 0.044387, grad_norm: 0.286170
[[34m2025-10-04 12:33:12[0m] Step: 7100, Training Logs: loss_final: 0.904477, loss_mean: 0.861465, loss_mean_cls: 0.043012, grad_norm: 0.450005
[[34m2025-10-04 12:33:12[0m] Step: 7101, Training Logs: loss_final: 0.863032, loss_mean: 0.817369, loss_mean_cls: 0.045664, grad_norm: 0.317710
[[34m2025-10-04 12:33:13[0m] Step: 7102, Training Logs: loss_final: 0.889099, loss_mean: 0.845842, loss_mean_cls: 0.043257, grad_norm: 0.402244
[[34m2025-10-04 12:33:13[0m] Step: 7103, Training Logs: loss_final: 0.889162, loss_mean: 0.843774, loss_mean_cls: 0.045388, grad_norm: 0.319322
[[34m2025-10-04 12:33:13[0m] Step: 7104, Training Logs: loss_final: 0.877149, loss_mean: 0.834320, loss_mean_cls: 0.042829, grad_norm: 0.301374
[[34m2025-10-04 12:33:13[0m] Step: 7105, Training Logs: loss_final: 0.882010, loss_mean: 0.839002, loss_mean_cls: 0.043008, grad_norm: 0.250035
[[34m2025-10-04 12:33:14[0m] Step: 7106, Training Logs: loss_final: 0.904120, loss_mean: 0.861556, loss_mean_cls: 0.042563, grad_norm: 0.323298
[[34m2025-10-04 12:33:14[0m] Step: 7107, Training Logs: loss_final: 0.862217, loss_mean: 0.818484, loss_mean_cls: 0.043733, grad_norm: 0.302942
[[34m2025-10-04 12:33:14[0m] Step: 7108, Training Logs: loss_final: 0.876068, loss_mean: 0.831329, loss_mean_cls: 0.044739, grad_norm: 0.450912
[[34m2025-10-04 12:33:15[0m] Step: 7109, Training Logs: loss_final: 0.871778, loss_mean: 0.828222, loss_mean_cls: 0.043556, grad_norm: 0.308413
[[34m2025-10-04 12:33:15[0m] Step: 7110, Training Logs: loss_final: 0.883247, loss_mean: 0.839936, loss_mean_cls: 0.043311, grad_norm: 0.427663
[[34m2025-10-04 12:33:15[0m] Step: 7111, Training Logs: loss_final: 0.878239, loss_mean: 0.834827, loss_mean_cls: 0.043412, grad_norm: 0.382517
[[34m2025-10-04 12:33:15[0m] Step: 7112, Training Logs: loss_final: 0.873263, loss_mean: 0.830015, loss_mean_cls: 0.043247, grad_norm: 0.466951
[[34m2025-10-04 12:33:16[0m] Step: 7113, Training Logs: loss_final: 0.877306, loss_mean: 0.832787, loss_mean_cls: 0.044518, grad_norm: 0.296703
[[34m2025-10-04 12:33:16[0m] Step: 7114, Training Logs: loss_final: 0.901918, loss_mean: 0.859124, loss_mean_cls: 0.042794, grad_norm: 0.341532
[[34m2025-10-04 12:33:16[0m] Step: 7115, Training Logs: loss_final: 0.882335, loss_mean: 0.839552, loss_mean_cls: 0.042784, grad_norm: 0.353399
[[34m2025-10-04 12:33:17[0m] Step: 7116, Training Logs: loss_final: 0.879050, loss_mean: 0.835112, loss_mean_cls: 0.043938, grad_norm: 0.390237
[[34m2025-10-04 12:33:17[0m] Step: 7117, Training Logs: loss_final: 0.881299, loss_mean: 0.837765, loss_mean_cls: 0.043534, grad_norm: 0.324535
[[34m2025-10-04 12:33:17[0m] Step: 7118, Training Logs: loss_final: 0.886912, loss_mean: 0.842748, loss_mean_cls: 0.044164, grad_norm: 0.389575
[[34m2025-10-04 12:33:17[0m] Step: 7119, Training Logs: loss_final: 0.858779, loss_mean: 0.814804, loss_mean_cls: 0.043975, grad_norm: 0.317345
[[34m2025-10-04 12:33:18[0m] Step: 7120, Training Logs: loss_final: 0.877700, loss_mean: 0.834482, loss_mean_cls: 0.043217, grad_norm: 0.386683
[[34m2025-10-04 12:33:18[0m] Step: 7121, Training Logs: loss_final: 0.894826, loss_mean: 0.852446, loss_mean_cls: 0.042380, grad_norm: 0.513427
[[34m2025-10-04 12:33:18[0m] Step: 7122, Training Logs: loss_final: 0.893515, loss_mean: 0.849521, loss_mean_cls: 0.043995, grad_norm: 0.232657
[[34m2025-10-04 12:33:19[0m] Step: 7123, Training Logs: loss_final: 0.876259, loss_mean: 0.832968, loss_mean_cls: 0.043292, grad_norm: 0.391726
[[34m2025-10-04 12:33:19[0m] Step: 7124, Training Logs: loss_final: 0.873069, loss_mean: 0.828785, loss_mean_cls: 0.044284, grad_norm: 0.282616
[[34m2025-10-04 12:33:19[0m] Step: 7125, Training Logs: loss_final: 0.864510, loss_mean: 0.820587, loss_mean_cls: 0.043923, grad_norm: 0.470862
[[34m2025-10-04 12:33:20[0m] Step: 7126, Training Logs: loss_final: 0.900338, loss_mean: 0.856943, loss_mean_cls: 0.043395, grad_norm: 0.276035
[[34m2025-10-04 12:33:20[0m] Step: 7127, Training Logs: loss_final: 0.881100, loss_mean: 0.836856, loss_mean_cls: 0.044244, grad_norm: 0.455914
[[34m2025-10-04 12:33:20[0m] Step: 7128, Training Logs: loss_final: 0.876570, loss_mean: 0.833152, loss_mean_cls: 0.043418, grad_norm: 0.194527
[[34m2025-10-04 12:33:20[0m] Step: 7129, Training Logs: loss_final: 0.865563, loss_mean: 0.822515, loss_mean_cls: 0.043047, grad_norm: 0.308019
[[34m2025-10-04 12:33:21[0m] Step: 7130, Training Logs: loss_final: 0.881132, loss_mean: 0.837222, loss_mean_cls: 0.043909, grad_norm: 0.310562
[[34m2025-10-04 12:33:21[0m] Step: 7131, Training Logs: loss_final: 0.901886, loss_mean: 0.859413, loss_mean_cls: 0.042473, grad_norm: 0.330759
[[34m2025-10-04 12:33:21[0m] Step: 7132, Training Logs: loss_final: 0.855028, loss_mean: 0.811556, loss_mean_cls: 0.043472, grad_norm: 0.199341
[[34m2025-10-04 12:33:22[0m] Step: 7133, Training Logs: loss_final: 0.878907, loss_mean: 0.835812, loss_mean_cls: 0.043095, grad_norm: 0.396144
[[34m2025-10-04 12:33:22[0m] Step: 7134, Training Logs: loss_final: 0.883026, loss_mean: 0.840081, loss_mean_cls: 0.042945, grad_norm: 0.269913
[[34m2025-10-04 12:33:22[0m] Step: 7135, Training Logs: loss_final: 0.884222, loss_mean: 0.841094, loss_mean_cls: 0.043128, grad_norm: 0.335341
[[34m2025-10-04 12:33:22[0m] Step: 7136, Training Logs: loss_final: 0.886877, loss_mean: 0.843176, loss_mean_cls: 0.043701, grad_norm: 0.215359
[[34m2025-10-04 12:33:23[0m] Step: 7137, Training Logs: loss_final: 0.918511, loss_mean: 0.876386, loss_mean_cls: 0.042126, grad_norm: 0.373940
[[34m2025-10-04 12:33:23[0m] Step: 7138, Training Logs: loss_final: 0.876552, loss_mean: 0.831923, loss_mean_cls: 0.044629, grad_norm: 0.298302
[[34m2025-10-04 12:33:23[0m] Step: 7139, Training Logs: loss_final: 0.877899, loss_mean: 0.834655, loss_mean_cls: 0.043244, grad_norm: 0.228084
[[34m2025-10-04 12:33:24[0m] Step: 7140, Training Logs: loss_final: 0.886505, loss_mean: 0.841984, loss_mean_cls: 0.044521, grad_norm: 0.224628
[[34m2025-10-04 12:33:24[0m] Step: 7141, Training Logs: loss_final: 0.880289, loss_mean: 0.837121, loss_mean_cls: 0.043168, grad_norm: 0.208944
[[34m2025-10-04 12:33:24[0m] Step: 7142, Training Logs: loss_final: 0.857555, loss_mean: 0.813242, loss_mean_cls: 0.044313, grad_norm: 0.228337
[[34m2025-10-04 12:33:25[0m] Step: 7143, Training Logs: loss_final: 0.871993, loss_mean: 0.827963, loss_mean_cls: 0.044030, grad_norm: 0.215289
[[34m2025-10-04 12:33:25[0m] Step: 7144, Training Logs: loss_final: 0.874646, loss_mean: 0.831248, loss_mean_cls: 0.043399, grad_norm: 0.203254
[[34m2025-10-04 12:33:25[0m] Step: 7145, Training Logs: loss_final: 0.885587, loss_mean: 0.843617, loss_mean_cls: 0.041970, grad_norm: 0.212336
[[34m2025-10-04 12:33:25[0m] Step: 7146, Training Logs: loss_final: 0.878359, loss_mean: 0.835343, loss_mean_cls: 0.043015, grad_norm: 0.245042
[[34m2025-10-04 12:33:26[0m] Step: 7147, Training Logs: loss_final: 0.888558, loss_mean: 0.844525, loss_mean_cls: 0.044033, grad_norm: 0.275859
[[34m2025-10-04 12:33:26[0m] Step: 7148, Training Logs: loss_final: 0.871184, loss_mean: 0.825914, loss_mean_cls: 0.045270, grad_norm: 0.212916
[[34m2025-10-04 12:33:26[0m] Step: 7149, Training Logs: loss_final: 0.874801, loss_mean: 0.829533, loss_mean_cls: 0.045269, grad_norm: 0.212374
[[34m2025-10-04 12:33:27[0m] Step: 7150, Training Logs: loss_final: 0.880501, loss_mean: 0.836485, loss_mean_cls: 0.044016, grad_norm: 0.218247
[[34m2025-10-04 12:33:27[0m] Step: 7151, Training Logs: loss_final: 0.882156, loss_mean: 0.838996, loss_mean_cls: 0.043160, grad_norm: 0.252230
[[34m2025-10-04 12:33:27[0m] Step: 7152, Training Logs: loss_final: 0.867335, loss_mean: 0.823239, loss_mean_cls: 0.044096, grad_norm: 0.256644
[[34m2025-10-04 12:33:27[0m] Step: 7153, Training Logs: loss_final: 0.883122, loss_mean: 0.840069, loss_mean_cls: 0.043054, grad_norm: 0.194434
[[34m2025-10-04 12:33:28[0m] Step: 7154, Training Logs: loss_final: 0.877063, loss_mean: 0.833534, loss_mean_cls: 0.043529, grad_norm: 0.364206
[[34m2025-10-04 12:33:28[0m] Step: 7155, Training Logs: loss_final: 0.879271, loss_mean: 0.835312, loss_mean_cls: 0.043959, grad_norm: 0.306994
[[34m2025-10-04 12:33:28[0m] Step: 7156, Training Logs: loss_final: 0.876446, loss_mean: 0.832450, loss_mean_cls: 0.043995, grad_norm: 0.280074
[[34m2025-10-04 12:33:29[0m] Step: 7157, Training Logs: loss_final: 0.891791, loss_mean: 0.848919, loss_mean_cls: 0.042872, grad_norm: 0.331879
[[34m2025-10-04 12:33:29[0m] Step: 7158, Training Logs: loss_final: 0.886099, loss_mean: 0.843009, loss_mean_cls: 0.043090, grad_norm: 0.371317
[[34m2025-10-04 12:33:29[0m] Step: 7159, Training Logs: loss_final: 0.873550, loss_mean: 0.830112, loss_mean_cls: 0.043438, grad_norm: 0.365748
[[34m2025-10-04 12:33:29[0m] Step: 7160, Training Logs: loss_final: 0.870071, loss_mean: 0.825797, loss_mean_cls: 0.044274, grad_norm: 0.288974
[[34m2025-10-04 12:33:30[0m] Step: 7161, Training Logs: loss_final: 0.886447, loss_mean: 0.843680, loss_mean_cls: 0.042767, grad_norm: 0.359617
[[34m2025-10-04 12:33:30[0m] Step: 7162, Training Logs: loss_final: 0.852825, loss_mean: 0.807942, loss_mean_cls: 0.044883, grad_norm: 0.297866
[[34m2025-10-04 12:33:30[0m] Step: 7163, Training Logs: loss_final: 0.856475, loss_mean: 0.811965, loss_mean_cls: 0.044510, grad_norm: 0.289090
[[34m2025-10-04 12:33:31[0m] Step: 7164, Training Logs: loss_final: 0.859750, loss_mean: 0.815133, loss_mean_cls: 0.044617, grad_norm: 0.317652
[[34m2025-10-04 12:33:31[0m] Step: 7165, Training Logs: loss_final: 0.884646, loss_mean: 0.840931, loss_mean_cls: 0.043716, grad_norm: 0.321240
[[34m2025-10-04 12:33:31[0m] Step: 7166, Training Logs: loss_final: 0.880340, loss_mean: 0.835876, loss_mean_cls: 0.044463, grad_norm: 0.301677
[[34m2025-10-04 12:33:32[0m] Step: 7167, Training Logs: loss_final: 0.859559, loss_mean: 0.814982, loss_mean_cls: 0.044577, grad_norm: 0.267383
[[34m2025-10-04 12:33:32[0m] Step: 7168, Training Logs: loss_final: 0.872539, loss_mean: 0.826929, loss_mean_cls: 0.045611, grad_norm: 0.276908
[[34m2025-10-04 12:33:32[0m] Step: 7169, Training Logs: loss_final: 0.883968, loss_mean: 0.839471, loss_mean_cls: 0.044497, grad_norm: 0.313490
[[34m2025-10-04 12:33:32[0m] Step: 7170, Training Logs: loss_final: 0.861429, loss_mean: 0.815868, loss_mean_cls: 0.045560, grad_norm: 0.295136
[[34m2025-10-04 12:33:33[0m] Step: 7171, Training Logs: loss_final: 0.883727, loss_mean: 0.840539, loss_mean_cls: 0.043188, grad_norm: 0.235104
[[34m2025-10-04 12:33:33[0m] Step: 7172, Training Logs: loss_final: 0.861465, loss_mean: 0.818147, loss_mean_cls: 0.043318, grad_norm: 0.280832
[[34m2025-10-04 12:33:33[0m] Step: 7173, Training Logs: loss_final: 0.883355, loss_mean: 0.839083, loss_mean_cls: 0.044273, grad_norm: 0.275827
[[34m2025-10-04 12:33:34[0m] Step: 7174, Training Logs: loss_final: 0.881451, loss_mean: 0.837306, loss_mean_cls: 0.044144, grad_norm: 0.344202
[[34m2025-10-04 12:33:34[0m] Step: 7175, Training Logs: loss_final: 0.855192, loss_mean: 0.810434, loss_mean_cls: 0.044758, grad_norm: 0.303708
[[34m2025-10-04 12:33:34[0m] Step: 7176, Training Logs: loss_final: 0.872863, loss_mean: 0.829945, loss_mean_cls: 0.042917, grad_norm: 0.264869
[[34m2025-10-04 12:33:34[0m] Step: 7177, Training Logs: loss_final: 0.882429, loss_mean: 0.838798, loss_mean_cls: 0.043631, grad_norm: 0.327949
[[34m2025-10-04 12:33:35[0m] Step: 7178, Training Logs: loss_final: 0.877454, loss_mean: 0.833233, loss_mean_cls: 0.044221, grad_norm: 0.300891
[[34m2025-10-04 12:33:35[0m] Step: 7179, Training Logs: loss_final: 0.873079, loss_mean: 0.829360, loss_mean_cls: 0.043719, grad_norm: 0.243972
[[34m2025-10-04 12:33:35[0m] Step: 7180, Training Logs: loss_final: 0.880644, loss_mean: 0.836913, loss_mean_cls: 0.043731, grad_norm: 0.253694
[[34m2025-10-04 12:33:36[0m] Step: 7181, Training Logs: loss_final: 0.873327, loss_mean: 0.829027, loss_mean_cls: 0.044300, grad_norm: 0.303864
[[34m2025-10-04 12:33:36[0m] Step: 7182, Training Logs: loss_final: 0.869207, loss_mean: 0.824826, loss_mean_cls: 0.044381, grad_norm: 0.214754
[[34m2025-10-04 12:33:36[0m] Step: 7183, Training Logs: loss_final: 0.882701, loss_mean: 0.839660, loss_mean_cls: 0.043041, grad_norm: 0.264058
[[34m2025-10-04 12:33:36[0m] Step: 7184, Training Logs: loss_final: 0.885895, loss_mean: 0.842903, loss_mean_cls: 0.042991, grad_norm: 0.249637
[[34m2025-10-04 12:33:37[0m] Step: 7185, Training Logs: loss_final: 0.891654, loss_mean: 0.848383, loss_mean_cls: 0.043271, grad_norm: 0.381056
[[34m2025-10-04 12:33:37[0m] Step: 7186, Training Logs: loss_final: 0.887009, loss_mean: 0.844085, loss_mean_cls: 0.042923, grad_norm: 0.274544
[[34m2025-10-04 12:33:37[0m] Step: 7187, Training Logs: loss_final: 0.884242, loss_mean: 0.839580, loss_mean_cls: 0.044662, grad_norm: 0.374405
[[34m2025-10-04 12:33:38[0m] Step: 7188, Training Logs: loss_final: 0.868949, loss_mean: 0.825784, loss_mean_cls: 0.043165, grad_norm: 0.601480
[[34m2025-10-04 12:33:38[0m] Step: 7189, Training Logs: loss_final: 0.872788, loss_mean: 0.829694, loss_mean_cls: 0.043095, grad_norm: 0.348616
[[34m2025-10-04 12:33:38[0m] Step: 7190, Training Logs: loss_final: 0.865184, loss_mean: 0.821941, loss_mean_cls: 0.043243, grad_norm: 0.294135
[[34m2025-10-04 12:33:39[0m] Step: 7191, Training Logs: loss_final: 0.896297, loss_mean: 0.853613, loss_mean_cls: 0.042684, grad_norm: 0.254034
[[34m2025-10-04 12:33:39[0m] Step: 7192, Training Logs: loss_final: 0.889477, loss_mean: 0.846647, loss_mean_cls: 0.042831, grad_norm: 0.304356
[[34m2025-10-04 12:33:39[0m] Step: 7193, Training Logs: loss_final: 0.870427, loss_mean: 0.826743, loss_mean_cls: 0.043684, grad_norm: 0.467848
[[34m2025-10-04 12:33:39[0m] Step: 7194, Training Logs: loss_final: 0.877567, loss_mean: 0.833385, loss_mean_cls: 0.044182, grad_norm: 0.250339
[[34m2025-10-04 12:33:40[0m] Step: 7195, Training Logs: loss_final: 0.867984, loss_mean: 0.823034, loss_mean_cls: 0.044950, grad_norm: 0.459580
[[34m2025-10-04 12:33:40[0m] Step: 7196, Training Logs: loss_final: 0.904200, loss_mean: 0.862170, loss_mean_cls: 0.042030, grad_norm: 0.254071
[[34m2025-10-04 12:33:40[0m] Step: 7197, Training Logs: loss_final: 0.862117, loss_mean: 0.816865, loss_mean_cls: 0.045252, grad_norm: 0.351330
[[34m2025-10-04 12:33:41[0m] Step: 7198, Training Logs: loss_final: 0.887139, loss_mean: 0.842560, loss_mean_cls: 0.044579, grad_norm: 0.363125
[[34m2025-10-04 12:33:41[0m] Step: 7199, Training Logs: loss_final: 0.887247, loss_mean: 0.843175, loss_mean_cls: 0.044072, grad_norm: 0.484452
[[34m2025-10-04 12:33:41[0m] Step: 7200, Training Logs: loss_final: 0.883944, loss_mean: 0.840308, loss_mean_cls: 0.043636, grad_norm: 0.258386
[[34m2025-10-04 12:33:41[0m] Step: 7201, Training Logs: loss_final: 0.868080, loss_mean: 0.824367, loss_mean_cls: 0.043713, grad_norm: 0.335635
[[34m2025-10-04 12:33:42[0m] Step: 7202, Training Logs: loss_final: 0.860666, loss_mean: 0.815878, loss_mean_cls: 0.044788, grad_norm: 0.246009
[[34m2025-10-04 12:33:42[0m] Step: 7203, Training Logs: loss_final: 0.867724, loss_mean: 0.824563, loss_mean_cls: 0.043161, grad_norm: 0.360547
[[34m2025-10-04 12:33:42[0m] Step: 7204, Training Logs: loss_final: 0.885003, loss_mean: 0.840656, loss_mean_cls: 0.044347, grad_norm: 0.356839
[[34m2025-10-04 12:33:43[0m] Step: 7205, Training Logs: loss_final: 0.861691, loss_mean: 0.815181, loss_mean_cls: 0.046510, grad_norm: 0.256887
[[34m2025-10-04 12:33:43[0m] Step: 7206, Training Logs: loss_final: 0.896471, loss_mean: 0.853591, loss_mean_cls: 0.042880, grad_norm: 0.358888
[[34m2025-10-04 12:33:43[0m] Step: 7207, Training Logs: loss_final: 0.883741, loss_mean: 0.839896, loss_mean_cls: 0.043845, grad_norm: 0.250010
[[34m2025-10-04 12:33:43[0m] Step: 7208, Training Logs: loss_final: 0.898051, loss_mean: 0.855178, loss_mean_cls: 0.042873, grad_norm: 0.209018
[[34m2025-10-04 12:33:44[0m] Step: 7209, Training Logs: loss_final: 0.905679, loss_mean: 0.861857, loss_mean_cls: 0.043822, grad_norm: 0.269698
[[34m2025-10-04 12:33:44[0m] Step: 7210, Training Logs: loss_final: 0.896316, loss_mean: 0.852036, loss_mean_cls: 0.044280, grad_norm: 0.230403
[[34m2025-10-04 12:33:44[0m] Step: 7211, Training Logs: loss_final: 0.882980, loss_mean: 0.839128, loss_mean_cls: 0.043852, grad_norm: 0.356348
[[34m2025-10-04 12:33:45[0m] Step: 7212, Training Logs: loss_final: 0.857491, loss_mean: 0.814419, loss_mean_cls: 0.043071, grad_norm: 0.249772
[[34m2025-10-04 12:33:45[0m] Step: 7213, Training Logs: loss_final: 0.875533, loss_mean: 0.831260, loss_mean_cls: 0.044274, grad_norm: 0.217283
[[34m2025-10-04 12:33:45[0m] Step: 7214, Training Logs: loss_final: 0.871971, loss_mean: 0.827407, loss_mean_cls: 0.044564, grad_norm: 0.376485
[[34m2025-10-04 12:33:46[0m] Step: 7215, Training Logs: loss_final: 0.889150, loss_mean: 0.845398, loss_mean_cls: 0.043752, grad_norm: 0.388367
[[34m2025-10-04 12:33:46[0m] Step: 7216, Training Logs: loss_final: 0.872957, loss_mean: 0.829107, loss_mean_cls: 0.043850, grad_norm: 0.287232
[[34m2025-10-04 12:33:46[0m] Step: 7217, Training Logs: loss_final: 0.889908, loss_mean: 0.846795, loss_mean_cls: 0.043113, grad_norm: 0.313154
[[34m2025-10-04 12:33:46[0m] Step: 7218, Training Logs: loss_final: 0.872737, loss_mean: 0.829344, loss_mean_cls: 0.043393, grad_norm: 0.281799
[[34m2025-10-04 12:33:47[0m] Step: 7219, Training Logs: loss_final: 0.882767, loss_mean: 0.839319, loss_mean_cls: 0.043449, grad_norm: 0.246621
[[34m2025-10-04 12:33:47[0m] Step: 7220, Training Logs: loss_final: 0.894725, loss_mean: 0.851289, loss_mean_cls: 0.043436, grad_norm: 0.253772
[[34m2025-10-04 12:33:47[0m] Step: 7221, Training Logs: loss_final: 0.873577, loss_mean: 0.828227, loss_mean_cls: 0.045350, grad_norm: 0.315550
[[34m2025-10-04 12:33:48[0m] Step: 7222, Training Logs: loss_final: 0.871972, loss_mean: 0.827220, loss_mean_cls: 0.044753, grad_norm: 0.248479
[[34m2025-10-04 12:33:48[0m] Step: 7223, Training Logs: loss_final: 0.880314, loss_mean: 0.838033, loss_mean_cls: 0.042281, grad_norm: 0.259161
[[34m2025-10-04 12:33:48[0m] Step: 7224, Training Logs: loss_final: 0.880403, loss_mean: 0.837750, loss_mean_cls: 0.042654, grad_norm: 0.284505
[[34m2025-10-04 12:33:48[0m] Step: 7225, Training Logs: loss_final: 0.861218, loss_mean: 0.816909, loss_mean_cls: 0.044310, grad_norm: 0.347560
[[34m2025-10-04 12:33:49[0m] Step: 7226, Training Logs: loss_final: 0.867327, loss_mean: 0.824716, loss_mean_cls: 0.042611, grad_norm: 0.206449
[[34m2025-10-04 12:33:49[0m] Step: 7227, Training Logs: loss_final: 0.879664, loss_mean: 0.835765, loss_mean_cls: 0.043899, grad_norm: 0.405876
[[34m2025-10-04 12:33:49[0m] Step: 7228, Training Logs: loss_final: 0.875952, loss_mean: 0.833157, loss_mean_cls: 0.042795, grad_norm: 0.288792
[[34m2025-10-04 12:33:50[0m] Step: 7229, Training Logs: loss_final: 0.874183, loss_mean: 0.830336, loss_mean_cls: 0.043847, grad_norm: 0.266705
[[34m2025-10-04 12:33:50[0m] Step: 7230, Training Logs: loss_final: 0.872369, loss_mean: 0.828209, loss_mean_cls: 0.044160, grad_norm: 0.277621
[[34m2025-10-04 12:33:50[0m] Step: 7231, Training Logs: loss_final: 0.884875, loss_mean: 0.841421, loss_mean_cls: 0.043455, grad_norm: 0.372834
[[34m2025-10-04 12:33:50[0m] Step: 7232, Training Logs: loss_final: 0.881671, loss_mean: 0.837683, loss_mean_cls: 0.043987, grad_norm: 0.265528
[[34m2025-10-04 12:33:51[0m] Step: 7233, Training Logs: loss_final: 0.867032, loss_mean: 0.822148, loss_mean_cls: 0.044884, grad_norm: 0.294278
[[34m2025-10-04 12:33:51[0m] Step: 7234, Training Logs: loss_final: 0.883325, loss_mean: 0.840764, loss_mean_cls: 0.042561, grad_norm: 0.271480
[[34m2025-10-04 12:33:51[0m] Step: 7235, Training Logs: loss_final: 0.875952, loss_mean: 0.833172, loss_mean_cls: 0.042780, grad_norm: 0.374205
[[34m2025-10-04 12:33:52[0m] Step: 7236, Training Logs: loss_final: 0.886400, loss_mean: 0.843017, loss_mean_cls: 0.043383, grad_norm: 0.628652
[[34m2025-10-04 12:33:52[0m] Step: 7237, Training Logs: loss_final: 0.878071, loss_mean: 0.834211, loss_mean_cls: 0.043860, grad_norm: 0.277454
[[34m2025-10-04 12:33:52[0m] Step: 7238, Training Logs: loss_final: 0.880889, loss_mean: 0.837471, loss_mean_cls: 0.043418, grad_norm: 0.316396
[[34m2025-10-04 12:33:53[0m] Step: 7239, Training Logs: loss_final: 0.860661, loss_mean: 0.816394, loss_mean_cls: 0.044267, grad_norm: 0.350681
[[34m2025-10-04 12:33:53[0m] Step: 7240, Training Logs: loss_final: 0.891264, loss_mean: 0.848452, loss_mean_cls: 0.042812, grad_norm: 0.287745
[[34m2025-10-04 12:33:53[0m] Step: 7241, Training Logs: loss_final: 0.877012, loss_mean: 0.833601, loss_mean_cls: 0.043411, grad_norm: 0.215387
[[34m2025-10-04 12:33:53[0m] Step: 7242, Training Logs: loss_final: 0.890242, loss_mean: 0.848280, loss_mean_cls: 0.041963, grad_norm: 0.356754
[[34m2025-10-04 12:33:54[0m] Step: 7243, Training Logs: loss_final: 0.887362, loss_mean: 0.842949, loss_mean_cls: 0.044413, grad_norm: 0.447795
[[34m2025-10-04 12:33:54[0m] Step: 7244, Training Logs: loss_final: 0.891302, loss_mean: 0.846373, loss_mean_cls: 0.044929, grad_norm: 0.236807
[[34m2025-10-04 12:33:54[0m] Step: 7245, Training Logs: loss_final: 0.853621, loss_mean: 0.808694, loss_mean_cls: 0.044927, grad_norm: 0.369239
[[34m2025-10-04 12:33:55[0m] Step: 7246, Training Logs: loss_final: 0.866923, loss_mean: 0.822377, loss_mean_cls: 0.044545, grad_norm: 0.277310
[[34m2025-10-04 12:33:55[0m] Step: 7247, Training Logs: loss_final: 0.870996, loss_mean: 0.826802, loss_mean_cls: 0.044194, grad_norm: 0.231163
[[34m2025-10-04 12:33:55[0m] Step: 7248, Training Logs: loss_final: 0.888312, loss_mean: 0.845494, loss_mean_cls: 0.042818, grad_norm: 0.319510
[[34m2025-10-04 12:33:55[0m] Step: 7249, Training Logs: loss_final: 0.873335, loss_mean: 0.827821, loss_mean_cls: 0.045513, grad_norm: 0.328344
[[34m2025-10-04 12:33:56[0m] Step: 7250, Training Logs: loss_final: 0.846039, loss_mean: 0.802059, loss_mean_cls: 0.043980, grad_norm: 0.211511
[[34m2025-10-04 12:33:56[0m] Step: 7251, Training Logs: loss_final: 0.861346, loss_mean: 0.817172, loss_mean_cls: 0.044173, grad_norm: 0.242075
[[34m2025-10-04 12:33:56[0m] Step: 7252, Training Logs: loss_final: 0.875019, loss_mean: 0.830574, loss_mean_cls: 0.044445, grad_norm: 0.259045
[[34m2025-10-04 12:33:57[0m] Step: 7253, Training Logs: loss_final: 0.869815, loss_mean: 0.825917, loss_mean_cls: 0.043897, grad_norm: 0.242869
[[34m2025-10-04 12:33:57[0m] Step: 7254, Training Logs: loss_final: 0.878436, loss_mean: 0.834522, loss_mean_cls: 0.043914, grad_norm: 0.312589
[[34m2025-10-04 12:33:57[0m] Step: 7255, Training Logs: loss_final: 0.866229, loss_mean: 0.821721, loss_mean_cls: 0.044508, grad_norm: 0.274694
[[34m2025-10-04 12:33:58[0m] Step: 7256, Training Logs: loss_final: 0.863292, loss_mean: 0.819184, loss_mean_cls: 0.044108, grad_norm: 0.342087
[[34m2025-10-04 12:33:58[0m] Step: 7257, Training Logs: loss_final: 0.899422, loss_mean: 0.856933, loss_mean_cls: 0.042489, grad_norm: 0.158253
[[34m2025-10-04 12:33:58[0m] Step: 7258, Training Logs: loss_final: 0.872882, loss_mean: 0.829654, loss_mean_cls: 0.043228, grad_norm: 0.342588
[[34m2025-10-04 12:33:58[0m] Step: 7259, Training Logs: loss_final: 0.886320, loss_mean: 0.843637, loss_mean_cls: 0.042683, grad_norm: 0.257756
[[34m2025-10-04 12:33:59[0m] Step: 7260, Training Logs: loss_final: 0.880172, loss_mean: 0.836129, loss_mean_cls: 0.044043, grad_norm: 0.243993
[[34m2025-10-04 12:33:59[0m] Step: 7261, Training Logs: loss_final: 0.885729, loss_mean: 0.842737, loss_mean_cls: 0.042992, grad_norm: 0.247979
[[34m2025-10-04 12:33:59[0m] Step: 7262, Training Logs: loss_final: 0.873010, loss_mean: 0.828752, loss_mean_cls: 0.044258, grad_norm: 0.256377
[[34m2025-10-04 12:34:00[0m] Step: 7263, Training Logs: loss_final: 0.873941, loss_mean: 0.830277, loss_mean_cls: 0.043664, grad_norm: 0.222042
[[34m2025-10-04 12:34:00[0m] Step: 7264, Training Logs: loss_final: 0.874818, loss_mean: 0.830334, loss_mean_cls: 0.044484, grad_norm: 0.248773
[[34m2025-10-04 12:34:00[0m] Step: 7265, Training Logs: loss_final: 0.856198, loss_mean: 0.812227, loss_mean_cls: 0.043971, grad_norm: 0.201958
[[34m2025-10-04 12:34:00[0m] Step: 7266, Training Logs: loss_final: 0.875326, loss_mean: 0.832718, loss_mean_cls: 0.042608, grad_norm: 0.289473
[[34m2025-10-04 12:34:01[0m] Step: 7267, Training Logs: loss_final: 0.874916, loss_mean: 0.829458, loss_mean_cls: 0.045458, grad_norm: 0.252970
[[34m2025-10-04 12:34:01[0m] Step: 7268, Training Logs: loss_final: 0.872250, loss_mean: 0.829300, loss_mean_cls: 0.042950, grad_norm: 0.301059
[[34m2025-10-04 12:34:01[0m] Step: 7269, Training Logs: loss_final: 0.856449, loss_mean: 0.812478, loss_mean_cls: 0.043970, grad_norm: 0.386122
[[34m2025-10-04 12:34:02[0m] Step: 7270, Training Logs: loss_final: 0.890863, loss_mean: 0.847440, loss_mean_cls: 0.043423, grad_norm: 0.209367
[[34m2025-10-04 12:34:02[0m] Step: 7271, Training Logs: loss_final: 0.872728, loss_mean: 0.828161, loss_mean_cls: 0.044567, grad_norm: 0.293795
[[34m2025-10-04 12:34:02[0m] Step: 7272, Training Logs: loss_final: 0.882986, loss_mean: 0.838903, loss_mean_cls: 0.044082, grad_norm: 0.242872
[[34m2025-10-04 12:34:03[0m] Step: 7273, Training Logs: loss_final: 0.889411, loss_mean: 0.846710, loss_mean_cls: 0.042701, grad_norm: 0.256681
[[34m2025-10-04 12:34:03[0m] Step: 7274, Training Logs: loss_final: 0.875828, loss_mean: 0.831579, loss_mean_cls: 0.044249, grad_norm: 0.279146
[[34m2025-10-04 12:34:03[0m] Step: 7275, Training Logs: loss_final: 0.875824, loss_mean: 0.830798, loss_mean_cls: 0.045026, grad_norm: 0.395579
[[34m2025-10-04 12:34:03[0m] Step: 7276, Training Logs: loss_final: 0.892784, loss_mean: 0.848469, loss_mean_cls: 0.044315, grad_norm: 0.271663
[[34m2025-10-04 12:34:04[0m] Step: 7277, Training Logs: loss_final: 0.892140, loss_mean: 0.848266, loss_mean_cls: 0.043874, grad_norm: 0.401502
[[34m2025-10-04 12:34:04[0m] Step: 7278, Training Logs: loss_final: 0.881879, loss_mean: 0.837554, loss_mean_cls: 0.044325, grad_norm: 0.405060
[[34m2025-10-04 12:34:04[0m] Step: 7279, Training Logs: loss_final: 0.879769, loss_mean: 0.835485, loss_mean_cls: 0.044284, grad_norm: 0.305929
[[34m2025-10-04 12:34:05[0m] Step: 7280, Training Logs: loss_final: 0.876505, loss_mean: 0.833588, loss_mean_cls: 0.042918, grad_norm: 0.489305
[[34m2025-10-04 12:34:05[0m] Step: 7281, Training Logs: loss_final: 0.873653, loss_mean: 0.830187, loss_mean_cls: 0.043466, grad_norm: 0.268936
[[34m2025-10-04 12:34:05[0m] Step: 7282, Training Logs: loss_final: 0.893506, loss_mean: 0.851191, loss_mean_cls: 0.042315, grad_norm: 0.304843
[[34m2025-10-04 12:34:06[0m] Step: 7283, Training Logs: loss_final: 0.880104, loss_mean: 0.836280, loss_mean_cls: 0.043824, grad_norm: 0.204035
[[34m2025-10-04 12:34:06[0m] Step: 7284, Training Logs: loss_final: 0.866475, loss_mean: 0.822705, loss_mean_cls: 0.043770, grad_norm: 0.421939
[[34m2025-10-04 12:34:06[0m] Step: 7285, Training Logs: loss_final: 0.891005, loss_mean: 0.848041, loss_mean_cls: 0.042963, grad_norm: 0.296031
[[34m2025-10-04 12:34:06[0m] Step: 7286, Training Logs: loss_final: 0.877073, loss_mean: 0.832889, loss_mean_cls: 0.044184, grad_norm: 0.190475
[[34m2025-10-04 12:34:07[0m] Step: 7287, Training Logs: loss_final: 0.875453, loss_mean: 0.831773, loss_mean_cls: 0.043679, grad_norm: 0.333864
[[34m2025-10-04 12:34:07[0m] Step: 7288, Training Logs: loss_final: 0.895045, loss_mean: 0.852184, loss_mean_cls: 0.042861, grad_norm: 0.276981
[[34m2025-10-04 12:34:07[0m] Step: 7289, Training Logs: loss_final: 0.879961, loss_mean: 0.835264, loss_mean_cls: 0.044697, grad_norm: 0.260360
[[34m2025-10-04 12:34:08[0m] Step: 7290, Training Logs: loss_final: 0.890863, loss_mean: 0.848083, loss_mean_cls: 0.042780, grad_norm: 0.287638
[[34m2025-10-04 12:34:08[0m] Step: 7291, Training Logs: loss_final: 0.879099, loss_mean: 0.836772, loss_mean_cls: 0.042326, grad_norm: 0.229708
[[34m2025-10-04 12:34:08[0m] Step: 7292, Training Logs: loss_final: 0.871656, loss_mean: 0.827771, loss_mean_cls: 0.043885, grad_norm: 0.295950
[[34m2025-10-04 12:34:09[0m] Step: 7293, Training Logs: loss_final: 0.887151, loss_mean: 0.844493, loss_mean_cls: 0.042657, grad_norm: 0.301778
[[34m2025-10-04 12:34:09[0m] Step: 7294, Training Logs: loss_final: 0.885306, loss_mean: 0.841443, loss_mean_cls: 0.043863, grad_norm: 0.437939
[[34m2025-10-04 12:34:09[0m] Step: 7295, Training Logs: loss_final: 0.900045, loss_mean: 0.856535, loss_mean_cls: 0.043509, grad_norm: 0.265071
[[34m2025-10-04 12:34:09[0m] Step: 7296, Training Logs: loss_final: 0.890975, loss_mean: 0.846175, loss_mean_cls: 0.044800, grad_norm: 0.454437
[[34m2025-10-04 12:34:10[0m] Step: 7297, Training Logs: loss_final: 0.882574, loss_mean: 0.839139, loss_mean_cls: 0.043436, grad_norm: 0.272386
[[34m2025-10-04 12:34:10[0m] Step: 7298, Training Logs: loss_final: 0.868954, loss_mean: 0.823591, loss_mean_cls: 0.045364, grad_norm: 0.348119
[[34m2025-10-04 12:34:10[0m] Step: 7299, Training Logs: loss_final: 0.872442, loss_mean: 0.827844, loss_mean_cls: 0.044598, grad_norm: 0.266601
[[34m2025-10-04 12:34:11[0m] Step: 7300, Training Logs: loss_final: 0.869051, loss_mean: 0.825441, loss_mean_cls: 0.043610, grad_norm: 0.369570
[[34m2025-10-04 12:34:11[0m] Step: 7301, Training Logs: loss_final: 0.887263, loss_mean: 0.844258, loss_mean_cls: 0.043005, grad_norm: 0.233472
[[34m2025-10-04 12:34:11[0m] Step: 7302, Training Logs: loss_final: 0.859249, loss_mean: 0.814997, loss_mean_cls: 0.044252, grad_norm: 0.391513
[[34m2025-10-04 12:34:11[0m] Step: 7303, Training Logs: loss_final: 0.887573, loss_mean: 0.844255, loss_mean_cls: 0.043318, grad_norm: 0.366545
[[34m2025-10-04 12:34:12[0m] Step: 7304, Training Logs: loss_final: 0.890931, loss_mean: 0.847013, loss_mean_cls: 0.043918, grad_norm: 0.363935
[[34m2025-10-04 12:34:12[0m] Step: 7305, Training Logs: loss_final: 0.882515, loss_mean: 0.840196, loss_mean_cls: 0.042319, grad_norm: 0.286038
[[34m2025-10-04 12:34:12[0m] Step: 7306, Training Logs: loss_final: 0.871027, loss_mean: 0.828853, loss_mean_cls: 0.042174, grad_norm: 0.364233
[[34m2025-10-04 12:34:13[0m] Step: 7307, Training Logs: loss_final: 0.878417, loss_mean: 0.833947, loss_mean_cls: 0.044470, grad_norm: 0.262369
[[34m2025-10-04 12:34:13[0m] Step: 7308, Training Logs: loss_final: 0.861435, loss_mean: 0.817586, loss_mean_cls: 0.043850, grad_norm: 0.282059
[[34m2025-10-04 12:34:13[0m] Step: 7309, Training Logs: loss_final: 0.875403, loss_mean: 0.831856, loss_mean_cls: 0.043547, grad_norm: 0.261741
[[34m2025-10-04 12:34:13[0m] Step: 7310, Training Logs: loss_final: 0.871696, loss_mean: 0.827620, loss_mean_cls: 0.044076, grad_norm: 0.323998
[[34m2025-10-04 12:34:14[0m] Step: 7311, Training Logs: loss_final: 0.882830, loss_mean: 0.839510, loss_mean_cls: 0.043320, grad_norm: 0.279857
[[34m2025-10-04 12:34:14[0m] Step: 7312, Training Logs: loss_final: 0.895291, loss_mean: 0.851358, loss_mean_cls: 0.043933, grad_norm: 0.230379
[[34m2025-10-04 12:34:14[0m] Step: 7313, Training Logs: loss_final: 0.879263, loss_mean: 0.835522, loss_mean_cls: 0.043740, grad_norm: 0.402592
[[34m2025-10-04 12:34:15[0m] Step: 7314, Training Logs: loss_final: 0.873908, loss_mean: 0.830115, loss_mean_cls: 0.043793, grad_norm: 0.299494
[[34m2025-10-04 12:34:15[0m] Step: 7315, Training Logs: loss_final: 0.900907, loss_mean: 0.857585, loss_mean_cls: 0.043322, grad_norm: 0.333740
[[34m2025-10-04 12:34:15[0m] Step: 7316, Training Logs: loss_final: 0.896659, loss_mean: 0.854277, loss_mean_cls: 0.042383, grad_norm: 0.317912
[[34m2025-10-04 12:34:16[0m] Step: 7317, Training Logs: loss_final: 0.873062, loss_mean: 0.828983, loss_mean_cls: 0.044079, grad_norm: 0.394026
[[34m2025-10-04 12:34:16[0m] Step: 7318, Training Logs: loss_final: 0.873635, loss_mean: 0.830097, loss_mean_cls: 0.043538, grad_norm: 0.268056
[[34m2025-10-04 12:34:16[0m] Step: 7319, Training Logs: loss_final: 0.881618, loss_mean: 0.838298, loss_mean_cls: 0.043321, grad_norm: 0.332577
[[34m2025-10-04 12:34:16[0m] Step: 7320, Training Logs: loss_final: 0.876265, loss_mean: 0.832087, loss_mean_cls: 0.044178, grad_norm: 0.325426
[[34m2025-10-04 12:34:17[0m] Step: 7321, Training Logs: loss_final: 0.879686, loss_mean: 0.837356, loss_mean_cls: 0.042331, grad_norm: 0.315459
[[34m2025-10-04 12:34:17[0m] Step: 7322, Training Logs: loss_final: 0.872471, loss_mean: 0.829630, loss_mean_cls: 0.042841, grad_norm: 0.237639
[[34m2025-10-04 12:34:17[0m] Step: 7323, Training Logs: loss_final: 0.881143, loss_mean: 0.837943, loss_mean_cls: 0.043200, grad_norm: 0.438870
[[34m2025-10-04 12:34:18[0m] Step: 7324, Training Logs: loss_final: 0.866791, loss_mean: 0.822471, loss_mean_cls: 0.044320, grad_norm: 0.280365
[[34m2025-10-04 12:34:18[0m] Step: 7325, Training Logs: loss_final: 0.876835, loss_mean: 0.833786, loss_mean_cls: 0.043049, grad_norm: 0.335748
[[34m2025-10-04 12:34:18[0m] Step: 7326, Training Logs: loss_final: 0.852626, loss_mean: 0.808820, loss_mean_cls: 0.043807, grad_norm: 0.213805
[[34m2025-10-04 12:34:18[0m] Step: 7327, Training Logs: loss_final: 0.883550, loss_mean: 0.838593, loss_mean_cls: 0.044958, grad_norm: 0.376786
[[34m2025-10-04 12:34:19[0m] Step: 7328, Training Logs: loss_final: 0.870646, loss_mean: 0.826628, loss_mean_cls: 0.044018, grad_norm: 0.381097
[[34m2025-10-04 12:34:19[0m] Step: 7329, Training Logs: loss_final: 0.884112, loss_mean: 0.840555, loss_mean_cls: 0.043557, grad_norm: 0.296012
[[34m2025-10-04 12:34:19[0m] Step: 7330, Training Logs: loss_final: 0.878360, loss_mean: 0.834932, loss_mean_cls: 0.043429, grad_norm: 0.398083
[[34m2025-10-04 12:34:20[0m] Step: 7331, Training Logs: loss_final: 0.878486, loss_mean: 0.834216, loss_mean_cls: 0.044270, grad_norm: 0.345996
[[34m2025-10-04 12:34:20[0m] Step: 7332, Training Logs: loss_final: 0.885533, loss_mean: 0.842098, loss_mean_cls: 0.043435, grad_norm: 0.461121
[[34m2025-10-04 12:34:20[0m] Step: 7333, Training Logs: loss_final: 0.855977, loss_mean: 0.811730, loss_mean_cls: 0.044247, grad_norm: 0.279635
[[34m2025-10-04 12:34:20[0m] Step: 7334, Training Logs: loss_final: 0.886356, loss_mean: 0.842607, loss_mean_cls: 0.043749, grad_norm: 0.328959
[[34m2025-10-04 12:34:21[0m] Step: 7335, Training Logs: loss_final: 0.875718, loss_mean: 0.831397, loss_mean_cls: 0.044321, grad_norm: 0.252914
[[34m2025-10-04 12:34:21[0m] Step: 7336, Training Logs: loss_final: 0.911110, loss_mean: 0.867470, loss_mean_cls: 0.043641, grad_norm: 0.240691
[[34m2025-10-04 12:34:21[0m] Step: 7337, Training Logs: loss_final: 0.873637, loss_mean: 0.830251, loss_mean_cls: 0.043386, grad_norm: 0.226141
[[34m2025-10-04 12:34:22[0m] Step: 7338, Training Logs: loss_final: 0.873602, loss_mean: 0.830306, loss_mean_cls: 0.043296, grad_norm: 0.313617
[[34m2025-10-04 12:34:23[0m] Step: 7339, Training Logs: loss_final: 0.885998, loss_mean: 0.841441, loss_mean_cls: 0.044556, grad_norm: 0.233408
[[34m2025-10-04 12:34:23[0m] Step: 7340, Training Logs: loss_final: 0.867321, loss_mean: 0.821994, loss_mean_cls: 0.045326, grad_norm: 0.378236
[[34m2025-10-04 12:34:23[0m] Step: 7341, Training Logs: loss_final: 0.879706, loss_mean: 0.835689, loss_mean_cls: 0.044017, grad_norm: 0.353575
[[34m2025-10-04 12:34:23[0m] Step: 7342, Training Logs: loss_final: 0.886614, loss_mean: 0.842849, loss_mean_cls: 0.043765, grad_norm: 0.291648
[[34m2025-10-04 12:34:24[0m] Step: 7343, Training Logs: loss_final: 0.865555, loss_mean: 0.821515, loss_mean_cls: 0.044040, grad_norm: 0.385626
[[34m2025-10-04 12:34:24[0m] Step: 7344, Training Logs: loss_final: 0.877338, loss_mean: 0.833390, loss_mean_cls: 0.043947, grad_norm: 0.241979
[[34m2025-10-04 12:34:24[0m] Step: 7345, Training Logs: loss_final: 0.901871, loss_mean: 0.856224, loss_mean_cls: 0.045647, grad_norm: 0.396057
[[34m2025-10-04 12:34:25[0m] Step: 7346, Training Logs: loss_final: 0.868821, loss_mean: 0.823778, loss_mean_cls: 0.045042, grad_norm: 0.198186
[[34m2025-10-04 12:34:25[0m] Step: 7347, Training Logs: loss_final: 0.871603, loss_mean: 0.827916, loss_mean_cls: 0.043687, grad_norm: 0.331252
[[34m2025-10-04 12:34:25[0m] Step: 7348, Training Logs: loss_final: 0.857956, loss_mean: 0.814641, loss_mean_cls: 0.043315, grad_norm: 0.440128
[[34m2025-10-04 12:34:26[0m] Step: 7349, Training Logs: loss_final: 0.870842, loss_mean: 0.826484, loss_mean_cls: 0.044358, grad_norm: 0.456723
[[34m2025-10-04 12:34:26[0m] Step: 7350, Training Logs: loss_final: 0.854580, loss_mean: 0.809403, loss_mean_cls: 0.045178, grad_norm: 0.301675
[[34m2025-10-04 12:34:26[0m] Step: 7351, Training Logs: loss_final: 0.872331, loss_mean: 0.828179, loss_mean_cls: 0.044152, grad_norm: 0.487196
[[34m2025-10-04 12:34:26[0m] Step: 7352, Training Logs: loss_final: 0.885984, loss_mean: 0.843416, loss_mean_cls: 0.042567, grad_norm: 0.389041
[[34m2025-10-04 12:34:27[0m] Step: 7353, Training Logs: loss_final: 0.861099, loss_mean: 0.816837, loss_mean_cls: 0.044262, grad_norm: 0.342931
[[34m2025-10-04 12:34:27[0m] Step: 7354, Training Logs: loss_final: 0.887681, loss_mean: 0.843507, loss_mean_cls: 0.044173, grad_norm: 0.255394
[[34m2025-10-04 12:34:27[0m] Step: 7355, Training Logs: loss_final: 0.877962, loss_mean: 0.833965, loss_mean_cls: 0.043997, grad_norm: 0.334425
[[34m2025-10-04 12:34:28[0m] Step: 7356, Training Logs: loss_final: 0.871311, loss_mean: 0.827368, loss_mean_cls: 0.043943, grad_norm: 0.303513
[[34m2025-10-04 12:34:28[0m] Step: 7357, Training Logs: loss_final: 0.866140, loss_mean: 0.821208, loss_mean_cls: 0.044932, grad_norm: 0.452009
[[34m2025-10-04 12:34:28[0m] Step: 7358, Training Logs: loss_final: 0.885533, loss_mean: 0.841734, loss_mean_cls: 0.043799, grad_norm: 0.231369
[[34m2025-10-04 12:34:28[0m] Step: 7359, Training Logs: loss_final: 0.862327, loss_mean: 0.818294, loss_mean_cls: 0.044033, grad_norm: 0.331473
[[34m2025-10-04 12:34:29[0m] Step: 7360, Training Logs: loss_final: 0.876433, loss_mean: 0.832674, loss_mean_cls: 0.043759, grad_norm: 0.290274
[[34m2025-10-04 12:34:29[0m] Step: 7361, Training Logs: loss_final: 0.879496, loss_mean: 0.835090, loss_mean_cls: 0.044406, grad_norm: 0.387570
[[34m2025-10-04 12:34:29[0m] Step: 7362, Training Logs: loss_final: 0.870822, loss_mean: 0.826316, loss_mean_cls: 0.044506, grad_norm: 0.270515
[[34m2025-10-04 12:34:30[0m] Step: 7363, Training Logs: loss_final: 0.877336, loss_mean: 0.834908, loss_mean_cls: 0.042428, grad_norm: 0.286823
[[34m2025-10-04 12:34:30[0m] Step: 7364, Training Logs: loss_final: 0.891779, loss_mean: 0.848526, loss_mean_cls: 0.043253, grad_norm: 0.322422
[[34m2025-10-04 12:34:30[0m] Step: 7365, Training Logs: loss_final: 0.873683, loss_mean: 0.829732, loss_mean_cls: 0.043951, grad_norm: 0.336515
[[34m2025-10-04 12:34:30[0m] Step: 7366, Training Logs: loss_final: 0.859206, loss_mean: 0.815344, loss_mean_cls: 0.043862, grad_norm: 0.308317
[[34m2025-10-04 12:34:31[0m] Step: 7367, Training Logs: loss_final: 0.877527, loss_mean: 0.833664, loss_mean_cls: 0.043863, grad_norm: 0.316216
[[34m2025-10-04 12:34:31[0m] Step: 7368, Training Logs: loss_final: 0.897824, loss_mean: 0.854440, loss_mean_cls: 0.043385, grad_norm: 0.336428
[[34m2025-10-04 12:34:31[0m] Step: 7369, Training Logs: loss_final: 0.876231, loss_mean: 0.831952, loss_mean_cls: 0.044279, grad_norm: 0.305121
[[34m2025-10-04 12:34:32[0m] Step: 7370, Training Logs: loss_final: 0.889019, loss_mean: 0.845559, loss_mean_cls: 0.043460, grad_norm: 0.493304
[[34m2025-10-04 12:34:32[0m] Step: 7371, Training Logs: loss_final: 0.895115, loss_mean: 0.850794, loss_mean_cls: 0.044321, grad_norm: 0.388615
[[34m2025-10-04 12:34:32[0m] Step: 7372, Training Logs: loss_final: 0.888553, loss_mean: 0.845906, loss_mean_cls: 0.042646, grad_norm: 0.305930
[[34m2025-10-04 12:34:32[0m] Step: 7373, Training Logs: loss_final: 0.874010, loss_mean: 0.830837, loss_mean_cls: 0.043172, grad_norm: 0.208555
[[34m2025-10-04 12:34:33[0m] Step: 7374, Training Logs: loss_final: 0.863868, loss_mean: 0.819587, loss_mean_cls: 0.044281, grad_norm: 0.517314
[[34m2025-10-04 12:34:33[0m] Step: 7375, Training Logs: loss_final: 0.900655, loss_mean: 0.857140, loss_mean_cls: 0.043515, grad_norm: 0.340978
[[34m2025-10-04 12:34:33[0m] Step: 7376, Training Logs: loss_final: 0.881423, loss_mean: 0.836917, loss_mean_cls: 0.044506, grad_norm: 0.378723
[[34m2025-10-04 12:34:34[0m] Step: 7377, Training Logs: loss_final: 0.862084, loss_mean: 0.818440, loss_mean_cls: 0.043643, grad_norm: 0.295217
[[34m2025-10-04 12:34:34[0m] Step: 7378, Training Logs: loss_final: 0.863317, loss_mean: 0.819245, loss_mean_cls: 0.044072, grad_norm: 0.426215
[[34m2025-10-04 12:34:34[0m] Step: 7379, Training Logs: loss_final: 0.860687, loss_mean: 0.815405, loss_mean_cls: 0.045282, grad_norm: 0.354464
[[34m2025-10-04 12:34:35[0m] Step: 7380, Training Logs: loss_final: 0.884247, loss_mean: 0.841325, loss_mean_cls: 0.042921, grad_norm: 0.287078
[[34m2025-10-04 12:34:35[0m] Step: 7381, Training Logs: loss_final: 0.873783, loss_mean: 0.829656, loss_mean_cls: 0.044127, grad_norm: 0.263567
[[34m2025-10-04 12:34:35[0m] Step: 7382, Training Logs: loss_final: 0.880044, loss_mean: 0.835993, loss_mean_cls: 0.044050, grad_norm: 0.310631
[[34m2025-10-04 12:34:35[0m] Step: 7383, Training Logs: loss_final: 0.887072, loss_mean: 0.843472, loss_mean_cls: 0.043600, grad_norm: 0.244636
[[34m2025-10-04 12:34:36[0m] Step: 7384, Training Logs: loss_final: 0.883988, loss_mean: 0.841155, loss_mean_cls: 0.042833, grad_norm: 0.383984
[[34m2025-10-04 12:34:36[0m] Step: 7385, Training Logs: loss_final: 0.869788, loss_mean: 0.826038, loss_mean_cls: 0.043750, grad_norm: 0.423198
[[34m2025-10-04 12:34:36[0m] Step: 7386, Training Logs: loss_final: 0.882455, loss_mean: 0.839102, loss_mean_cls: 0.043353, grad_norm: 0.209654
[[34m2025-10-04 12:34:37[0m] Step: 7387, Training Logs: loss_final: 0.864034, loss_mean: 0.819860, loss_mean_cls: 0.044173, grad_norm: 0.240477
[[34m2025-10-04 12:34:37[0m] Step: 7388, Training Logs: loss_final: 0.858511, loss_mean: 0.814232, loss_mean_cls: 0.044279, grad_norm: 0.290453
[[34m2025-10-04 12:34:37[0m] Step: 7389, Training Logs: loss_final: 0.875275, loss_mean: 0.832001, loss_mean_cls: 0.043275, grad_norm: 0.253496
[[34m2025-10-04 12:34:37[0m] Step: 7390, Training Logs: loss_final: 0.864553, loss_mean: 0.820462, loss_mean_cls: 0.044091, grad_norm: 0.308624
[[34m2025-10-04 12:34:38[0m] Step: 7391, Training Logs: loss_final: 0.847790, loss_mean: 0.801982, loss_mean_cls: 0.045808, grad_norm: 0.371622
[[34m2025-10-04 12:34:38[0m] Step: 7392, Training Logs: loss_final: 0.871556, loss_mean: 0.826911, loss_mean_cls: 0.044645, grad_norm: 0.400028
[[34m2025-10-04 12:34:38[0m] Step: 7393, Training Logs: loss_final: 0.887673, loss_mean: 0.843401, loss_mean_cls: 0.044272, grad_norm: 0.313128
[[34m2025-10-04 12:34:39[0m] Step: 7394, Training Logs: loss_final: 0.888993, loss_mean: 0.845792, loss_mean_cls: 0.043200, grad_norm: 0.390927
[[34m2025-10-04 12:34:39[0m] Step: 7395, Training Logs: loss_final: 0.862874, loss_mean: 0.818579, loss_mean_cls: 0.044295, grad_norm: 0.319790
[[34m2025-10-04 12:34:39[0m] Step: 7396, Training Logs: loss_final: 0.875260, loss_mean: 0.831201, loss_mean_cls: 0.044059, grad_norm: 0.292053
[[34m2025-10-04 12:34:39[0m] Step: 7397, Training Logs: loss_final: 0.870687, loss_mean: 0.826123, loss_mean_cls: 0.044564, grad_norm: 0.346356
[[34m2025-10-04 12:34:40[0m] Step: 7398, Training Logs: loss_final: 0.879474, loss_mean: 0.836256, loss_mean_cls: 0.043218, grad_norm: 0.363617
[[34m2025-10-04 12:34:40[0m] Step: 7399, Training Logs: loss_final: 0.890406, loss_mean: 0.846364, loss_mean_cls: 0.044042, grad_norm: 0.316788
[[34m2025-10-04 12:34:40[0m] Step: 7400, Training Logs: loss_final: 0.884206, loss_mean: 0.840575, loss_mean_cls: 0.043631, grad_norm: 0.391344
[[34m2025-10-04 12:34:41[0m] Step: 7401, Training Logs: loss_final: 0.889102, loss_mean: 0.845466, loss_mean_cls: 0.043636, grad_norm: 0.242105
[[34m2025-10-04 12:34:41[0m] Step: 7402, Training Logs: loss_final: 0.869899, loss_mean: 0.825420, loss_mean_cls: 0.044479, grad_norm: 0.340936
[[34m2025-10-04 12:34:41[0m] Step: 7403, Training Logs: loss_final: 0.876791, loss_mean: 0.832850, loss_mean_cls: 0.043941, grad_norm: 0.254093
[[34m2025-10-04 12:34:41[0m] Step: 7404, Training Logs: loss_final: 0.864291, loss_mean: 0.820467, loss_mean_cls: 0.043824, grad_norm: 0.304549
[[34m2025-10-04 12:34:42[0m] Step: 7405, Training Logs: loss_final: 0.888631, loss_mean: 0.844819, loss_mean_cls: 0.043813, grad_norm: 0.272856
[[34m2025-10-04 12:34:42[0m] Step: 7406, Training Logs: loss_final: 0.893451, loss_mean: 0.849629, loss_mean_cls: 0.043822, grad_norm: 0.307089
[[34m2025-10-04 12:34:42[0m] Step: 7407, Training Logs: loss_final: 0.884644, loss_mean: 0.842130, loss_mean_cls: 0.042514, grad_norm: 0.344760
[[34m2025-10-04 12:34:43[0m] Step: 7408, Training Logs: loss_final: 0.878457, loss_mean: 0.834781, loss_mean_cls: 0.043676, grad_norm: 0.277535
[[34m2025-10-04 12:34:43[0m] Step: 7409, Training Logs: loss_final: 0.871179, loss_mean: 0.825583, loss_mean_cls: 0.045597, grad_norm: 0.228406
[[34m2025-10-04 12:34:43[0m] Step: 7410, Training Logs: loss_final: 0.879383, loss_mean: 0.835848, loss_mean_cls: 0.043534, grad_norm: 0.398131
[[34m2025-10-04 12:34:43[0m] Step: 7411, Training Logs: loss_final: 0.891573, loss_mean: 0.849035, loss_mean_cls: 0.042538, grad_norm: 0.280653
[[34m2025-10-04 12:34:44[0m] Step: 7412, Training Logs: loss_final: 0.882055, loss_mean: 0.837340, loss_mean_cls: 0.044716, grad_norm: 0.251443
[[34m2025-10-04 12:34:44[0m] Step: 7413, Training Logs: loss_final: 0.876419, loss_mean: 0.833666, loss_mean_cls: 0.042753, grad_norm: 0.324353
[[34m2025-10-04 12:34:44[0m] Step: 7414, Training Logs: loss_final: 0.877711, loss_mean: 0.833875, loss_mean_cls: 0.043835, grad_norm: 0.226196
[[34m2025-10-04 12:34:45[0m] Step: 7415, Training Logs: loss_final: 0.897707, loss_mean: 0.854847, loss_mean_cls: 0.042860, grad_norm: 0.244369
[[34m2025-10-04 12:34:45[0m] Step: 7416, Training Logs: loss_final: 0.871831, loss_mean: 0.827997, loss_mean_cls: 0.043834, grad_norm: 0.277195
[[34m2025-10-04 12:34:45[0m] Step: 7417, Training Logs: loss_final: 0.864627, loss_mean: 0.822001, loss_mean_cls: 0.042626, grad_norm: 0.311721
[[34m2025-10-04 12:34:45[0m] Step: 7418, Training Logs: loss_final: 0.891835, loss_mean: 0.848398, loss_mean_cls: 0.043437, grad_norm: 0.196676
[[34m2025-10-04 12:34:46[0m] Step: 7419, Training Logs: loss_final: 0.876852, loss_mean: 0.833667, loss_mean_cls: 0.043184, grad_norm: 0.275928
[[34m2025-10-04 12:34:46[0m] Step: 7420, Training Logs: loss_final: 0.861961, loss_mean: 0.817026, loss_mean_cls: 0.044935, grad_norm: 0.275639
[[34m2025-10-04 12:34:46[0m] Step: 7421, Training Logs: loss_final: 0.860408, loss_mean: 0.815787, loss_mean_cls: 0.044620, grad_norm: 0.253027
[[34m2025-10-04 12:34:47[0m] Step: 7422, Training Logs: loss_final: 0.856018, loss_mean: 0.812330, loss_mean_cls: 0.043688, grad_norm: 0.236889
[[34m2025-10-04 12:34:47[0m] Step: 7423, Training Logs: loss_final: 0.887115, loss_mean: 0.841983, loss_mean_cls: 0.045131, grad_norm: 0.267125
[[34m2025-10-04 12:34:47[0m] Step: 7424, Training Logs: loss_final: 0.858004, loss_mean: 0.812573, loss_mean_cls: 0.045431, grad_norm: 0.327037
[[34m2025-10-04 12:34:48[0m] Step: 7425, Training Logs: loss_final: 0.880103, loss_mean: 0.836908, loss_mean_cls: 0.043195, grad_norm: 0.221090
[[34m2025-10-04 12:34:48[0m] Step: 7426, Training Logs: loss_final: 0.876228, loss_mean: 0.831895, loss_mean_cls: 0.044333, grad_norm: 0.257188
[[34m2025-10-04 12:34:48[0m] Step: 7427, Training Logs: loss_final: 0.886701, loss_mean: 0.842736, loss_mean_cls: 0.043965, grad_norm: 0.287348
[[34m2025-10-04 12:34:49[0m] Step: 7428, Training Logs: loss_final: 0.878990, loss_mean: 0.835149, loss_mean_cls: 0.043842, grad_norm: 0.342886
[[34m2025-10-04 12:34:49[0m] Step: 7429, Training Logs: loss_final: 0.873847, loss_mean: 0.830544, loss_mean_cls: 0.043303, grad_norm: 0.273972
[[34m2025-10-04 12:34:49[0m] Step: 7430, Training Logs: loss_final: 0.870990, loss_mean: 0.827267, loss_mean_cls: 0.043722, grad_norm: 0.246670
[[34m2025-10-04 12:34:49[0m] Step: 7431, Training Logs: loss_final: 0.872760, loss_mean: 0.828523, loss_mean_cls: 0.044237, grad_norm: 0.200871
[[34m2025-10-04 12:34:50[0m] Step: 7432, Training Logs: loss_final: 0.880454, loss_mean: 0.837411, loss_mean_cls: 0.043043, grad_norm: 0.314735
[[34m2025-10-04 12:34:50[0m] Step: 7433, Training Logs: loss_final: 0.873927, loss_mean: 0.830239, loss_mean_cls: 0.043688, grad_norm: 0.351923
[[34m2025-10-04 12:34:50[0m] Step: 7434, Training Logs: loss_final: 0.896430, loss_mean: 0.854093, loss_mean_cls: 0.042337, grad_norm: 0.296708
[[34m2025-10-04 12:34:51[0m] Step: 7435, Training Logs: loss_final: 0.873566, loss_mean: 0.830389, loss_mean_cls: 0.043178, grad_norm: 0.340273
[[34m2025-10-04 12:34:51[0m] Step: 7436, Training Logs: loss_final: 0.868245, loss_mean: 0.825018, loss_mean_cls: 0.043227, grad_norm: 0.329690
[[34m2025-10-04 12:34:51[0m] Step: 7437, Training Logs: loss_final: 0.865253, loss_mean: 0.820777, loss_mean_cls: 0.044476, grad_norm: 0.308571
[[34m2025-10-04 12:34:51[0m] Step: 7438, Training Logs: loss_final: 0.872752, loss_mean: 0.829544, loss_mean_cls: 0.043208, grad_norm: 0.245696
[[34m2025-10-04 12:34:52[0m] Step: 7439, Training Logs: loss_final: 0.886557, loss_mean: 0.842415, loss_mean_cls: 0.044142, grad_norm: 0.326796
[[34m2025-10-04 12:34:52[0m] Step: 7440, Training Logs: loss_final: 0.870109, loss_mean: 0.825362, loss_mean_cls: 0.044747, grad_norm: 0.205944
[[34m2025-10-04 12:34:52[0m] Step: 7441, Training Logs: loss_final: 0.888379, loss_mean: 0.844286, loss_mean_cls: 0.044093, grad_norm: 0.345007
[[34m2025-10-04 12:34:53[0m] Step: 7442, Training Logs: loss_final: 0.896640, loss_mean: 0.852595, loss_mean_cls: 0.044045, grad_norm: 0.172945
[[34m2025-10-04 12:34:53[0m] Step: 7443, Training Logs: loss_final: 0.876217, loss_mean: 0.832639, loss_mean_cls: 0.043578, grad_norm: 0.280855
[[34m2025-10-04 12:34:53[0m] Step: 7444, Training Logs: loss_final: 0.882415, loss_mean: 0.838678, loss_mean_cls: 0.043737, grad_norm: 0.219582
[[34m2025-10-04 12:34:53[0m] Step: 7445, Training Logs: loss_final: 0.882295, loss_mean: 0.838686, loss_mean_cls: 0.043609, grad_norm: 0.368003
[[34m2025-10-04 12:34:54[0m] Step: 7446, Training Logs: loss_final: 0.900129, loss_mean: 0.857046, loss_mean_cls: 0.043083, grad_norm: 0.258902
[[34m2025-10-04 12:34:54[0m] Step: 7447, Training Logs: loss_final: 0.873250, loss_mean: 0.830045, loss_mean_cls: 0.043205, grad_norm: 0.197169
[[34m2025-10-04 12:34:54[0m] Step: 7448, Training Logs: loss_final: 0.880433, loss_mean: 0.836980, loss_mean_cls: 0.043452, grad_norm: 0.295348
[[34m2025-10-04 12:34:55[0m] Step: 7449, Training Logs: loss_final: 0.885257, loss_mean: 0.839995, loss_mean_cls: 0.045263, grad_norm: 0.270003
[[34m2025-10-04 12:34:55[0m] Step: 7450, Training Logs: loss_final: 0.875887, loss_mean: 0.831882, loss_mean_cls: 0.044005, grad_norm: 0.223913
[[34m2025-10-04 12:34:55[0m] Step: 7451, Training Logs: loss_final: 0.879045, loss_mean: 0.835109, loss_mean_cls: 0.043936, grad_norm: 0.306593
[[34m2025-10-04 12:34:56[0m] Step: 7452, Training Logs: loss_final: 0.864749, loss_mean: 0.820887, loss_mean_cls: 0.043862, grad_norm: 0.237873
[[34m2025-10-04 12:34:56[0m] Step: 7453, Training Logs: loss_final: 0.867259, loss_mean: 0.822808, loss_mean_cls: 0.044451, grad_norm: 0.260629
[[34m2025-10-04 12:34:56[0m] Step: 7454, Training Logs: loss_final: 0.911356, loss_mean: 0.868411, loss_mean_cls: 0.042945, grad_norm: 0.214948
[[34m2025-10-04 12:34:56[0m] Step: 7455, Training Logs: loss_final: 0.875872, loss_mean: 0.832130, loss_mean_cls: 0.043742, grad_norm: 0.231075
[[34m2025-10-04 12:34:57[0m] Step: 7456, Training Logs: loss_final: 0.871597, loss_mean: 0.827967, loss_mean_cls: 0.043630, grad_norm: 0.296260
[[34m2025-10-04 12:34:57[0m] Step: 7457, Training Logs: loss_final: 0.899242, loss_mean: 0.855922, loss_mean_cls: 0.043320, grad_norm: 0.238878
[[34m2025-10-04 12:34:57[0m] Step: 7458, Training Logs: loss_final: 0.851776, loss_mean: 0.807588, loss_mean_cls: 0.044189, grad_norm: 0.383345
[[34m2025-10-04 12:34:58[0m] Step: 7459, Training Logs: loss_final: 0.890389, loss_mean: 0.845166, loss_mean_cls: 0.045223, grad_norm: 0.272381
[[34m2025-10-04 12:34:58[0m] Step: 7460, Training Logs: loss_final: 0.877224, loss_mean: 0.833729, loss_mean_cls: 0.043494, grad_norm: 0.310327
[[34m2025-10-04 12:34:58[0m] Step: 7461, Training Logs: loss_final: 0.883692, loss_mean: 0.840511, loss_mean_cls: 0.043181, grad_norm: 0.235686
[[34m2025-10-04 12:34:58[0m] Step: 7462, Training Logs: loss_final: 0.873590, loss_mean: 0.830278, loss_mean_cls: 0.043312, grad_norm: 0.351151
[[34m2025-10-04 12:34:59[0m] Step: 7463, Training Logs: loss_final: 0.894230, loss_mean: 0.850731, loss_mean_cls: 0.043499, grad_norm: 0.252034
[[34m2025-10-04 12:34:59[0m] Step: 7464, Training Logs: loss_final: 0.877036, loss_mean: 0.833907, loss_mean_cls: 0.043129, grad_norm: 0.330590
[[34m2025-10-04 12:34:59[0m] Step: 7465, Training Logs: loss_final: 0.889229, loss_mean: 0.846747, loss_mean_cls: 0.042482, grad_norm: 0.190492
[[34m2025-10-04 12:35:00[0m] Step: 7466, Training Logs: loss_final: 0.878149, loss_mean: 0.834339, loss_mean_cls: 0.043810, grad_norm: 0.353055
[[34m2025-10-04 12:35:00[0m] Step: 7467, Training Logs: loss_final: 0.874648, loss_mean: 0.831421, loss_mean_cls: 0.043227, grad_norm: 0.230673
[[34m2025-10-04 12:35:00[0m] Step: 7468, Training Logs: loss_final: 0.889595, loss_mean: 0.847039, loss_mean_cls: 0.042556, grad_norm: 0.348253
[[34m2025-10-04 12:35:00[0m] Step: 7469, Training Logs: loss_final: 0.870245, loss_mean: 0.827569, loss_mean_cls: 0.042676, grad_norm: 0.385007
[[34m2025-10-04 12:35:01[0m] Step: 7470, Training Logs: loss_final: 0.872708, loss_mean: 0.829381, loss_mean_cls: 0.043327, grad_norm: 0.270405
[[34m2025-10-04 12:35:01[0m] Step: 7471, Training Logs: loss_final: 0.884941, loss_mean: 0.840849, loss_mean_cls: 0.044092, grad_norm: 0.249326
[[34m2025-10-04 12:35:01[0m] Step: 7472, Training Logs: loss_final: 0.894093, loss_mean: 0.850867, loss_mean_cls: 0.043226, grad_norm: 0.296904
[[34m2025-10-04 12:35:02[0m] Step: 7473, Training Logs: loss_final: 0.875392, loss_mean: 0.830875, loss_mean_cls: 0.044517, grad_norm: 0.377785
[[34m2025-10-04 12:35:02[0m] Step: 7474, Training Logs: loss_final: 0.872899, loss_mean: 0.828864, loss_mean_cls: 0.044035, grad_norm: 0.341106
[[34m2025-10-04 12:35:02[0m] Step: 7475, Training Logs: loss_final: 0.877062, loss_mean: 0.833461, loss_mean_cls: 0.043601, grad_norm: 0.481162
[[34m2025-10-04 12:35:02[0m] Step: 7476, Training Logs: loss_final: 0.861093, loss_mean: 0.816315, loss_mean_cls: 0.044779, grad_norm: 0.306654
[[34m2025-10-04 12:35:03[0m] Step: 7477, Training Logs: loss_final: 0.890226, loss_mean: 0.846429, loss_mean_cls: 0.043797, grad_norm: 0.289878
[[34m2025-10-04 12:35:03[0m] Step: 7478, Training Logs: loss_final: 0.876916, loss_mean: 0.833001, loss_mean_cls: 0.043915, grad_norm: 0.291895
[[34m2025-10-04 12:35:03[0m] Step: 7479, Training Logs: loss_final: 0.884977, loss_mean: 0.841630, loss_mean_cls: 0.043348, grad_norm: 0.320514
[[34m2025-10-04 12:35:04[0m] Step: 7480, Training Logs: loss_final: 0.895594, loss_mean: 0.852449, loss_mean_cls: 0.043145, grad_norm: 0.407953
[[34m2025-10-04 12:35:04[0m] Step: 7481, Training Logs: loss_final: 0.879926, loss_mean: 0.836783, loss_mean_cls: 0.043143, grad_norm: 0.234866
[[34m2025-10-04 12:35:04[0m] Step: 7482, Training Logs: loss_final: 0.865109, loss_mean: 0.821006, loss_mean_cls: 0.044103, grad_norm: 0.358878
[[34m2025-10-04 12:35:05[0m] Step: 7483, Training Logs: loss_final: 0.868759, loss_mean: 0.824895, loss_mean_cls: 0.043864, grad_norm: 0.292977
[[34m2025-10-04 12:35:05[0m] Step: 7484, Training Logs: loss_final: 0.879071, loss_mean: 0.835380, loss_mean_cls: 0.043691, grad_norm: 0.316162
[[34m2025-10-04 12:35:05[0m] Step: 7485, Training Logs: loss_final: 0.889716, loss_mean: 0.847620, loss_mean_cls: 0.042096, grad_norm: 0.272064
[[34m2025-10-04 12:35:05[0m] Step: 7486, Training Logs: loss_final: 0.867980, loss_mean: 0.824341, loss_mean_cls: 0.043639, grad_norm: 0.203083
[[34m2025-10-04 12:35:06[0m] Step: 7487, Training Logs: loss_final: 0.887613, loss_mean: 0.844585, loss_mean_cls: 0.043027, grad_norm: 0.259070
[[34m2025-10-04 12:35:06[0m] Step: 7488, Training Logs: loss_final: 0.874025, loss_mean: 0.829672, loss_mean_cls: 0.044353, grad_norm: 0.225155
[[34m2025-10-04 12:35:06[0m] Step: 7489, Training Logs: loss_final: 0.853040, loss_mean: 0.810393, loss_mean_cls: 0.042647, grad_norm: 0.243453
[[34m2025-10-04 12:35:07[0m] Step: 7490, Training Logs: loss_final: 0.879245, loss_mean: 0.834892, loss_mean_cls: 0.044354, grad_norm: 0.259554
[[34m2025-10-04 12:35:07[0m] Step: 7491, Training Logs: loss_final: 0.857305, loss_mean: 0.813550, loss_mean_cls: 0.043755, grad_norm: 0.466094
[[34m2025-10-04 12:35:07[0m] Step: 7492, Training Logs: loss_final: 0.882520, loss_mean: 0.839605, loss_mean_cls: 0.042915, grad_norm: 0.350126
[[34m2025-10-04 12:35:07[0m] Step: 7493, Training Logs: loss_final: 0.863742, loss_mean: 0.821272, loss_mean_cls: 0.042470, grad_norm: 0.333499
[[34m2025-10-04 12:35:08[0m] Step: 7494, Training Logs: loss_final: 0.883931, loss_mean: 0.840901, loss_mean_cls: 0.043030, grad_norm: 0.375602
[[34m2025-10-04 12:35:08[0m] Step: 7495, Training Logs: loss_final: 0.873534, loss_mean: 0.828563, loss_mean_cls: 0.044971, grad_norm: 0.276841
[[34m2025-10-04 12:35:08[0m] Step: 7496, Training Logs: loss_final: 0.895134, loss_mean: 0.851622, loss_mean_cls: 0.043512, grad_norm: 0.340824
[[34m2025-10-04 12:35:09[0m] Step: 7497, Training Logs: loss_final: 0.867085, loss_mean: 0.821828, loss_mean_cls: 0.045257, grad_norm: 0.305488
[[34m2025-10-04 12:35:09[0m] Step: 7498, Training Logs: loss_final: 0.869892, loss_mean: 0.825899, loss_mean_cls: 0.043992, grad_norm: 0.409222
[[34m2025-10-04 12:35:09[0m] Step: 7499, Training Logs: loss_final: 0.874609, loss_mean: 0.831356, loss_mean_cls: 0.043253, grad_norm: 0.317458
[[34m2025-10-04 12:35:09[0m] Step: 7500, Training Logs: loss_final: 0.915317, loss_mean: 0.873447, loss_mean_cls: 0.041870, grad_norm: 0.330082
[[34m2025-10-04 12:35:10[0m] Step: 7501, Training Logs: loss_final: 0.883000, loss_mean: 0.837873, loss_mean_cls: 0.045128, grad_norm: 0.243113
[[34m2025-10-04 12:35:10[0m] Step: 7502, Training Logs: loss_final: 0.867962, loss_mean: 0.823766, loss_mean_cls: 0.044196, grad_norm: 0.280759
[[34m2025-10-04 12:35:10[0m] Step: 7503, Training Logs: loss_final: 0.875226, loss_mean: 0.831242, loss_mean_cls: 0.043984, grad_norm: 0.305772
[[34m2025-10-04 12:35:11[0m] Step: 7504, Training Logs: loss_final: 0.889521, loss_mean: 0.845958, loss_mean_cls: 0.043564, grad_norm: 0.378781
[[34m2025-10-04 12:35:11[0m] Step: 7505, Training Logs: loss_final: 0.884168, loss_mean: 0.838806, loss_mean_cls: 0.045362, grad_norm: 0.364963
[[34m2025-10-04 12:35:11[0m] Step: 7506, Training Logs: loss_final: 0.876323, loss_mean: 0.832625, loss_mean_cls: 0.043698, grad_norm: 0.262968
[[34m2025-10-04 12:35:12[0m] Step: 7507, Training Logs: loss_final: 0.870538, loss_mean: 0.826661, loss_mean_cls: 0.043877, grad_norm: 0.293133
[[34m2025-10-04 12:35:12[0m] Step: 7508, Training Logs: loss_final: 0.886630, loss_mean: 0.844536, loss_mean_cls: 0.042094, grad_norm: 0.330764
[[34m2025-10-04 12:35:12[0m] Step: 7509, Training Logs: loss_final: 0.879376, loss_mean: 0.836043, loss_mean_cls: 0.043333, grad_norm: 0.351816
[[34m2025-10-04 12:35:12[0m] Step: 7510, Training Logs: loss_final: 0.870559, loss_mean: 0.827031, loss_mean_cls: 0.043527, grad_norm: 0.236914
[[34m2025-10-04 12:35:13[0m] Step: 7511, Training Logs: loss_final: 0.854655, loss_mean: 0.811885, loss_mean_cls: 0.042770, grad_norm: 0.200252
[[34m2025-10-04 12:35:13[0m] Step: 7512, Training Logs: loss_final: 0.866390, loss_mean: 0.822029, loss_mean_cls: 0.044361, grad_norm: 0.273211
[[34m2025-10-04 12:35:13[0m] Step: 7513, Training Logs: loss_final: 0.900813, loss_mean: 0.857957, loss_mean_cls: 0.042856, grad_norm: 0.318396
[[34m2025-10-04 12:35:14[0m] Step: 7514, Training Logs: loss_final: 0.869604, loss_mean: 0.825300, loss_mean_cls: 0.044304, grad_norm: 0.219486
[[34m2025-10-04 12:35:14[0m] Step: 7515, Training Logs: loss_final: 0.871652, loss_mean: 0.827047, loss_mean_cls: 0.044605, grad_norm: 0.212955
[[34m2025-10-04 12:35:14[0m] Step: 7516, Training Logs: loss_final: 0.861392, loss_mean: 0.818463, loss_mean_cls: 0.042929, grad_norm: 0.247757
[[34m2025-10-04 12:35:15[0m] Step: 7517, Training Logs: loss_final: 0.890985, loss_mean: 0.848671, loss_mean_cls: 0.042313, grad_norm: 0.265650
[[34m2025-10-04 12:35:15[0m] Step: 7518, Training Logs: loss_final: 0.878849, loss_mean: 0.836290, loss_mean_cls: 0.042558, grad_norm: 0.321947
[[34m2025-10-04 12:35:15[0m] Step: 7519, Training Logs: loss_final: 0.889265, loss_mean: 0.845606, loss_mean_cls: 0.043659, grad_norm: 0.195933
[[34m2025-10-04 12:35:15[0m] Step: 7520, Training Logs: loss_final: 0.870733, loss_mean: 0.826960, loss_mean_cls: 0.043773, grad_norm: 0.339558
[[34m2025-10-04 12:35:16[0m] Step: 7521, Training Logs: loss_final: 0.892852, loss_mean: 0.848561, loss_mean_cls: 0.044291, grad_norm: 0.356945
[[34m2025-10-04 12:35:16[0m] Step: 7522, Training Logs: loss_final: 0.878051, loss_mean: 0.834341, loss_mean_cls: 0.043710, grad_norm: 0.448313
[[34m2025-10-04 12:35:16[0m] Step: 7523, Training Logs: loss_final: 0.886853, loss_mean: 0.843499, loss_mean_cls: 0.043353, grad_norm: 0.304275
[[34m2025-10-04 12:35:17[0m] Step: 7524, Training Logs: loss_final: 0.879305, loss_mean: 0.835578, loss_mean_cls: 0.043728, grad_norm: 0.364879
[[34m2025-10-04 12:35:17[0m] Step: 7525, Training Logs: loss_final: 0.882635, loss_mean: 0.839478, loss_mean_cls: 0.043157, grad_norm: 0.427757
[[34m2025-10-04 12:35:17[0m] Step: 7526, Training Logs: loss_final: 0.864797, loss_mean: 0.820913, loss_mean_cls: 0.043883, grad_norm: 0.248918
[[34m2025-10-04 12:35:18[0m] Step: 7527, Training Logs: loss_final: 0.882473, loss_mean: 0.838157, loss_mean_cls: 0.044317, grad_norm: 0.394214
[[34m2025-10-04 12:35:18[0m] Step: 7528, Training Logs: loss_final: 0.874888, loss_mean: 0.830979, loss_mean_cls: 0.043909, grad_norm: 0.318659
[[34m2025-10-04 12:35:18[0m] Step: 7529, Training Logs: loss_final: 0.849020, loss_mean: 0.806448, loss_mean_cls: 0.042572, grad_norm: 0.322422
[[34m2025-10-04 12:35:18[0m] Step: 7530, Training Logs: loss_final: 0.890162, loss_mean: 0.846327, loss_mean_cls: 0.043836, grad_norm: 0.352634
[[34m2025-10-04 12:35:19[0m] Step: 7531, Training Logs: loss_final: 0.878097, loss_mean: 0.834241, loss_mean_cls: 0.043855, grad_norm: 0.384192
[[34m2025-10-04 12:35:19[0m] Step: 7532, Training Logs: loss_final: 0.893553, loss_mean: 0.851106, loss_mean_cls: 0.042447, grad_norm: 0.358129
[[34m2025-10-04 12:35:19[0m] Step: 7533, Training Logs: loss_final: 0.869777, loss_mean: 0.825210, loss_mean_cls: 0.044568, grad_norm: 0.279800
[[34m2025-10-04 12:35:20[0m] Step: 7534, Training Logs: loss_final: 0.861643, loss_mean: 0.819459, loss_mean_cls: 0.042185, grad_norm: 0.363028
[[34m2025-10-04 12:35:20[0m] Step: 7535, Training Logs: loss_final: 0.865290, loss_mean: 0.822297, loss_mean_cls: 0.042993, grad_norm: 0.349830
[[34m2025-10-04 12:35:20[0m] Step: 7536, Training Logs: loss_final: 0.869134, loss_mean: 0.825625, loss_mean_cls: 0.043508, grad_norm: 0.325003
[[34m2025-10-04 12:35:20[0m] Step: 7537, Training Logs: loss_final: 0.882834, loss_mean: 0.839641, loss_mean_cls: 0.043194, grad_norm: 0.327788
[[34m2025-10-04 12:35:21[0m] Step: 7538, Training Logs: loss_final: 0.872883, loss_mean: 0.828908, loss_mean_cls: 0.043975, grad_norm: 0.251163
[[34m2025-10-04 12:35:21[0m] Step: 7539, Training Logs: loss_final: 0.863140, loss_mean: 0.817888, loss_mean_cls: 0.045253, grad_norm: 0.275721
[[34m2025-10-04 12:35:21[0m] Step: 7540, Training Logs: loss_final: 0.869163, loss_mean: 0.824860, loss_mean_cls: 0.044303, grad_norm: 0.345994
[[34m2025-10-04 12:35:22[0m] Step: 7541, Training Logs: loss_final: 0.890987, loss_mean: 0.847646, loss_mean_cls: 0.043340, grad_norm: 0.315338
[[34m2025-10-04 12:35:22[0m] Step: 7542, Training Logs: loss_final: 0.882159, loss_mean: 0.837505, loss_mean_cls: 0.044654, grad_norm: 0.249914
[[34m2025-10-04 12:35:22[0m] Step: 7543, Training Logs: loss_final: 0.883802, loss_mean: 0.839571, loss_mean_cls: 0.044231, grad_norm: 0.302728
[[34m2025-10-04 12:35:23[0m] Step: 7544, Training Logs: loss_final: 0.893303, loss_mean: 0.849149, loss_mean_cls: 0.044154, grad_norm: 0.310521
[[34m2025-10-04 12:35:23[0m] Step: 7545, Training Logs: loss_final: 0.884138, loss_mean: 0.841222, loss_mean_cls: 0.042917, grad_norm: 0.227490
[[34m2025-10-04 12:35:23[0m] Step: 7546, Training Logs: loss_final: 0.874651, loss_mean: 0.830732, loss_mean_cls: 0.043919, grad_norm: 0.186968
[[34m2025-10-04 12:35:24[0m] Step: 7547, Training Logs: loss_final: 0.879757, loss_mean: 0.836266, loss_mean_cls: 0.043491, grad_norm: 0.326792
[[34m2025-10-04 12:35:24[0m] Step: 7548, Training Logs: loss_final: 0.863328, loss_mean: 0.819568, loss_mean_cls: 0.043760, grad_norm: 0.220812
[[34m2025-10-04 12:35:24[0m] Step: 7549, Training Logs: loss_final: 0.877805, loss_mean: 0.834556, loss_mean_cls: 0.043249, grad_norm: 0.291953
[[34m2025-10-04 12:35:24[0m] Step: 7550, Training Logs: loss_final: 0.887678, loss_mean: 0.843759, loss_mean_cls: 0.043919, grad_norm: 0.306341
[[34m2025-10-04 12:35:25[0m] Step: 7551, Training Logs: loss_final: 0.855289, loss_mean: 0.811375, loss_mean_cls: 0.043914, grad_norm: 0.281874
[[34m2025-10-04 12:35:25[0m] Step: 7552, Training Logs: loss_final: 0.882226, loss_mean: 0.839187, loss_mean_cls: 0.043038, grad_norm: 0.281002
[[34m2025-10-04 12:35:25[0m] Step: 7553, Training Logs: loss_final: 0.856042, loss_mean: 0.811970, loss_mean_cls: 0.044071, grad_norm: 0.338647
[[34m2025-10-04 12:35:26[0m] Step: 7554, Training Logs: loss_final: 0.881588, loss_mean: 0.838003, loss_mean_cls: 0.043585, grad_norm: 0.341333
[[34m2025-10-04 12:35:26[0m] Step: 7555, Training Logs: loss_final: 0.892843, loss_mean: 0.848722, loss_mean_cls: 0.044121, grad_norm: 0.298464
[[34m2025-10-04 12:35:26[0m] Step: 7556, Training Logs: loss_final: 0.873887, loss_mean: 0.829175, loss_mean_cls: 0.044712, grad_norm: 0.441500
[[34m2025-10-04 12:35:27[0m] Step: 7557, Training Logs: loss_final: 0.905230, loss_mean: 0.861604, loss_mean_cls: 0.043626, grad_norm: 0.328969
[[34m2025-10-04 12:35:27[0m] Step: 7558, Training Logs: loss_final: 0.885495, loss_mean: 0.842054, loss_mean_cls: 0.043441, grad_norm: 0.235706
[[34m2025-10-04 12:35:27[0m] Step: 7559, Training Logs: loss_final: 0.895029, loss_mean: 0.851171, loss_mean_cls: 0.043858, grad_norm: 0.343887
[[34m2025-10-04 12:35:27[0m] Step: 7560, Training Logs: loss_final: 0.866060, loss_mean: 0.822002, loss_mean_cls: 0.044058, grad_norm: 0.446012
[[34m2025-10-04 12:35:28[0m] Step: 7561, Training Logs: loss_final: 0.867155, loss_mean: 0.822095, loss_mean_cls: 0.045059, grad_norm: 0.330532
[[34m2025-10-04 12:35:28[0m] Step: 7562, Training Logs: loss_final: 0.871627, loss_mean: 0.828265, loss_mean_cls: 0.043361, grad_norm: 0.379059
[[34m2025-10-04 12:35:28[0m] Step: 7563, Training Logs: loss_final: 0.879846, loss_mean: 0.837312, loss_mean_cls: 0.042534, grad_norm: 0.335890
[[34m2025-10-04 12:35:29[0m] Step: 7564, Training Logs: loss_final: 0.867082, loss_mean: 0.822163, loss_mean_cls: 0.044919, grad_norm: 0.493278
[[34m2025-10-04 12:35:29[0m] Step: 7565, Training Logs: loss_final: 0.868916, loss_mean: 0.823789, loss_mean_cls: 0.045127, grad_norm: 0.338023
[[34m2025-10-04 12:35:29[0m] Step: 7566, Training Logs: loss_final: 0.874050, loss_mean: 0.829103, loss_mean_cls: 0.044947, grad_norm: 0.375556
[[34m2025-10-04 12:35:30[0m] Step: 7567, Training Logs: loss_final: 0.883802, loss_mean: 0.840563, loss_mean_cls: 0.043239, grad_norm: 0.382579
[[34m2025-10-04 12:35:30[0m] Step: 7568, Training Logs: loss_final: 0.883724, loss_mean: 0.839217, loss_mean_cls: 0.044507, grad_norm: 0.474841
[[34m2025-10-04 12:35:30[0m] Step: 7569, Training Logs: loss_final: 0.877952, loss_mean: 0.835173, loss_mean_cls: 0.042779, grad_norm: 0.234028
[[34m2025-10-04 12:35:30[0m] Step: 7570, Training Logs: loss_final: 0.869339, loss_mean: 0.824849, loss_mean_cls: 0.044490, grad_norm: 0.348107
[[34m2025-10-04 12:35:31[0m] Step: 7571, Training Logs: loss_final: 0.881200, loss_mean: 0.837572, loss_mean_cls: 0.043629, grad_norm: 0.482359
[[34m2025-10-04 12:35:31[0m] Step: 7572, Training Logs: loss_final: 0.888204, loss_mean: 0.844430, loss_mean_cls: 0.043774, grad_norm: 0.299652
[[34m2025-10-04 12:35:31[0m] Step: 7573, Training Logs: loss_final: 0.879138, loss_mean: 0.835851, loss_mean_cls: 0.043287, grad_norm: 0.370538
[[34m2025-10-04 12:35:32[0m] Step: 7574, Training Logs: loss_final: 0.883684, loss_mean: 0.838862, loss_mean_cls: 0.044822, grad_norm: 0.225914
[[34m2025-10-04 12:35:32[0m] Step: 7575, Training Logs: loss_final: 0.892175, loss_mean: 0.849967, loss_mean_cls: 0.042209, grad_norm: 0.366462
[[34m2025-10-04 12:35:32[0m] Step: 7576, Training Logs: loss_final: 0.862794, loss_mean: 0.819175, loss_mean_cls: 0.043619, grad_norm: 0.270367
[[34m2025-10-04 12:35:32[0m] Step: 7577, Training Logs: loss_final: 0.872725, loss_mean: 0.829000, loss_mean_cls: 0.043725, grad_norm: 0.224162
[[34m2025-10-04 12:35:33[0m] Step: 7578, Training Logs: loss_final: 0.873443, loss_mean: 0.831121, loss_mean_cls: 0.042322, grad_norm: 0.430217
[[34m2025-10-04 12:35:33[0m] Step: 7579, Training Logs: loss_final: 0.871178, loss_mean: 0.827517, loss_mean_cls: 0.043662, grad_norm: 0.380176
[[34m2025-10-04 12:35:33[0m] Step: 7580, Training Logs: loss_final: 0.873794, loss_mean: 0.829935, loss_mean_cls: 0.043859, grad_norm: 0.372213
[[34m2025-10-04 12:35:34[0m] Step: 7581, Training Logs: loss_final: 0.858140, loss_mean: 0.814971, loss_mean_cls: 0.043170, grad_norm: 0.286006
[[34m2025-10-04 12:35:34[0m] Step: 7582, Training Logs: loss_final: 0.866619, loss_mean: 0.822890, loss_mean_cls: 0.043730, grad_norm: 0.342875
[[34m2025-10-04 12:35:34[0m] Step: 7583, Training Logs: loss_final: 0.867498, loss_mean: 0.824725, loss_mean_cls: 0.042772, grad_norm: 0.273479
[[34m2025-10-04 12:35:35[0m] Step: 7584, Training Logs: loss_final: 0.879686, loss_mean: 0.836330, loss_mean_cls: 0.043356, grad_norm: 0.333603
[[34m2025-10-04 12:35:35[0m] Step: 7585, Training Logs: loss_final: 0.878552, loss_mean: 0.834852, loss_mean_cls: 0.043700, grad_norm: 0.402910
[[34m2025-10-04 12:35:35[0m] Step: 7586, Training Logs: loss_final: 0.878667, loss_mean: 0.834627, loss_mean_cls: 0.044040, grad_norm: 0.284183
[[34m2025-10-04 12:35:35[0m] Step: 7587, Training Logs: loss_final: 0.881285, loss_mean: 0.837708, loss_mean_cls: 0.043576, grad_norm: 0.339140
[[34m2025-10-04 12:35:36[0m] Step: 7588, Training Logs: loss_final: 0.878201, loss_mean: 0.833534, loss_mean_cls: 0.044668, grad_norm: 0.316940
[[34m2025-10-04 12:35:36[0m] Step: 7589, Training Logs: loss_final: 0.865980, loss_mean: 0.823415, loss_mean_cls: 0.042565, grad_norm: 0.302030
[[34m2025-10-04 12:35:36[0m] Step: 7590, Training Logs: loss_final: 0.883963, loss_mean: 0.840966, loss_mean_cls: 0.042997, grad_norm: 0.329672
[[34m2025-10-04 12:35:37[0m] Step: 7591, Training Logs: loss_final: 0.880537, loss_mean: 0.836769, loss_mean_cls: 0.043768, grad_norm: 0.257330
[[34m2025-10-04 12:35:37[0m] Step: 7592, Training Logs: loss_final: 0.853899, loss_mean: 0.807680, loss_mean_cls: 0.046219, grad_norm: 0.370720
[[34m2025-10-04 12:35:37[0m] Step: 7593, Training Logs: loss_final: 0.897208, loss_mean: 0.854446, loss_mean_cls: 0.042762, grad_norm: 0.352488
[[34m2025-10-04 12:35:38[0m] Step: 7594, Training Logs: loss_final: 0.874002, loss_mean: 0.830461, loss_mean_cls: 0.043541, grad_norm: 0.364141
[[34m2025-10-04 12:35:38[0m] Step: 7595, Training Logs: loss_final: 0.900532, loss_mean: 0.856840, loss_mean_cls: 0.043692, grad_norm: 0.400028
[[34m2025-10-04 12:35:38[0m] Step: 7596, Training Logs: loss_final: 0.860986, loss_mean: 0.817206, loss_mean_cls: 0.043781, grad_norm: 0.289061
[[34m2025-10-04 12:35:39[0m] Step: 7597, Training Logs: loss_final: 0.886008, loss_mean: 0.842301, loss_mean_cls: 0.043707, grad_norm: 0.316885
[[34m2025-10-04 12:35:39[0m] Step: 7598, Training Logs: loss_final: 0.870261, loss_mean: 0.826891, loss_mean_cls: 0.043371, grad_norm: 0.262907
[[34m2025-10-04 12:35:39[0m] Step: 7599, Training Logs: loss_final: 0.880308, loss_mean: 0.835982, loss_mean_cls: 0.044326, grad_norm: 0.280213
[[34m2025-10-04 12:35:39[0m] Step: 7600, Training Logs: loss_final: 0.880763, loss_mean: 0.838487, loss_mean_cls: 0.042276, grad_norm: 0.327068
[[34m2025-10-04 12:35:40[0m] Step: 7601, Training Logs: loss_final: 0.877113, loss_mean: 0.833532, loss_mean_cls: 0.043581, grad_norm: 0.241200
[[34m2025-10-04 12:35:40[0m] Step: 7602, Training Logs: loss_final: 0.880180, loss_mean: 0.836757, loss_mean_cls: 0.043424, grad_norm: 0.274503
[[34m2025-10-04 12:35:40[0m] Step: 7603, Training Logs: loss_final: 0.880560, loss_mean: 0.837491, loss_mean_cls: 0.043068, grad_norm: 0.251074
[[34m2025-10-04 12:35:41[0m] Step: 7604, Training Logs: loss_final: 0.875048, loss_mean: 0.831037, loss_mean_cls: 0.044010, grad_norm: 0.223143
[[34m2025-10-04 12:35:41[0m] Step: 7605, Training Logs: loss_final: 0.898376, loss_mean: 0.855124, loss_mean_cls: 0.043252, grad_norm: 0.333299
[[34m2025-10-04 12:35:41[0m] Step: 7606, Training Logs: loss_final: 0.874240, loss_mean: 0.830907, loss_mean_cls: 0.043332, grad_norm: 0.317988
[[34m2025-10-04 12:35:41[0m] Step: 7607, Training Logs: loss_final: 0.866523, loss_mean: 0.822362, loss_mean_cls: 0.044161, grad_norm: 0.301132
[[34m2025-10-04 12:35:42[0m] Step: 7608, Training Logs: loss_final: 0.897492, loss_mean: 0.853382, loss_mean_cls: 0.044110, grad_norm: 0.238322
[[34m2025-10-04 12:35:42[0m] Step: 7609, Training Logs: loss_final: 0.879256, loss_mean: 0.835961, loss_mean_cls: 0.043295, grad_norm: 0.217453
[[34m2025-10-04 12:35:42[0m] Step: 7610, Training Logs: loss_final: 0.873300, loss_mean: 0.829978, loss_mean_cls: 0.043322, grad_norm: 0.384640
[[34m2025-10-04 12:35:43[0m] Step: 7611, Training Logs: loss_final: 0.875057, loss_mean: 0.832280, loss_mean_cls: 0.042778, grad_norm: 0.234679
[[34m2025-10-04 12:35:43[0m] Step: 7612, Training Logs: loss_final: 0.890647, loss_mean: 0.846622, loss_mean_cls: 0.044025, grad_norm: 0.266884
[[34m2025-10-04 12:35:43[0m] Step: 7613, Training Logs: loss_final: 0.878100, loss_mean: 0.835092, loss_mean_cls: 0.043007, grad_norm: 0.397817
[[34m2025-10-04 12:35:43[0m] Step: 7614, Training Logs: loss_final: 0.865410, loss_mean: 0.822114, loss_mean_cls: 0.043296, grad_norm: 0.247864
[[34m2025-10-04 12:35:44[0m] Step: 7615, Training Logs: loss_final: 0.885872, loss_mean: 0.843248, loss_mean_cls: 0.042624, grad_norm: 0.196230
[[34m2025-10-04 12:35:44[0m] Step: 7616, Training Logs: loss_final: 0.875890, loss_mean: 0.832941, loss_mean_cls: 0.042949, grad_norm: 0.200572
[[34m2025-10-04 12:35:44[0m] Step: 7617, Training Logs: loss_final: 0.891137, loss_mean: 0.847455, loss_mean_cls: 0.043681, grad_norm: 0.250572
[[34m2025-10-04 12:35:45[0m] Step: 7618, Training Logs: loss_final: 0.859201, loss_mean: 0.814601, loss_mean_cls: 0.044600, grad_norm: 0.227535
[[34m2025-10-04 12:35:45[0m] Step: 7619, Training Logs: loss_final: 0.882132, loss_mean: 0.838086, loss_mean_cls: 0.044046, grad_norm: 0.237406
[[34m2025-10-04 12:35:45[0m] Step: 7620, Training Logs: loss_final: 0.874803, loss_mean: 0.831487, loss_mean_cls: 0.043316, grad_norm: 0.211108
[[34m2025-10-04 12:35:46[0m] Step: 7621, Training Logs: loss_final: 0.860759, loss_mean: 0.817099, loss_mean_cls: 0.043660, grad_norm: 0.332886
[[34m2025-10-04 12:35:46[0m] Step: 7622, Training Logs: loss_final: 0.895086, loss_mean: 0.852446, loss_mean_cls: 0.042640, grad_norm: 0.225563
[[34m2025-10-04 12:35:46[0m] Step: 7623, Training Logs: loss_final: 0.888446, loss_mean: 0.843648, loss_mean_cls: 0.044799, grad_norm: 0.235743
[[34m2025-10-04 12:35:46[0m] Step: 7624, Training Logs: loss_final: 0.858524, loss_mean: 0.814643, loss_mean_cls: 0.043881, grad_norm: 0.353258
[[34m2025-10-04 12:35:47[0m] Step: 7625, Training Logs: loss_final: 0.861421, loss_mean: 0.816559, loss_mean_cls: 0.044862, grad_norm: 0.440142
[[34m2025-10-04 12:35:47[0m] Step: 7626, Training Logs: loss_final: 0.871708, loss_mean: 0.827760, loss_mean_cls: 0.043948, grad_norm: 0.295374
[[34m2025-10-04 12:35:47[0m] Step: 7627, Training Logs: loss_final: 0.886158, loss_mean: 0.842322, loss_mean_cls: 0.043836, grad_norm: 0.370261
[[34m2025-10-04 12:35:48[0m] Step: 7628, Training Logs: loss_final: 0.881939, loss_mean: 0.837754, loss_mean_cls: 0.044185, grad_norm: 0.200883
[[34m2025-10-04 12:35:48[0m] Step: 7629, Training Logs: loss_final: 0.878886, loss_mean: 0.834564, loss_mean_cls: 0.044321, grad_norm: 0.463199
[[34m2025-10-04 12:35:48[0m] Step: 7630, Training Logs: loss_final: 0.873153, loss_mean: 0.829056, loss_mean_cls: 0.044097, grad_norm: 0.343724
[[34m2025-10-04 12:35:48[0m] Step: 7631, Training Logs: loss_final: 0.866299, loss_mean: 0.823647, loss_mean_cls: 0.042653, grad_norm: 0.224726
[[34m2025-10-04 12:35:49[0m] Step: 7632, Training Logs: loss_final: 0.891724, loss_mean: 0.848600, loss_mean_cls: 0.043124, grad_norm: 0.309717
[[34m2025-10-04 12:35:49[0m] Step: 7633, Training Logs: loss_final: 0.865766, loss_mean: 0.821632, loss_mean_cls: 0.044134, grad_norm: 0.277365
[[34m2025-10-04 12:35:49[0m] Step: 7634, Training Logs: loss_final: 0.880540, loss_mean: 0.836656, loss_mean_cls: 0.043884, grad_norm: 0.312595
[[34m2025-10-04 12:35:50[0m] Step: 7635, Training Logs: loss_final: 0.875439, loss_mean: 0.831638, loss_mean_cls: 0.043801, grad_norm: 0.274286
[[34m2025-10-04 12:35:50[0m] Step: 7636, Training Logs: loss_final: 0.890284, loss_mean: 0.847523, loss_mean_cls: 0.042761, grad_norm: 0.296172
[[34m2025-10-04 12:35:50[0m] Step: 7637, Training Logs: loss_final: 0.867781, loss_mean: 0.823094, loss_mean_cls: 0.044687, grad_norm: 0.260159
[[34m2025-10-04 12:35:51[0m] Step: 7638, Training Logs: loss_final: 0.874499, loss_mean: 0.832202, loss_mean_cls: 0.042297, grad_norm: 0.351092
[[34m2025-10-04 12:35:51[0m] Step: 7639, Training Logs: loss_final: 0.874689, loss_mean: 0.830178, loss_mean_cls: 0.044511, grad_norm: 0.250491
[[34m2025-10-04 12:35:51[0m] Step: 7640, Training Logs: loss_final: 0.853942, loss_mean: 0.811345, loss_mean_cls: 0.042597, grad_norm: 0.325519
[[34m2025-10-04 12:35:51[0m] Step: 7641, Training Logs: loss_final: 0.872498, loss_mean: 0.828413, loss_mean_cls: 0.044085, grad_norm: 0.218122
[[34m2025-10-04 12:35:52[0m] Step: 7642, Training Logs: loss_final: 0.890264, loss_mean: 0.848448, loss_mean_cls: 0.041816, grad_norm: 0.232943
[[34m2025-10-04 12:35:52[0m] Step: 7643, Training Logs: loss_final: 0.893700, loss_mean: 0.850715, loss_mean_cls: 0.042985, grad_norm: 0.260861
[[34m2025-10-04 12:35:52[0m] Step: 7644, Training Logs: loss_final: 0.877651, loss_mean: 0.834226, loss_mean_cls: 0.043425, grad_norm: 0.251756
[[34m2025-10-04 12:35:53[0m] Step: 7645, Training Logs: loss_final: 0.894867, loss_mean: 0.852040, loss_mean_cls: 0.042828, grad_norm: 0.256912
[[34m2025-10-04 12:35:53[0m] Step: 7646, Training Logs: loss_final: 0.891125, loss_mean: 0.847750, loss_mean_cls: 0.043375, grad_norm: 0.282491
[[34m2025-10-04 12:35:53[0m] Step: 7647, Training Logs: loss_final: 0.874954, loss_mean: 0.831356, loss_mean_cls: 0.043598, grad_norm: 0.238081
[[34m2025-10-04 12:35:53[0m] Step: 7648, Training Logs: loss_final: 0.894068, loss_mean: 0.851179, loss_mean_cls: 0.042890, grad_norm: 0.293029
[[34m2025-10-04 12:35:54[0m] Step: 7649, Training Logs: loss_final: 0.888027, loss_mean: 0.844473, loss_mean_cls: 0.043554, grad_norm: 0.241611
[[34m2025-10-04 12:35:54[0m] Step: 7650, Training Logs: loss_final: 0.877815, loss_mean: 0.834612, loss_mean_cls: 0.043202, grad_norm: 0.284531
[[34m2025-10-04 12:35:54[0m] Step: 7651, Training Logs: loss_final: 0.874136, loss_mean: 0.830806, loss_mean_cls: 0.043330, grad_norm: 0.203807
[[34m2025-10-04 12:35:55[0m] Step: 7652, Training Logs: loss_final: 0.861345, loss_mean: 0.818310, loss_mean_cls: 0.043034, grad_norm: 0.248762
[[34m2025-10-04 12:35:55[0m] Step: 7653, Training Logs: loss_final: 0.898507, loss_mean: 0.855526, loss_mean_cls: 0.042981, grad_norm: 0.257834
[[34m2025-10-04 12:35:55[0m] Step: 7654, Training Logs: loss_final: 0.868377, loss_mean: 0.824842, loss_mean_cls: 0.043535, grad_norm: 0.236729
[[34m2025-10-04 12:35:56[0m] Step: 7655, Training Logs: loss_final: 0.880548, loss_mean: 0.836163, loss_mean_cls: 0.044385, grad_norm: 0.202637
[[34m2025-10-04 12:35:56[0m] Step: 7656, Training Logs: loss_final: 0.878008, loss_mean: 0.833990, loss_mean_cls: 0.044017, grad_norm: 0.229944
[[34m2025-10-04 12:35:56[0m] Step: 7657, Training Logs: loss_final: 0.863525, loss_mean: 0.819177, loss_mean_cls: 0.044348, grad_norm: 0.242884
[[34m2025-10-04 12:35:56[0m] Step: 7658, Training Logs: loss_final: 0.897965, loss_mean: 0.856539, loss_mean_cls: 0.041426, grad_norm: 0.319580
[[34m2025-10-04 12:35:57[0m] Step: 7659, Training Logs: loss_final: 0.861599, loss_mean: 0.818043, loss_mean_cls: 0.043556, grad_norm: 0.233840
[[34m2025-10-04 12:35:57[0m] Step: 7660, Training Logs: loss_final: 0.886305, loss_mean: 0.842791, loss_mean_cls: 0.043514, grad_norm: 0.290760
[[34m2025-10-04 12:35:57[0m] Step: 7661, Training Logs: loss_final: 0.868622, loss_mean: 0.824610, loss_mean_cls: 0.044012, grad_norm: 0.356035
[[34m2025-10-04 12:35:58[0m] Step: 7662, Training Logs: loss_final: 0.866450, loss_mean: 0.821756, loss_mean_cls: 0.044694, grad_norm: 0.254990
[[34m2025-10-04 12:35:58[0m] Step: 7663, Training Logs: loss_final: 0.860163, loss_mean: 0.816072, loss_mean_cls: 0.044091, grad_norm: 0.239170
[[34m2025-10-04 12:35:58[0m] Step: 7664, Training Logs: loss_final: 0.871097, loss_mean: 0.827647, loss_mean_cls: 0.043450, grad_norm: 0.342329
[[34m2025-10-04 12:35:58[0m] Step: 7665, Training Logs: loss_final: 0.868318, loss_mean: 0.825272, loss_mean_cls: 0.043045, grad_norm: 0.198420
[[34m2025-10-04 12:35:59[0m] Step: 7666, Training Logs: loss_final: 0.882441, loss_mean: 0.838798, loss_mean_cls: 0.043643, grad_norm: 0.201764
[[34m2025-10-04 12:35:59[0m] Step: 7667, Training Logs: loss_final: 0.882137, loss_mean: 0.837877, loss_mean_cls: 0.044260, grad_norm: 0.190024
[[34m2025-10-04 12:35:59[0m] Step: 7668, Training Logs: loss_final: 0.881795, loss_mean: 0.838316, loss_mean_cls: 0.043479, grad_norm: 0.264365
[[34m2025-10-04 12:36:00[0m] Step: 7669, Training Logs: loss_final: 0.872078, loss_mean: 0.827213, loss_mean_cls: 0.044865, grad_norm: 0.406263
[[34m2025-10-04 12:36:00[0m] Step: 7670, Training Logs: loss_final: 0.863227, loss_mean: 0.819205, loss_mean_cls: 0.044022, grad_norm: 0.235870
[[34m2025-10-04 12:36:00[0m] Step: 7671, Training Logs: loss_final: 0.884521, loss_mean: 0.842261, loss_mean_cls: 0.042260, grad_norm: 0.211117
[[34m2025-10-04 12:36:00[0m] Step: 7672, Training Logs: loss_final: 0.855640, loss_mean: 0.811195, loss_mean_cls: 0.044445, grad_norm: 0.284044
[[34m2025-10-04 12:36:01[0m] Step: 7673, Training Logs: loss_final: 0.885457, loss_mean: 0.841352, loss_mean_cls: 0.044105, grad_norm: 0.245942
[[34m2025-10-04 12:36:01[0m] Step: 7674, Training Logs: loss_final: 0.868581, loss_mean: 0.825573, loss_mean_cls: 0.043008, grad_norm: 0.216643
[[34m2025-10-04 12:36:01[0m] Step: 7675, Training Logs: loss_final: 0.873109, loss_mean: 0.829863, loss_mean_cls: 0.043246, grad_norm: 0.286061
[[34m2025-10-04 12:36:02[0m] Step: 7676, Training Logs: loss_final: 0.875847, loss_mean: 0.830891, loss_mean_cls: 0.044956, grad_norm: 0.314667
[[34m2025-10-04 12:36:02[0m] Step: 7677, Training Logs: loss_final: 0.885118, loss_mean: 0.842455, loss_mean_cls: 0.042663, grad_norm: 0.286683
[[34m2025-10-04 12:36:02[0m] Step: 7678, Training Logs: loss_final: 0.877561, loss_mean: 0.832560, loss_mean_cls: 0.045001, grad_norm: 0.256803
[[34m2025-10-04 12:36:02[0m] Step: 7679, Training Logs: loss_final: 0.864857, loss_mean: 0.821661, loss_mean_cls: 0.043196, grad_norm: 0.317833
[[34m2025-10-04 12:36:03[0m] Step: 7680, Training Logs: loss_final: 0.875826, loss_mean: 0.832622, loss_mean_cls: 0.043204, grad_norm: 0.279390
[[34m2025-10-04 12:36:03[0m] Step: 7681, Training Logs: loss_final: 0.867020, loss_mean: 0.823427, loss_mean_cls: 0.043593, grad_norm: 0.247597
[[34m2025-10-04 12:36:03[0m] Step: 7682, Training Logs: loss_final: 0.876654, loss_mean: 0.833314, loss_mean_cls: 0.043340, grad_norm: 0.238692
[[34m2025-10-04 12:36:04[0m] Step: 7683, Training Logs: loss_final: 0.880525, loss_mean: 0.836438, loss_mean_cls: 0.044087, grad_norm: 0.243945
[[34m2025-10-04 12:36:04[0m] Step: 7684, Training Logs: loss_final: 0.871239, loss_mean: 0.827026, loss_mean_cls: 0.044214, grad_norm: 0.506273
[[34m2025-10-04 12:36:04[0m] Step: 7685, Training Logs: loss_final: 0.869768, loss_mean: 0.824941, loss_mean_cls: 0.044827, grad_norm: 0.412430
[[34m2025-10-04 12:36:04[0m] Step: 7686, Training Logs: loss_final: 0.881765, loss_mean: 0.837578, loss_mean_cls: 0.044187, grad_norm: 0.284505
[[34m2025-10-04 12:36:05[0m] Step: 7687, Training Logs: loss_final: 0.886577, loss_mean: 0.843150, loss_mean_cls: 0.043427, grad_norm: 0.351704
[[34m2025-10-04 12:36:05[0m] Step: 7688, Training Logs: loss_final: 0.888185, loss_mean: 0.843934, loss_mean_cls: 0.044250, grad_norm: 0.250086
[[34m2025-10-04 12:36:05[0m] Step: 7689, Training Logs: loss_final: 0.856880, loss_mean: 0.813800, loss_mean_cls: 0.043080, grad_norm: 0.261713
[[34m2025-10-04 12:36:06[0m] Step: 7690, Training Logs: loss_final: 0.880996, loss_mean: 0.838356, loss_mean_cls: 0.042640, grad_norm: 0.343971
[[34m2025-10-04 12:36:06[0m] Step: 7691, Training Logs: loss_final: 0.884142, loss_mean: 0.840783, loss_mean_cls: 0.043360, grad_norm: 0.394241
[[34m2025-10-04 12:36:06[0m] Step: 7692, Training Logs: loss_final: 0.871809, loss_mean: 0.827788, loss_mean_cls: 0.044021, grad_norm: 0.352750
[[34m2025-10-04 12:36:07[0m] Step: 7693, Training Logs: loss_final: 0.880511, loss_mean: 0.836813, loss_mean_cls: 0.043698, grad_norm: 0.564014
[[34m2025-10-04 12:36:07[0m] Step: 7694, Training Logs: loss_final: 0.904690, loss_mean: 0.860881, loss_mean_cls: 0.043809, grad_norm: 0.220126
[[34m2025-10-04 12:36:07[0m] Step: 7695, Training Logs: loss_final: 0.881637, loss_mean: 0.838192, loss_mean_cls: 0.043445, grad_norm: 0.674012
[[34m2025-10-04 12:36:07[0m] Step: 7696, Training Logs: loss_final: 0.887960, loss_mean: 0.844310, loss_mean_cls: 0.043650, grad_norm: 0.249777
[[34m2025-10-04 12:36:08[0m] Step: 7697, Training Logs: loss_final: 0.886689, loss_mean: 0.842891, loss_mean_cls: 0.043798, grad_norm: 0.606505
[[34m2025-10-04 12:36:08[0m] Step: 7698, Training Logs: loss_final: 0.905657, loss_mean: 0.862185, loss_mean_cls: 0.043472, grad_norm: 0.318988
[[34m2025-10-04 12:36:08[0m] Step: 7699, Training Logs: loss_final: 0.872293, loss_mean: 0.828729, loss_mean_cls: 0.043564, grad_norm: 0.604154
[[34m2025-10-04 12:36:09[0m] Step: 7700, Training Logs: loss_final: 0.885594, loss_mean: 0.843042, loss_mean_cls: 0.042552, grad_norm: 0.386846
[[34m2025-10-04 12:36:09[0m] Step: 7701, Training Logs: loss_final: 0.878921, loss_mean: 0.834959, loss_mean_cls: 0.043962, grad_norm: 0.476821
[[34m2025-10-04 12:36:09[0m] Step: 7702, Training Logs: loss_final: 0.877529, loss_mean: 0.832510, loss_mean_cls: 0.045018, grad_norm: 0.448636
[[34m2025-10-04 12:36:09[0m] Step: 7703, Training Logs: loss_final: 0.867273, loss_mean: 0.823083, loss_mean_cls: 0.044190, grad_norm: 0.468332
[[34m2025-10-04 12:36:10[0m] Step: 7704, Training Logs: loss_final: 0.888018, loss_mean: 0.844897, loss_mean_cls: 0.043121, grad_norm: 0.675978
[[34m2025-10-04 12:36:10[0m] Step: 7705, Training Logs: loss_final: 0.878727, loss_mean: 0.833773, loss_mean_cls: 0.044955, grad_norm: 0.310564
[[34m2025-10-04 12:36:10[0m] Step: 7706, Training Logs: loss_final: 0.873773, loss_mean: 0.830307, loss_mean_cls: 0.043465, grad_norm: 0.415930
[[34m2025-10-04 12:36:11[0m] Step: 7707, Training Logs: loss_final: 0.875894, loss_mean: 0.832660, loss_mean_cls: 0.043234, grad_norm: 0.330897
[[34m2025-10-04 12:36:11[0m] Step: 7708, Training Logs: loss_final: 0.874706, loss_mean: 0.831137, loss_mean_cls: 0.043569, grad_norm: 0.588198
[[34m2025-10-04 12:36:11[0m] Step: 7709, Training Logs: loss_final: 0.876929, loss_mean: 0.833600, loss_mean_cls: 0.043329, grad_norm: 0.170189
[[34m2025-10-04 12:36:12[0m] Step: 7710, Training Logs: loss_final: 0.877439, loss_mean: 0.834320, loss_mean_cls: 0.043118, grad_norm: 0.534801
[[34m2025-10-04 12:36:12[0m] Step: 7711, Training Logs: loss_final: 0.881811, loss_mean: 0.839136, loss_mean_cls: 0.042675, grad_norm: 0.213174
[[34m2025-10-04 12:36:12[0m] Step: 7712, Training Logs: loss_final: 0.887294, loss_mean: 0.843326, loss_mean_cls: 0.043968, grad_norm: 0.519956
[[34m2025-10-04 12:36:12[0m] Step: 7713, Training Logs: loss_final: 0.886205, loss_mean: 0.843261, loss_mean_cls: 0.042944, grad_norm: 0.301779
[[34m2025-10-04 12:36:13[0m] Step: 7714, Training Logs: loss_final: 0.877738, loss_mean: 0.834441, loss_mean_cls: 0.043297, grad_norm: 0.407709
[[34m2025-10-04 12:36:13[0m] Step: 7715, Training Logs: loss_final: 0.864162, loss_mean: 0.819552, loss_mean_cls: 0.044610, grad_norm: 0.285800
[[34m2025-10-04 12:36:13[0m] Step: 7716, Training Logs: loss_final: 0.883997, loss_mean: 0.840708, loss_mean_cls: 0.043289, grad_norm: 0.548166
[[34m2025-10-04 12:36:14[0m] Step: 7717, Training Logs: loss_final: 0.892328, loss_mean: 0.849666, loss_mean_cls: 0.042662, grad_norm: 0.205546
[[34m2025-10-04 12:36:14[0m] Step: 7718, Training Logs: loss_final: 0.867638, loss_mean: 0.825519, loss_mean_cls: 0.042119, grad_norm: 0.669868
[[34m2025-10-04 12:36:14[0m] Step: 7719, Training Logs: loss_final: 0.864890, loss_mean: 0.819295, loss_mean_cls: 0.045594, grad_norm: 0.238343
[[34m2025-10-04 12:36:15[0m] Step: 7720, Training Logs: loss_final: 0.888747, loss_mean: 0.845723, loss_mean_cls: 0.043024, grad_norm: 0.620516
[[34m2025-10-04 12:36:15[0m] Step: 7721, Training Logs: loss_final: 0.867708, loss_mean: 0.824379, loss_mean_cls: 0.043329, grad_norm: 0.285319
[[34m2025-10-04 12:36:15[0m] Step: 7722, Training Logs: loss_final: 0.884582, loss_mean: 0.840707, loss_mean_cls: 0.043875, grad_norm: 0.605808
[[34m2025-10-04 12:36:15[0m] Step: 7723, Training Logs: loss_final: 0.891005, loss_mean: 0.847045, loss_mean_cls: 0.043959, grad_norm: 0.486589
[[34m2025-10-04 12:36:16[0m] Step: 7724, Training Logs: loss_final: 0.889923, loss_mean: 0.847523, loss_mean_cls: 0.042400, grad_norm: 0.426876
[[34m2025-10-04 12:36:16[0m] Step: 7725, Training Logs: loss_final: 0.903601, loss_mean: 0.860184, loss_mean_cls: 0.043418, grad_norm: 0.260698
[[34m2025-10-04 12:36:16[0m] Step: 7726, Training Logs: loss_final: 0.863436, loss_mean: 0.819276, loss_mean_cls: 0.044160, grad_norm: 0.268268
[[34m2025-10-04 12:36:17[0m] Step: 7727, Training Logs: loss_final: 0.888039, loss_mean: 0.843670, loss_mean_cls: 0.044369, grad_norm: 0.394946
[[34m2025-10-04 12:36:17[0m] Step: 7728, Training Logs: loss_final: 0.878506, loss_mean: 0.835228, loss_mean_cls: 0.043278, grad_norm: 0.384957
[[34m2025-10-04 12:36:17[0m] Step: 7729, Training Logs: loss_final: 0.863484, loss_mean: 0.820326, loss_mean_cls: 0.043158, grad_norm: 0.221839
[[34m2025-10-04 12:36:18[0m] Step: 7730, Training Logs: loss_final: 0.860668, loss_mean: 0.817198, loss_mean_cls: 0.043470, grad_norm: 0.300097
[[34m2025-10-04 12:36:18[0m] Step: 7731, Training Logs: loss_final: 0.869149, loss_mean: 0.825964, loss_mean_cls: 0.043185, grad_norm: 0.303200
[[34m2025-10-04 12:36:18[0m] Step: 7732, Training Logs: loss_final: 0.880299, loss_mean: 0.835847, loss_mean_cls: 0.044453, grad_norm: 0.288824
[[34m2025-10-04 12:36:18[0m] Step: 7733, Training Logs: loss_final: 0.862862, loss_mean: 0.817352, loss_mean_cls: 0.045510, grad_norm: 0.253588
[[34m2025-10-04 12:36:19[0m] Step: 7734, Training Logs: loss_final: 0.875982, loss_mean: 0.833089, loss_mean_cls: 0.042893, grad_norm: 0.240566
[[34m2025-10-04 12:36:19[0m] Step: 7735, Training Logs: loss_final: 0.858260, loss_mean: 0.814106, loss_mean_cls: 0.044154, grad_norm: 0.353328
[[34m2025-10-04 12:36:19[0m] Step: 7736, Training Logs: loss_final: 0.897676, loss_mean: 0.855606, loss_mean_cls: 0.042070, grad_norm: 0.270336
[[34m2025-10-04 12:36:20[0m] Step: 7737, Training Logs: loss_final: 0.865477, loss_mean: 0.821274, loss_mean_cls: 0.044203, grad_norm: 0.275048
[[34m2025-10-04 12:36:20[0m] Step: 7738, Training Logs: loss_final: 0.903204, loss_mean: 0.860106, loss_mean_cls: 0.043098, grad_norm: 0.408350
[[34m2025-10-04 12:36:20[0m] Step: 7739, Training Logs: loss_final: 0.896937, loss_mean: 0.855140, loss_mean_cls: 0.041797, grad_norm: 0.382154
[[34m2025-10-04 12:36:20[0m] Step: 7740, Training Logs: loss_final: 0.883493, loss_mean: 0.839450, loss_mean_cls: 0.044044, grad_norm: 0.235364
[[34m2025-10-04 12:36:21[0m] Step: 7741, Training Logs: loss_final: 0.875060, loss_mean: 0.832195, loss_mean_cls: 0.042865, grad_norm: 0.410939
[[34m2025-10-04 12:36:21[0m] Step: 7742, Training Logs: loss_final: 0.877960, loss_mean: 0.834543, loss_mean_cls: 0.043417, grad_norm: 0.311250
[[34m2025-10-04 12:36:21[0m] Step: 7743, Training Logs: loss_final: 0.865296, loss_mean: 0.820827, loss_mean_cls: 0.044469, grad_norm: 0.241044
[[34m2025-10-04 12:36:22[0m] Step: 7744, Training Logs: loss_final: 0.873147, loss_mean: 0.829471, loss_mean_cls: 0.043675, grad_norm: 0.292100
[[34m2025-10-04 12:36:22[0m] Step: 7745, Training Logs: loss_final: 0.868638, loss_mean: 0.825327, loss_mean_cls: 0.043310, grad_norm: 0.300901
[[34m2025-10-04 12:36:22[0m] Step: 7746, Training Logs: loss_final: 0.863889, loss_mean: 0.819949, loss_mean_cls: 0.043940, grad_norm: 0.244990
[[34m2025-10-04 12:36:22[0m] Step: 7747, Training Logs: loss_final: 0.875833, loss_mean: 0.832651, loss_mean_cls: 0.043182, grad_norm: 0.236872
[[34m2025-10-04 12:36:23[0m] Step: 7748, Training Logs: loss_final: 0.878867, loss_mean: 0.835860, loss_mean_cls: 0.043006, grad_norm: 0.314236
[[34m2025-10-04 12:36:23[0m] Step: 7749, Training Logs: loss_final: 0.871693, loss_mean: 0.829255, loss_mean_cls: 0.042438, grad_norm: 0.216401
[[34m2025-10-04 12:36:23[0m] Step: 7750, Training Logs: loss_final: 0.897518, loss_mean: 0.854378, loss_mean_cls: 0.043140, grad_norm: 0.234577
[[34m2025-10-04 12:36:24[0m] Step: 7751, Training Logs: loss_final: 0.878022, loss_mean: 0.833094, loss_mean_cls: 0.044928, grad_norm: 0.339492
[[34m2025-10-04 12:36:24[0m] Step: 7752, Training Logs: loss_final: 0.877913, loss_mean: 0.834873, loss_mean_cls: 0.043039, grad_norm: 0.203362
[[34m2025-10-04 12:36:24[0m] Step: 7753, Training Logs: loss_final: 0.881208, loss_mean: 0.837712, loss_mean_cls: 0.043497, grad_norm: 0.254994
[[34m2025-10-04 12:36:25[0m] Step: 7754, Training Logs: loss_final: 0.882617, loss_mean: 0.839330, loss_mean_cls: 0.043287, grad_norm: 0.272852
[[34m2025-10-04 12:36:25[0m] Step: 7755, Training Logs: loss_final: 0.887263, loss_mean: 0.843112, loss_mean_cls: 0.044151, grad_norm: 0.201241
[[34m2025-10-04 12:36:25[0m] Step: 7756, Training Logs: loss_final: 0.897286, loss_mean: 0.855862, loss_mean_cls: 0.041424, grad_norm: 0.206523
[[34m2025-10-04 12:36:25[0m] Step: 7757, Training Logs: loss_final: 0.869232, loss_mean: 0.825210, loss_mean_cls: 0.044022, grad_norm: 0.232916
[[34m2025-10-04 12:36:26[0m] Step: 7758, Training Logs: loss_final: 0.876951, loss_mean: 0.833495, loss_mean_cls: 0.043456, grad_norm: 0.282660
[[34m2025-10-04 12:36:26[0m] Step: 7759, Training Logs: loss_final: 0.888065, loss_mean: 0.843660, loss_mean_cls: 0.044405, grad_norm: 0.244662
[[34m2025-10-04 12:36:26[0m] Step: 7760, Training Logs: loss_final: 0.873329, loss_mean: 0.831077, loss_mean_cls: 0.042252, grad_norm: 0.276467
[[34m2025-10-04 12:36:27[0m] Step: 7761, Training Logs: loss_final: 0.873712, loss_mean: 0.831061, loss_mean_cls: 0.042652, grad_norm: 0.273411
[[34m2025-10-04 12:36:27[0m] Step: 7762, Training Logs: loss_final: 0.888487, loss_mean: 0.845421, loss_mean_cls: 0.043066, grad_norm: 0.257329
[[34m2025-10-04 12:36:27[0m] Step: 7763, Training Logs: loss_final: 0.881404, loss_mean: 0.837618, loss_mean_cls: 0.043786, grad_norm: 0.325184
[[34m2025-10-04 12:36:27[0m] Step: 7764, Training Logs: loss_final: 0.859487, loss_mean: 0.814952, loss_mean_cls: 0.044535, grad_norm: 0.190566
[[34m2025-10-04 12:36:28[0m] Step: 7765, Training Logs: loss_final: 0.883605, loss_mean: 0.840986, loss_mean_cls: 0.042618, grad_norm: 0.327713
[[34m2025-10-04 12:36:28[0m] Step: 7766, Training Logs: loss_final: 0.865873, loss_mean: 0.822158, loss_mean_cls: 0.043715, grad_norm: 0.309720
[[34m2025-10-04 12:36:28[0m] Step: 7767, Training Logs: loss_final: 0.886511, loss_mean: 0.842737, loss_mean_cls: 0.043774, grad_norm: 0.265789
[[34m2025-10-04 12:36:29[0m] Step: 7768, Training Logs: loss_final: 0.893847, loss_mean: 0.850579, loss_mean_cls: 0.043268, grad_norm: 0.345418
[[34m2025-10-04 12:36:29[0m] Step: 7769, Training Logs: loss_final: 0.869345, loss_mean: 0.826066, loss_mean_cls: 0.043279, grad_norm: 0.313295
[[34m2025-10-04 12:36:29[0m] Step: 7770, Training Logs: loss_final: 0.901496, loss_mean: 0.858124, loss_mean_cls: 0.043373, grad_norm: 0.313913
[[34m2025-10-04 12:36:29[0m] Step: 7771, Training Logs: loss_final: 0.883960, loss_mean: 0.842521, loss_mean_cls: 0.041439, grad_norm: 0.325404
[[34m2025-10-04 12:36:30[0m] Step: 7772, Training Logs: loss_final: 0.890850, loss_mean: 0.846883, loss_mean_cls: 0.043967, grad_norm: 0.270397
[[34m2025-10-04 12:36:30[0m] Step: 7773, Training Logs: loss_final: 0.869622, loss_mean: 0.824625, loss_mean_cls: 0.044998, grad_norm: 0.288841
[[34m2025-10-04 12:36:30[0m] Step: 7774, Training Logs: loss_final: 0.873378, loss_mean: 0.829054, loss_mean_cls: 0.044325, grad_norm: 0.388305
[[34m2025-10-04 12:36:31[0m] Step: 7775, Training Logs: loss_final: 0.858622, loss_mean: 0.813903, loss_mean_cls: 0.044719, grad_norm: 0.422563
[[34m2025-10-04 12:36:31[0m] Step: 7776, Training Logs: loss_final: 0.854060, loss_mean: 0.809888, loss_mean_cls: 0.044171, grad_norm: 0.248459
[[34m2025-10-04 12:36:31[0m] Step: 7777, Training Logs: loss_final: 0.867774, loss_mean: 0.824827, loss_mean_cls: 0.042947, grad_norm: 0.431793
[[34m2025-10-04 12:36:32[0m] Step: 7778, Training Logs: loss_final: 0.856506, loss_mean: 0.813354, loss_mean_cls: 0.043152, grad_norm: 0.345876
[[34m2025-10-04 12:36:32[0m] Step: 7779, Training Logs: loss_final: 0.881506, loss_mean: 0.837990, loss_mean_cls: 0.043515, grad_norm: 0.319388
[[34m2025-10-04 12:36:32[0m] Step: 7780, Training Logs: loss_final: 0.888105, loss_mean: 0.843730, loss_mean_cls: 0.044374, grad_norm: 0.347952
[[34m2025-10-04 12:36:32[0m] Step: 7781, Training Logs: loss_final: 0.886236, loss_mean: 0.842173, loss_mean_cls: 0.044063, grad_norm: 0.359773
[[34m2025-10-04 12:36:33[0m] Step: 7782, Training Logs: loss_final: 0.872829, loss_mean: 0.829274, loss_mean_cls: 0.043555, grad_norm: 0.271244
[[34m2025-10-04 12:36:33[0m] Step: 7783, Training Logs: loss_final: 0.867397, loss_mean: 0.823801, loss_mean_cls: 0.043597, grad_norm: 0.329755
[[34m2025-10-04 12:36:33[0m] Step: 7784, Training Logs: loss_final: 0.864540, loss_mean: 0.821429, loss_mean_cls: 0.043111, grad_norm: 0.351341
[[34m2025-10-04 12:36:34[0m] Step: 7785, Training Logs: loss_final: 0.880333, loss_mean: 0.836264, loss_mean_cls: 0.044069, grad_norm: 0.222844
[[34m2025-10-04 12:36:34[0m] Step: 7786, Training Logs: loss_final: 0.881264, loss_mean: 0.838517, loss_mean_cls: 0.042747, grad_norm: 0.365466
[[34m2025-10-04 12:36:34[0m] Step: 7787, Training Logs: loss_final: 0.882461, loss_mean: 0.837112, loss_mean_cls: 0.045350, grad_norm: 0.318666
[[34m2025-10-04 12:36:34[0m] Step: 7788, Training Logs: loss_final: 0.869665, loss_mean: 0.826348, loss_mean_cls: 0.043317, grad_norm: 0.269738
[[34m2025-10-04 12:36:35[0m] Step: 7789, Training Logs: loss_final: 0.863447, loss_mean: 0.820093, loss_mean_cls: 0.043355, grad_norm: 0.253998
[[34m2025-10-04 12:36:35[0m] Step: 7790, Training Logs: loss_final: 0.886458, loss_mean: 0.842361, loss_mean_cls: 0.044097, grad_norm: 0.332046
[[34m2025-10-04 12:36:35[0m] Step: 7791, Training Logs: loss_final: 0.866420, loss_mean: 0.822872, loss_mean_cls: 0.043548, grad_norm: 0.295265
[[34m2025-10-04 12:36:36[0m] Step: 7792, Training Logs: loss_final: 0.895586, loss_mean: 0.852395, loss_mean_cls: 0.043191, grad_norm: 0.249297
[[34m2025-10-04 12:36:36[0m] Step: 7793, Training Logs: loss_final: 0.875694, loss_mean: 0.831244, loss_mean_cls: 0.044450, grad_norm: 0.307586
[[34m2025-10-04 12:36:36[0m] Step: 7794, Training Logs: loss_final: 0.871913, loss_mean: 0.828015, loss_mean_cls: 0.043898, grad_norm: 0.283607
[[34m2025-10-04 12:36:36[0m] Step: 7795, Training Logs: loss_final: 0.874171, loss_mean: 0.830066, loss_mean_cls: 0.044105, grad_norm: 0.306318
[[34m2025-10-04 12:36:37[0m] Step: 7796, Training Logs: loss_final: 0.896661, loss_mean: 0.854558, loss_mean_cls: 0.042103, grad_norm: 0.246736
[[34m2025-10-04 12:36:37[0m] Step: 7797, Training Logs: loss_final: 0.871546, loss_mean: 0.826783, loss_mean_cls: 0.044763, grad_norm: 0.269226
[[34m2025-10-04 12:36:37[0m] Step: 7798, Training Logs: loss_final: 0.866790, loss_mean: 0.821738, loss_mean_cls: 0.045052, grad_norm: 0.264412
[[34m2025-10-04 12:36:38[0m] Step: 7799, Training Logs: loss_final: 0.870687, loss_mean: 0.828363, loss_mean_cls: 0.042324, grad_norm: 0.205477
[[34m2025-10-04 12:36:38[0m] Step: 7800, Training Logs: loss_final: 0.904845, loss_mean: 0.860910, loss_mean_cls: 0.043936, grad_norm: 0.206167
[[34m2025-10-04 12:36:38[0m] Step: 7801, Training Logs: loss_final: 0.867434, loss_mean: 0.822984, loss_mean_cls: 0.044450, grad_norm: 0.219549
[[34m2025-10-04 12:36:38[0m] Step: 7802, Training Logs: loss_final: 0.862776, loss_mean: 0.817405, loss_mean_cls: 0.045371, grad_norm: 0.190459
[[34m2025-10-04 12:36:39[0m] Step: 7803, Training Logs: loss_final: 0.868148, loss_mean: 0.826928, loss_mean_cls: 0.041220, grad_norm: 0.248513
[[34m2025-10-04 12:36:39[0m] Step: 7804, Training Logs: loss_final: 0.888149, loss_mean: 0.845415, loss_mean_cls: 0.042734, grad_norm: 0.219506
[[34m2025-10-04 12:36:39[0m] Step: 7805, Training Logs: loss_final: 0.882802, loss_mean: 0.840758, loss_mean_cls: 0.042044, grad_norm: 0.329919
[[34m2025-10-04 12:36:40[0m] Step: 7806, Training Logs: loss_final: 0.874343, loss_mean: 0.830942, loss_mean_cls: 0.043401, grad_norm: 0.393433
[[34m2025-10-04 12:36:40[0m] Step: 7807, Training Logs: loss_final: 0.868099, loss_mean: 0.823000, loss_mean_cls: 0.045099, grad_norm: 0.204771
[[34m2025-10-04 12:36:40[0m] Step: 7808, Training Logs: loss_final: 0.886640, loss_mean: 0.844893, loss_mean_cls: 0.041748, grad_norm: 0.274245
[[34m2025-10-04 12:36:40[0m] Step: 7809, Training Logs: loss_final: 0.863094, loss_mean: 0.819270, loss_mean_cls: 0.043824, grad_norm: 0.194045
[[34m2025-10-04 12:36:41[0m] Step: 7810, Training Logs: loss_final: 0.853179, loss_mean: 0.809810, loss_mean_cls: 0.043370, grad_norm: 0.321664
[[34m2025-10-04 12:36:41[0m] Step: 7811, Training Logs: loss_final: 0.864765, loss_mean: 0.821166, loss_mean_cls: 0.043598, grad_norm: 0.156343
[[34m2025-10-04 12:36:41[0m] Step: 7812, Training Logs: loss_final: 0.868585, loss_mean: 0.825068, loss_mean_cls: 0.043517, grad_norm: 0.274392
[[34m2025-10-04 12:36:42[0m] Step: 7813, Training Logs: loss_final: 0.870620, loss_mean: 0.828369, loss_mean_cls: 0.042251, grad_norm: 0.251793
[[34m2025-10-04 12:36:42[0m] Step: 7814, Training Logs: loss_final: 0.857898, loss_mean: 0.814048, loss_mean_cls: 0.043850, grad_norm: 0.307502
[[34m2025-10-04 12:36:42[0m] Step: 7815, Training Logs: loss_final: 0.867172, loss_mean: 0.824123, loss_mean_cls: 0.043050, grad_norm: 0.241398
[[34m2025-10-04 12:36:42[0m] Step: 7816, Training Logs: loss_final: 0.869963, loss_mean: 0.827127, loss_mean_cls: 0.042836, grad_norm: 0.284162
[[34m2025-10-04 12:36:43[0m] Step: 7817, Training Logs: loss_final: 0.844272, loss_mean: 0.799512, loss_mean_cls: 0.044760, grad_norm: 0.204090
[[34m2025-10-04 12:36:43[0m] Step: 7818, Training Logs: loss_final: 0.885987, loss_mean: 0.842953, loss_mean_cls: 0.043035, grad_norm: 0.471066
[[34m2025-10-04 12:36:43[0m] Step: 7819, Training Logs: loss_final: 0.860934, loss_mean: 0.816759, loss_mean_cls: 0.044174, grad_norm: 0.327915
[[34m2025-10-04 12:36:44[0m] Step: 7820, Training Logs: loss_final: 0.868696, loss_mean: 0.825058, loss_mean_cls: 0.043638, grad_norm: 0.412547
[[34m2025-10-04 12:36:44[0m] Step: 7821, Training Logs: loss_final: 0.876356, loss_mean: 0.832483, loss_mean_cls: 0.043874, grad_norm: 0.284379
[[34m2025-10-04 12:36:44[0m] Step: 7822, Training Logs: loss_final: 0.885824, loss_mean: 0.843440, loss_mean_cls: 0.042384, grad_norm: 0.342038
[[34m2025-10-04 12:36:44[0m] Step: 7823, Training Logs: loss_final: 0.874440, loss_mean: 0.830717, loss_mean_cls: 0.043724, grad_norm: 0.334246
[[34m2025-10-04 12:36:45[0m] Step: 7824, Training Logs: loss_final: 0.887727, loss_mean: 0.845218, loss_mean_cls: 0.042509, grad_norm: 0.201988
[[34m2025-10-04 12:36:45[0m] Step: 7825, Training Logs: loss_final: 0.868203, loss_mean: 0.825008, loss_mean_cls: 0.043194, grad_norm: 0.239884
[[34m2025-10-04 12:36:45[0m] Step: 7826, Training Logs: loss_final: 0.900707, loss_mean: 0.857373, loss_mean_cls: 0.043334, grad_norm: 0.341005
[[34m2025-10-04 12:36:46[0m] Step: 7827, Training Logs: loss_final: 0.875565, loss_mean: 0.832837, loss_mean_cls: 0.042727, grad_norm: 0.271606
[[34m2025-10-04 12:36:46[0m] Step: 7828, Training Logs: loss_final: 0.887251, loss_mean: 0.844322, loss_mean_cls: 0.042930, grad_norm: 0.285005
[[34m2025-10-04 12:36:46[0m] Step: 7829, Training Logs: loss_final: 0.889364, loss_mean: 0.845513, loss_mean_cls: 0.043851, grad_norm: 0.226641
[[34m2025-10-04 12:36:47[0m] Step: 7830, Training Logs: loss_final: 0.880743, loss_mean: 0.837285, loss_mean_cls: 0.043458, grad_norm: 0.273065
[[34m2025-10-04 12:36:47[0m] Step: 7831, Training Logs: loss_final: 0.857221, loss_mean: 0.812052, loss_mean_cls: 0.045169, grad_norm: 0.344730
[[34m2025-10-04 12:36:47[0m] Step: 7832, Training Logs: loss_final: 0.876993, loss_mean: 0.833252, loss_mean_cls: 0.043741, grad_norm: 0.246431
[[34m2025-10-04 12:36:47[0m] Step: 7833, Training Logs: loss_final: 0.895671, loss_mean: 0.850877, loss_mean_cls: 0.044795, grad_norm: 0.374915
[[34m2025-10-04 12:36:48[0m] Step: 7834, Training Logs: loss_final: 0.870648, loss_mean: 0.827385, loss_mean_cls: 0.043263, grad_norm: 0.234827
[[34m2025-10-04 12:36:48[0m] Step: 7835, Training Logs: loss_final: 0.896069, loss_mean: 0.852628, loss_mean_cls: 0.043441, grad_norm: 0.358604
[[34m2025-10-04 12:36:48[0m] Step: 7836, Training Logs: loss_final: 0.867030, loss_mean: 0.823099, loss_mean_cls: 0.043932, grad_norm: 0.293671
[[34m2025-10-04 12:36:49[0m] Step: 7837, Training Logs: loss_final: 0.877703, loss_mean: 0.834405, loss_mean_cls: 0.043298, grad_norm: 0.181743
[[34m2025-10-04 12:36:49[0m] Step: 7838, Training Logs: loss_final: 0.870600, loss_mean: 0.827698, loss_mean_cls: 0.042902, grad_norm: 0.259616
[[34m2025-10-04 12:36:49[0m] Step: 7839, Training Logs: loss_final: 0.893436, loss_mean: 0.849764, loss_mean_cls: 0.043672, grad_norm: 0.276136
[[34m2025-10-04 12:36:49[0m] Step: 7840, Training Logs: loss_final: 0.873804, loss_mean: 0.830056, loss_mean_cls: 0.043748, grad_norm: 0.247864
[[34m2025-10-04 12:36:50[0m] Step: 7841, Training Logs: loss_final: 0.880965, loss_mean: 0.838053, loss_mean_cls: 0.042912, grad_norm: 0.342519
[[34m2025-10-04 12:36:50[0m] Step: 7842, Training Logs: loss_final: 0.874244, loss_mean: 0.830605, loss_mean_cls: 0.043639, grad_norm: 0.413148
[[34m2025-10-04 12:36:50[0m] Step: 7843, Training Logs: loss_final: 0.891757, loss_mean: 0.848370, loss_mean_cls: 0.043387, grad_norm: 0.227965
[[34m2025-10-04 12:36:51[0m] Step: 7844, Training Logs: loss_final: 0.871247, loss_mean: 0.826982, loss_mean_cls: 0.044265, grad_norm: 0.261979
[[34m2025-10-04 12:36:51[0m] Step: 7845, Training Logs: loss_final: 0.883824, loss_mean: 0.841270, loss_mean_cls: 0.042554, grad_norm: 0.273517
[[34m2025-10-04 12:36:51[0m] Step: 7846, Training Logs: loss_final: 0.860144, loss_mean: 0.815568, loss_mean_cls: 0.044576, grad_norm: 0.294568
[[34m2025-10-04 12:36:52[0m] Step: 7847, Training Logs: loss_final: 0.864130, loss_mean: 0.821249, loss_mean_cls: 0.042881, grad_norm: 0.311087
[[34m2025-10-04 12:36:52[0m] Step: 7848, Training Logs: loss_final: 0.868357, loss_mean: 0.825168, loss_mean_cls: 0.043189, grad_norm: 0.328907
[[34m2025-10-04 12:36:52[0m] Step: 7849, Training Logs: loss_final: 0.885002, loss_mean: 0.841874, loss_mean_cls: 0.043128, grad_norm: 0.292284
[[34m2025-10-04 12:36:52[0m] Step: 7850, Training Logs: loss_final: 0.880337, loss_mean: 0.836162, loss_mean_cls: 0.044174, grad_norm: 0.253248
[[34m2025-10-04 12:36:53[0m] Step: 7851, Training Logs: loss_final: 0.884848, loss_mean: 0.840769, loss_mean_cls: 0.044080, grad_norm: 0.306484
[[34m2025-10-04 12:36:53[0m] Step: 7852, Training Logs: loss_final: 0.893267, loss_mean: 0.849594, loss_mean_cls: 0.043673, grad_norm: 0.236481
[[34m2025-10-04 12:36:53[0m] Step: 7853, Training Logs: loss_final: 0.841069, loss_mean: 0.797612, loss_mean_cls: 0.043457, grad_norm: 0.262781
[[34m2025-10-04 12:36:54[0m] Step: 7854, Training Logs: loss_final: 0.875344, loss_mean: 0.833152, loss_mean_cls: 0.042192, grad_norm: 0.208617
[[34m2025-10-04 12:36:54[0m] Step: 7855, Training Logs: loss_final: 0.855003, loss_mean: 0.810678, loss_mean_cls: 0.044324, grad_norm: 0.352339
[[34m2025-10-04 12:36:54[0m] Step: 7856, Training Logs: loss_final: 0.879103, loss_mean: 0.835003, loss_mean_cls: 0.044101, grad_norm: 0.205307
[[34m2025-10-04 12:36:54[0m] Step: 7857, Training Logs: loss_final: 0.864327, loss_mean: 0.821385, loss_mean_cls: 0.042942, grad_norm: 0.268390
[[34m2025-10-04 12:36:55[0m] Step: 7858, Training Logs: loss_final: 0.869767, loss_mean: 0.826301, loss_mean_cls: 0.043466, grad_norm: 0.202161
[[34m2025-10-04 12:36:55[0m] Step: 7859, Training Logs: loss_final: 0.856438, loss_mean: 0.812629, loss_mean_cls: 0.043809, grad_norm: 0.205649
[[34m2025-10-04 12:36:55[0m] Step: 7860, Training Logs: loss_final: 0.873581, loss_mean: 0.830030, loss_mean_cls: 0.043550, grad_norm: 0.267097
[[34m2025-10-04 12:36:56[0m] Step: 7861, Training Logs: loss_final: 0.846021, loss_mean: 0.800348, loss_mean_cls: 0.045672, grad_norm: 0.181991
[[34m2025-10-04 12:36:56[0m] Step: 7862, Training Logs: loss_final: 0.880271, loss_mean: 0.837402, loss_mean_cls: 0.042869, grad_norm: 0.265174
[[34m2025-10-04 12:36:56[0m] Step: 7863, Training Logs: loss_final: 0.862631, loss_mean: 0.818519, loss_mean_cls: 0.044112, grad_norm: 0.217284
[[34m2025-10-04 12:36:56[0m] Step: 7864, Training Logs: loss_final: 0.866677, loss_mean: 0.824154, loss_mean_cls: 0.042522, grad_norm: 0.192575
[[34m2025-10-04 12:36:57[0m] Step: 7865, Training Logs: loss_final: 0.868663, loss_mean: 0.823866, loss_mean_cls: 0.044797, grad_norm: 0.259092
[[34m2025-10-04 12:36:57[0m] Step: 7866, Training Logs: loss_final: 0.877123, loss_mean: 0.835004, loss_mean_cls: 0.042119, grad_norm: 0.210145
[[34m2025-10-04 12:36:57[0m] Step: 7867, Training Logs: loss_final: 0.885681, loss_mean: 0.841330, loss_mean_cls: 0.044352, grad_norm: 0.234834
[[34m2025-10-04 12:36:58[0m] Step: 7868, Training Logs: loss_final: 0.873640, loss_mean: 0.829269, loss_mean_cls: 0.044372, grad_norm: 0.184609
[[34m2025-10-04 12:36:58[0m] Step: 7869, Training Logs: loss_final: 0.884595, loss_mean: 0.841029, loss_mean_cls: 0.043566, grad_norm: 0.204119
[[34m2025-10-04 12:36:58[0m] Step: 7870, Training Logs: loss_final: 0.870056, loss_mean: 0.826512, loss_mean_cls: 0.043544, grad_norm: 0.313672
[[34m2025-10-04 12:36:59[0m] Step: 7871, Training Logs: loss_final: 0.855784, loss_mean: 0.812559, loss_mean_cls: 0.043225, grad_norm: 0.198729
[[34m2025-10-04 12:36:59[0m] Step: 7872, Training Logs: loss_final: 0.883056, loss_mean: 0.839144, loss_mean_cls: 0.043912, grad_norm: 0.263697
[[34m2025-10-04 12:36:59[0m] Step: 7873, Training Logs: loss_final: 0.879786, loss_mean: 0.836639, loss_mean_cls: 0.043147, grad_norm: 0.242520
[[34m2025-10-04 12:36:59[0m] Step: 7874, Training Logs: loss_final: 0.854313, loss_mean: 0.810255, loss_mean_cls: 0.044057, grad_norm: 0.346694
[[34m2025-10-04 12:37:00[0m] Step: 7875, Training Logs: loss_final: 0.863449, loss_mean: 0.819843, loss_mean_cls: 0.043607, grad_norm: 0.337071
[[34m2025-10-04 12:37:00[0m] Step: 7876, Training Logs: loss_final: 0.899877, loss_mean: 0.856838, loss_mean_cls: 0.043039, grad_norm: 0.290966
[[34m2025-10-04 12:37:00[0m] Step: 7877, Training Logs: loss_final: 0.872144, loss_mean: 0.827914, loss_mean_cls: 0.044230, grad_norm: 0.373449
[[34m2025-10-04 12:37:01[0m] Step: 7878, Training Logs: loss_final: 0.857853, loss_mean: 0.814795, loss_mean_cls: 0.043058, grad_norm: 0.237677
[[34m2025-10-04 12:37:01[0m] Step: 7879, Training Logs: loss_final: 0.865897, loss_mean: 0.822728, loss_mean_cls: 0.043169, grad_norm: 0.408928
[[34m2025-10-04 12:37:01[0m] Step: 7880, Training Logs: loss_final: 0.857293, loss_mean: 0.812824, loss_mean_cls: 0.044468, grad_norm: 0.405203
[[34m2025-10-04 12:37:01[0m] Step: 7881, Training Logs: loss_final: 0.872164, loss_mean: 0.828074, loss_mean_cls: 0.044090, grad_norm: 0.307590
[[34m2025-10-04 12:37:02[0m] Step: 7882, Training Logs: loss_final: 0.864618, loss_mean: 0.819859, loss_mean_cls: 0.044759, grad_norm: 0.282119
[[34m2025-10-04 12:37:02[0m] Step: 7883, Training Logs: loss_final: 0.902665, loss_mean: 0.860213, loss_mean_cls: 0.042452, grad_norm: 0.373988
[[34m2025-10-04 12:37:02[0m] Step: 7884, Training Logs: loss_final: 0.870064, loss_mean: 0.826538, loss_mean_cls: 0.043526, grad_norm: 0.566599
[[34m2025-10-04 12:37:03[0m] Step: 7885, Training Logs: loss_final: 0.859085, loss_mean: 0.814639, loss_mean_cls: 0.044445, grad_norm: 0.339292
[[34m2025-10-04 12:37:03[0m] Step: 7886, Training Logs: loss_final: 0.882656, loss_mean: 0.838802, loss_mean_cls: 0.043854, grad_norm: 0.393229
[[34m2025-10-04 12:37:03[0m] Step: 7887, Training Logs: loss_final: 0.874696, loss_mean: 0.831413, loss_mean_cls: 0.043283, grad_norm: 0.304775
[[34m2025-10-04 12:37:03[0m] Step: 7888, Training Logs: loss_final: 0.867567, loss_mean: 0.824548, loss_mean_cls: 0.043020, grad_norm: 0.493898
[[34m2025-10-04 12:37:04[0m] Step: 7889, Training Logs: loss_final: 0.893225, loss_mean: 0.851253, loss_mean_cls: 0.041972, grad_norm: 0.263263
[[34m2025-10-04 12:37:04[0m] Step: 7890, Training Logs: loss_final: 0.875078, loss_mean: 0.830612, loss_mean_cls: 0.044466, grad_norm: 0.370946
[[34m2025-10-04 12:37:04[0m] Step: 7891, Training Logs: loss_final: 0.865431, loss_mean: 0.821236, loss_mean_cls: 0.044195, grad_norm: 0.284829
[[34m2025-10-04 12:37:05[0m] Step: 7892, Training Logs: loss_final: 0.866408, loss_mean: 0.822258, loss_mean_cls: 0.044150, grad_norm: 0.436189
[[34m2025-10-04 12:37:05[0m] Step: 7893, Training Logs: loss_final: 0.896202, loss_mean: 0.853925, loss_mean_cls: 0.042277, grad_norm: 0.225083
[[34m2025-10-04 12:37:05[0m] Step: 7894, Training Logs: loss_final: 0.893988, loss_mean: 0.852375, loss_mean_cls: 0.041613, grad_norm: 0.422274
[[34m2025-10-04 12:37:06[0m] Step: 7895, Training Logs: loss_final: 0.893991, loss_mean: 0.851200, loss_mean_cls: 0.042791, grad_norm: 0.312623
[[34m2025-10-04 12:37:06[0m] Step: 7896, Training Logs: loss_final: 0.911272, loss_mean: 0.869055, loss_mean_cls: 0.042216, grad_norm: 0.278374
[[34m2025-10-04 12:37:06[0m] Step: 7897, Training Logs: loss_final: 0.894399, loss_mean: 0.852527, loss_mean_cls: 0.041872, grad_norm: 0.304709
[[34m2025-10-04 12:37:06[0m] Step: 7898, Training Logs: loss_final: 0.850747, loss_mean: 0.807348, loss_mean_cls: 0.043399, grad_norm: 0.210692
[[34m2025-10-04 12:37:07[0m] Step: 7899, Training Logs: loss_final: 0.866625, loss_mean: 0.823171, loss_mean_cls: 0.043455, grad_norm: 0.314086
[[34m2025-10-04 12:37:07[0m] Step: 7900, Training Logs: loss_final: 0.864188, loss_mean: 0.819677, loss_mean_cls: 0.044510, grad_norm: 0.344107
[[34m2025-10-04 12:37:07[0m] Step: 7901, Training Logs: loss_final: 0.886963, loss_mean: 0.843677, loss_mean_cls: 0.043286, grad_norm: 0.208072
[[34m2025-10-04 12:37:08[0m] Step: 7902, Training Logs: loss_final: 0.878135, loss_mean: 0.834432, loss_mean_cls: 0.043703, grad_norm: 0.270394
[[34m2025-10-04 12:37:08[0m] Step: 7903, Training Logs: loss_final: 0.880250, loss_mean: 0.836595, loss_mean_cls: 0.043654, grad_norm: 0.240530
[[34m2025-10-04 12:37:08[0m] Step: 7904, Training Logs: loss_final: 0.861677, loss_mean: 0.817028, loss_mean_cls: 0.044648, grad_norm: 0.226471
[[34m2025-10-04 12:37:08[0m] Step: 7905, Training Logs: loss_final: 0.884111, loss_mean: 0.840726, loss_mean_cls: 0.043385, grad_norm: 0.215802
[[34m2025-10-04 12:37:09[0m] Step: 7906, Training Logs: loss_final: 0.859203, loss_mean: 0.814505, loss_mean_cls: 0.044698, grad_norm: 0.229409
[[34m2025-10-04 12:37:09[0m] Step: 7907, Training Logs: loss_final: 0.888344, loss_mean: 0.845231, loss_mean_cls: 0.043113, grad_norm: 0.232015
[[34m2025-10-04 12:37:09[0m] Step: 7908, Training Logs: loss_final: 0.887352, loss_mean: 0.842298, loss_mean_cls: 0.045054, grad_norm: 0.238701
[[34m2025-10-04 12:37:10[0m] Step: 7909, Training Logs: loss_final: 0.884876, loss_mean: 0.841412, loss_mean_cls: 0.043464, grad_norm: 0.246019
[[34m2025-10-04 12:37:10[0m] Step: 7910, Training Logs: loss_final: 0.875210, loss_mean: 0.831353, loss_mean_cls: 0.043856, grad_norm: 0.260530
[[34m2025-10-04 12:37:10[0m] Step: 7911, Training Logs: loss_final: 0.890448, loss_mean: 0.847796, loss_mean_cls: 0.042652, grad_norm: 0.240766
[[34m2025-10-04 12:37:11[0m] Step: 7912, Training Logs: loss_final: 0.881466, loss_mean: 0.838029, loss_mean_cls: 0.043437, grad_norm: 0.287452
[[34m2025-10-04 12:37:11[0m] Step: 7913, Training Logs: loss_final: 0.881462, loss_mean: 0.837186, loss_mean_cls: 0.044275, grad_norm: 0.239082
[[34m2025-10-04 12:37:11[0m] Step: 7914, Training Logs: loss_final: 0.866211, loss_mean: 0.823412, loss_mean_cls: 0.042799, grad_norm: 0.223503
[[34m2025-10-04 12:37:11[0m] Step: 7915, Training Logs: loss_final: 0.885673, loss_mean: 0.842121, loss_mean_cls: 0.043552, grad_norm: 0.223395
[[34m2025-10-04 12:37:12[0m] Step: 7916, Training Logs: loss_final: 0.875842, loss_mean: 0.833331, loss_mean_cls: 0.042511, grad_norm: 0.300321
[[34m2025-10-04 12:37:12[0m] Step: 7917, Training Logs: loss_final: 0.867099, loss_mean: 0.825168, loss_mean_cls: 0.041930, grad_norm: 0.222150
[[34m2025-10-04 12:37:12[0m] Step: 7918, Training Logs: loss_final: 0.881319, loss_mean: 0.837386, loss_mean_cls: 0.043932, grad_norm: 0.473654
[[34m2025-10-04 12:37:13[0m] Step: 7919, Training Logs: loss_final: 0.881675, loss_mean: 0.838010, loss_mean_cls: 0.043665, grad_norm: 0.237897
[[34m2025-10-04 12:37:13[0m] Step: 7920, Training Logs: loss_final: 0.867362, loss_mean: 0.823355, loss_mean_cls: 0.044007, grad_norm: 0.313983
[[34m2025-10-04 12:37:13[0m] Step: 7921, Training Logs: loss_final: 0.873384, loss_mean: 0.829635, loss_mean_cls: 0.043749, grad_norm: 0.255869
[[34m2025-10-04 12:37:14[0m] Step: 7922, Training Logs: loss_final: 0.872125, loss_mean: 0.829178, loss_mean_cls: 0.042947, grad_norm: 0.218384
[[34m2025-10-04 12:37:14[0m] Step: 7923, Training Logs: loss_final: 0.867466, loss_mean: 0.823814, loss_mean_cls: 0.043652, grad_norm: 0.220013
[[34m2025-10-04 12:37:14[0m] Step: 7924, Training Logs: loss_final: 0.850199, loss_mean: 0.806732, loss_mean_cls: 0.043467, grad_norm: 0.313954
[[34m2025-10-04 12:37:14[0m] Step: 7925, Training Logs: loss_final: 0.889582, loss_mean: 0.847254, loss_mean_cls: 0.042329, grad_norm: 0.243520
[[34m2025-10-04 12:37:15[0m] Step: 7926, Training Logs: loss_final: 0.877519, loss_mean: 0.834677, loss_mean_cls: 0.042842, grad_norm: 0.273974
[[34m2025-10-04 12:37:15[0m] Step: 7927, Training Logs: loss_final: 0.897354, loss_mean: 0.855409, loss_mean_cls: 0.041945, grad_norm: 0.298049
[[34m2025-10-04 12:37:15[0m] Step: 7928, Training Logs: loss_final: 0.863904, loss_mean: 0.820403, loss_mean_cls: 0.043501, grad_norm: 0.237622
[[34m2025-10-04 12:37:16[0m] Step: 7929, Training Logs: loss_final: 0.868243, loss_mean: 0.824975, loss_mean_cls: 0.043267, grad_norm: 0.374988
[[34m2025-10-04 12:37:16[0m] Step: 7930, Training Logs: loss_final: 0.865674, loss_mean: 0.822627, loss_mean_cls: 0.043047, grad_norm: 0.313376
[[34m2025-10-04 12:37:16[0m] Step: 7931, Training Logs: loss_final: 0.864690, loss_mean: 0.821675, loss_mean_cls: 0.043015, grad_norm: 0.300859
[[34m2025-10-04 12:37:16[0m] Step: 7932, Training Logs: loss_final: 0.864987, loss_mean: 0.821405, loss_mean_cls: 0.043583, grad_norm: 0.236890
[[34m2025-10-04 12:37:17[0m] Step: 7933, Training Logs: loss_final: 0.890178, loss_mean: 0.847019, loss_mean_cls: 0.043158, grad_norm: 0.291857
[[34m2025-10-04 12:37:17[0m] Step: 7934, Training Logs: loss_final: 0.890235, loss_mean: 0.847381, loss_mean_cls: 0.042854, grad_norm: 0.195737
[[34m2025-10-04 12:37:17[0m] Step: 7935, Training Logs: loss_final: 0.863931, loss_mean: 0.821375, loss_mean_cls: 0.042555, grad_norm: 0.333003
[[34m2025-10-04 12:37:18[0m] Step: 7936, Training Logs: loss_final: 0.863469, loss_mean: 0.820772, loss_mean_cls: 0.042698, grad_norm: 0.264947
[[34m2025-10-04 12:37:18[0m] Step: 7937, Training Logs: loss_final: 0.882373, loss_mean: 0.837928, loss_mean_cls: 0.044445, grad_norm: 0.462644
[[34m2025-10-04 12:37:18[0m] Step: 7938, Training Logs: loss_final: 0.877858, loss_mean: 0.834125, loss_mean_cls: 0.043733, grad_norm: 0.281236
[[34m2025-10-04 12:37:18[0m] Step: 7939, Training Logs: loss_final: 0.862196, loss_mean: 0.817402, loss_mean_cls: 0.044794, grad_norm: 0.254308
[[34m2025-10-04 12:37:19[0m] Step: 7940, Training Logs: loss_final: 0.878680, loss_mean: 0.834491, loss_mean_cls: 0.044189, grad_norm: 0.275519
[[34m2025-10-04 12:37:19[0m] Step: 7941, Training Logs: loss_final: 0.859729, loss_mean: 0.817217, loss_mean_cls: 0.042512, grad_norm: 0.318868
[[34m2025-10-04 12:37:19[0m] Step: 7942, Training Logs: loss_final: 0.874548, loss_mean: 0.831765, loss_mean_cls: 0.042782, grad_norm: 0.239522
[[34m2025-10-04 12:37:20[0m] Step: 7943, Training Logs: loss_final: 0.880692, loss_mean: 0.837200, loss_mean_cls: 0.043492, grad_norm: 0.299578
[[34m2025-10-04 12:37:20[0m] Step: 7944, Training Logs: loss_final: 0.864350, loss_mean: 0.819896, loss_mean_cls: 0.044454, grad_norm: 0.270273
[[34m2025-10-04 12:37:20[0m] Step: 7945, Training Logs: loss_final: 0.872504, loss_mean: 0.831110, loss_mean_cls: 0.041394, grad_norm: 0.206847
[[34m2025-10-04 12:37:20[0m] Step: 7946, Training Logs: loss_final: 0.883117, loss_mean: 0.840818, loss_mean_cls: 0.042299, grad_norm: 0.194305
[[34m2025-10-04 12:37:21[0m] Step: 7947, Training Logs: loss_final: 0.876212, loss_mean: 0.832402, loss_mean_cls: 0.043810, grad_norm: 0.278070
[[34m2025-10-04 12:37:21[0m] Step: 7948, Training Logs: loss_final: 0.866591, loss_mean: 0.822995, loss_mean_cls: 0.043596, grad_norm: 0.157070
[[34m2025-10-04 12:37:21[0m] Step: 7949, Training Logs: loss_final: 0.870650, loss_mean: 0.827443, loss_mean_cls: 0.043207, grad_norm: 0.235435
[[34m2025-10-04 12:37:22[0m] Step: 7950, Training Logs: loss_final: 0.878346, loss_mean: 0.835143, loss_mean_cls: 0.043203, grad_norm: 0.238022
[[34m2025-10-04 12:37:22[0m] Step: 7951, Training Logs: loss_final: 0.882497, loss_mean: 0.839510, loss_mean_cls: 0.042987, grad_norm: 0.212383
[[34m2025-10-04 12:37:22[0m] Step: 7952, Training Logs: loss_final: 0.872072, loss_mean: 0.829368, loss_mean_cls: 0.042705, grad_norm: 0.225330
[[34m2025-10-04 12:37:22[0m] Step: 7953, Training Logs: loss_final: 0.880642, loss_mean: 0.838710, loss_mean_cls: 0.041932, grad_norm: 0.195821
[[34m2025-10-04 12:37:23[0m] Step: 7954, Training Logs: loss_final: 0.872027, loss_mean: 0.828986, loss_mean_cls: 0.043041, grad_norm: 0.270114
[[34m2025-10-04 12:37:23[0m] Step: 7955, Training Logs: loss_final: 0.885294, loss_mean: 0.841490, loss_mean_cls: 0.043804, grad_norm: 0.282100
[[34m2025-10-04 12:37:23[0m] Step: 7956, Training Logs: loss_final: 0.873515, loss_mean: 0.831161, loss_mean_cls: 0.042353, grad_norm: 0.210307
[[34m2025-10-04 12:37:24[0m] Step: 7957, Training Logs: loss_final: 0.896764, loss_mean: 0.852965, loss_mean_cls: 0.043799, grad_norm: 0.224770
[[34m2025-10-04 12:37:24[0m] Step: 7958, Training Logs: loss_final: 0.886789, loss_mean: 0.844725, loss_mean_cls: 0.042064, grad_norm: 0.283790
[[34m2025-10-04 12:37:24[0m] Step: 7959, Training Logs: loss_final: 0.888954, loss_mean: 0.846121, loss_mean_cls: 0.042833, grad_norm: 0.262813
[[34m2025-10-04 12:37:25[0m] Step: 7960, Training Logs: loss_final: 0.890337, loss_mean: 0.845842, loss_mean_cls: 0.044496, grad_norm: 0.257565
[[34m2025-10-04 12:37:25[0m] Step: 7961, Training Logs: loss_final: 0.873630, loss_mean: 0.829552, loss_mean_cls: 0.044078, grad_norm: 0.262125
[[34m2025-10-04 12:37:25[0m] Step: 7962, Training Logs: loss_final: 0.882427, loss_mean: 0.837979, loss_mean_cls: 0.044448, grad_norm: 0.281945
[[34m2025-10-04 12:37:25[0m] Step: 7963, Training Logs: loss_final: 0.897754, loss_mean: 0.855310, loss_mean_cls: 0.042444, grad_norm: 0.231793
[[34m2025-10-04 12:37:26[0m] Step: 7964, Training Logs: loss_final: 0.882301, loss_mean: 0.839535, loss_mean_cls: 0.042765, grad_norm: 0.272072
[[34m2025-10-04 12:37:26[0m] Step: 7965, Training Logs: loss_final: 0.872305, loss_mean: 0.829598, loss_mean_cls: 0.042707, grad_norm: 0.231620
[[34m2025-10-04 12:37:26[0m] Step: 7966, Training Logs: loss_final: 0.885343, loss_mean: 0.843071, loss_mean_cls: 0.042272, grad_norm: 0.256719
[[34m2025-10-04 12:37:27[0m] Step: 7967, Training Logs: loss_final: 0.876452, loss_mean: 0.833088, loss_mean_cls: 0.043364, grad_norm: 0.225647
[[34m2025-10-04 12:37:27[0m] Step: 7968, Training Logs: loss_final: 0.885796, loss_mean: 0.841500, loss_mean_cls: 0.044296, grad_norm: 0.190681
[[34m2025-10-04 12:37:27[0m] Step: 7969, Training Logs: loss_final: 0.871221, loss_mean: 0.826748, loss_mean_cls: 0.044473, grad_norm: 0.291931
[[34m2025-10-04 12:37:28[0m] Step: 7970, Training Logs: loss_final: 0.883148, loss_mean: 0.840972, loss_mean_cls: 0.042175, grad_norm: 0.319981
[[34m2025-10-04 12:37:28[0m] Step: 7971, Training Logs: loss_final: 0.876881, loss_mean: 0.834000, loss_mean_cls: 0.042881, grad_norm: 0.358563
[[34m2025-10-04 12:37:28[0m] Step: 7972, Training Logs: loss_final: 0.877107, loss_mean: 0.832587, loss_mean_cls: 0.044520, grad_norm: 0.269083
[[34m2025-10-04 12:37:28[0m] Step: 7973, Training Logs: loss_final: 0.890796, loss_mean: 0.846950, loss_mean_cls: 0.043847, grad_norm: 0.392507
[[34m2025-10-04 12:37:29[0m] Step: 7974, Training Logs: loss_final: 0.883939, loss_mean: 0.840938, loss_mean_cls: 0.043000, grad_norm: 0.192311
[[34m2025-10-04 12:37:29[0m] Step: 7975, Training Logs: loss_final: 0.859594, loss_mean: 0.814754, loss_mean_cls: 0.044840, grad_norm: 0.313411
[[34m2025-10-04 12:37:29[0m] Step: 7976, Training Logs: loss_final: 0.880721, loss_mean: 0.837812, loss_mean_cls: 0.042910, grad_norm: 0.323904
[[34m2025-10-04 12:37:30[0m] Step: 7977, Training Logs: loss_final: 0.870858, loss_mean: 0.825162, loss_mean_cls: 0.045696, grad_norm: 0.377925
[[34m2025-10-04 12:37:30[0m] Step: 7978, Training Logs: loss_final: 0.870055, loss_mean: 0.826592, loss_mean_cls: 0.043463, grad_norm: 0.224006
[[34m2025-10-04 12:37:30[0m] Step: 7979, Training Logs: loss_final: 0.893390, loss_mean: 0.851341, loss_mean_cls: 0.042049, grad_norm: 0.455451
[[34m2025-10-04 12:37:30[0m] Step: 7980, Training Logs: loss_final: 0.888880, loss_mean: 0.846788, loss_mean_cls: 0.042092, grad_norm: 0.240272
[[34m2025-10-04 12:37:31[0m] Step: 7981, Training Logs: loss_final: 0.889236, loss_mean: 0.845968, loss_mean_cls: 0.043267, grad_norm: 0.313363
[[34m2025-10-04 12:37:31[0m] Step: 7982, Training Logs: loss_final: 0.882308, loss_mean: 0.838859, loss_mean_cls: 0.043449, grad_norm: 0.355871
[[34m2025-10-04 12:37:31[0m] Step: 7983, Training Logs: loss_final: 0.889911, loss_mean: 0.847173, loss_mean_cls: 0.042737, grad_norm: 0.298156
[[34m2025-10-04 12:37:32[0m] Step: 7984, Training Logs: loss_final: 0.884183, loss_mean: 0.840596, loss_mean_cls: 0.043586, grad_norm: 0.304414
[[34m2025-10-04 12:37:32[0m] Step: 7985, Training Logs: loss_final: 0.872905, loss_mean: 0.829421, loss_mean_cls: 0.043484, grad_norm: 0.306364
[[34m2025-10-04 12:37:32[0m] Step: 7986, Training Logs: loss_final: 0.872221, loss_mean: 0.828676, loss_mean_cls: 0.043545, grad_norm: 0.337130
[[34m2025-10-04 12:37:32[0m] Step: 7987, Training Logs: loss_final: 0.882578, loss_mean: 0.838981, loss_mean_cls: 0.043597, grad_norm: 0.268317
[[34m2025-10-04 12:37:33[0m] Step: 7988, Training Logs: loss_final: 0.873970, loss_mean: 0.830485, loss_mean_cls: 0.043485, grad_norm: 0.184063
[[34m2025-10-04 12:37:33[0m] Step: 7989, Training Logs: loss_final: 0.892137, loss_mean: 0.848142, loss_mean_cls: 0.043995, grad_norm: 0.397261
[[34m2025-10-04 12:37:33[0m] Step: 7990, Training Logs: loss_final: 0.857155, loss_mean: 0.814298, loss_mean_cls: 0.042857, grad_norm: 0.300258
[[34m2025-10-04 12:37:34[0m] Step: 7991, Training Logs: loss_final: 0.864015, loss_mean: 0.820720, loss_mean_cls: 0.043295, grad_norm: 0.440577
[[34m2025-10-04 12:37:34[0m] Step: 7992, Training Logs: loss_final: 0.874046, loss_mean: 0.829542, loss_mean_cls: 0.044504, grad_norm: 0.439466
[[34m2025-10-04 12:37:34[0m] Step: 7993, Training Logs: loss_final: 0.870939, loss_mean: 0.827433, loss_mean_cls: 0.043506, grad_norm: 0.254023
[[34m2025-10-04 12:37:34[0m] Step: 7994, Training Logs: loss_final: 0.842592, loss_mean: 0.798484, loss_mean_cls: 0.044108, grad_norm: 0.336158
[[34m2025-10-04 12:37:35[0m] Step: 7995, Training Logs: loss_final: 0.894951, loss_mean: 0.851049, loss_mean_cls: 0.043902, grad_norm: 0.387184
[[34m2025-10-04 12:37:35[0m] Step: 7996, Training Logs: loss_final: 0.869622, loss_mean: 0.825498, loss_mean_cls: 0.044125, grad_norm: 0.390878
[[34m2025-10-04 12:37:35[0m] Step: 7997, Training Logs: loss_final: 0.850984, loss_mean: 0.808300, loss_mean_cls: 0.042684, grad_norm: 0.381478
[[34m2025-10-04 12:37:36[0m] Step: 7998, Training Logs: loss_final: 0.886573, loss_mean: 0.844071, loss_mean_cls: 0.042503, grad_norm: 0.570630
[[34m2025-10-04 12:37:36[0m] Step: 7999, Training Logs: loss_final: 0.876244, loss_mean: 0.834114, loss_mean_cls: 0.042130, grad_norm: 0.294192
[[34m2025-10-04 12:37:36[0m] Step: 8000, Training Logs: loss_final: 0.883358, loss_mean: 0.840641, loss_mean_cls: 0.042717, grad_norm: 0.484912
[[34m2025-10-04 12:37:36[0m] Step: 8001, Training Logs: loss_final: 0.858004, loss_mean: 0.813579, loss_mean_cls: 0.044425, grad_norm: 0.282941
[[34m2025-10-04 12:37:37[0m] Step: 8002, Training Logs: loss_final: 0.860560, loss_mean: 0.816688, loss_mean_cls: 0.043873, grad_norm: 0.311632
[[34m2025-10-04 12:37:37[0m] Step: 8003, Training Logs: loss_final: 0.871197, loss_mean: 0.828601, loss_mean_cls: 0.042596, grad_norm: 0.317931
[[34m2025-10-04 12:37:37[0m] Step: 8004, Training Logs: loss_final: 0.857991, loss_mean: 0.812209, loss_mean_cls: 0.045782, grad_norm: 0.488060
[[34m2025-10-04 12:37:38[0m] Step: 8005, Training Logs: loss_final: 0.869511, loss_mean: 0.826692, loss_mean_cls: 0.042820, grad_norm: 0.255632
[[34m2025-10-04 12:37:38[0m] Step: 8006, Training Logs: loss_final: 0.887348, loss_mean: 0.843170, loss_mean_cls: 0.044177, grad_norm: 0.552821
[[34m2025-10-04 12:37:38[0m] Step: 8007, Training Logs: loss_final: 0.870401, loss_mean: 0.826210, loss_mean_cls: 0.044191, grad_norm: 0.212322
[[34m2025-10-04 12:37:38[0m] Step: 8008, Training Logs: loss_final: 0.879390, loss_mean: 0.835897, loss_mean_cls: 0.043493, grad_norm: 0.252551
[[34m2025-10-04 12:37:39[0m] Step: 8009, Training Logs: loss_final: 0.884608, loss_mean: 0.841055, loss_mean_cls: 0.043553, grad_norm: 0.320230
[[34m2025-10-04 12:37:39[0m] Step: 8010, Training Logs: loss_final: 0.882121, loss_mean: 0.838005, loss_mean_cls: 0.044116, grad_norm: 0.337570
[[34m2025-10-04 12:37:39[0m] Step: 8011, Training Logs: loss_final: 0.887486, loss_mean: 0.842878, loss_mean_cls: 0.044608, grad_norm: 0.387025
[[34m2025-10-04 12:37:40[0m] Step: 8012, Training Logs: loss_final: 0.857445, loss_mean: 0.814477, loss_mean_cls: 0.042968, grad_norm: 0.398405
[[34m2025-10-04 12:37:40[0m] Step: 8013, Training Logs: loss_final: 0.866211, loss_mean: 0.823348, loss_mean_cls: 0.042863, grad_norm: 0.229815
[[34m2025-10-04 12:37:40[0m] Step: 8014, Training Logs: loss_final: 0.876483, loss_mean: 0.834384, loss_mean_cls: 0.042099, grad_norm: 0.439336
[[34m2025-10-04 12:37:40[0m] Step: 8015, Training Logs: loss_final: 0.888567, loss_mean: 0.844763, loss_mean_cls: 0.043804, grad_norm: 0.391853
[[34m2025-10-04 12:37:41[0m] Step: 8016, Training Logs: loss_final: 0.885170, loss_mean: 0.841178, loss_mean_cls: 0.043992, grad_norm: 0.313614
[[34m2025-10-04 12:37:41[0m] Step: 8017, Training Logs: loss_final: 0.865540, loss_mean: 0.821156, loss_mean_cls: 0.044384, grad_norm: 0.217685
[[34m2025-10-04 12:37:41[0m] Step: 8018, Training Logs: loss_final: 0.871644, loss_mean: 0.828231, loss_mean_cls: 0.043413, grad_norm: 0.317952
[[34m2025-10-04 12:37:42[0m] Step: 8019, Training Logs: loss_final: 0.878983, loss_mean: 0.835076, loss_mean_cls: 0.043908, grad_norm: 0.234400
[[34m2025-10-04 12:37:42[0m] Step: 8020, Training Logs: loss_final: 0.891961, loss_mean: 0.849639, loss_mean_cls: 0.042323, grad_norm: 0.253768
[[34m2025-10-04 12:37:42[0m] Step: 8021, Training Logs: loss_final: 0.897431, loss_mean: 0.854268, loss_mean_cls: 0.043164, grad_norm: 0.233102
[[34m2025-10-04 12:37:42[0m] Step: 8022, Training Logs: loss_final: 0.860105, loss_mean: 0.816350, loss_mean_cls: 0.043755, grad_norm: 0.228990
[[34m2025-10-04 12:37:43[0m] Step: 8023, Training Logs: loss_final: 0.885206, loss_mean: 0.842295, loss_mean_cls: 0.042911, grad_norm: 0.287642
[[34m2025-10-04 12:37:43[0m] Step: 8024, Training Logs: loss_final: 0.891511, loss_mean: 0.849040, loss_mean_cls: 0.042471, grad_norm: 0.405873
[[34m2025-10-04 12:37:43[0m] Step: 8025, Training Logs: loss_final: 0.835693, loss_mean: 0.792139, loss_mean_cls: 0.043554, grad_norm: 0.187236
[[34m2025-10-04 12:37:44[0m] Step: 8026, Training Logs: loss_final: 0.868318, loss_mean: 0.825399, loss_mean_cls: 0.042920, grad_norm: 0.170757
[[34m2025-10-04 12:37:44[0m] Step: 8027, Training Logs: loss_final: 0.886178, loss_mean: 0.843229, loss_mean_cls: 0.042949, grad_norm: 0.389929
[[34m2025-10-04 12:37:44[0m] Step: 8028, Training Logs: loss_final: 0.883195, loss_mean: 0.840888, loss_mean_cls: 0.042307, grad_norm: 0.277381
[[34m2025-10-04 12:37:45[0m] Step: 8029, Training Logs: loss_final: 0.876642, loss_mean: 0.834357, loss_mean_cls: 0.042284, grad_norm: 0.203387
[[34m2025-10-04 12:37:45[0m] Step: 8030, Training Logs: loss_final: 0.872157, loss_mean: 0.830324, loss_mean_cls: 0.041833, grad_norm: 0.189279
[[34m2025-10-04 12:37:45[0m] Step: 8031, Training Logs: loss_final: 0.869979, loss_mean: 0.827471, loss_mean_cls: 0.042508, grad_norm: 0.347167
[[34m2025-10-04 12:37:45[0m] Step: 8032, Training Logs: loss_final: 0.854217, loss_mean: 0.810352, loss_mean_cls: 0.043866, grad_norm: 0.273374
[[34m2025-10-04 12:37:46[0m] Step: 8033, Training Logs: loss_final: 0.884398, loss_mean: 0.841760, loss_mean_cls: 0.042638, grad_norm: 0.209696
[[34m2025-10-04 12:37:46[0m] Step: 8034, Training Logs: loss_final: 0.889314, loss_mean: 0.846281, loss_mean_cls: 0.043032, grad_norm: 0.218976
[[34m2025-10-04 12:37:46[0m] Step: 8035, Training Logs: loss_final: 0.885959, loss_mean: 0.842768, loss_mean_cls: 0.043190, grad_norm: 0.267325
[[34m2025-10-04 12:37:47[0m] Step: 8036, Training Logs: loss_final: 0.842353, loss_mean: 0.799098, loss_mean_cls: 0.043255, grad_norm: 0.280758
[[34m2025-10-04 12:37:47[0m] Step: 8037, Training Logs: loss_final: 0.875359, loss_mean: 0.831164, loss_mean_cls: 0.044195, grad_norm: 0.264444
[[34m2025-10-04 12:37:47[0m] Step: 8038, Training Logs: loss_final: 0.880832, loss_mean: 0.838236, loss_mean_cls: 0.042596, grad_norm: 0.242095
[[34m2025-10-04 12:37:48[0m] Step: 8039, Training Logs: loss_final: 0.863233, loss_mean: 0.819408, loss_mean_cls: 0.043825, grad_norm: 0.266080
[[34m2025-10-04 12:37:48[0m] Step: 8040, Training Logs: loss_final: 0.869938, loss_mean: 0.825106, loss_mean_cls: 0.044833, grad_norm: 0.308757
[[34m2025-10-04 12:37:48[0m] Step: 8041, Training Logs: loss_final: 0.872310, loss_mean: 0.829166, loss_mean_cls: 0.043144, grad_norm: 0.339052
[[34m2025-10-04 12:37:48[0m] Step: 8042, Training Logs: loss_final: 0.872695, loss_mean: 0.829011, loss_mean_cls: 0.043684, grad_norm: 0.351317
[[34m2025-10-04 12:37:49[0m] Step: 8043, Training Logs: loss_final: 0.895247, loss_mean: 0.852115, loss_mean_cls: 0.043132, grad_norm: 0.237952
[[34m2025-10-04 12:37:49[0m] Step: 8044, Training Logs: loss_final: 0.889967, loss_mean: 0.846903, loss_mean_cls: 0.043063, grad_norm: 0.267992
[[34m2025-10-04 12:37:49[0m] Step: 8045, Training Logs: loss_final: 0.873093, loss_mean: 0.829356, loss_mean_cls: 0.043738, grad_norm: 0.358930
[[34m2025-10-04 12:37:50[0m] Step: 8046, Training Logs: loss_final: 0.889509, loss_mean: 0.846550, loss_mean_cls: 0.042959, grad_norm: 0.304402
[[34m2025-10-04 12:37:50[0m] Step: 8047, Training Logs: loss_final: 0.872304, loss_mean: 0.828628, loss_mean_cls: 0.043676, grad_norm: 0.213974
[[34m2025-10-04 12:37:50[0m] Step: 8048, Training Logs: loss_final: 0.875773, loss_mean: 0.831138, loss_mean_cls: 0.044635, grad_norm: 0.449741
[[34m2025-10-04 12:37:51[0m] Step: 8049, Training Logs: loss_final: 0.889318, loss_mean: 0.845308, loss_mean_cls: 0.044010, grad_norm: 0.468024
[[34m2025-10-04 12:37:51[0m] Step: 8050, Training Logs: loss_final: 0.869778, loss_mean: 0.824806, loss_mean_cls: 0.044972, grad_norm: 0.245276
[[34m2025-10-04 12:37:51[0m] Step: 8051, Training Logs: loss_final: 0.872107, loss_mean: 0.828408, loss_mean_cls: 0.043698, grad_norm: 0.398849
[[34m2025-10-04 12:37:51[0m] Step: 8052, Training Logs: loss_final: 0.873605, loss_mean: 0.829755, loss_mean_cls: 0.043850, grad_norm: 0.343024
[[34m2025-10-04 12:37:52[0m] Step: 8053, Training Logs: loss_final: 0.890877, loss_mean: 0.847838, loss_mean_cls: 0.043039, grad_norm: 0.350807
[[34m2025-10-04 12:37:52[0m] Step: 8054, Training Logs: loss_final: 0.868195, loss_mean: 0.824624, loss_mean_cls: 0.043571, grad_norm: 0.188902
[[34m2025-10-04 12:37:52[0m] Step: 8055, Training Logs: loss_final: 0.880755, loss_mean: 0.837951, loss_mean_cls: 0.042804, grad_norm: 0.298914
[[34m2025-10-04 12:37:53[0m] Step: 8056, Training Logs: loss_final: 0.887283, loss_mean: 0.844469, loss_mean_cls: 0.042814, grad_norm: 0.249469
[[34m2025-10-04 12:37:53[0m] Step: 8057, Training Logs: loss_final: 0.883958, loss_mean: 0.840475, loss_mean_cls: 0.043482, grad_norm: 0.295639
[[34m2025-10-04 12:37:53[0m] Step: 8058, Training Logs: loss_final: 0.887000, loss_mean: 0.842838, loss_mean_cls: 0.044162, grad_norm: 0.245617
[[34m2025-10-04 12:37:53[0m] Step: 8059, Training Logs: loss_final: 0.878751, loss_mean: 0.834641, loss_mean_cls: 0.044110, grad_norm: 0.272177
[[34m2025-10-04 12:37:54[0m] Step: 8060, Training Logs: loss_final: 0.874733, loss_mean: 0.831362, loss_mean_cls: 0.043371, grad_norm: 0.255019
[[34m2025-10-04 12:37:54[0m] Step: 8061, Training Logs: loss_final: 0.869847, loss_mean: 0.826096, loss_mean_cls: 0.043751, grad_norm: 0.269656
[[34m2025-10-04 12:37:54[0m] Step: 8062, Training Logs: loss_final: 0.882792, loss_mean: 0.839229, loss_mean_cls: 0.043564, grad_norm: 0.214226
[[34m2025-10-04 12:37:55[0m] Step: 8063, Training Logs: loss_final: 0.862697, loss_mean: 0.817631, loss_mean_cls: 0.045066, grad_norm: 0.217685
[[34m2025-10-04 12:37:55[0m] Step: 8064, Training Logs: loss_final: 0.890169, loss_mean: 0.846906, loss_mean_cls: 0.043262, grad_norm: 0.292661
[[34m2025-10-04 12:37:55[0m] Step: 8065, Training Logs: loss_final: 0.883825, loss_mean: 0.839649, loss_mean_cls: 0.044176, grad_norm: 0.219945
[[34m2025-10-04 12:37:55[0m] Step: 8066, Training Logs: loss_final: 0.872135, loss_mean: 0.829351, loss_mean_cls: 0.042784, grad_norm: 0.266874
[[34m2025-10-04 12:37:56[0m] Step: 8067, Training Logs: loss_final: 0.873841, loss_mean: 0.829306, loss_mean_cls: 0.044535, grad_norm: 0.303204
[[34m2025-10-04 12:37:56[0m] Step: 8068, Training Logs: loss_final: 0.884549, loss_mean: 0.841233, loss_mean_cls: 0.043316, grad_norm: 0.195305
[[34m2025-10-04 12:37:56[0m] Step: 8069, Training Logs: loss_final: 0.888011, loss_mean: 0.845452, loss_mean_cls: 0.042559, grad_norm: 0.228240
[[34m2025-10-04 12:37:57[0m] Step: 8070, Training Logs: loss_final: 0.875328, loss_mean: 0.832166, loss_mean_cls: 0.043162, grad_norm: 0.274681
[[34m2025-10-04 12:37:57[0m] Step: 8071, Training Logs: loss_final: 0.869829, loss_mean: 0.826877, loss_mean_cls: 0.042952, grad_norm: 0.172121
[[34m2025-10-04 12:37:57[0m] Step: 8072, Training Logs: loss_final: 0.891093, loss_mean: 0.846781, loss_mean_cls: 0.044312, grad_norm: 0.219333
[[34m2025-10-04 12:37:57[0m] Step: 8073, Training Logs: loss_final: 0.873975, loss_mean: 0.831112, loss_mean_cls: 0.042863, grad_norm: 0.262537
[[34m2025-10-04 12:37:58[0m] Step: 8074, Training Logs: loss_final: 0.874891, loss_mean: 0.831061, loss_mean_cls: 0.043829, grad_norm: 0.236065
[[34m2025-10-04 12:37:58[0m] Step: 8075, Training Logs: loss_final: 0.872526, loss_mean: 0.828843, loss_mean_cls: 0.043683, grad_norm: 0.153598
[[34m2025-10-04 12:37:58[0m] Step: 8076, Training Logs: loss_final: 0.887200, loss_mean: 0.843820, loss_mean_cls: 0.043380, grad_norm: 0.206292
[[34m2025-10-04 12:37:59[0m] Step: 8077, Training Logs: loss_final: 0.870672, loss_mean: 0.829026, loss_mean_cls: 0.041646, grad_norm: 0.255579
[[34m2025-10-04 12:37:59[0m] Step: 8078, Training Logs: loss_final: 0.870894, loss_mean: 0.826903, loss_mean_cls: 0.043991, grad_norm: 0.251676
[[34m2025-10-04 12:37:59[0m] Step: 8079, Training Logs: loss_final: 0.876488, loss_mean: 0.831653, loss_mean_cls: 0.044835, grad_norm: 0.326739
[[34m2025-10-04 12:38:00[0m] Step: 8080, Training Logs: loss_final: 0.870501, loss_mean: 0.827759, loss_mean_cls: 0.042743, grad_norm: 0.202290
[[34m2025-10-04 12:38:00[0m] Step: 8081, Training Logs: loss_final: 0.864265, loss_mean: 0.819961, loss_mean_cls: 0.044303, grad_norm: 0.315365
[[34m2025-10-04 12:38:00[0m] Step: 8082, Training Logs: loss_final: 0.883483, loss_mean: 0.841322, loss_mean_cls: 0.042161, grad_norm: 0.304328
[[34m2025-10-04 12:38:00[0m] Step: 8083, Training Logs: loss_final: 0.865046, loss_mean: 0.822189, loss_mean_cls: 0.042857, grad_norm: 0.300320
[[34m2025-10-04 12:38:01[0m] Step: 8084, Training Logs: loss_final: 0.869573, loss_mean: 0.826140, loss_mean_cls: 0.043432, grad_norm: 0.217990
[[34m2025-10-04 12:38:01[0m] Step: 8085, Training Logs: loss_final: 0.872007, loss_mean: 0.828712, loss_mean_cls: 0.043295, grad_norm: 0.489402
[[34m2025-10-04 12:38:01[0m] Step: 8086, Training Logs: loss_final: 0.872062, loss_mean: 0.829141, loss_mean_cls: 0.042921, grad_norm: 0.290776
[[34m2025-10-04 12:38:02[0m] Step: 8087, Training Logs: loss_final: 0.892701, loss_mean: 0.849052, loss_mean_cls: 0.043648, grad_norm: 0.311575
[[34m2025-10-04 12:38:02[0m] Step: 8088, Training Logs: loss_final: 0.879417, loss_mean: 0.838646, loss_mean_cls: 0.040771, grad_norm: 0.307317
[[34m2025-10-04 12:38:02[0m] Step: 8089, Training Logs: loss_final: 0.863330, loss_mean: 0.820110, loss_mean_cls: 0.043221, grad_norm: 0.203684
[[34m2025-10-04 12:38:03[0m] Step: 8090, Training Logs: loss_final: 0.875035, loss_mean: 0.832005, loss_mean_cls: 0.043030, grad_norm: 0.280805
[[34m2025-10-04 12:38:03[0m] Step: 8091, Training Logs: loss_final: 0.884165, loss_mean: 0.840206, loss_mean_cls: 0.043959, grad_norm: 0.244012
[[34m2025-10-04 12:38:03[0m] Step: 8092, Training Logs: loss_final: 0.864211, loss_mean: 0.819660, loss_mean_cls: 0.044551, grad_norm: 0.262139
[[34m2025-10-04 12:38:03[0m] Step: 8093, Training Logs: loss_final: 0.891611, loss_mean: 0.848397, loss_mean_cls: 0.043213, grad_norm: 0.226211
[[34m2025-10-04 12:38:04[0m] Step: 8094, Training Logs: loss_final: 0.869733, loss_mean: 0.826441, loss_mean_cls: 0.043292, grad_norm: 0.259656
[[34m2025-10-04 12:38:04[0m] Step: 8095, Training Logs: loss_final: 0.892027, loss_mean: 0.848726, loss_mean_cls: 0.043300, grad_norm: 0.360728
[[34m2025-10-04 12:38:04[0m] Step: 8096, Training Logs: loss_final: 0.887846, loss_mean: 0.844158, loss_mean_cls: 0.043688, grad_norm: 0.340517
[[34m2025-10-04 12:38:05[0m] Step: 8097, Training Logs: loss_final: 0.891293, loss_mean: 0.849265, loss_mean_cls: 0.042028, grad_norm: 0.255673
[[34m2025-10-04 12:38:05[0m] Step: 8098, Training Logs: loss_final: 0.895515, loss_mean: 0.853494, loss_mean_cls: 0.042021, grad_norm: 0.323270
[[34m2025-10-04 12:38:05[0m] Step: 8099, Training Logs: loss_final: 0.874889, loss_mean: 0.832354, loss_mean_cls: 0.042534, grad_norm: 0.259279
[[34m2025-10-04 12:38:05[0m] Step: 8100, Training Logs: loss_final: 0.890336, loss_mean: 0.847724, loss_mean_cls: 0.042612, grad_norm: 0.292343
[[34m2025-10-04 12:38:06[0m] Step: 8101, Training Logs: loss_final: 0.894524, loss_mean: 0.852128, loss_mean_cls: 0.042395, grad_norm: 0.276730
[[34m2025-10-04 12:38:06[0m] Step: 8102, Training Logs: loss_final: 0.894028, loss_mean: 0.851472, loss_mean_cls: 0.042557, grad_norm: 0.299285
[[34m2025-10-04 12:38:06[0m] Step: 8103, Training Logs: loss_final: 0.865403, loss_mean: 0.822082, loss_mean_cls: 0.043321, grad_norm: 0.276584
[[34m2025-10-04 12:38:07[0m] Step: 8104, Training Logs: loss_final: 0.868464, loss_mean: 0.826163, loss_mean_cls: 0.042301, grad_norm: 0.318110
[[34m2025-10-04 12:38:07[0m] Step: 8105, Training Logs: loss_final: 0.857636, loss_mean: 0.815195, loss_mean_cls: 0.042440, grad_norm: 0.231846
[[34m2025-10-04 12:38:07[0m] Step: 8106, Training Logs: loss_final: 0.890303, loss_mean: 0.847264, loss_mean_cls: 0.043039, grad_norm: 0.306252
[[34m2025-10-04 12:38:08[0m] Step: 8107, Training Logs: loss_final: 0.861089, loss_mean: 0.816880, loss_mean_cls: 0.044208, grad_norm: 0.286960
[[34m2025-10-04 12:38:08[0m] Step: 8108, Training Logs: loss_final: 0.886707, loss_mean: 0.843468, loss_mean_cls: 0.043239, grad_norm: 0.200450
[[34m2025-10-04 12:38:08[0m] Step: 8109, Training Logs: loss_final: 0.882120, loss_mean: 0.839643, loss_mean_cls: 0.042478, grad_norm: 0.295613
[[34m2025-10-04 12:38:08[0m] Step: 8110, Training Logs: loss_final: 0.865865, loss_mean: 0.821677, loss_mean_cls: 0.044188, grad_norm: 0.210428
[[34m2025-10-04 12:38:09[0m] Step: 8111, Training Logs: loss_final: 0.886895, loss_mean: 0.843542, loss_mean_cls: 0.043353, grad_norm: 0.369196
[[34m2025-10-04 12:38:09[0m] Step: 8112, Training Logs: loss_final: 0.890905, loss_mean: 0.848978, loss_mean_cls: 0.041928, grad_norm: 0.295013
[[34m2025-10-04 12:38:09[0m] Step: 8113, Training Logs: loss_final: 0.880384, loss_mean: 0.837553, loss_mean_cls: 0.042831, grad_norm: 0.267233
[[34m2025-10-04 12:38:10[0m] Step: 8114, Training Logs: loss_final: 0.864463, loss_mean: 0.820820, loss_mean_cls: 0.043644, grad_norm: 0.259774
[[34m2025-10-04 12:38:10[0m] Step: 8115, Training Logs: loss_final: 0.880956, loss_mean: 0.836586, loss_mean_cls: 0.044369, grad_norm: 0.316845
[[34m2025-10-04 12:38:10[0m] Step: 8116, Training Logs: loss_final: 0.886298, loss_mean: 0.843031, loss_mean_cls: 0.043267, grad_norm: 0.422909
[[34m2025-10-04 12:38:10[0m] Step: 8117, Training Logs: loss_final: 0.876977, loss_mean: 0.833598, loss_mean_cls: 0.043378, grad_norm: 0.257679
[[34m2025-10-04 12:38:11[0m] Step: 8118, Training Logs: loss_final: 0.864719, loss_mean: 0.820562, loss_mean_cls: 0.044157, grad_norm: 0.308745
[[34m2025-10-04 12:38:11[0m] Step: 8119, Training Logs: loss_final: 0.851822, loss_mean: 0.808593, loss_mean_cls: 0.043229, grad_norm: 0.248615
[[34m2025-10-04 12:38:11[0m] Step: 8120, Training Logs: loss_final: 0.854261, loss_mean: 0.809501, loss_mean_cls: 0.044760, grad_norm: 0.385969
[[34m2025-10-04 12:38:12[0m] Step: 8121, Training Logs: loss_final: 0.877985, loss_mean: 0.833518, loss_mean_cls: 0.044467, grad_norm: 0.197013
[[34m2025-10-04 12:38:12[0m] Step: 8122, Training Logs: loss_final: 0.881024, loss_mean: 0.837299, loss_mean_cls: 0.043725, grad_norm: 0.374826
[[34m2025-10-04 12:38:12[0m] Step: 8123, Training Logs: loss_final: 0.870319, loss_mean: 0.826603, loss_mean_cls: 0.043716, grad_norm: 0.262081
[[34m2025-10-04 12:38:12[0m] Step: 8124, Training Logs: loss_final: 0.885333, loss_mean: 0.841545, loss_mean_cls: 0.043788, grad_norm: 0.494152
[[34m2025-10-04 12:38:13[0m] Step: 8125, Training Logs: loss_final: 0.895918, loss_mean: 0.852439, loss_mean_cls: 0.043479, grad_norm: 0.184174
[[34m2025-10-04 12:38:13[0m] Step: 8126, Training Logs: loss_final: 0.872821, loss_mean: 0.829306, loss_mean_cls: 0.043515, grad_norm: 0.438542
[[34m2025-10-04 12:38:13[0m] Step: 8127, Training Logs: loss_final: 0.890203, loss_mean: 0.847647, loss_mean_cls: 0.042557, grad_norm: 0.305945
[[34m2025-10-04 12:38:14[0m] Step: 8128, Training Logs: loss_final: 0.858822, loss_mean: 0.815054, loss_mean_cls: 0.043768, grad_norm: 0.273581
[[34m2025-10-04 12:38:14[0m] Step: 8129, Training Logs: loss_final: 0.872294, loss_mean: 0.829451, loss_mean_cls: 0.042844, grad_norm: 0.345979
[[34m2025-10-04 12:38:14[0m] Step: 8130, Training Logs: loss_final: 0.860128, loss_mean: 0.814823, loss_mean_cls: 0.045305, grad_norm: 0.387467
[[34m2025-10-04 12:38:15[0m] Step: 8131, Training Logs: loss_final: 0.886760, loss_mean: 0.843394, loss_mean_cls: 0.043366, grad_norm: 0.325363
[[34m2025-10-04 12:38:15[0m] Step: 8132, Training Logs: loss_final: 0.867210, loss_mean: 0.824755, loss_mean_cls: 0.042456, grad_norm: 0.258204
[[34m2025-10-04 12:38:15[0m] Step: 8133, Training Logs: loss_final: 0.878211, loss_mean: 0.833717, loss_mean_cls: 0.044493, grad_norm: 0.236518
[[34m2025-10-04 12:38:15[0m] Step: 8134, Training Logs: loss_final: 0.869683, loss_mean: 0.827036, loss_mean_cls: 0.042647, grad_norm: 0.304678
[[34m2025-10-04 12:38:16[0m] Step: 8135, Training Logs: loss_final: 0.886208, loss_mean: 0.842614, loss_mean_cls: 0.043594, grad_norm: 0.375164
[[34m2025-10-04 12:38:16[0m] Step: 8136, Training Logs: loss_final: 0.872792, loss_mean: 0.829540, loss_mean_cls: 0.043251, grad_norm: 0.319792
[[34m2025-10-04 12:38:16[0m] Step: 8137, Training Logs: loss_final: 0.867128, loss_mean: 0.823418, loss_mean_cls: 0.043710, grad_norm: 0.223298
[[34m2025-10-04 12:38:17[0m] Step: 8138, Training Logs: loss_final: 0.884671, loss_mean: 0.839751, loss_mean_cls: 0.044919, grad_norm: 0.273994
[[34m2025-10-04 12:38:17[0m] Step: 8139, Training Logs: loss_final: 0.864413, loss_mean: 0.820943, loss_mean_cls: 0.043470, grad_norm: 0.303962
[[34m2025-10-04 12:38:17[0m] Step: 8140, Training Logs: loss_final: 0.867906, loss_mean: 0.823316, loss_mean_cls: 0.044590, grad_norm: 0.232890
[[34m2025-10-04 12:38:17[0m] Step: 8141, Training Logs: loss_final: 0.866412, loss_mean: 0.823275, loss_mean_cls: 0.043137, grad_norm: 0.252175
[[34m2025-10-04 12:38:18[0m] Step: 8142, Training Logs: loss_final: 0.885273, loss_mean: 0.841252, loss_mean_cls: 0.044021, grad_norm: 0.302510
[[34m2025-10-04 12:38:18[0m] Step: 8143, Training Logs: loss_final: 0.870910, loss_mean: 0.827409, loss_mean_cls: 0.043501, grad_norm: 0.230540
[[34m2025-10-04 12:38:18[0m] Step: 8144, Training Logs: loss_final: 0.866647, loss_mean: 0.822713, loss_mean_cls: 0.043934, grad_norm: 0.229421
[[34m2025-10-04 12:38:19[0m] Step: 8145, Training Logs: loss_final: 0.877590, loss_mean: 0.833835, loss_mean_cls: 0.043755, grad_norm: 0.267674
[[34m2025-10-04 12:38:19[0m] Step: 8146, Training Logs: loss_final: 0.872519, loss_mean: 0.829207, loss_mean_cls: 0.043312, grad_norm: 0.309578
[[34m2025-10-04 12:38:19[0m] Step: 8147, Training Logs: loss_final: 0.864053, loss_mean: 0.820457, loss_mean_cls: 0.043597, grad_norm: 0.329763
[[34m2025-10-04 12:38:19[0m] Step: 8148, Training Logs: loss_final: 0.896931, loss_mean: 0.853729, loss_mean_cls: 0.043203, grad_norm: 0.253179
[[34m2025-10-04 12:38:20[0m] Step: 8149, Training Logs: loss_final: 0.876680, loss_mean: 0.833753, loss_mean_cls: 0.042927, grad_norm: 0.300197
[[34m2025-10-04 12:38:20[0m] Step: 8150, Training Logs: loss_final: 0.873480, loss_mean: 0.830437, loss_mean_cls: 0.043043, grad_norm: 0.349283
[[34m2025-10-04 12:38:20[0m] Step: 8151, Training Logs: loss_final: 0.882593, loss_mean: 0.838742, loss_mean_cls: 0.043851, grad_norm: 0.273602
[[34m2025-10-04 12:38:21[0m] Step: 8152, Training Logs: loss_final: 0.879095, loss_mean: 0.835820, loss_mean_cls: 0.043276, grad_norm: 0.236350
[[34m2025-10-04 12:38:21[0m] Step: 8153, Training Logs: loss_final: 0.865898, loss_mean: 0.824206, loss_mean_cls: 0.041691, grad_norm: 0.370205
[[34m2025-10-04 12:38:21[0m] Step: 8154, Training Logs: loss_final: 0.866007, loss_mean: 0.822396, loss_mean_cls: 0.043611, grad_norm: 0.349778
[[34m2025-10-04 12:38:22[0m] Step: 8155, Training Logs: loss_final: 0.875483, loss_mean: 0.832232, loss_mean_cls: 0.043250, grad_norm: 0.376169
[[34m2025-10-04 12:38:22[0m] Step: 8156, Training Logs: loss_final: 0.867461, loss_mean: 0.823721, loss_mean_cls: 0.043740, grad_norm: 0.386457
[[34m2025-10-04 12:38:22[0m] Step: 8157, Training Logs: loss_final: 0.888733, loss_mean: 0.846067, loss_mean_cls: 0.042666, grad_norm: 0.306966
[[34m2025-10-04 12:38:22[0m] Step: 8158, Training Logs: loss_final: 0.876786, loss_mean: 0.834629, loss_mean_cls: 0.042157, grad_norm: 0.260651
[[34m2025-10-04 12:38:23[0m] Step: 8159, Training Logs: loss_final: 0.900208, loss_mean: 0.856695, loss_mean_cls: 0.043513, grad_norm: 0.411700
[[34m2025-10-04 12:38:23[0m] Step: 8160, Training Logs: loss_final: 0.870644, loss_mean: 0.826924, loss_mean_cls: 0.043720, grad_norm: 0.207182
[[34m2025-10-04 12:38:23[0m] Step: 8161, Training Logs: loss_final: 0.883299, loss_mean: 0.840497, loss_mean_cls: 0.042803, grad_norm: 0.350122
[[34m2025-10-04 12:38:24[0m] Step: 8162, Training Logs: loss_final: 0.877356, loss_mean: 0.833272, loss_mean_cls: 0.044084, grad_norm: 0.268349
[[34m2025-10-04 12:38:24[0m] Step: 8163, Training Logs: loss_final: 0.869979, loss_mean: 0.826169, loss_mean_cls: 0.043810, grad_norm: 0.536891
[[34m2025-10-04 12:38:24[0m] Step: 8164, Training Logs: loss_final: 0.875096, loss_mean: 0.833261, loss_mean_cls: 0.041836, grad_norm: 0.309938
[[34m2025-10-04 12:38:24[0m] Step: 8165, Training Logs: loss_final: 0.863564, loss_mean: 0.819841, loss_mean_cls: 0.043723, grad_norm: 0.606699
[[34m2025-10-04 12:38:25[0m] Step: 8166, Training Logs: loss_final: 0.875413, loss_mean: 0.832963, loss_mean_cls: 0.042451, grad_norm: 0.240567
[[34m2025-10-04 12:38:25[0m] Step: 8167, Training Logs: loss_final: 0.874919, loss_mean: 0.831041, loss_mean_cls: 0.043878, grad_norm: 0.447202
[[34m2025-10-04 12:38:25[0m] Step: 8168, Training Logs: loss_final: 0.873551, loss_mean: 0.830192, loss_mean_cls: 0.043359, grad_norm: 0.245219
[[34m2025-10-04 12:38:26[0m] Step: 8169, Training Logs: loss_final: 0.864013, loss_mean: 0.819395, loss_mean_cls: 0.044617, grad_norm: 0.433887
[[34m2025-10-04 12:38:26[0m] Step: 8170, Training Logs: loss_final: 0.870832, loss_mean: 0.826486, loss_mean_cls: 0.044346, grad_norm: 0.298964
[[34m2025-10-04 12:38:26[0m] Step: 8171, Training Logs: loss_final: 0.888082, loss_mean: 0.844632, loss_mean_cls: 0.043450, grad_norm: 0.441699
[[34m2025-10-04 12:38:26[0m] Step: 8172, Training Logs: loss_final: 0.879468, loss_mean: 0.837180, loss_mean_cls: 0.042288, grad_norm: 0.259898
[[34m2025-10-04 12:38:27[0m] Step: 8173, Training Logs: loss_final: 0.870265, loss_mean: 0.826003, loss_mean_cls: 0.044262, grad_norm: 0.324734
[[34m2025-10-04 12:38:27[0m] Step: 8174, Training Logs: loss_final: 0.884873, loss_mean: 0.841518, loss_mean_cls: 0.043355, grad_norm: 0.272837
[[34m2025-10-04 12:38:27[0m] Step: 8175, Training Logs: loss_final: 0.867070, loss_mean: 0.822660, loss_mean_cls: 0.044409, grad_norm: 0.362284
[[34m2025-10-04 12:38:28[0m] Step: 8176, Training Logs: loss_final: 0.865455, loss_mean: 0.821325, loss_mean_cls: 0.044129, grad_norm: 0.209836
[[34m2025-10-04 12:38:28[0m] Step: 8177, Training Logs: loss_final: 0.893472, loss_mean: 0.850842, loss_mean_cls: 0.042630, grad_norm: 0.359971
[[34m2025-10-04 12:38:28[0m] Step: 8178, Training Logs: loss_final: 0.894754, loss_mean: 0.852215, loss_mean_cls: 0.042539, grad_norm: 0.339393
[[34m2025-10-04 12:38:28[0m] Step: 8179, Training Logs: loss_final: 0.870455, loss_mean: 0.827454, loss_mean_cls: 0.043001, grad_norm: 0.299644
[[34m2025-10-04 12:38:29[0m] Step: 8180, Training Logs: loss_final: 0.853856, loss_mean: 0.809093, loss_mean_cls: 0.044763, grad_norm: 0.351148
[[34m2025-10-04 12:38:29[0m] Step: 8181, Training Logs: loss_final: 0.886986, loss_mean: 0.843656, loss_mean_cls: 0.043330, grad_norm: 0.189193
[[34m2025-10-04 12:38:29[0m] Step: 8182, Training Logs: loss_final: 0.857006, loss_mean: 0.813188, loss_mean_cls: 0.043817, grad_norm: 0.278844
[[34m2025-10-04 12:38:30[0m] Step: 8183, Training Logs: loss_final: 0.887360, loss_mean: 0.844281, loss_mean_cls: 0.043079, grad_norm: 0.310084
[[34m2025-10-04 12:38:30[0m] Step: 8184, Training Logs: loss_final: 0.884259, loss_mean: 0.841610, loss_mean_cls: 0.042649, grad_norm: 0.211172
[[34m2025-10-04 12:38:30[0m] Step: 8185, Training Logs: loss_final: 0.888339, loss_mean: 0.845430, loss_mean_cls: 0.042909, grad_norm: 0.256164
[[34m2025-10-04 12:38:31[0m] Step: 8186, Training Logs: loss_final: 0.864567, loss_mean: 0.821777, loss_mean_cls: 0.042790, grad_norm: 0.274110
[[34m2025-10-04 12:38:31[0m] Step: 8187, Training Logs: loss_final: 0.873743, loss_mean: 0.829911, loss_mean_cls: 0.043832, grad_norm: 0.244324
[[34m2025-10-04 12:38:31[0m] Step: 8188, Training Logs: loss_final: 0.863417, loss_mean: 0.819940, loss_mean_cls: 0.043477, grad_norm: 0.234922
[[34m2025-10-04 12:38:31[0m] Step: 8189, Training Logs: loss_final: 0.887468, loss_mean: 0.843795, loss_mean_cls: 0.043673, grad_norm: 0.339701
[[34m2025-10-04 12:38:32[0m] Step: 8190, Training Logs: loss_final: 0.881701, loss_mean: 0.838670, loss_mean_cls: 0.043031, grad_norm: 0.268764
[[34m2025-10-04 12:38:32[0m] Step: 8191, Training Logs: loss_final: 0.889511, loss_mean: 0.846161, loss_mean_cls: 0.043349, grad_norm: 0.229107
[[34m2025-10-04 12:38:32[0m] Step: 8192, Training Logs: loss_final: 0.871895, loss_mean: 0.828177, loss_mean_cls: 0.043718, grad_norm: 0.229868
[[34m2025-10-04 12:38:33[0m] Step: 8193, Training Logs: loss_final: 0.886097, loss_mean: 0.843591, loss_mean_cls: 0.042506, grad_norm: 0.211156
[[34m2025-10-04 12:38:33[0m] Step: 8194, Training Logs: loss_final: 0.859930, loss_mean: 0.816423, loss_mean_cls: 0.043507, grad_norm: 0.290721
[[34m2025-10-04 12:38:33[0m] Step: 8195, Training Logs: loss_final: 0.889250, loss_mean: 0.845860, loss_mean_cls: 0.043390, grad_norm: 0.300065
[[34m2025-10-04 12:38:33[0m] Step: 8196, Training Logs: loss_final: 0.894088, loss_mean: 0.852310, loss_mean_cls: 0.041778, grad_norm: 0.240402
[[34m2025-10-04 12:38:34[0m] Step: 8197, Training Logs: loss_final: 0.868094, loss_mean: 0.824691, loss_mean_cls: 0.043403, grad_norm: 0.194990
[[34m2025-10-04 12:38:34[0m] Step: 8198, Training Logs: loss_final: 0.873917, loss_mean: 0.830451, loss_mean_cls: 0.043466, grad_norm: 0.293608
[[34m2025-10-04 12:38:34[0m] Step: 8199, Training Logs: loss_final: 0.868829, loss_mean: 0.825157, loss_mean_cls: 0.043672, grad_norm: 0.293351
[[34m2025-10-04 12:38:35[0m] Step: 8200, Training Logs: loss_final: 0.870205, loss_mean: 0.827205, loss_mean_cls: 0.043000, grad_norm: 0.243233
[[34m2025-10-04 12:38:35[0m] Step: 8201, Training Logs: loss_final: 0.890507, loss_mean: 0.848363, loss_mean_cls: 0.042144, grad_norm: 0.216464
[[34m2025-10-04 12:38:35[0m] Step: 8202, Training Logs: loss_final: 0.866520, loss_mean: 0.822455, loss_mean_cls: 0.044065, grad_norm: 0.276780
[[34m2025-10-04 12:38:35[0m] Step: 8203, Training Logs: loss_final: 0.854046, loss_mean: 0.810265, loss_mean_cls: 0.043781, grad_norm: 0.206115
[[34m2025-10-04 12:38:36[0m] Step: 8204, Training Logs: loss_final: 0.859942, loss_mean: 0.815418, loss_mean_cls: 0.044524, grad_norm: 0.281933
[[34m2025-10-04 12:38:36[0m] Step: 8205, Training Logs: loss_final: 0.866780, loss_mean: 0.822433, loss_mean_cls: 0.044347, grad_norm: 0.285871
[[34m2025-10-04 12:38:36[0m] Step: 8206, Training Logs: loss_final: 0.875334, loss_mean: 0.832580, loss_mean_cls: 0.042754, grad_norm: 0.326298
[[34m2025-10-04 12:38:37[0m] Step: 8207, Training Logs: loss_final: 0.876940, loss_mean: 0.833117, loss_mean_cls: 0.043823, grad_norm: 0.300191
[[34m2025-10-04 12:38:37[0m] Step: 8208, Training Logs: loss_final: 0.876810, loss_mean: 0.833466, loss_mean_cls: 0.043343, grad_norm: 0.209962
[[34m2025-10-04 12:38:37[0m] Step: 8209, Training Logs: loss_final: 0.889950, loss_mean: 0.846043, loss_mean_cls: 0.043907, grad_norm: 0.224503
[[34m2025-10-04 12:38:37[0m] Step: 8210, Training Logs: loss_final: 0.870804, loss_mean: 0.826517, loss_mean_cls: 0.044287, grad_norm: 0.282277
[[34m2025-10-04 12:38:38[0m] Step: 8211, Training Logs: loss_final: 0.882199, loss_mean: 0.837702, loss_mean_cls: 0.044497, grad_norm: 0.301722
[[34m2025-10-04 12:38:38[0m] Step: 8212, Training Logs: loss_final: 0.914931, loss_mean: 0.872182, loss_mean_cls: 0.042749, grad_norm: 0.236429
[[34m2025-10-04 12:38:38[0m] Step: 8213, Training Logs: loss_final: 0.867815, loss_mean: 0.824054, loss_mean_cls: 0.043761, grad_norm: 0.325205
[[34m2025-10-04 12:38:39[0m] Step: 8214, Training Logs: loss_final: 0.869237, loss_mean: 0.825218, loss_mean_cls: 0.044019, grad_norm: 0.292091
[[34m2025-10-04 12:38:39[0m] Step: 8215, Training Logs: loss_final: 0.863271, loss_mean: 0.819146, loss_mean_cls: 0.044125, grad_norm: 0.265396
[[34m2025-10-04 12:38:39[0m] Step: 8216, Training Logs: loss_final: 0.890059, loss_mean: 0.847042, loss_mean_cls: 0.043017, grad_norm: 0.244916
[[34m2025-10-04 12:38:39[0m] Step: 8217, Training Logs: loss_final: 0.879109, loss_mean: 0.835948, loss_mean_cls: 0.043161, grad_norm: 0.390917
[[34m2025-10-04 12:38:40[0m] Step: 8218, Training Logs: loss_final: 0.884777, loss_mean: 0.841412, loss_mean_cls: 0.043365, grad_norm: 0.251540
[[34m2025-10-04 12:38:40[0m] Step: 8219, Training Logs: loss_final: 0.895667, loss_mean: 0.852954, loss_mean_cls: 0.042713, grad_norm: 0.365328
[[34m2025-10-04 12:38:40[0m] Step: 8220, Training Logs: loss_final: 0.885923, loss_mean: 0.842899, loss_mean_cls: 0.043024, grad_norm: 0.277207
[[34m2025-10-04 12:38:41[0m] Step: 8221, Training Logs: loss_final: 0.868187, loss_mean: 0.823687, loss_mean_cls: 0.044500, grad_norm: 0.399011
[[34m2025-10-04 12:38:41[0m] Step: 8222, Training Logs: loss_final: 0.870245, loss_mean: 0.827055, loss_mean_cls: 0.043190, grad_norm: 0.274349
[[34m2025-10-04 12:38:41[0m] Step: 8223, Training Logs: loss_final: 0.898097, loss_mean: 0.854073, loss_mean_cls: 0.044025, grad_norm: 0.275381
[[34m2025-10-04 12:38:42[0m] Step: 8224, Training Logs: loss_final: 0.871645, loss_mean: 0.828988, loss_mean_cls: 0.042657, grad_norm: 0.301687
[[34m2025-10-04 12:38:42[0m] Step: 8225, Training Logs: loss_final: 0.872976, loss_mean: 0.830191, loss_mean_cls: 0.042785, grad_norm: 0.267395
[[34m2025-10-04 12:38:42[0m] Step: 8226, Training Logs: loss_final: 0.856376, loss_mean: 0.811729, loss_mean_cls: 0.044647, grad_norm: 0.231900
[[34m2025-10-04 12:38:42[0m] Step: 8227, Training Logs: loss_final: 0.870241, loss_mean: 0.826623, loss_mean_cls: 0.043618, grad_norm: 0.231429
[[34m2025-10-04 12:38:43[0m] Step: 8228, Training Logs: loss_final: 0.864294, loss_mean: 0.821108, loss_mean_cls: 0.043185, grad_norm: 0.258222
[[34m2025-10-04 12:38:43[0m] Step: 8229, Training Logs: loss_final: 0.894565, loss_mean: 0.851553, loss_mean_cls: 0.043011, grad_norm: 0.273337
[[34m2025-10-04 12:38:43[0m] Step: 8230, Training Logs: loss_final: 0.888525, loss_mean: 0.845631, loss_mean_cls: 0.042894, grad_norm: 0.244360
[[34m2025-10-04 12:38:44[0m] Step: 8231, Training Logs: loss_final: 0.879778, loss_mean: 0.836832, loss_mean_cls: 0.042946, grad_norm: 0.250493
[[34m2025-10-04 12:38:44[0m] Step: 8232, Training Logs: loss_final: 0.866138, loss_mean: 0.821589, loss_mean_cls: 0.044550, grad_norm: 0.316634
[[34m2025-10-04 12:38:44[0m] Step: 8233, Training Logs: loss_final: 0.880875, loss_mean: 0.839150, loss_mean_cls: 0.041725, grad_norm: 0.296452
[[34m2025-10-04 12:38:44[0m] Step: 8234, Training Logs: loss_final: 0.889385, loss_mean: 0.846153, loss_mean_cls: 0.043232, grad_norm: 0.233548
[[34m2025-10-04 12:38:45[0m] Step: 8235, Training Logs: loss_final: 0.879857, loss_mean: 0.835613, loss_mean_cls: 0.044244, grad_norm: 0.296747
[[34m2025-10-04 12:38:45[0m] Step: 8236, Training Logs: loss_final: 0.865003, loss_mean: 0.820737, loss_mean_cls: 0.044266, grad_norm: 0.279012
[[34m2025-10-04 12:38:45[0m] Step: 8237, Training Logs: loss_final: 0.896120, loss_mean: 0.852137, loss_mean_cls: 0.043982, grad_norm: 0.277920
[[34m2025-10-04 12:38:46[0m] Step: 8238, Training Logs: loss_final: 0.886832, loss_mean: 0.843721, loss_mean_cls: 0.043111, grad_norm: 0.260465
[[34m2025-10-04 12:38:46[0m] Step: 8239, Training Logs: loss_final: 0.893443, loss_mean: 0.850453, loss_mean_cls: 0.042990, grad_norm: 0.362634
[[34m2025-10-04 12:38:46[0m] Step: 8240, Training Logs: loss_final: 0.881949, loss_mean: 0.838941, loss_mean_cls: 0.043008, grad_norm: 0.189107
[[34m2025-10-04 12:38:47[0m] Step: 8241, Training Logs: loss_final: 0.882884, loss_mean: 0.838778, loss_mean_cls: 0.044106, grad_norm: 0.360557
[[34m2025-10-04 12:38:47[0m] Step: 8242, Training Logs: loss_final: 0.897500, loss_mean: 0.855178, loss_mean_cls: 0.042322, grad_norm: 0.307010
[[34m2025-10-04 12:38:47[0m] Step: 8243, Training Logs: loss_final: 0.881422, loss_mean: 0.837673, loss_mean_cls: 0.043749, grad_norm: 0.341547
[[34m2025-10-04 12:38:47[0m] Step: 8244, Training Logs: loss_final: 0.864984, loss_mean: 0.820247, loss_mean_cls: 0.044737, grad_norm: 0.308955
[[34m2025-10-04 12:38:48[0m] Step: 8245, Training Logs: loss_final: 0.877707, loss_mean: 0.835167, loss_mean_cls: 0.042541, grad_norm: 0.459018
[[34m2025-10-04 12:38:48[0m] Step: 8246, Training Logs: loss_final: 0.868709, loss_mean: 0.825452, loss_mean_cls: 0.043257, grad_norm: 0.311344
[[34m2025-10-04 12:38:48[0m] Step: 8247, Training Logs: loss_final: 0.882057, loss_mean: 0.840010, loss_mean_cls: 0.042047, grad_norm: 0.305772
[[34m2025-10-04 12:38:49[0m] Step: 8248, Training Logs: loss_final: 0.881675, loss_mean: 0.838328, loss_mean_cls: 0.043347, grad_norm: 0.300620
[[34m2025-10-04 12:38:49[0m] Step: 8249, Training Logs: loss_final: 0.898896, loss_mean: 0.856165, loss_mean_cls: 0.042731, grad_norm: 0.354073
[[34m2025-10-04 12:38:49[0m] Step: 8250, Training Logs: loss_final: 0.867727, loss_mean: 0.823243, loss_mean_cls: 0.044485, grad_norm: 0.307682
[[34m2025-10-04 12:38:49[0m] Step: 8251, Training Logs: loss_final: 0.865455, loss_mean: 0.821858, loss_mean_cls: 0.043597, grad_norm: 0.354514
[[34m2025-10-04 12:38:50[0m] Step: 8252, Training Logs: loss_final: 0.892538, loss_mean: 0.848729, loss_mean_cls: 0.043809, grad_norm: 0.424605
[[34m2025-10-04 12:38:50[0m] Step: 8253, Training Logs: loss_final: 0.889559, loss_mean: 0.847228, loss_mean_cls: 0.042331, grad_norm: 0.331042
[[34m2025-10-04 12:38:50[0m] Step: 8254, Training Logs: loss_final: 0.879089, loss_mean: 0.835863, loss_mean_cls: 0.043226, grad_norm: 0.318850
[[34m2025-10-04 12:38:51[0m] Step: 8255, Training Logs: loss_final: 0.863949, loss_mean: 0.820838, loss_mean_cls: 0.043112, grad_norm: 0.296957
[[34m2025-10-04 12:38:51[0m] Step: 8256, Training Logs: loss_final: 0.874410, loss_mean: 0.831192, loss_mean_cls: 0.043218, grad_norm: 0.355760
[[34m2025-10-04 12:38:51[0m] Step: 8257, Training Logs: loss_final: 0.880060, loss_mean: 0.837885, loss_mean_cls: 0.042175, grad_norm: 0.228653
[[34m2025-10-04 12:38:51[0m] Step: 8258, Training Logs: loss_final: 0.869491, loss_mean: 0.825494, loss_mean_cls: 0.043997, grad_norm: 0.278819
[[34m2025-10-04 12:38:52[0m] Step: 8259, Training Logs: loss_final: 0.874787, loss_mean: 0.830522, loss_mean_cls: 0.044266, grad_norm: 0.285951
[[34m2025-10-04 12:38:52[0m] Step: 8260, Training Logs: loss_final: 0.881866, loss_mean: 0.839362, loss_mean_cls: 0.042503, grad_norm: 0.291866
[[34m2025-10-04 12:38:52[0m] Step: 8261, Training Logs: loss_final: 0.855280, loss_mean: 0.812574, loss_mean_cls: 0.042706, grad_norm: 0.271810
[[34m2025-10-04 12:38:53[0m] Step: 8262, Training Logs: loss_final: 0.868335, loss_mean: 0.825110, loss_mean_cls: 0.043224, grad_norm: 0.241705
[[34m2025-10-04 12:38:53[0m] Step: 8263, Training Logs: loss_final: 0.873587, loss_mean: 0.829135, loss_mean_cls: 0.044452, grad_norm: 0.247907
[[34m2025-10-04 12:38:53[0m] Step: 8264, Training Logs: loss_final: 0.883894, loss_mean: 0.841472, loss_mean_cls: 0.042422, grad_norm: 0.243666
[[34m2025-10-04 12:38:54[0m] Step: 8265, Training Logs: loss_final: 0.862483, loss_mean: 0.820147, loss_mean_cls: 0.042336, grad_norm: 0.198562
[[34m2025-10-04 12:38:54[0m] Step: 8266, Training Logs: loss_final: 0.877852, loss_mean: 0.835244, loss_mean_cls: 0.042608, grad_norm: 0.247997
[[34m2025-10-04 12:38:54[0m] Step: 8267, Training Logs: loss_final: 0.886747, loss_mean: 0.843264, loss_mean_cls: 0.043483, grad_norm: 0.298684
[[34m2025-10-04 12:38:54[0m] Step: 8268, Training Logs: loss_final: 0.872565, loss_mean: 0.829332, loss_mean_cls: 0.043233, grad_norm: 0.215224
[[34m2025-10-04 12:38:55[0m] Step: 8269, Training Logs: loss_final: 0.897933, loss_mean: 0.853807, loss_mean_cls: 0.044125, grad_norm: 0.181705
[[34m2025-10-04 12:38:55[0m] Step: 8270, Training Logs: loss_final: 0.888107, loss_mean: 0.843697, loss_mean_cls: 0.044410, grad_norm: 0.218633
[[34m2025-10-04 12:38:55[0m] Step: 8271, Training Logs: loss_final: 0.873947, loss_mean: 0.830162, loss_mean_cls: 0.043786, grad_norm: 0.203053
[[34m2025-10-04 12:38:56[0m] Step: 8272, Training Logs: loss_final: 0.871492, loss_mean: 0.828965, loss_mean_cls: 0.042528, grad_norm: 0.185725
[[34m2025-10-04 12:38:56[0m] Step: 8273, Training Logs: loss_final: 0.888128, loss_mean: 0.843587, loss_mean_cls: 0.044541, grad_norm: 0.184180
[[34m2025-10-04 12:38:56[0m] Step: 8274, Training Logs: loss_final: 0.879255, loss_mean: 0.837109, loss_mean_cls: 0.042146, grad_norm: 0.265449
[[34m2025-10-04 12:38:56[0m] Step: 8275, Training Logs: loss_final: 0.856754, loss_mean: 0.813107, loss_mean_cls: 0.043647, grad_norm: 0.254890
[[34m2025-10-04 12:38:57[0m] Step: 8276, Training Logs: loss_final: 0.891078, loss_mean: 0.847908, loss_mean_cls: 0.043170, grad_norm: 0.351168
[[34m2025-10-04 12:38:57[0m] Step: 8277, Training Logs: loss_final: 0.888287, loss_mean: 0.845203, loss_mean_cls: 0.043085, grad_norm: 0.262193
[[34m2025-10-04 12:38:57[0m] Step: 8278, Training Logs: loss_final: 0.885443, loss_mean: 0.844008, loss_mean_cls: 0.041435, grad_norm: 0.210388
[[34m2025-10-04 12:38:58[0m] Step: 8279, Training Logs: loss_final: 0.879937, loss_mean: 0.836449, loss_mean_cls: 0.043488, grad_norm: 0.367107
[[34m2025-10-04 12:38:58[0m] Step: 8280, Training Logs: loss_final: 0.875418, loss_mean: 0.831460, loss_mean_cls: 0.043959, grad_norm: 0.323746
[[34m2025-10-04 12:38:58[0m] Step: 8281, Training Logs: loss_final: 0.872713, loss_mean: 0.830779, loss_mean_cls: 0.041934, grad_norm: 0.354150
[[34m2025-10-04 12:38:59[0m] Step: 8282, Training Logs: loss_final: 0.869420, loss_mean: 0.826394, loss_mean_cls: 0.043026, grad_norm: 0.292940
[[34m2025-10-04 12:38:59[0m] Step: 8283, Training Logs: loss_final: 0.876926, loss_mean: 0.835209, loss_mean_cls: 0.041716, grad_norm: 0.338613
[[34m2025-10-04 12:38:59[0m] Step: 8284, Training Logs: loss_final: 0.873492, loss_mean: 0.830727, loss_mean_cls: 0.042765, grad_norm: 0.307436
[[34m2025-10-04 12:38:59[0m] Step: 8285, Training Logs: loss_final: 0.878026, loss_mean: 0.834579, loss_mean_cls: 0.043448, grad_norm: 0.231082
[[34m2025-10-04 12:39:00[0m] Step: 8286, Training Logs: loss_final: 0.886808, loss_mean: 0.844346, loss_mean_cls: 0.042462, grad_norm: 0.265975
[[34m2025-10-04 12:39:00[0m] Step: 8287, Training Logs: loss_final: 0.873029, loss_mean: 0.829146, loss_mean_cls: 0.043883, grad_norm: 0.198271
[[34m2025-10-04 12:39:00[0m] Step: 8288, Training Logs: loss_final: 0.882252, loss_mean: 0.838472, loss_mean_cls: 0.043779, grad_norm: 0.274723
[[34m2025-10-04 12:39:01[0m] Step: 8289, Training Logs: loss_final: 0.861798, loss_mean: 0.817346, loss_mean_cls: 0.044452, grad_norm: 0.272921
[[34m2025-10-04 12:39:01[0m] Step: 8290, Training Logs: loss_final: 0.842235, loss_mean: 0.798683, loss_mean_cls: 0.043553, grad_norm: 0.217146
[[34m2025-10-04 12:39:01[0m] Step: 8291, Training Logs: loss_final: 0.876213, loss_mean: 0.832555, loss_mean_cls: 0.043658, grad_norm: 0.257222
[[34m2025-10-04 12:39:01[0m] Step: 8292, Training Logs: loss_final: 0.891350, loss_mean: 0.848108, loss_mean_cls: 0.043242, grad_norm: 0.320809
[[34m2025-10-04 12:39:02[0m] Step: 8293, Training Logs: loss_final: 0.862418, loss_mean: 0.817688, loss_mean_cls: 0.044730, grad_norm: 0.369062
[[34m2025-10-04 12:39:02[0m] Step: 8294, Training Logs: loss_final: 0.879904, loss_mean: 0.836332, loss_mean_cls: 0.043572, grad_norm: 0.246064
[[34m2025-10-04 12:39:02[0m] Step: 8295, Training Logs: loss_final: 0.878823, loss_mean: 0.835276, loss_mean_cls: 0.043546, grad_norm: 0.235313
[[34m2025-10-04 12:39:03[0m] Step: 8296, Training Logs: loss_final: 0.884478, loss_mean: 0.840007, loss_mean_cls: 0.044471, grad_norm: 0.237648
[[34m2025-10-04 12:39:03[0m] Step: 8297, Training Logs: loss_final: 0.872365, loss_mean: 0.829898, loss_mean_cls: 0.042467, grad_norm: 0.275690
[[34m2025-10-04 12:39:03[0m] Step: 8298, Training Logs: loss_final: 0.889533, loss_mean: 0.845893, loss_mean_cls: 0.043640, grad_norm: 0.245018
[[34m2025-10-04 12:39:03[0m] Step: 8299, Training Logs: loss_final: 0.864132, loss_mean: 0.820009, loss_mean_cls: 0.044123, grad_norm: 0.354234
[[34m2025-10-04 12:39:04[0m] Step: 8300, Training Logs: loss_final: 0.883269, loss_mean: 0.841230, loss_mean_cls: 0.042040, grad_norm: 0.153803
[[34m2025-10-04 12:39:04[0m] Step: 8301, Training Logs: loss_final: 0.875364, loss_mean: 0.832158, loss_mean_cls: 0.043206, grad_norm: 0.178340
[[34m2025-10-04 12:39:04[0m] Step: 8302, Training Logs: loss_final: 0.892171, loss_mean: 0.848981, loss_mean_cls: 0.043190, grad_norm: 0.240726
[[34m2025-10-04 12:39:05[0m] Step: 8303, Training Logs: loss_final: 0.862688, loss_mean: 0.820145, loss_mean_cls: 0.042542, grad_norm: 0.249790
[[34m2025-10-04 12:39:05[0m] Step: 8304, Training Logs: loss_final: 0.880598, loss_mean: 0.838379, loss_mean_cls: 0.042219, grad_norm: 0.196260
[[34m2025-10-04 12:39:05[0m] Step: 8305, Training Logs: loss_final: 0.862160, loss_mean: 0.819405, loss_mean_cls: 0.042754, grad_norm: 0.185793
[[34m2025-10-04 12:39:06[0m] Step: 8306, Training Logs: loss_final: 0.888980, loss_mean: 0.845174, loss_mean_cls: 0.043807, grad_norm: 0.214850
[[34m2025-10-04 12:39:06[0m] Step: 8307, Training Logs: loss_final: 0.888993, loss_mean: 0.846632, loss_mean_cls: 0.042360, grad_norm: 0.205018
[[34m2025-10-04 12:39:06[0m] Step: 8308, Training Logs: loss_final: 0.867188, loss_mean: 0.824258, loss_mean_cls: 0.042930, grad_norm: 0.288912
[[34m2025-10-04 12:39:06[0m] Step: 8309, Training Logs: loss_final: 0.872055, loss_mean: 0.828889, loss_mean_cls: 0.043167, grad_norm: 0.378580
[[34m2025-10-04 12:39:07[0m] Step: 8310, Training Logs: loss_final: 0.867002, loss_mean: 0.824724, loss_mean_cls: 0.042278, grad_norm: 0.460644
[[34m2025-10-04 12:39:07[0m] Step: 8311, Training Logs: loss_final: 0.885190, loss_mean: 0.842082, loss_mean_cls: 0.043108, grad_norm: 0.242033
[[34m2025-10-04 12:39:07[0m] Step: 8312, Training Logs: loss_final: 0.871533, loss_mean: 0.828335, loss_mean_cls: 0.043199, grad_norm: 0.394144
[[34m2025-10-04 12:39:08[0m] Step: 8313, Training Logs: loss_final: 0.861492, loss_mean: 0.817361, loss_mean_cls: 0.044131, grad_norm: 0.294660
[[34m2025-10-04 12:39:08[0m] Step: 8314, Training Logs: loss_final: 0.875534, loss_mean: 0.833425, loss_mean_cls: 0.042110, grad_norm: 0.511052
[[34m2025-10-04 12:39:08[0m] Step: 8315, Training Logs: loss_final: 0.882000, loss_mean: 0.838483, loss_mean_cls: 0.043517, grad_norm: 0.290911
[[34m2025-10-04 12:39:08[0m] Step: 8316, Training Logs: loss_final: 0.883469, loss_mean: 0.840074, loss_mean_cls: 0.043394, grad_norm: 0.444789
[[34m2025-10-04 12:39:09[0m] Step: 8317, Training Logs: loss_final: 0.896215, loss_mean: 0.852627, loss_mean_cls: 0.043588, grad_norm: 0.308475
[[34m2025-10-04 12:39:09[0m] Step: 8318, Training Logs: loss_final: 0.881868, loss_mean: 0.838496, loss_mean_cls: 0.043372, grad_norm: 0.391345
[[34m2025-10-04 12:39:09[0m] Step: 8319, Training Logs: loss_final: 0.865299, loss_mean: 0.820881, loss_mean_cls: 0.044418, grad_norm: 0.248689
[[34m2025-10-04 12:39:10[0m] Step: 8320, Training Logs: loss_final: 0.862898, loss_mean: 0.819015, loss_mean_cls: 0.043883, grad_norm: 0.365560
[[34m2025-10-04 12:39:10[0m] Step: 8321, Training Logs: loss_final: 0.872586, loss_mean: 0.828789, loss_mean_cls: 0.043797, grad_norm: 0.279820
[[34m2025-10-04 12:39:10[0m] Step: 8322, Training Logs: loss_final: 0.871691, loss_mean: 0.827725, loss_mean_cls: 0.043966, grad_norm: 0.435418
[[34m2025-10-04 12:39:10[0m] Step: 8323, Training Logs: loss_final: 0.873569, loss_mean: 0.830157, loss_mean_cls: 0.043412, grad_norm: 0.405281
[[34m2025-10-04 12:39:11[0m] Step: 8324, Training Logs: loss_final: 0.856553, loss_mean: 0.812146, loss_mean_cls: 0.044407, grad_norm: 0.305871
[[34m2025-10-04 12:39:11[0m] Step: 8325, Training Logs: loss_final: 0.878150, loss_mean: 0.836189, loss_mean_cls: 0.041961, grad_norm: 0.437094
[[34m2025-10-04 12:39:11[0m] Step: 8326, Training Logs: loss_final: 0.865451, loss_mean: 0.822202, loss_mean_cls: 0.043249, grad_norm: 0.227055
[[34m2025-10-04 12:39:12[0m] Step: 8327, Training Logs: loss_final: 0.868279, loss_mean: 0.823935, loss_mean_cls: 0.044344, grad_norm: 0.336433
[[34m2025-10-04 12:39:12[0m] Step: 8328, Training Logs: loss_final: 0.868318, loss_mean: 0.823587, loss_mean_cls: 0.044732, grad_norm: 0.245891
[[34m2025-10-04 12:39:12[0m] Step: 8329, Training Logs: loss_final: 0.866589, loss_mean: 0.823606, loss_mean_cls: 0.042983, grad_norm: 0.289348
[[34m2025-10-04 12:39:13[0m] Step: 8330, Training Logs: loss_final: 0.905613, loss_mean: 0.862387, loss_mean_cls: 0.043226, grad_norm: 0.242968
[[34m2025-10-04 12:39:13[0m] Step: 8331, Training Logs: loss_final: 0.879618, loss_mean: 0.835360, loss_mean_cls: 0.044258, grad_norm: 0.277233
[[34m2025-10-04 12:39:13[0m] Step: 8332, Training Logs: loss_final: 0.879306, loss_mean: 0.835980, loss_mean_cls: 0.043326, grad_norm: 0.190322
[[34m2025-10-04 12:39:13[0m] Step: 8333, Training Logs: loss_final: 0.862034, loss_mean: 0.818163, loss_mean_cls: 0.043871, grad_norm: 0.247974
[[34m2025-10-04 12:39:14[0m] Step: 8334, Training Logs: loss_final: 0.877505, loss_mean: 0.834263, loss_mean_cls: 0.043242, grad_norm: 0.203088
[[34m2025-10-04 12:39:14[0m] Step: 8335, Training Logs: loss_final: 0.863141, loss_mean: 0.820011, loss_mean_cls: 0.043130, grad_norm: 0.290304
[[34m2025-10-04 12:39:14[0m] Step: 8336, Training Logs: loss_final: 0.887133, loss_mean: 0.843641, loss_mean_cls: 0.043492, grad_norm: 0.247840
[[34m2025-10-04 12:39:15[0m] Step: 8337, Training Logs: loss_final: 0.875564, loss_mean: 0.832081, loss_mean_cls: 0.043484, grad_norm: 0.245470
[[34m2025-10-04 12:39:15[0m] Step: 8338, Training Logs: loss_final: 0.876377, loss_mean: 0.832476, loss_mean_cls: 0.043900, grad_norm: 0.208127
[[34m2025-10-04 12:39:15[0m] Step: 8339, Training Logs: loss_final: 0.879794, loss_mean: 0.835955, loss_mean_cls: 0.043839, grad_norm: 0.257113
[[34m2025-10-04 12:39:15[0m] Step: 8340, Training Logs: loss_final: 0.861918, loss_mean: 0.818255, loss_mean_cls: 0.043663, grad_norm: 0.270597
[[34m2025-10-04 12:39:16[0m] Step: 8341, Training Logs: loss_final: 0.870278, loss_mean: 0.826660, loss_mean_cls: 0.043618, grad_norm: 0.203482
[[34m2025-10-04 12:39:16[0m] Step: 8342, Training Logs: loss_final: 0.854665, loss_mean: 0.809893, loss_mean_cls: 0.044771, grad_norm: 0.307384
[[34m2025-10-04 12:39:16[0m] Step: 8343, Training Logs: loss_final: 0.870710, loss_mean: 0.826370, loss_mean_cls: 0.044340, grad_norm: 0.324506
[[34m2025-10-04 12:39:17[0m] Step: 8344, Training Logs: loss_final: 0.880190, loss_mean: 0.836786, loss_mean_cls: 0.043404, grad_norm: 0.195565
[[34m2025-10-04 12:39:17[0m] Step: 8345, Training Logs: loss_final: 0.873993, loss_mean: 0.829717, loss_mean_cls: 0.044276, grad_norm: 0.384577
[[34m2025-10-04 12:39:17[0m] Step: 8346, Training Logs: loss_final: 0.867690, loss_mean: 0.824777, loss_mean_cls: 0.042913, grad_norm: 0.170147
[[34m2025-10-04 12:39:18[0m] Step: 8347, Training Logs: loss_final: 0.875043, loss_mean: 0.832433, loss_mean_cls: 0.042610, grad_norm: 0.377412
[[34m2025-10-04 12:39:18[0m] Step: 8348, Training Logs: loss_final: 0.886657, loss_mean: 0.844080, loss_mean_cls: 0.042577, grad_norm: 0.209339
[[34m2025-10-04 12:39:18[0m] Step: 8349, Training Logs: loss_final: 0.871600, loss_mean: 0.829085, loss_mean_cls: 0.042516, grad_norm: 0.239254
[[34m2025-10-04 12:39:19[0m] Step: 8350, Training Logs: loss_final: 0.880366, loss_mean: 0.837094, loss_mean_cls: 0.043273, grad_norm: 0.238556
[[34m2025-10-04 12:39:19[0m] Step: 8351, Training Logs: loss_final: 0.871649, loss_mean: 0.827960, loss_mean_cls: 0.043688, grad_norm: 0.271372
[[34m2025-10-04 12:39:19[0m] Step: 8352, Training Logs: loss_final: 0.859328, loss_mean: 0.816323, loss_mean_cls: 0.043005, grad_norm: 0.294074
[[34m2025-10-04 12:39:19[0m] Step: 8353, Training Logs: loss_final: 0.862808, loss_mean: 0.819859, loss_mean_cls: 0.042948, grad_norm: 0.229988
[[34m2025-10-04 12:39:20[0m] Step: 8354, Training Logs: loss_final: 0.882310, loss_mean: 0.840670, loss_mean_cls: 0.041640, grad_norm: 0.259981
[[34m2025-10-04 12:39:20[0m] Step: 8355, Training Logs: loss_final: 0.860942, loss_mean: 0.817688, loss_mean_cls: 0.043254, grad_norm: 0.184718
[[34m2025-10-04 12:39:20[0m] Step: 8356, Training Logs: loss_final: 0.848291, loss_mean: 0.805067, loss_mean_cls: 0.043224, grad_norm: 0.322989
[[34m2025-10-04 12:39:21[0m] Step: 8357, Training Logs: loss_final: 0.859870, loss_mean: 0.816065, loss_mean_cls: 0.043805, grad_norm: 0.302476
[[34m2025-10-04 12:39:21[0m] Step: 8358, Training Logs: loss_final: 0.867742, loss_mean: 0.824199, loss_mean_cls: 0.043543, grad_norm: 0.199661
[[34m2025-10-04 12:39:21[0m] Step: 8359, Training Logs: loss_final: 0.865530, loss_mean: 0.822634, loss_mean_cls: 0.042897, grad_norm: 0.202677
[[34m2025-10-04 12:39:22[0m] Step: 8360, Training Logs: loss_final: 0.894798, loss_mean: 0.851207, loss_mean_cls: 0.043591, grad_norm: 0.263023
[[34m2025-10-04 12:39:22[0m] Step: 8361, Training Logs: loss_final: 0.862176, loss_mean: 0.818629, loss_mean_cls: 0.043547, grad_norm: 0.225841
[[34m2025-10-04 12:39:22[0m] Step: 8362, Training Logs: loss_final: 0.870200, loss_mean: 0.826525, loss_mean_cls: 0.043674, grad_norm: 0.188998
[[34m2025-10-04 12:39:22[0m] Step: 8363, Training Logs: loss_final: 0.868642, loss_mean: 0.823955, loss_mean_cls: 0.044687, grad_norm: 0.265579
[[34m2025-10-04 12:39:23[0m] Step: 8364, Training Logs: loss_final: 0.858604, loss_mean: 0.813662, loss_mean_cls: 0.044942, grad_norm: 0.255060
[[34m2025-10-04 12:39:23[0m] Step: 8365, Training Logs: loss_final: 0.890566, loss_mean: 0.846968, loss_mean_cls: 0.043598, grad_norm: 0.258140
[[34m2025-10-04 12:39:23[0m] Step: 8366, Training Logs: loss_final: 0.865604, loss_mean: 0.822155, loss_mean_cls: 0.043449, grad_norm: 0.204704
[[34m2025-10-04 12:39:24[0m] Step: 8367, Training Logs: loss_final: 0.878155, loss_mean: 0.833964, loss_mean_cls: 0.044191, grad_norm: 0.186275
[[34m2025-10-04 12:39:24[0m] Step: 8368, Training Logs: loss_final: 0.898010, loss_mean: 0.854881, loss_mean_cls: 0.043129, grad_norm: 0.232289
[[34m2025-10-04 12:39:24[0m] Step: 8369, Training Logs: loss_final: 0.899516, loss_mean: 0.855809, loss_mean_cls: 0.043706, grad_norm: 0.351740
[[34m2025-10-04 12:39:25[0m] Step: 8370, Training Logs: loss_final: 0.906562, loss_mean: 0.865004, loss_mean_cls: 0.041557, grad_norm: 0.383606
[[34m2025-10-04 12:39:25[0m] Step: 8371, Training Logs: loss_final: 0.885888, loss_mean: 0.843019, loss_mean_cls: 0.042868, grad_norm: 0.269536
[[34m2025-10-04 12:39:25[0m] Step: 8372, Training Logs: loss_final: 0.855973, loss_mean: 0.812061, loss_mean_cls: 0.043912, grad_norm: 0.295735
[[34m2025-10-04 12:39:25[0m] Step: 8373, Training Logs: loss_final: 0.856806, loss_mean: 0.813772, loss_mean_cls: 0.043034, grad_norm: 0.223958
[[34m2025-10-04 12:39:26[0m] Step: 8374, Training Logs: loss_final: 0.869936, loss_mean: 0.825420, loss_mean_cls: 0.044516, grad_norm: 0.286857
[[34m2025-10-04 12:39:26[0m] Step: 8375, Training Logs: loss_final: 0.856839, loss_mean: 0.813064, loss_mean_cls: 0.043774, grad_norm: 0.262042
[[34m2025-10-04 12:39:26[0m] Step: 8376, Training Logs: loss_final: 0.879409, loss_mean: 0.835824, loss_mean_cls: 0.043585, grad_norm: 0.452594
[[34m2025-10-04 12:39:27[0m] Step: 8377, Training Logs: loss_final: 0.885497, loss_mean: 0.841893, loss_mean_cls: 0.043604, grad_norm: 0.314710
[[34m2025-10-04 12:39:27[0m] Step: 8378, Training Logs: loss_final: 0.862771, loss_mean: 0.818526, loss_mean_cls: 0.044245, grad_norm: 0.267324
[[34m2025-10-04 12:39:27[0m] Step: 8379, Training Logs: loss_final: 0.858507, loss_mean: 0.815316, loss_mean_cls: 0.043192, grad_norm: 0.246338
[[34m2025-10-04 12:39:28[0m] Step: 8380, Training Logs: loss_final: 0.915131, loss_mean: 0.872272, loss_mean_cls: 0.042859, grad_norm: 0.235945
[[34m2025-10-04 12:39:28[0m] Step: 8381, Training Logs: loss_final: 0.899246, loss_mean: 0.856937, loss_mean_cls: 0.042309, grad_norm: 0.239512
[[34m2025-10-04 12:39:28[0m] Step: 8382, Training Logs: loss_final: 0.874277, loss_mean: 0.830082, loss_mean_cls: 0.044195, grad_norm: 0.244885
[[34m2025-10-04 12:39:28[0m] Step: 8383, Training Logs: loss_final: 0.900076, loss_mean: 0.857670, loss_mean_cls: 0.042405, grad_norm: 0.218048
[[34m2025-10-04 12:39:29[0m] Step: 8384, Training Logs: loss_final: 0.854472, loss_mean: 0.810085, loss_mean_cls: 0.044387, grad_norm: 0.260143
[[34m2025-10-04 12:39:29[0m] Step: 8385, Training Logs: loss_final: 0.888776, loss_mean: 0.845653, loss_mean_cls: 0.043124, grad_norm: 0.341416
[[34m2025-10-04 12:39:29[0m] Step: 8386, Training Logs: loss_final: 0.873580, loss_mean: 0.830058, loss_mean_cls: 0.043522, grad_norm: 0.310122
[[34m2025-10-04 12:39:30[0m] Step: 8387, Training Logs: loss_final: 0.876678, loss_mean: 0.832740, loss_mean_cls: 0.043938, grad_norm: 0.236646
[[34m2025-10-04 12:39:30[0m] Step: 8388, Training Logs: loss_final: 0.874250, loss_mean: 0.830674, loss_mean_cls: 0.043575, grad_norm: 0.329277
[[34m2025-10-04 12:39:30[0m] Step: 8389, Training Logs: loss_final: 0.875281, loss_mean: 0.831907, loss_mean_cls: 0.043374, grad_norm: 0.315661
[[34m2025-10-04 12:39:30[0m] Step: 8390, Training Logs: loss_final: 0.880408, loss_mean: 0.836556, loss_mean_cls: 0.043852, grad_norm: 0.228612
[[34m2025-10-04 12:39:31[0m] Step: 8391, Training Logs: loss_final: 0.873296, loss_mean: 0.829850, loss_mean_cls: 0.043446, grad_norm: 0.253046
[[34m2025-10-04 12:39:31[0m] Step: 8392, Training Logs: loss_final: 0.878554, loss_mean: 0.834124, loss_mean_cls: 0.044430, grad_norm: 0.247015
[[34m2025-10-04 12:39:31[0m] Step: 8393, Training Logs: loss_final: 0.871291, loss_mean: 0.827998, loss_mean_cls: 0.043293, grad_norm: 0.268822
[[34m2025-10-04 12:39:32[0m] Step: 8394, Training Logs: loss_final: 0.871046, loss_mean: 0.827814, loss_mean_cls: 0.043232, grad_norm: 0.325241
[[34m2025-10-04 12:39:32[0m] Step: 8395, Training Logs: loss_final: 0.869124, loss_mean: 0.825216, loss_mean_cls: 0.043908, grad_norm: 0.193442
[[34m2025-10-04 12:39:32[0m] Step: 8396, Training Logs: loss_final: 0.872889, loss_mean: 0.829091, loss_mean_cls: 0.043798, grad_norm: 0.316964
[[34m2025-10-04 12:39:33[0m] Step: 8397, Training Logs: loss_final: 0.892090, loss_mean: 0.849838, loss_mean_cls: 0.042252, grad_norm: 0.273574
[[34m2025-10-04 12:39:33[0m] Step: 8398, Training Logs: loss_final: 0.866024, loss_mean: 0.823357, loss_mean_cls: 0.042667, grad_norm: 0.236579
[[34m2025-10-04 12:39:33[0m] Step: 8399, Training Logs: loss_final: 0.885621, loss_mean: 0.842944, loss_mean_cls: 0.042677, grad_norm: 0.253015
[[34m2025-10-04 12:39:33[0m] Step: 8400, Training Logs: loss_final: 0.862718, loss_mean: 0.819145, loss_mean_cls: 0.043574, grad_norm: 0.249809
[[34m2025-10-04 12:39:34[0m] Step: 8401, Training Logs: loss_final: 0.895675, loss_mean: 0.852263, loss_mean_cls: 0.043413, grad_norm: 0.265189
[[34m2025-10-04 12:39:34[0m] Step: 8402, Training Logs: loss_final: 0.856838, loss_mean: 0.812774, loss_mean_cls: 0.044064, grad_norm: 0.280099
[[34m2025-10-04 12:39:34[0m] Step: 8403, Training Logs: loss_final: 0.872240, loss_mean: 0.829440, loss_mean_cls: 0.042800, grad_norm: 0.167558
[[34m2025-10-04 12:39:35[0m] Step: 8404, Training Logs: loss_final: 0.875985, loss_mean: 0.832380, loss_mean_cls: 0.043605, grad_norm: 0.298945
[[34m2025-10-04 12:39:35[0m] Step: 8405, Training Logs: loss_final: 0.858274, loss_mean: 0.814467, loss_mean_cls: 0.043807, grad_norm: 0.342247
[[34m2025-10-04 12:39:35[0m] Step: 8406, Training Logs: loss_final: 0.875422, loss_mean: 0.832332, loss_mean_cls: 0.043090, grad_norm: 0.176813
[[34m2025-10-04 12:39:35[0m] Step: 8407, Training Logs: loss_final: 0.878722, loss_mean: 0.835524, loss_mean_cls: 0.043198, grad_norm: 0.309989
[[34m2025-10-04 12:39:36[0m] Step: 8408, Training Logs: loss_final: 0.880265, loss_mean: 0.836996, loss_mean_cls: 0.043269, grad_norm: 0.212299
[[34m2025-10-04 12:39:36[0m] Step: 8409, Training Logs: loss_final: 0.881421, loss_mean: 0.839164, loss_mean_cls: 0.042257, grad_norm: 0.244480
[[34m2025-10-04 12:39:36[0m] Step: 8410, Training Logs: loss_final: 0.880859, loss_mean: 0.837704, loss_mean_cls: 0.043155, grad_norm: 0.227267
[[34m2025-10-04 12:39:37[0m] Step: 8411, Training Logs: loss_final: 0.860504, loss_mean: 0.817133, loss_mean_cls: 0.043370, grad_norm: 0.304810
[[34m2025-10-04 12:39:37[0m] Step: 8412, Training Logs: loss_final: 0.880364, loss_mean: 0.837029, loss_mean_cls: 0.043335, grad_norm: 0.283982
[[34m2025-10-04 12:39:37[0m] Step: 8413, Training Logs: loss_final: 0.873596, loss_mean: 0.829697, loss_mean_cls: 0.043899, grad_norm: 0.269734
[[34m2025-10-04 12:39:37[0m] Step: 8414, Training Logs: loss_final: 0.881711, loss_mean: 0.839430, loss_mean_cls: 0.042282, grad_norm: 0.229977
[[34m2025-10-04 12:39:38[0m] Step: 8415, Training Logs: loss_final: 0.883909, loss_mean: 0.840300, loss_mean_cls: 0.043608, grad_norm: 0.360714
[[34m2025-10-04 12:39:38[0m] Step: 8416, Training Logs: loss_final: 0.872699, loss_mean: 0.829028, loss_mean_cls: 0.043670, grad_norm: 0.227331
[[34m2025-10-04 12:39:38[0m] Step: 8417, Training Logs: loss_final: 0.863377, loss_mean: 0.819326, loss_mean_cls: 0.044052, grad_norm: 0.252785
[[34m2025-10-04 12:39:39[0m] Step: 8418, Training Logs: loss_final: 0.880777, loss_mean: 0.839019, loss_mean_cls: 0.041757, grad_norm: 0.160545
[[34m2025-10-04 12:39:39[0m] Step: 8419, Training Logs: loss_final: 0.860328, loss_mean: 0.816031, loss_mean_cls: 0.044297, grad_norm: 0.272389
[[34m2025-10-04 12:39:39[0m] Step: 8420, Training Logs: loss_final: 0.892925, loss_mean: 0.850746, loss_mean_cls: 0.042179, grad_norm: 0.275759
[[34m2025-10-04 12:39:40[0m] Step: 8421, Training Logs: loss_final: 0.865603, loss_mean: 0.822973, loss_mean_cls: 0.042630, grad_norm: 0.277095
[[34m2025-10-04 12:39:40[0m] Step: 8422, Training Logs: loss_final: 0.889657, loss_mean: 0.846325, loss_mean_cls: 0.043332, grad_norm: 0.278096
[[34m2025-10-04 12:39:40[0m] Step: 8423, Training Logs: loss_final: 0.879147, loss_mean: 0.836477, loss_mean_cls: 0.042670, grad_norm: 0.416098
[[34m2025-10-04 12:39:40[0m] Step: 8424, Training Logs: loss_final: 0.846331, loss_mean: 0.801990, loss_mean_cls: 0.044341, grad_norm: 0.194519
[[34m2025-10-04 12:39:41[0m] Step: 8425, Training Logs: loss_final: 0.862993, loss_mean: 0.820304, loss_mean_cls: 0.042689, grad_norm: 0.301677
[[34m2025-10-04 12:39:41[0m] Step: 8426, Training Logs: loss_final: 0.880066, loss_mean: 0.836684, loss_mean_cls: 0.043382, grad_norm: 0.227468
[[34m2025-10-04 12:39:41[0m] Step: 8427, Training Logs: loss_final: 0.884542, loss_mean: 0.842992, loss_mean_cls: 0.041549, grad_norm: 0.221912
[[34m2025-10-04 12:39:42[0m] Step: 8428, Training Logs: loss_final: 0.879617, loss_mean: 0.837610, loss_mean_cls: 0.042007, grad_norm: 0.204824
[[34m2025-10-04 12:39:42[0m] Step: 8429, Training Logs: loss_final: 0.891740, loss_mean: 0.848520, loss_mean_cls: 0.043220, grad_norm: 0.240493
[[34m2025-10-04 12:39:42[0m] Step: 8430, Training Logs: loss_final: 0.869395, loss_mean: 0.825424, loss_mean_cls: 0.043971, grad_norm: 0.261994
[[34m2025-10-04 12:39:43[0m] Step: 8431, Training Logs: loss_final: 0.863047, loss_mean: 0.820896, loss_mean_cls: 0.042151, grad_norm: 0.209754
[[34m2025-10-04 12:39:43[0m] Step: 8432, Training Logs: loss_final: 0.883952, loss_mean: 0.840755, loss_mean_cls: 0.043198, grad_norm: 0.316028
[[34m2025-10-04 12:39:43[0m] Step: 8433, Training Logs: loss_final: 0.874716, loss_mean: 0.831339, loss_mean_cls: 0.043376, grad_norm: 0.181625
[[34m2025-10-04 12:39:43[0m] Step: 8434, Training Logs: loss_final: 0.880542, loss_mean: 0.837972, loss_mean_cls: 0.042569, grad_norm: 0.222085
[[34m2025-10-04 12:39:44[0m] Step: 8435, Training Logs: loss_final: 0.867394, loss_mean: 0.824426, loss_mean_cls: 0.042969, grad_norm: 0.270140
[[34m2025-10-04 12:39:44[0m] Step: 8436, Training Logs: loss_final: 0.887877, loss_mean: 0.844917, loss_mean_cls: 0.042961, grad_norm: 0.290497
[[34m2025-10-04 12:39:44[0m] Step: 8437, Training Logs: loss_final: 0.859342, loss_mean: 0.814995, loss_mean_cls: 0.044347, grad_norm: 0.287771
[[34m2025-10-04 12:39:45[0m] Step: 8438, Training Logs: loss_final: 0.863125, loss_mean: 0.819319, loss_mean_cls: 0.043806, grad_norm: 0.314080
[[34m2025-10-04 12:39:45[0m] Step: 8439, Training Logs: loss_final: 0.882446, loss_mean: 0.839251, loss_mean_cls: 0.043195, grad_norm: 0.317030
[[34m2025-10-04 12:39:45[0m] Step: 8440, Training Logs: loss_final: 0.876102, loss_mean: 0.834386, loss_mean_cls: 0.041716, grad_norm: 0.196392
[[34m2025-10-04 12:39:45[0m] Step: 8441, Training Logs: loss_final: 0.888773, loss_mean: 0.844563, loss_mean_cls: 0.044210, grad_norm: 0.426838
[[34m2025-10-04 12:39:46[0m] Step: 8442, Training Logs: loss_final: 0.880720, loss_mean: 0.839044, loss_mean_cls: 0.041676, grad_norm: 0.176066
[[34m2025-10-04 12:39:46[0m] Step: 8443, Training Logs: loss_final: 0.864082, loss_mean: 0.820121, loss_mean_cls: 0.043961, grad_norm: 0.299009
[[34m2025-10-04 12:39:46[0m] Step: 8444, Training Logs: loss_final: 0.870775, loss_mean: 0.827121, loss_mean_cls: 0.043654, grad_norm: 0.342393
[[34m2025-10-04 12:39:47[0m] Step: 8445, Training Logs: loss_final: 0.882366, loss_mean: 0.839272, loss_mean_cls: 0.043093, grad_norm: 0.478292
[[34m2025-10-04 12:39:47[0m] Step: 8446, Training Logs: loss_final: 0.852197, loss_mean: 0.809551, loss_mean_cls: 0.042646, grad_norm: 0.280079
[[34m2025-10-04 12:39:47[0m] Step: 8447, Training Logs: loss_final: 0.880693, loss_mean: 0.836695, loss_mean_cls: 0.043998, grad_norm: 0.400896
[[34m2025-10-04 12:39:48[0m] Step: 8448, Training Logs: loss_final: 0.863855, loss_mean: 0.821250, loss_mean_cls: 0.042605, grad_norm: 0.312629
[[34m2025-10-04 12:39:48[0m] Step: 8449, Training Logs: loss_final: 0.876381, loss_mean: 0.833357, loss_mean_cls: 0.043023, grad_norm: 0.317999
[[34m2025-10-04 12:39:48[0m] Step: 8450, Training Logs: loss_final: 0.885476, loss_mean: 0.842465, loss_mean_cls: 0.043011, grad_norm: 0.395074
[[34m2025-10-04 12:39:48[0m] Step: 8451, Training Logs: loss_final: 0.878696, loss_mean: 0.835565, loss_mean_cls: 0.043131, grad_norm: 0.242048
[[34m2025-10-04 12:39:49[0m] Step: 8452, Training Logs: loss_final: 0.874485, loss_mean: 0.831448, loss_mean_cls: 0.043038, grad_norm: 0.295588
[[34m2025-10-04 12:39:49[0m] Step: 8453, Training Logs: loss_final: 0.888016, loss_mean: 0.844473, loss_mean_cls: 0.043544, grad_norm: 0.238086
[[34m2025-10-04 12:39:49[0m] Step: 8454, Training Logs: loss_final: 0.895768, loss_mean: 0.852183, loss_mean_cls: 0.043585, grad_norm: 0.271779
[[34m2025-10-04 12:39:50[0m] Step: 8455, Training Logs: loss_final: 0.886230, loss_mean: 0.842857, loss_mean_cls: 0.043373, grad_norm: 0.303783
[[34m2025-10-04 12:39:50[0m] Step: 8456, Training Logs: loss_final: 0.864043, loss_mean: 0.820279, loss_mean_cls: 0.043764, grad_norm: 0.261528
[[34m2025-10-04 12:39:50[0m] Step: 8457, Training Logs: loss_final: 0.885573, loss_mean: 0.842726, loss_mean_cls: 0.042847, grad_norm: 0.287541
[[34m2025-10-04 12:39:51[0m] Step: 8458, Training Logs: loss_final: 0.866952, loss_mean: 0.822672, loss_mean_cls: 0.044280, grad_norm: 0.282764
[[34m2025-10-04 12:39:51[0m] Step: 8459, Training Logs: loss_final: 0.887427, loss_mean: 0.844230, loss_mean_cls: 0.043196, grad_norm: 0.211006
[[34m2025-10-04 12:39:51[0m] Step: 8460, Training Logs: loss_final: 0.883685, loss_mean: 0.841352, loss_mean_cls: 0.042333, grad_norm: 0.221725
[[34m2025-10-04 12:39:51[0m] Step: 8461, Training Logs: loss_final: 0.872625, loss_mean: 0.829340, loss_mean_cls: 0.043285, grad_norm: 0.219176
[[34m2025-10-04 12:39:52[0m] Step: 8462, Training Logs: loss_final: 0.880307, loss_mean: 0.837008, loss_mean_cls: 0.043299, grad_norm: 0.244016
[[34m2025-10-04 12:39:52[0m] Step: 8463, Training Logs: loss_final: 0.866308, loss_mean: 0.822763, loss_mean_cls: 0.043545, grad_norm: 0.211101
[[34m2025-10-04 12:39:52[0m] Step: 8464, Training Logs: loss_final: 0.893777, loss_mean: 0.851842, loss_mean_cls: 0.041935, grad_norm: 0.205354
[[34m2025-10-04 12:39:53[0m] Step: 8465, Training Logs: loss_final: 0.869182, loss_mean: 0.825919, loss_mean_cls: 0.043263, grad_norm: 0.224698
[[34m2025-10-04 12:39:53[0m] Step: 8466, Training Logs: loss_final: 0.885780, loss_mean: 0.843851, loss_mean_cls: 0.041928, grad_norm: 0.209008
[[34m2025-10-04 12:39:53[0m] Step: 8467, Training Logs: loss_final: 0.888721, loss_mean: 0.845353, loss_mean_cls: 0.043368, grad_norm: 0.212558
[[34m2025-10-04 12:39:53[0m] Step: 8468, Training Logs: loss_final: 0.873421, loss_mean: 0.831732, loss_mean_cls: 0.041690, grad_norm: 0.211269
[[34m2025-10-04 12:39:54[0m] Step: 8469, Training Logs: loss_final: 0.879866, loss_mean: 0.837045, loss_mean_cls: 0.042821, grad_norm: 0.291297
[[34m2025-10-04 12:39:54[0m] Step: 8470, Training Logs: loss_final: 0.874006, loss_mean: 0.829779, loss_mean_cls: 0.044227, grad_norm: 0.237427
[[34m2025-10-04 12:39:54[0m] Step: 8471, Training Logs: loss_final: 0.886726, loss_mean: 0.842999, loss_mean_cls: 0.043727, grad_norm: 0.272806
[[34m2025-10-04 12:39:55[0m] Step: 8472, Training Logs: loss_final: 0.898179, loss_mean: 0.854305, loss_mean_cls: 0.043874, grad_norm: 0.207225
[[34m2025-10-04 12:39:55[0m] Step: 8473, Training Logs: loss_final: 0.866336, loss_mean: 0.821396, loss_mean_cls: 0.044940, grad_norm: 0.355054
[[34m2025-10-04 12:39:55[0m] Step: 8474, Training Logs: loss_final: 0.885237, loss_mean: 0.842103, loss_mean_cls: 0.043134, grad_norm: 0.293823
[[34m2025-10-04 12:39:56[0m] Step: 8475, Training Logs: loss_final: 0.871996, loss_mean: 0.828502, loss_mean_cls: 0.043494, grad_norm: 0.168032
[[34m2025-10-04 12:39:56[0m] Step: 8476, Training Logs: loss_final: 0.874716, loss_mean: 0.832832, loss_mean_cls: 0.041884, grad_norm: 0.292792
[[34m2025-10-04 12:39:56[0m] Step: 8477, Training Logs: loss_final: 0.867913, loss_mean: 0.824123, loss_mean_cls: 0.043789, grad_norm: 0.195190
[[34m2025-10-04 12:39:56[0m] Step: 8478, Training Logs: loss_final: 0.869038, loss_mean: 0.825385, loss_mean_cls: 0.043653, grad_norm: 0.258531
[[34m2025-10-04 12:39:57[0m] Step: 8479, Training Logs: loss_final: 0.857004, loss_mean: 0.812540, loss_mean_cls: 0.044464, grad_norm: 0.232472
[[34m2025-10-04 12:39:57[0m] Step: 8480, Training Logs: loss_final: 0.876450, loss_mean: 0.833696, loss_mean_cls: 0.042754, grad_norm: 0.211340
[[34m2025-10-04 12:39:57[0m] Step: 8481, Training Logs: loss_final: 0.893249, loss_mean: 0.849710, loss_mean_cls: 0.043540, grad_norm: 0.296045
[[34m2025-10-04 12:39:58[0m] Step: 8482, Training Logs: loss_final: 0.870450, loss_mean: 0.827796, loss_mean_cls: 0.042655, grad_norm: 0.264537
[[34m2025-10-04 12:39:58[0m] Step: 8483, Training Logs: loss_final: 0.873198, loss_mean: 0.829120, loss_mean_cls: 0.044079, grad_norm: 0.306003
[[34m2025-10-04 12:39:58[0m] Step: 8484, Training Logs: loss_final: 0.870646, loss_mean: 0.827745, loss_mean_cls: 0.042900, grad_norm: 0.317001
[[34m2025-10-04 12:39:58[0m] Step: 8485, Training Logs: loss_final: 0.861975, loss_mean: 0.819195, loss_mean_cls: 0.042780, grad_norm: 0.285303
[[34m2025-10-04 12:39:59[0m] Step: 8486, Training Logs: loss_final: 0.873054, loss_mean: 0.828702, loss_mean_cls: 0.044352, grad_norm: inf
[[34m2025-10-04 12:39:59[0m] Step: 8487, Training Logs: loss_final: 0.870387, loss_mean: 0.827373, loss_mean_cls: 0.043014, grad_norm: 0.368780
[[34m2025-10-04 12:39:59[0m] Step: 8488, Training Logs: loss_final: 0.895769, loss_mean: 0.853612, loss_mean_cls: 0.042157, grad_norm: 0.258422
[[34m2025-10-04 12:40:00[0m] Step: 8489, Training Logs: loss_final: 0.876066, loss_mean: 0.832472, loss_mean_cls: 0.043594, grad_norm: 0.239006
[[34m2025-10-04 12:40:00[0m] Step: 8490, Training Logs: loss_final: 0.874800, loss_mean: 0.830824, loss_mean_cls: 0.043975, grad_norm: 0.319482
[[34m2025-10-04 12:40:00[0m] Step: 8491, Training Logs: loss_final: 0.859515, loss_mean: 0.815858, loss_mean_cls: 0.043657, grad_norm: 0.364335
[[34m2025-10-04 12:40:01[0m] Step: 8492, Training Logs: loss_final: 0.873208, loss_mean: 0.829343, loss_mean_cls: 0.043865, grad_norm: 0.178835
[[34m2025-10-04 12:40:01[0m] Step: 8493, Training Logs: loss_final: 0.854783, loss_mean: 0.811544, loss_mean_cls: 0.043239, grad_norm: 0.317660
[[34m2025-10-04 12:40:01[0m] Step: 8494, Training Logs: loss_final: 0.864529, loss_mean: 0.820363, loss_mean_cls: 0.044167, grad_norm: 0.319097
[[34m2025-10-04 12:40:01[0m] Step: 8495, Training Logs: loss_final: 0.862715, loss_mean: 0.819307, loss_mean_cls: 0.043409, grad_norm: 0.397671
[[34m2025-10-04 12:40:02[0m] Step: 8496, Training Logs: loss_final: 0.882819, loss_mean: 0.839034, loss_mean_cls: 0.043785, grad_norm: 0.398587
[[34m2025-10-04 12:40:02[0m] Step: 8497, Training Logs: loss_final: 0.878614, loss_mean: 0.836558, loss_mean_cls: 0.042057, grad_norm: 0.405328
[[34m2025-10-04 12:40:02[0m] Step: 8498, Training Logs: loss_final: 0.862297, loss_mean: 0.818177, loss_mean_cls: 0.044120, grad_norm: 0.618839
[[34m2025-10-04 12:40:03[0m] Step: 8499, Training Logs: loss_final: 0.881654, loss_mean: 0.837864, loss_mean_cls: 0.043790, grad_norm: 0.381121
[[34m2025-10-04 12:40:03[0m] Step: 8500, Training Logs: loss_final: 0.878875, loss_mean: 0.836376, loss_mean_cls: 0.042499, grad_norm: 0.615231
[[34m2025-10-04 12:40:03[0m] Step: 8501, Training Logs: loss_final: 0.878366, loss_mean: 0.835695, loss_mean_cls: 0.042671, grad_norm: 0.171536
[[34m2025-10-04 12:40:04[0m] Step: 8502, Training Logs: loss_final: 0.883135, loss_mean: 0.838975, loss_mean_cls: 0.044160, grad_norm: 0.339411
[[34m2025-10-04 12:40:04[0m] Step: 8503, Training Logs: loss_final: 0.863480, loss_mean: 0.820088, loss_mean_cls: 0.043392, grad_norm: 0.310799
[[34m2025-10-04 12:40:04[0m] Step: 8504, Training Logs: loss_final: 0.855118, loss_mean: 0.811307, loss_mean_cls: 0.043811, grad_norm: 0.417974
[[34m2025-10-04 12:40:04[0m] Step: 8505, Training Logs: loss_final: 0.877774, loss_mean: 0.833282, loss_mean_cls: 0.044492, grad_norm: 0.292183
[[34m2025-10-04 12:40:05[0m] Step: 8506, Training Logs: loss_final: 0.870829, loss_mean: 0.828063, loss_mean_cls: 0.042767, grad_norm: 0.450837
[[34m2025-10-04 12:40:05[0m] Step: 8507, Training Logs: loss_final: 0.895842, loss_mean: 0.853342, loss_mean_cls: 0.042500, grad_norm: 0.304589
[[34m2025-10-04 12:40:05[0m] Step: 8508, Training Logs: loss_final: 0.888262, loss_mean: 0.844728, loss_mean_cls: 0.043534, grad_norm: 0.514900
[[34m2025-10-04 12:40:06[0m] Step: 8509, Training Logs: loss_final: 0.865562, loss_mean: 0.823406, loss_mean_cls: 0.042156, grad_norm: 0.255102
[[34m2025-10-04 12:40:06[0m] Step: 8510, Training Logs: loss_final: 0.862841, loss_mean: 0.818993, loss_mean_cls: 0.043849, grad_norm: 0.621429
[[34m2025-10-04 12:40:06[0m] Step: 8511, Training Logs: loss_final: 0.865353, loss_mean: 0.821811, loss_mean_cls: 0.043543, grad_norm: 0.170296
[[34m2025-10-04 12:40:07[0m] Step: 8512, Training Logs: loss_final: 0.882420, loss_mean: 0.839213, loss_mean_cls: 0.043207, grad_norm: 0.486568
[[34m2025-10-04 12:40:07[0m] Step: 8513, Training Logs: loss_final: 0.878858, loss_mean: 0.835403, loss_mean_cls: 0.043455, grad_norm: 0.190389
[[34m2025-10-04 12:40:07[0m] Step: 8514, Training Logs: loss_final: 0.866894, loss_mean: 0.823462, loss_mean_cls: 0.043432, grad_norm: 0.321000
[[34m2025-10-04 12:40:08[0m] Step: 8515, Training Logs: loss_final: 0.871493, loss_mean: 0.828019, loss_mean_cls: 0.043474, grad_norm: 0.265213
[[34m2025-10-04 12:40:08[0m] Step: 8516, Training Logs: loss_final: 0.872943, loss_mean: 0.829055, loss_mean_cls: 0.043888, grad_norm: 0.356500
[[34m2025-10-04 12:40:08[0m] Step: 8517, Training Logs: loss_final: 0.855923, loss_mean: 0.813809, loss_mean_cls: 0.042114, grad_norm: 0.301196
[[34m2025-10-04 12:40:08[0m] Step: 8518, Training Logs: loss_final: 0.882617, loss_mean: 0.838043, loss_mean_cls: 0.044574, grad_norm: 0.336755
[[34m2025-10-04 12:40:09[0m] Step: 8519, Training Logs: loss_final: 0.876346, loss_mean: 0.832380, loss_mean_cls: 0.043966, grad_norm: 0.312625
[[34m2025-10-04 12:40:09[0m] Step: 8520, Training Logs: loss_final: 0.874972, loss_mean: 0.831619, loss_mean_cls: 0.043352, grad_norm: 0.240298
[[34m2025-10-04 12:40:09[0m] Step: 8521, Training Logs: loss_final: 0.884771, loss_mean: 0.841579, loss_mean_cls: 0.043192, grad_norm: 0.378486
[[34m2025-10-04 12:40:10[0m] Step: 8522, Training Logs: loss_final: 0.874647, loss_mean: 0.831427, loss_mean_cls: 0.043220, grad_norm: 0.283128
[[34m2025-10-04 12:40:10[0m] Step: 8523, Training Logs: loss_final: 0.867185, loss_mean: 0.824753, loss_mean_cls: 0.042432, grad_norm: 0.255327
[[34m2025-10-04 12:40:10[0m] Step: 8524, Training Logs: loss_final: 0.865705, loss_mean: 0.822729, loss_mean_cls: 0.042976, grad_norm: 0.221094
[[34m2025-10-04 12:40:10[0m] Step: 8525, Training Logs: loss_final: 0.866092, loss_mean: 0.823004, loss_mean_cls: 0.043088, grad_norm: 0.354355
[[34m2025-10-04 12:40:11[0m] Step: 8526, Training Logs: loss_final: 0.836870, loss_mean: 0.791248, loss_mean_cls: 0.045622, grad_norm: 0.384390
[[34m2025-10-04 12:40:11[0m] Step: 8527, Training Logs: loss_final: 0.857277, loss_mean: 0.813377, loss_mean_cls: 0.043900, grad_norm: 0.199143
[[34m2025-10-04 12:40:11[0m] Step: 8528, Training Logs: loss_final: 0.881080, loss_mean: 0.838275, loss_mean_cls: 0.042805, grad_norm: 0.350963
[[34m2025-10-04 12:40:12[0m] Step: 8529, Training Logs: loss_final: 0.854909, loss_mean: 0.812187, loss_mean_cls: 0.042722, grad_norm: 0.226144
[[34m2025-10-04 12:40:12[0m] Step: 8530, Training Logs: loss_final: 0.857462, loss_mean: 0.813385, loss_mean_cls: 0.044077, grad_norm: 0.411744
[[34m2025-10-04 12:40:12[0m] Step: 8531, Training Logs: loss_final: 0.859839, loss_mean: 0.816587, loss_mean_cls: 0.043251, grad_norm: 0.341653
[[34m2025-10-04 12:40:12[0m] Step: 8532, Training Logs: loss_final: 0.888303, loss_mean: 0.845242, loss_mean_cls: 0.043061, grad_norm: 0.218422
[[34m2025-10-04 12:40:13[0m] Step: 8533, Training Logs: loss_final: 0.887068, loss_mean: 0.844982, loss_mean_cls: 0.042086, grad_norm: 0.347846
[[34m2025-10-04 12:40:13[0m] Step: 8534, Training Logs: loss_final: 0.873936, loss_mean: 0.830759, loss_mean_cls: 0.043177, grad_norm: 0.332595
[[34m2025-10-04 12:40:13[0m] Step: 8535, Training Logs: loss_final: 0.891086, loss_mean: 0.848429, loss_mean_cls: 0.042657, grad_norm: 0.227978
[[34m2025-10-04 12:40:14[0m] Step: 8536, Training Logs: loss_final: 0.887310, loss_mean: 0.844599, loss_mean_cls: 0.042710, grad_norm: 0.239295
[[34m2025-10-04 12:40:14[0m] Step: 8537, Training Logs: loss_final: 0.879528, loss_mean: 0.837249, loss_mean_cls: 0.042279, grad_norm: 0.322681
[[34m2025-10-04 12:40:14[0m] Step: 8538, Training Logs: loss_final: 0.860516, loss_mean: 0.817437, loss_mean_cls: 0.043080, grad_norm: 0.327150
[[34m2025-10-04 12:40:15[0m] Step: 8539, Training Logs: loss_final: 0.859771, loss_mean: 0.816480, loss_mean_cls: 0.043291, grad_norm: 0.168630
[[34m2025-10-04 12:40:15[0m] Step: 8540, Training Logs: loss_final: 0.892322, loss_mean: 0.847797, loss_mean_cls: 0.044525, grad_norm: 0.263114
[[34m2025-10-04 12:40:15[0m] Step: 8541, Training Logs: loss_final: 0.888689, loss_mean: 0.845307, loss_mean_cls: 0.043382, grad_norm: 0.223632
[[34m2025-10-04 12:40:15[0m] Step: 8542, Training Logs: loss_final: 0.853165, loss_mean: 0.808726, loss_mean_cls: 0.044439, grad_norm: 0.300142
[[34m2025-10-04 12:40:16[0m] Step: 8543, Training Logs: loss_final: 0.881419, loss_mean: 0.838583, loss_mean_cls: 0.042836, grad_norm: 0.316611
[[34m2025-10-04 12:40:16[0m] Step: 8544, Training Logs: loss_final: 0.879268, loss_mean: 0.836391, loss_mean_cls: 0.042877, grad_norm: 0.219525
[[34m2025-10-04 12:40:16[0m] Step: 8545, Training Logs: loss_final: 0.884221, loss_mean: 0.840819, loss_mean_cls: 0.043402, grad_norm: 0.299110
[[34m2025-10-04 12:40:17[0m] Step: 8546, Training Logs: loss_final: 0.867685, loss_mean: 0.824454, loss_mean_cls: 0.043232, grad_norm: 0.216117
[[34m2025-10-04 12:40:17[0m] Step: 8547, Training Logs: loss_final: 0.876323, loss_mean: 0.833636, loss_mean_cls: 0.042687, grad_norm: 0.234764
[[34m2025-10-04 12:40:17[0m] Step: 8548, Training Logs: loss_final: 0.866801, loss_mean: 0.824521, loss_mean_cls: 0.042280, grad_norm: 0.183307
[[34m2025-10-04 12:40:18[0m] Step: 8549, Training Logs: loss_final: 0.892973, loss_mean: 0.849114, loss_mean_cls: 0.043858, grad_norm: 0.250287
[[34m2025-10-04 12:40:18[0m] Step: 8550, Training Logs: loss_final: 0.890855, loss_mean: 0.847136, loss_mean_cls: 0.043719, grad_norm: 0.321969
[[34m2025-10-04 12:40:18[0m] Step: 8551, Training Logs: loss_final: 0.855878, loss_mean: 0.812017, loss_mean_cls: 0.043861, grad_norm: 0.325525
[[34m2025-10-04 12:40:18[0m] Step: 8552, Training Logs: loss_final: 0.854831, loss_mean: 0.811192, loss_mean_cls: 0.043639, grad_norm: 0.256570
[[34m2025-10-04 12:40:19[0m] Step: 8553, Training Logs: loss_final: 0.880855, loss_mean: 0.837521, loss_mean_cls: 0.043334, grad_norm: 0.404649
[[34m2025-10-04 12:40:19[0m] Step: 8554, Training Logs: loss_final: 0.885185, loss_mean: 0.841197, loss_mean_cls: 0.043988, grad_norm: 0.242190
[[34m2025-10-04 12:40:19[0m] Step: 8555, Training Logs: loss_final: 0.884012, loss_mean: 0.841766, loss_mean_cls: 0.042245, grad_norm: 0.313728
[[34m2025-10-04 12:40:20[0m] Step: 8556, Training Logs: loss_final: 0.876693, loss_mean: 0.833881, loss_mean_cls: 0.042811, grad_norm: 0.252638
[[34m2025-10-04 12:40:20[0m] Step: 8557, Training Logs: loss_final: 0.866568, loss_mean: 0.823495, loss_mean_cls: 0.043073, grad_norm: 0.401353
[[34m2025-10-04 12:40:20[0m] Step: 8558, Training Logs: loss_final: 0.873919, loss_mean: 0.831398, loss_mean_cls: 0.042522, grad_norm: 0.232848
[[34m2025-10-04 12:40:21[0m] Step: 8559, Training Logs: loss_final: 0.888336, loss_mean: 0.843889, loss_mean_cls: 0.044447, grad_norm: 0.443496
[[34m2025-10-04 12:40:21[0m] Step: 8560, Training Logs: loss_final: 0.882864, loss_mean: 0.839646, loss_mean_cls: 0.043218, grad_norm: 0.390947
[[34m2025-10-04 12:40:21[0m] Step: 8561, Training Logs: loss_final: 0.858256, loss_mean: 0.814892, loss_mean_cls: 0.043364, grad_norm: 0.340372
[[34m2025-10-04 12:40:21[0m] Step: 8562, Training Logs: loss_final: 0.874987, loss_mean: 0.831370, loss_mean_cls: 0.043618, grad_norm: 0.372482
[[34m2025-10-04 12:40:22[0m] Step: 8563, Training Logs: loss_final: 0.877552, loss_mean: 0.834274, loss_mean_cls: 0.043278, grad_norm: 0.268998
[[34m2025-10-04 12:40:22[0m] Step: 8564, Training Logs: loss_final: 0.872343, loss_mean: 0.829484, loss_mean_cls: 0.042859, grad_norm: 0.316581
[[34m2025-10-04 12:40:22[0m] Step: 8565, Training Logs: loss_final: 0.864334, loss_mean: 0.821462, loss_mean_cls: 0.042872, grad_norm: 0.288590
[[34m2025-10-04 12:40:23[0m] Step: 8566, Training Logs: loss_final: 0.857959, loss_mean: 0.815391, loss_mean_cls: 0.042568, grad_norm: 0.417379
[[34m2025-10-04 12:40:23[0m] Step: 8567, Training Logs: loss_final: 0.889675, loss_mean: 0.848104, loss_mean_cls: 0.041571, grad_norm: 0.225060
[[34m2025-10-04 12:40:23[0m] Step: 8568, Training Logs: loss_final: 0.878224, loss_mean: 0.835074, loss_mean_cls: 0.043150, grad_norm: 0.391657
[[34m2025-10-04 12:40:24[0m] Step: 8569, Training Logs: loss_final: 0.868941, loss_mean: 0.825796, loss_mean_cls: 0.043146, grad_norm: 0.215153
[[34m2025-10-04 12:40:24[0m] Step: 8570, Training Logs: loss_final: 0.876248, loss_mean: 0.832539, loss_mean_cls: 0.043709, grad_norm: 0.322999
[[34m2025-10-04 12:40:24[0m] Step: 8571, Training Logs: loss_final: 0.864911, loss_mean: 0.821716, loss_mean_cls: 0.043194, grad_norm: 0.251869
[[34m2025-10-04 12:40:24[0m] Step: 8572, Training Logs: loss_final: 0.873728, loss_mean: 0.829934, loss_mean_cls: 0.043794, grad_norm: 0.547880
[[34m2025-10-04 12:40:25[0m] Step: 8573, Training Logs: loss_final: 0.875999, loss_mean: 0.833310, loss_mean_cls: 0.042690, grad_norm: 0.239289
[[34m2025-10-04 12:40:25[0m] Step: 8574, Training Logs: loss_final: 0.871626, loss_mean: 0.828013, loss_mean_cls: 0.043613, grad_norm: 0.305126
[[34m2025-10-04 12:40:25[0m] Step: 8575, Training Logs: loss_final: 0.879476, loss_mean: 0.835940, loss_mean_cls: 0.043536, grad_norm: 0.203978
[[34m2025-10-04 12:40:26[0m] Step: 8576, Training Logs: loss_final: 0.865301, loss_mean: 0.820789, loss_mean_cls: 0.044512, grad_norm: 0.331872
[[34m2025-10-04 12:40:26[0m] Step: 8577, Training Logs: loss_final: 0.859864, loss_mean: 0.817545, loss_mean_cls: 0.042319, grad_norm: 0.204206
[[34m2025-10-04 12:40:26[0m] Step: 8578, Training Logs: loss_final: 0.882794, loss_mean: 0.839614, loss_mean_cls: 0.043180, grad_norm: 0.332913
[[34m2025-10-04 12:40:26[0m] Step: 8579, Training Logs: loss_final: 0.873375, loss_mean: 0.830041, loss_mean_cls: 0.043334, grad_norm: 0.210581
[[34m2025-10-04 12:40:27[0m] Step: 8580, Training Logs: loss_final: 0.879277, loss_mean: 0.836271, loss_mean_cls: 0.043006, grad_norm: 0.371961
[[34m2025-10-04 12:40:27[0m] Step: 8581, Training Logs: loss_final: 0.879018, loss_mean: 0.836676, loss_mean_cls: 0.042342, grad_norm: 0.215200
[[34m2025-10-04 12:40:27[0m] Step: 8582, Training Logs: loss_final: 0.875882, loss_mean: 0.833553, loss_mean_cls: 0.042329, grad_norm: 0.370503
[[34m2025-10-04 12:40:28[0m] Step: 8583, Training Logs: loss_final: 0.872474, loss_mean: 0.829972, loss_mean_cls: 0.042503, grad_norm: 0.284109
[[34m2025-10-04 12:40:28[0m] Step: 8584, Training Logs: loss_final: 0.874711, loss_mean: 0.831175, loss_mean_cls: 0.043536, grad_norm: 0.477892
[[34m2025-10-04 12:40:28[0m] Step: 8585, Training Logs: loss_final: 0.867372, loss_mean: 0.824376, loss_mean_cls: 0.042996, grad_norm: 0.408711
[[34m2025-10-04 12:40:28[0m] Step: 8586, Training Logs: loss_final: 0.860319, loss_mean: 0.815829, loss_mean_cls: 0.044490, grad_norm: 0.300011
[[34m2025-10-04 12:40:29[0m] Step: 8587, Training Logs: loss_final: 0.869654, loss_mean: 0.827353, loss_mean_cls: 0.042301, grad_norm: 0.279720
[[34m2025-10-04 12:40:29[0m] Step: 8588, Training Logs: loss_final: 0.886830, loss_mean: 0.845333, loss_mean_cls: 0.041498, grad_norm: 0.275019
[[34m2025-10-04 12:40:29[0m] Step: 8589, Training Logs: loss_final: 0.871537, loss_mean: 0.828215, loss_mean_cls: 0.043322, grad_norm: 0.418487
[[34m2025-10-04 12:40:30[0m] Step: 8590, Training Logs: loss_final: 0.863000, loss_mean: 0.820325, loss_mean_cls: 0.042675, grad_norm: 0.282971
[[34m2025-10-04 12:40:30[0m] Step: 8591, Training Logs: loss_final: 0.857304, loss_mean: 0.812890, loss_mean_cls: 0.044414, grad_norm: 0.247028
[[34m2025-10-04 12:40:30[0m] Step: 8592, Training Logs: loss_final: 0.881860, loss_mean: 0.839049, loss_mean_cls: 0.042812, grad_norm: 0.278749
[[34m2025-10-04 12:40:31[0m] Step: 8593, Training Logs: loss_final: 0.876501, loss_mean: 0.832390, loss_mean_cls: 0.044112, grad_norm: 0.318566
[[34m2025-10-04 12:40:31[0m] Step: 8594, Training Logs: loss_final: 0.874394, loss_mean: 0.831199, loss_mean_cls: 0.043195, grad_norm: 0.330546
[[34m2025-10-04 12:40:31[0m] Step: 8595, Training Logs: loss_final: 0.865061, loss_mean: 0.821301, loss_mean_cls: 0.043760, grad_norm: 0.262393
[[34m2025-10-04 12:40:31[0m] Step: 8596, Training Logs: loss_final: 0.883869, loss_mean: 0.840621, loss_mean_cls: 0.043248, grad_norm: 0.247648
[[34m2025-10-04 12:40:32[0m] Step: 8597, Training Logs: loss_final: 0.868404, loss_mean: 0.824766, loss_mean_cls: 0.043638, grad_norm: 0.296379
[[34m2025-10-04 12:40:32[0m] Step: 8598, Training Logs: loss_final: 0.895068, loss_mean: 0.852546, loss_mean_cls: 0.042522, grad_norm: 0.313876
[[34m2025-10-04 12:40:32[0m] Step: 8599, Training Logs: loss_final: 0.887923, loss_mean: 0.844398, loss_mean_cls: 0.043525, grad_norm: 0.234433
[[34m2025-10-04 12:40:33[0m] Step: 8600, Training Logs: loss_final: 0.878084, loss_mean: 0.835436, loss_mean_cls: 0.042648, grad_norm: 0.220190
[[34m2025-10-04 12:40:33[0m] Step: 8601, Training Logs: loss_final: 0.882822, loss_mean: 0.840517, loss_mean_cls: 0.042305, grad_norm: 0.340532
[[34m2025-10-04 12:40:33[0m] Step: 8602, Training Logs: loss_final: 0.894190, loss_mean: 0.851144, loss_mean_cls: 0.043046, grad_norm: 0.297311
[[34m2025-10-04 12:40:33[0m] Step: 8603, Training Logs: loss_final: 0.883520, loss_mean: 0.840201, loss_mean_cls: 0.043318, grad_norm: 0.256816
[[34m2025-10-04 12:40:34[0m] Step: 8604, Training Logs: loss_final: 0.892762, loss_mean: 0.850561, loss_mean_cls: 0.042202, grad_norm: 0.210814
[[34m2025-10-04 12:40:34[0m] Step: 8605, Training Logs: loss_final: 0.890425, loss_mean: 0.847783, loss_mean_cls: 0.042642, grad_norm: 0.215830
[[34m2025-10-04 12:40:34[0m] Step: 8606, Training Logs: loss_final: 0.862130, loss_mean: 0.817875, loss_mean_cls: 0.044256, grad_norm: 0.295896
[[34m2025-10-04 12:40:35[0m] Step: 8607, Training Logs: loss_final: 0.866799, loss_mean: 0.823368, loss_mean_cls: 0.043431, grad_norm: 0.296585
[[34m2025-10-04 12:40:35[0m] Step: 8608, Training Logs: loss_final: 0.858679, loss_mean: 0.815202, loss_mean_cls: 0.043477, grad_norm: 0.196595
[[34m2025-10-04 12:40:35[0m] Step: 8609, Training Logs: loss_final: 0.869382, loss_mean: 0.825752, loss_mean_cls: 0.043630, grad_norm: 0.227185
[[34m2025-10-04 12:40:36[0m] Step: 8610, Training Logs: loss_final: 0.877980, loss_mean: 0.834890, loss_mean_cls: 0.043090, grad_norm: 0.275911
[[34m2025-10-04 12:40:36[0m] Step: 8611, Training Logs: loss_final: 0.871514, loss_mean: 0.827190, loss_mean_cls: 0.044324, grad_norm: 0.264331
[[34m2025-10-04 12:40:36[0m] Step: 8612, Training Logs: loss_final: 0.889225, loss_mean: 0.845431, loss_mean_cls: 0.043794, grad_norm: 0.263514
[[34m2025-10-04 12:40:36[0m] Step: 8613, Training Logs: loss_final: 0.873497, loss_mean: 0.830854, loss_mean_cls: 0.042643, grad_norm: 0.250761
[[34m2025-10-04 12:40:37[0m] Step: 8614, Training Logs: loss_final: 0.876644, loss_mean: 0.832378, loss_mean_cls: 0.044266, grad_norm: 0.262698
[[34m2025-10-04 12:40:37[0m] Step: 8615, Training Logs: loss_final: 0.864939, loss_mean: 0.821498, loss_mean_cls: 0.043441, grad_norm: 0.294607
[[34m2025-10-04 12:40:37[0m] Step: 8616, Training Logs: loss_final: 0.876045, loss_mean: 0.832176, loss_mean_cls: 0.043869, grad_norm: 0.286980
[[34m2025-10-04 12:40:38[0m] Step: 8617, Training Logs: loss_final: 0.879870, loss_mean: 0.837028, loss_mean_cls: 0.042842, grad_norm: 0.280096
[[34m2025-10-04 12:40:38[0m] Step: 8618, Training Logs: loss_final: 0.878209, loss_mean: 0.836351, loss_mean_cls: 0.041858, grad_norm: 0.262030
[[34m2025-10-04 12:40:38[0m] Step: 8619, Training Logs: loss_final: 0.869071, loss_mean: 0.825801, loss_mean_cls: 0.043270, grad_norm: 0.356075
[[34m2025-10-04 12:40:38[0m] Step: 8620, Training Logs: loss_final: 0.854226, loss_mean: 0.810196, loss_mean_cls: 0.044030, grad_norm: 0.203004
[[34m2025-10-04 12:40:39[0m] Step: 8621, Training Logs: loss_final: 0.865637, loss_mean: 0.821106, loss_mean_cls: 0.044531, grad_norm: 0.192842
[[34m2025-10-04 12:40:39[0m] Step: 8622, Training Logs: loss_final: 0.892341, loss_mean: 0.850240, loss_mean_cls: 0.042101, grad_norm: 0.252473
[[34m2025-10-04 12:40:39[0m] Step: 8623, Training Logs: loss_final: 0.849867, loss_mean: 0.806657, loss_mean_cls: 0.043210, grad_norm: 0.350792
[[34m2025-10-04 12:40:40[0m] Step: 8624, Training Logs: loss_final: 0.877132, loss_mean: 0.833948, loss_mean_cls: 0.043184, grad_norm: 0.256661
[[34m2025-10-04 12:40:40[0m] Step: 8625, Training Logs: loss_final: 0.893602, loss_mean: 0.850879, loss_mean_cls: 0.042722, grad_norm: 0.258656
[[34m2025-10-04 12:40:40[0m] Step: 8626, Training Logs: loss_final: 0.871352, loss_mean: 0.828398, loss_mean_cls: 0.042954, grad_norm: 0.185692
[[34m2025-10-04 12:40:40[0m] Step: 8627, Training Logs: loss_final: 0.882028, loss_mean: 0.838699, loss_mean_cls: 0.043329, grad_norm: 0.228606
[[34m2025-10-04 12:40:41[0m] Step: 8628, Training Logs: loss_final: 0.865861, loss_mean: 0.822469, loss_mean_cls: 0.043393, grad_norm: 0.307953
[[34m2025-10-04 12:40:41[0m] Step: 8629, Training Logs: loss_final: 0.870320, loss_mean: 0.827216, loss_mean_cls: 0.043103, grad_norm: 0.232696
[[34m2025-10-04 12:40:41[0m] Step: 8630, Training Logs: loss_final: 0.875510, loss_mean: 0.830484, loss_mean_cls: 0.045026, grad_norm: 0.188484
[[34m2025-10-04 12:40:42[0m] Step: 8631, Training Logs: loss_final: 0.877026, loss_mean: 0.834867, loss_mean_cls: 0.042159, grad_norm: 0.259203
[[34m2025-10-04 12:40:42[0m] Step: 8632, Training Logs: loss_final: 0.863903, loss_mean: 0.820826, loss_mean_cls: 0.043076, grad_norm: 0.290809
[[34m2025-10-04 12:40:42[0m] Step: 8633, Training Logs: loss_final: 0.888942, loss_mean: 0.846529, loss_mean_cls: 0.042413, grad_norm: 0.194701
[[34m2025-10-04 12:40:42[0m] Step: 8634, Training Logs: loss_final: 0.878746, loss_mean: 0.835165, loss_mean_cls: 0.043580, grad_norm: 0.345377
[[34m2025-10-04 12:40:43[0m] Step: 8635, Training Logs: loss_final: 0.877615, loss_mean: 0.835111, loss_mean_cls: 0.042504, grad_norm: 0.263568
[[34m2025-10-04 12:40:43[0m] Step: 8636, Training Logs: loss_final: 0.895321, loss_mean: 0.851575, loss_mean_cls: 0.043746, grad_norm: 0.208668
[[34m2025-10-04 12:40:43[0m] Step: 8637, Training Logs: loss_final: 0.865502, loss_mean: 0.822612, loss_mean_cls: 0.042890, grad_norm: 0.225364
[[34m2025-10-04 12:40:44[0m] Step: 8638, Training Logs: loss_final: 0.855888, loss_mean: 0.813058, loss_mean_cls: 0.042829, grad_norm: 0.213189
[[34m2025-10-04 12:40:44[0m] Step: 8639, Training Logs: loss_final: 0.887671, loss_mean: 0.845805, loss_mean_cls: 0.041866, grad_norm: 0.253400
[[34m2025-10-04 12:40:44[0m] Step: 8640, Training Logs: loss_final: 0.879439, loss_mean: 0.836201, loss_mean_cls: 0.043238, grad_norm: 0.284511
[[34m2025-10-04 12:40:45[0m] Step: 8641, Training Logs: loss_final: 0.864652, loss_mean: 0.821316, loss_mean_cls: 0.043336, grad_norm: 0.189901
[[34m2025-10-04 12:40:45[0m] Step: 8642, Training Logs: loss_final: 0.869205, loss_mean: 0.826435, loss_mean_cls: 0.042771, grad_norm: 0.268399
[[34m2025-10-04 12:40:45[0m] Step: 8643, Training Logs: loss_final: 0.876239, loss_mean: 0.833523, loss_mean_cls: 0.042716, grad_norm: 0.287788
[[34m2025-10-04 12:40:45[0m] Step: 8644, Training Logs: loss_final: 0.866328, loss_mean: 0.822916, loss_mean_cls: 0.043412, grad_norm: 0.291961
[[34m2025-10-04 12:40:46[0m] Step: 8645, Training Logs: loss_final: 0.869721, loss_mean: 0.825994, loss_mean_cls: 0.043726, grad_norm: 0.196381
[[34m2025-10-04 12:40:46[0m] Step: 8646, Training Logs: loss_final: 0.875491, loss_mean: 0.832468, loss_mean_cls: 0.043023, grad_norm: 0.214983
[[34m2025-10-04 12:40:46[0m] Step: 8647, Training Logs: loss_final: 0.866875, loss_mean: 0.823919, loss_mean_cls: 0.042956, grad_norm: 0.261636
[[34m2025-10-04 12:40:47[0m] Step: 8648, Training Logs: loss_final: 0.882094, loss_mean: 0.839540, loss_mean_cls: 0.042554, grad_norm: 0.264442
[[34m2025-10-04 12:40:47[0m] Step: 8649, Training Logs: loss_final: 0.872763, loss_mean: 0.829077, loss_mean_cls: 0.043686, grad_norm: 0.218083
[[34m2025-10-04 12:40:47[0m] Step: 8650, Training Logs: loss_final: 0.885256, loss_mean: 0.841765, loss_mean_cls: 0.043491, grad_norm: 0.400759
[[34m2025-10-04 12:40:47[0m] Step: 8651, Training Logs: loss_final: 0.865983, loss_mean: 0.821788, loss_mean_cls: 0.044195, grad_norm: 0.226307
[[34m2025-10-04 12:40:48[0m] Step: 8652, Training Logs: loss_final: 0.863863, loss_mean: 0.820322, loss_mean_cls: 0.043541, grad_norm: 0.321563
[[34m2025-10-04 12:40:48[0m] Step: 8653, Training Logs: loss_final: 0.871850, loss_mean: 0.828507, loss_mean_cls: 0.043343, grad_norm: 0.310724
[[34m2025-10-04 12:40:48[0m] Step: 8654, Training Logs: loss_final: 0.880246, loss_mean: 0.836748, loss_mean_cls: 0.043499, grad_norm: 0.327522
[[34m2025-10-04 12:40:49[0m] Step: 8655, Training Logs: loss_final: 0.857238, loss_mean: 0.814607, loss_mean_cls: 0.042631, grad_norm: 0.330518
[[34m2025-10-04 12:40:49[0m] Step: 8656, Training Logs: loss_final: 0.861970, loss_mean: 0.820642, loss_mean_cls: 0.041328, grad_norm: 0.344010
[[34m2025-10-04 12:40:49[0m] Step: 8657, Training Logs: loss_final: 0.867357, loss_mean: 0.823929, loss_mean_cls: 0.043428, grad_norm: 0.464322
[[34m2025-10-04 12:40:49[0m] Step: 8658, Training Logs: loss_final: 0.870368, loss_mean: 0.829012, loss_mean_cls: 0.041356, grad_norm: 0.236927
[[34m2025-10-04 12:40:50[0m] Step: 8659, Training Logs: loss_final: 0.877519, loss_mean: 0.834613, loss_mean_cls: 0.042906, grad_norm: 0.593091
[[34m2025-10-04 12:40:50[0m] Step: 8660, Training Logs: loss_final: 0.863799, loss_mean: 0.819193, loss_mean_cls: 0.044607, grad_norm: 0.241518
[[34m2025-10-04 12:40:50[0m] Step: 8661, Training Logs: loss_final: 0.862077, loss_mean: 0.820142, loss_mean_cls: 0.041935, grad_norm: 0.484606
[[34m2025-10-04 12:40:51[0m] Step: 8662, Training Logs: loss_final: 0.864135, loss_mean: 0.820600, loss_mean_cls: 0.043535, grad_norm: 0.238951
[[34m2025-10-04 12:40:51[0m] Step: 8663, Training Logs: loss_final: 0.883090, loss_mean: 0.840663, loss_mean_cls: 0.042427, grad_norm: 0.433495
[[34m2025-10-04 12:40:51[0m] Step: 8664, Training Logs: loss_final: 0.876789, loss_mean: 0.833561, loss_mean_cls: 0.043227, grad_norm: 0.343574
[[34m2025-10-04 12:40:51[0m] Step: 8665, Training Logs: loss_final: 0.911936, loss_mean: 0.869355, loss_mean_cls: 0.042581, grad_norm: 0.533124
[[34m2025-10-04 12:40:52[0m] Step: 8666, Training Logs: loss_final: 0.873064, loss_mean: 0.828893, loss_mean_cls: 0.044170, grad_norm: 0.237953
[[34m2025-10-04 12:40:52[0m] Step: 8667, Training Logs: loss_final: 0.876177, loss_mean: 0.833541, loss_mean_cls: 0.042636, grad_norm: 0.493796
[[34m2025-10-04 12:40:52[0m] Step: 8668, Training Logs: loss_final: 0.872583, loss_mean: 0.829728, loss_mean_cls: 0.042854, grad_norm: 0.245204
[[34m2025-10-04 12:40:53[0m] Step: 8669, Training Logs: loss_final: 0.880864, loss_mean: 0.838660, loss_mean_cls: 0.042205, grad_norm: 0.463856
[[34m2025-10-04 12:40:53[0m] Step: 8670, Training Logs: loss_final: 0.843398, loss_mean: 0.799565, loss_mean_cls: 0.043834, grad_norm: 0.197867
[[34m2025-10-04 12:40:53[0m] Step: 8671, Training Logs: loss_final: 0.869615, loss_mean: 0.826166, loss_mean_cls: 0.043449, grad_norm: 0.492544
[[34m2025-10-04 12:40:53[0m] Step: 8672, Training Logs: loss_final: 0.858077, loss_mean: 0.815265, loss_mean_cls: 0.042812, grad_norm: 0.206527
[[34m2025-10-04 12:40:54[0m] Step: 8673, Training Logs: loss_final: 0.879797, loss_mean: 0.837312, loss_mean_cls: 0.042485, grad_norm: 0.426341
[[34m2025-10-04 12:40:54[0m] Step: 8674, Training Logs: loss_final: 0.902321, loss_mean: 0.860391, loss_mean_cls: 0.041930, grad_norm: 0.298169
[[34m2025-10-04 12:40:54[0m] Step: 8675, Training Logs: loss_final: 0.903947, loss_mean: 0.860873, loss_mean_cls: 0.043074, grad_norm: 0.475122
[[34m2025-10-04 12:40:55[0m] Step: 8676, Training Logs: loss_final: 0.863648, loss_mean: 0.820193, loss_mean_cls: 0.043456, grad_norm: 0.211942
[[34m2025-10-04 12:40:55[0m] Step: 8677, Training Logs: loss_final: 0.877817, loss_mean: 0.834833, loss_mean_cls: 0.042983, grad_norm: 0.323945
[[34m2025-10-04 12:40:55[0m] Step: 8678, Training Logs: loss_final: 0.893618, loss_mean: 0.851086, loss_mean_cls: 0.042531, grad_norm: 0.197706
[[34m2025-10-04 12:40:56[0m] Step: 8679, Training Logs: loss_final: 0.870917, loss_mean: 0.827762, loss_mean_cls: 0.043155, grad_norm: 0.312304
[[34m2025-10-04 12:40:56[0m] Step: 8680, Training Logs: loss_final: 0.861394, loss_mean: 0.817969, loss_mean_cls: 0.043425, grad_norm: 0.193921
[[34m2025-10-04 12:40:56[0m] Step: 8681, Training Logs: loss_final: 0.870786, loss_mean: 0.827718, loss_mean_cls: 0.043068, grad_norm: 0.306826
[[34m2025-10-04 12:40:56[0m] Step: 8682, Training Logs: loss_final: 0.871319, loss_mean: 0.829090, loss_mean_cls: 0.042229, grad_norm: 0.193204
[[34m2025-10-04 12:40:57[0m] Step: 8683, Training Logs: loss_final: 0.892497, loss_mean: 0.850140, loss_mean_cls: 0.042357, grad_norm: 0.260105
[[34m2025-10-04 12:40:57[0m] Step: 8684, Training Logs: loss_final: 0.868227, loss_mean: 0.824943, loss_mean_cls: 0.043285, grad_norm: 0.219396
[[34m2025-10-04 12:40:57[0m] Step: 8685, Training Logs: loss_final: 0.861068, loss_mean: 0.817216, loss_mean_cls: 0.043852, grad_norm: 0.254057
[[34m2025-10-04 12:40:58[0m] Step: 8686, Training Logs: loss_final: 0.871768, loss_mean: 0.827882, loss_mean_cls: 0.043887, grad_norm: 0.335758
[[34m2025-10-04 12:40:58[0m] Step: 8687, Training Logs: loss_final: 0.890693, loss_mean: 0.847585, loss_mean_cls: 0.043107, grad_norm: 0.220338
[[34m2025-10-04 12:40:58[0m] Step: 8688, Training Logs: loss_final: 0.885145, loss_mean: 0.841202, loss_mean_cls: 0.043943, grad_norm: 0.302095
[[34m2025-10-04 12:40:58[0m] Step: 8689, Training Logs: loss_final: 0.880773, loss_mean: 0.838287, loss_mean_cls: 0.042486, grad_norm: 0.231942
[[34m2025-10-04 12:40:59[0m] Step: 8690, Training Logs: loss_final: 0.866392, loss_mean: 0.823177, loss_mean_cls: 0.043215, grad_norm: 0.292136
[[34m2025-10-04 12:40:59[0m] Step: 8691, Training Logs: loss_final: 0.866748, loss_mean: 0.824175, loss_mean_cls: 0.042573, grad_norm: 0.198635
[[34m2025-10-04 12:40:59[0m] Step: 8692, Training Logs: loss_final: 0.884507, loss_mean: 0.841063, loss_mean_cls: 0.043444, grad_norm: 0.343763
[[34m2025-10-04 12:41:00[0m] Step: 8693, Training Logs: loss_final: 0.861459, loss_mean: 0.819283, loss_mean_cls: 0.042175, grad_norm: 0.210196
[[34m2025-10-04 12:41:00[0m] Step: 8694, Training Logs: loss_final: 0.889247, loss_mean: 0.846405, loss_mean_cls: 0.042842, grad_norm: 0.262596
[[34m2025-10-04 12:41:00[0m] Step: 8695, Training Logs: loss_final: 0.848407, loss_mean: 0.804526, loss_mean_cls: 0.043881, grad_norm: 0.200860
[[34m2025-10-04 12:41:01[0m] Step: 8696, Training Logs: loss_final: 0.880390, loss_mean: 0.837898, loss_mean_cls: 0.042492, grad_norm: 0.227004
[[34m2025-10-04 12:41:01[0m] Step: 8697, Training Logs: loss_final: 0.893233, loss_mean: 0.849567, loss_mean_cls: 0.043666, grad_norm: 0.347791
[[34m2025-10-04 12:41:01[0m] Step: 8698, Training Logs: loss_final: 0.885912, loss_mean: 0.841717, loss_mean_cls: 0.044195, grad_norm: 0.218621
[[34m2025-10-04 12:41:01[0m] Step: 8699, Training Logs: loss_final: 0.879528, loss_mean: 0.836598, loss_mean_cls: 0.042930, grad_norm: 0.424328
[[34m2025-10-04 12:41:02[0m] Step: 8700, Training Logs: loss_final: 0.874646, loss_mean: 0.830545, loss_mean_cls: 0.044101, grad_norm: 0.353613
[[34m2025-10-04 12:41:02[0m] Step: 8701, Training Logs: loss_final: 0.883587, loss_mean: 0.841514, loss_mean_cls: 0.042073, grad_norm: 0.256452
[[34m2025-10-04 12:41:02[0m] Step: 8702, Training Logs: loss_final: 0.876647, loss_mean: 0.834413, loss_mean_cls: 0.042234, grad_norm: 0.293142
[[34m2025-10-04 12:41:03[0m] Step: 8703, Training Logs: loss_final: 0.870672, loss_mean: 0.829157, loss_mean_cls: 0.041515, grad_norm: 0.281773
[[34m2025-10-04 12:41:03[0m] Step: 8704, Training Logs: loss_final: 0.883881, loss_mean: 0.841761, loss_mean_cls: 0.042120, grad_norm: 0.327784
[[34m2025-10-04 12:41:03[0m] Step: 8705, Training Logs: loss_final: 0.850780, loss_mean: 0.807297, loss_mean_cls: 0.043483, grad_norm: 0.213174
[[34m2025-10-04 12:41:03[0m] Step: 8706, Training Logs: loss_final: 0.882130, loss_mean: 0.838922, loss_mean_cls: 0.043208, grad_norm: 0.233163
[[34m2025-10-04 12:41:04[0m] Step: 8707, Training Logs: loss_final: 0.890935, loss_mean: 0.848001, loss_mean_cls: 0.042934, grad_norm: 0.270999
[[34m2025-10-04 12:41:04[0m] Step: 8708, Training Logs: loss_final: 0.878113, loss_mean: 0.836522, loss_mean_cls: 0.041591, grad_norm: 0.264469
[[34m2025-10-04 12:41:04[0m] Step: 8709, Training Logs: loss_final: 0.878646, loss_mean: 0.836033, loss_mean_cls: 0.042612, grad_norm: 0.402436
[[34m2025-10-04 12:41:05[0m] Step: 8710, Training Logs: loss_final: 0.842155, loss_mean: 0.798621, loss_mean_cls: 0.043534, grad_norm: 0.263705
[[34m2025-10-04 12:41:05[0m] Step: 8711, Training Logs: loss_final: 0.868189, loss_mean: 0.824944, loss_mean_cls: 0.043246, grad_norm: 0.214808
[[34m2025-10-04 12:41:05[0m] Step: 8712, Training Logs: loss_final: 0.868687, loss_mean: 0.825412, loss_mean_cls: 0.043275, grad_norm: 0.314331
[[34m2025-10-04 12:41:05[0m] Step: 8713, Training Logs: loss_final: 0.879305, loss_mean: 0.836535, loss_mean_cls: 0.042770, grad_norm: 0.201620
[[34m2025-10-04 12:41:06[0m] Step: 8714, Training Logs: loss_final: 0.847121, loss_mean: 0.804100, loss_mean_cls: 0.043021, grad_norm: 0.299036
[[34m2025-10-04 12:41:06[0m] Step: 8715, Training Logs: loss_final: 0.872806, loss_mean: 0.830941, loss_mean_cls: 0.041865, grad_norm: 0.281546
[[34m2025-10-04 12:41:06[0m] Step: 8716, Training Logs: loss_final: 0.858726, loss_mean: 0.814828, loss_mean_cls: 0.043897, grad_norm: 0.244384
[[34m2025-10-04 12:41:07[0m] Step: 8717, Training Logs: loss_final: 0.867621, loss_mean: 0.824512, loss_mean_cls: 0.043109, grad_norm: 0.179991
[[34m2025-10-04 12:41:07[0m] Step: 8718, Training Logs: loss_final: 0.853414, loss_mean: 0.811267, loss_mean_cls: 0.042147, grad_norm: 0.257859
[[34m2025-10-04 12:41:07[0m] Step: 8719, Training Logs: loss_final: 0.877923, loss_mean: 0.835236, loss_mean_cls: 0.042686, grad_norm: 0.260302
[[34m2025-10-04 12:41:07[0m] Step: 8720, Training Logs: loss_final: 0.872357, loss_mean: 0.828597, loss_mean_cls: 0.043760, grad_norm: 0.301771
[[34m2025-10-04 12:41:08[0m] Step: 8721, Training Logs: loss_final: 0.870029, loss_mean: 0.826336, loss_mean_cls: 0.043693, grad_norm: 0.296781
[[34m2025-10-04 12:41:08[0m] Step: 8722, Training Logs: loss_final: 0.876563, loss_mean: 0.834887, loss_mean_cls: 0.041676, grad_norm: 0.225954
[[34m2025-10-04 12:41:08[0m] Step: 8723, Training Logs: loss_final: 0.911140, loss_mean: 0.868216, loss_mean_cls: 0.042924, grad_norm: 0.323600
[[34m2025-10-04 12:41:09[0m] Step: 8724, Training Logs: loss_final: 0.870845, loss_mean: 0.827153, loss_mean_cls: 0.043692, grad_norm: 0.188832
[[34m2025-10-04 12:41:09[0m] Step: 8725, Training Logs: loss_final: 0.878664, loss_mean: 0.837469, loss_mean_cls: 0.041195, grad_norm: 0.202897
[[34m2025-10-04 12:41:09[0m] Step: 8726, Training Logs: loss_final: 0.871480, loss_mean: 0.828664, loss_mean_cls: 0.042816, grad_norm: 0.328611
[[34m2025-10-04 12:41:10[0m] Step: 8727, Training Logs: loss_final: 0.856392, loss_mean: 0.813601, loss_mean_cls: 0.042791, grad_norm: 0.227096
[[34m2025-10-04 12:41:10[0m] Step: 8728, Training Logs: loss_final: 0.872916, loss_mean: 0.830038, loss_mean_cls: 0.042879, grad_norm: 0.339275
[[34m2025-10-04 12:41:10[0m] Step: 8729, Training Logs: loss_final: 0.883363, loss_mean: 0.839708, loss_mean_cls: 0.043655, grad_norm: 0.226245
[[34m2025-10-04 12:41:10[0m] Step: 8730, Training Logs: loss_final: 0.857688, loss_mean: 0.814282, loss_mean_cls: 0.043405, grad_norm: 0.278414
[[34m2025-10-04 12:41:11[0m] Step: 8731, Training Logs: loss_final: 0.868004, loss_mean: 0.825756, loss_mean_cls: 0.042248, grad_norm: 0.260045
[[34m2025-10-04 12:41:11[0m] Step: 8732, Training Logs: loss_final: 0.878165, loss_mean: 0.834236, loss_mean_cls: 0.043929, grad_norm: 0.302861
[[34m2025-10-04 12:41:11[0m] Step: 8733, Training Logs: loss_final: 0.862528, loss_mean: 0.819041, loss_mean_cls: 0.043487, grad_norm: 0.188950
[[34m2025-10-04 12:41:12[0m] Step: 8734, Training Logs: loss_final: 0.859871, loss_mean: 0.816300, loss_mean_cls: 0.043571, grad_norm: 0.216203
[[34m2025-10-04 12:41:12[0m] Step: 8735, Training Logs: loss_final: 0.887725, loss_mean: 0.844875, loss_mean_cls: 0.042850, grad_norm: 0.298707
[[34m2025-10-04 12:41:12[0m] Step: 8736, Training Logs: loss_final: 0.865595, loss_mean: 0.822679, loss_mean_cls: 0.042916, grad_norm: 0.364861
[[34m2025-10-04 12:41:12[0m] Step: 8737, Training Logs: loss_final: 0.875511, loss_mean: 0.831349, loss_mean_cls: 0.044162, grad_norm: 0.355960
[[34m2025-10-04 12:41:13[0m] Step: 8738, Training Logs: loss_final: 0.874014, loss_mean: 0.829939, loss_mean_cls: 0.044075, grad_norm: 0.435699
[[34m2025-10-04 12:41:13[0m] Step: 8739, Training Logs: loss_final: 0.869365, loss_mean: 0.826878, loss_mean_cls: 0.042487, grad_norm: 0.246366
[[34m2025-10-04 12:41:13[0m] Step: 8740, Training Logs: loss_final: 0.870649, loss_mean: 0.828176, loss_mean_cls: 0.042473, grad_norm: 0.309000
[[34m2025-10-04 12:41:14[0m] Step: 8741, Training Logs: loss_final: 0.873738, loss_mean: 0.830995, loss_mean_cls: 0.042744, grad_norm: 0.220826
[[34m2025-10-04 12:41:14[0m] Step: 8742, Training Logs: loss_final: 0.866249, loss_mean: 0.823224, loss_mean_cls: 0.043025, grad_norm: 0.330409
[[34m2025-10-04 12:41:14[0m] Step: 8743, Training Logs: loss_final: 0.869837, loss_mean: 0.827103, loss_mean_cls: 0.042734, grad_norm: 0.228587
[[34m2025-10-04 12:41:15[0m] Step: 8744, Training Logs: loss_final: 0.858775, loss_mean: 0.815913, loss_mean_cls: 0.042862, grad_norm: 0.489443
[[34m2025-10-04 12:41:15[0m] Step: 8745, Training Logs: loss_final: 0.868305, loss_mean: 0.826253, loss_mean_cls: 0.042053, grad_norm: 0.229792
[[34m2025-10-04 12:41:15[0m] Step: 8746, Training Logs: loss_final: 0.891542, loss_mean: 0.848974, loss_mean_cls: 0.042568, grad_norm: 0.387587
[[34m2025-10-04 12:41:15[0m] Step: 8747, Training Logs: loss_final: 0.871997, loss_mean: 0.828577, loss_mean_cls: 0.043421, grad_norm: 0.222196
[[34m2025-10-04 12:41:16[0m] Step: 8748, Training Logs: loss_final: 0.861917, loss_mean: 0.816777, loss_mean_cls: 0.045140, grad_norm: 0.253371
[[34m2025-10-04 12:41:16[0m] Step: 8749, Training Logs: loss_final: 0.862087, loss_mean: 0.819838, loss_mean_cls: 0.042250, grad_norm: 0.315545
[[34m2025-10-04 12:41:16[0m] Step: 8750, Training Logs: loss_final: 0.887774, loss_mean: 0.845004, loss_mean_cls: 0.042770, grad_norm: 0.169758
[[34m2025-10-04 12:41:17[0m] Step: 8751, Training Logs: loss_final: 0.887408, loss_mean: 0.843880, loss_mean_cls: 0.043529, grad_norm: 0.231453
[[34m2025-10-04 12:41:17[0m] Step: 8752, Training Logs: loss_final: 0.852424, loss_mean: 0.810454, loss_mean_cls: 0.041970, grad_norm: 0.257134
[[34m2025-10-04 12:41:17[0m] Step: 8753, Training Logs: loss_final: 0.867348, loss_mean: 0.823227, loss_mean_cls: 0.044120, grad_norm: 0.334671
[[34m2025-10-04 12:41:17[0m] Step: 8754, Training Logs: loss_final: 0.884956, loss_mean: 0.842683, loss_mean_cls: 0.042273, grad_norm: 0.251330
[[34m2025-10-04 12:41:18[0m] Step: 8755, Training Logs: loss_final: 0.879332, loss_mean: 0.835875, loss_mean_cls: 0.043457, grad_norm: 0.229650
[[34m2025-10-04 12:41:18[0m] Step: 8756, Training Logs: loss_final: 0.847107, loss_mean: 0.803509, loss_mean_cls: 0.043598, grad_norm: 0.350845
[[34m2025-10-04 12:41:18[0m] Step: 8757, Training Logs: loss_final: 0.873794, loss_mean: 0.831142, loss_mean_cls: 0.042652, grad_norm: 0.270139
[[34m2025-10-04 12:41:19[0m] Step: 8758, Training Logs: loss_final: 0.871363, loss_mean: 0.827783, loss_mean_cls: 0.043580, grad_norm: 0.188864
[[34m2025-10-04 12:41:19[0m] Step: 8759, Training Logs: loss_final: 0.874060, loss_mean: 0.830741, loss_mean_cls: 0.043319, grad_norm: 0.289794
[[34m2025-10-04 12:41:19[0m] Step: 8760, Training Logs: loss_final: 0.871441, loss_mean: 0.828249, loss_mean_cls: 0.043192, grad_norm: 0.287118
[[34m2025-10-04 12:41:20[0m] Step: 8761, Training Logs: loss_final: 0.854541, loss_mean: 0.809977, loss_mean_cls: 0.044563, grad_norm: 0.320333
[[34m2025-10-04 12:41:20[0m] Step: 8762, Training Logs: loss_final: 0.883711, loss_mean: 0.841009, loss_mean_cls: 0.042702, grad_norm: 0.258817
[[34m2025-10-04 12:41:20[0m] Step: 8763, Training Logs: loss_final: 0.866837, loss_mean: 0.823780, loss_mean_cls: 0.043057, grad_norm: 0.287850
[[34m2025-10-04 12:41:20[0m] Step: 8764, Training Logs: loss_final: 0.900030, loss_mean: 0.856586, loss_mean_cls: 0.043444, grad_norm: 0.322927
[[34m2025-10-04 12:41:21[0m] Step: 8765, Training Logs: loss_final: 0.866241, loss_mean: 0.821305, loss_mean_cls: 0.044937, grad_norm: 0.246282
[[34m2025-10-04 12:41:21[0m] Step: 8766, Training Logs: loss_final: 0.878912, loss_mean: 0.835528, loss_mean_cls: 0.043384, grad_norm: 0.245927
[[34m2025-10-04 12:41:21[0m] Step: 8767, Training Logs: loss_final: 0.862501, loss_mean: 0.819913, loss_mean_cls: 0.042588, grad_norm: 0.330596
[[34m2025-10-04 12:41:22[0m] Step: 8768, Training Logs: loss_final: 0.872295, loss_mean: 0.829771, loss_mean_cls: 0.042525, grad_norm: 0.366733
[[34m2025-10-04 12:41:22[0m] Step: 8769, Training Logs: loss_final: 0.861625, loss_mean: 0.818156, loss_mean_cls: 0.043469, grad_norm: 0.271496
[[34m2025-10-04 12:41:22[0m] Step: 8770, Training Logs: loss_final: 0.850285, loss_mean: 0.806599, loss_mean_cls: 0.043686, grad_norm: 0.212160
[[34m2025-10-04 12:41:22[0m] Step: 8771, Training Logs: loss_final: 0.862741, loss_mean: 0.819705, loss_mean_cls: 0.043037, grad_norm: 0.219570
[[34m2025-10-04 12:41:23[0m] Step: 8772, Training Logs: loss_final: 0.867747, loss_mean: 0.824091, loss_mean_cls: 0.043656, grad_norm: 0.308655
[[34m2025-10-04 12:41:23[0m] Step: 8773, Training Logs: loss_final: 0.873940, loss_mean: 0.830270, loss_mean_cls: 0.043670, grad_norm: 0.259859
[[34m2025-10-04 12:41:23[0m] Step: 8774, Training Logs: loss_final: 0.869488, loss_mean: 0.825791, loss_mean_cls: 0.043697, grad_norm: 0.199557
[[34m2025-10-04 12:41:24[0m] Step: 8775, Training Logs: loss_final: 0.888815, loss_mean: 0.845613, loss_mean_cls: 0.043202, grad_norm: 0.275476
[[34m2025-10-04 12:41:24[0m] Step: 8776, Training Logs: loss_final: 0.874279, loss_mean: 0.831745, loss_mean_cls: 0.042533, grad_norm: 0.288633
[[34m2025-10-04 12:41:24[0m] Step: 8777, Training Logs: loss_final: 0.841488, loss_mean: 0.797339, loss_mean_cls: 0.044150, grad_norm: 0.220581
[[34m2025-10-04 12:41:24[0m] Step: 8778, Training Logs: loss_final: 0.872795, loss_mean: 0.830373, loss_mean_cls: 0.042422, grad_norm: 0.264437
[[34m2025-10-04 12:41:25[0m] Step: 8779, Training Logs: loss_final: 0.889503, loss_mean: 0.847939, loss_mean_cls: 0.041564, grad_norm: 0.235402
[[34m2025-10-04 12:41:25[0m] Step: 8780, Training Logs: loss_final: 0.853172, loss_mean: 0.808463, loss_mean_cls: 0.044709, grad_norm: 0.273389
[[34m2025-10-04 12:41:25[0m] Step: 8781, Training Logs: loss_final: 0.877163, loss_mean: 0.834330, loss_mean_cls: 0.042833, grad_norm: 0.263458
[[34m2025-10-04 12:41:26[0m] Step: 8782, Training Logs: loss_final: 0.885963, loss_mean: 0.842881, loss_mean_cls: 0.043082, grad_norm: 0.184805
[[34m2025-10-04 12:41:26[0m] Step: 8783, Training Logs: loss_final: 0.878453, loss_mean: 0.835816, loss_mean_cls: 0.042638, grad_norm: 0.245362
[[34m2025-10-04 12:41:26[0m] Step: 8784, Training Logs: loss_final: 0.863532, loss_mean: 0.818825, loss_mean_cls: 0.044707, grad_norm: 0.230471
[[34m2025-10-04 12:41:26[0m] Step: 8785, Training Logs: loss_final: 0.862084, loss_mean: 0.818270, loss_mean_cls: 0.043813, grad_norm: 0.186314
[[34m2025-10-04 12:41:27[0m] Step: 8786, Training Logs: loss_final: 0.855087, loss_mean: 0.811929, loss_mean_cls: 0.043158, grad_norm: 0.232448
[[34m2025-10-04 12:41:27[0m] Step: 8787, Training Logs: loss_final: 0.874650, loss_mean: 0.830661, loss_mean_cls: 0.043989, grad_norm: 0.221995
[[34m2025-10-04 12:41:27[0m] Step: 8788, Training Logs: loss_final: 0.875800, loss_mean: 0.832631, loss_mean_cls: 0.043170, grad_norm: 0.283917
[[34m2025-10-04 12:41:28[0m] Step: 8789, Training Logs: loss_final: 0.857471, loss_mean: 0.813477, loss_mean_cls: 0.043993, grad_norm: 0.200409
[[34m2025-10-04 12:41:28[0m] Step: 8790, Training Logs: loss_final: 0.891038, loss_mean: 0.848136, loss_mean_cls: 0.042902, grad_norm: 0.209963
[[34m2025-10-04 12:41:28[0m] Step: 8791, Training Logs: loss_final: 0.878231, loss_mean: 0.834818, loss_mean_cls: 0.043412, grad_norm: 0.259541
[[34m2025-10-04 12:41:28[0m] Step: 8792, Training Logs: loss_final: 0.882810, loss_mean: 0.840551, loss_mean_cls: 0.042259, grad_norm: 0.217665
[[34m2025-10-04 12:41:29[0m] Step: 8793, Training Logs: loss_final: 0.878341, loss_mean: 0.835845, loss_mean_cls: 0.042495, grad_norm: 0.204637
[[34m2025-10-04 12:41:29[0m] Step: 8794, Training Logs: loss_final: 0.865417, loss_mean: 0.822420, loss_mean_cls: 0.042997, grad_norm: 0.313880
[[34m2025-10-04 12:41:29[0m] Step: 8795, Training Logs: loss_final: 0.872615, loss_mean: 0.829538, loss_mean_cls: 0.043076, grad_norm: 0.230882
[[34m2025-10-04 12:41:30[0m] Step: 8796, Training Logs: loss_final: 0.881096, loss_mean: 0.838669, loss_mean_cls: 0.042427, grad_norm: 0.214189
[[34m2025-10-04 12:41:30[0m] Step: 8797, Training Logs: loss_final: 0.874547, loss_mean: 0.831293, loss_mean_cls: 0.043254, grad_norm: 0.232127
[[34m2025-10-04 12:41:30[0m] Step: 8798, Training Logs: loss_final: 0.862904, loss_mean: 0.817852, loss_mean_cls: 0.045052, grad_norm: 0.258921
[[34m2025-10-04 12:41:31[0m] Step: 8799, Training Logs: loss_final: 0.887365, loss_mean: 0.843928, loss_mean_cls: 0.043437, grad_norm: 0.246839
[[34m2025-10-04 12:41:31[0m] Step: 8800, Training Logs: loss_final: 0.870339, loss_mean: 0.826772, loss_mean_cls: 0.043567, grad_norm: 0.323593
[[34m2025-10-04 12:41:31[0m] Step: 8801, Training Logs: loss_final: 0.881113, loss_mean: 0.837523, loss_mean_cls: 0.043590, grad_norm: 0.264884
[[34m2025-10-04 12:41:31[0m] Step: 8802, Training Logs: loss_final: 0.874522, loss_mean: 0.831640, loss_mean_cls: 0.042882, grad_norm: 0.236112
[[34m2025-10-04 12:41:32[0m] Step: 8803, Training Logs: loss_final: 0.853858, loss_mean: 0.809984, loss_mean_cls: 0.043874, grad_norm: 0.188436
[[34m2025-10-04 12:41:32[0m] Step: 8804, Training Logs: loss_final: 0.870060, loss_mean: 0.827489, loss_mean_cls: 0.042570, grad_norm: 0.211885
[[34m2025-10-04 12:41:32[0m] Step: 8805, Training Logs: loss_final: 0.877748, loss_mean: 0.834890, loss_mean_cls: 0.042858, grad_norm: 0.252830
[[34m2025-10-04 12:41:33[0m] Step: 8806, Training Logs: loss_final: 0.875380, loss_mean: 0.832169, loss_mean_cls: 0.043211, grad_norm: 0.190250
[[34m2025-10-04 12:41:33[0m] Step: 8807, Training Logs: loss_final: 0.848142, loss_mean: 0.804704, loss_mean_cls: 0.043437, grad_norm: 0.249055
[[34m2025-10-04 12:41:33[0m] Step: 8808, Training Logs: loss_final: 0.882728, loss_mean: 0.841170, loss_mean_cls: 0.041558, grad_norm: 0.312896
[[34m2025-10-04 12:41:34[0m] Step: 8809, Training Logs: loss_final: 0.882033, loss_mean: 0.839122, loss_mean_cls: 0.042911, grad_norm: 0.196126
[[34m2025-10-04 12:41:34[0m] Step: 8810, Training Logs: loss_final: 0.863685, loss_mean: 0.820603, loss_mean_cls: 0.043082, grad_norm: 0.241217
[[34m2025-10-04 12:41:34[0m] Step: 8811, Training Logs: loss_final: 0.878567, loss_mean: 0.835098, loss_mean_cls: 0.043468, grad_norm: 0.253001
[[34m2025-10-04 12:41:34[0m] Step: 8812, Training Logs: loss_final: 0.862806, loss_mean: 0.819679, loss_mean_cls: 0.043127, grad_norm: 0.254299
[[34m2025-10-04 12:41:35[0m] Step: 8813, Training Logs: loss_final: 0.882606, loss_mean: 0.840466, loss_mean_cls: 0.042140, grad_norm: 0.271612
[[34m2025-10-04 12:41:35[0m] Step: 8814, Training Logs: loss_final: 0.879758, loss_mean: 0.837021, loss_mean_cls: 0.042737, grad_norm: 0.278944
[[34m2025-10-04 12:41:35[0m] Step: 8815, Training Logs: loss_final: 0.874307, loss_mean: 0.830377, loss_mean_cls: 0.043930, grad_norm: 0.223551
[[34m2025-10-04 12:41:36[0m] Step: 8816, Training Logs: loss_final: 0.867298, loss_mean: 0.824396, loss_mean_cls: 0.042903, grad_norm: 0.194172
[[34m2025-10-04 12:41:36[0m] Step: 8817, Training Logs: loss_final: 0.885473, loss_mean: 0.842369, loss_mean_cls: 0.043104, grad_norm: 0.286157
[[34m2025-10-04 12:41:36[0m] Step: 8818, Training Logs: loss_final: 0.882816, loss_mean: 0.839854, loss_mean_cls: 0.042962, grad_norm: 0.210553
[[34m2025-10-04 12:41:36[0m] Step: 8819, Training Logs: loss_final: 0.860217, loss_mean: 0.816139, loss_mean_cls: 0.044078, grad_norm: 0.199161
[[34m2025-10-04 12:41:37[0m] Step: 8820, Training Logs: loss_final: 0.900517, loss_mean: 0.858358, loss_mean_cls: 0.042159, grad_norm: 0.190774
[[34m2025-10-04 12:41:37[0m] Step: 8821, Training Logs: loss_final: 0.863998, loss_mean: 0.819787, loss_mean_cls: 0.044210, grad_norm: 0.258036
[[34m2025-10-04 12:41:37[0m] Step: 8822, Training Logs: loss_final: 0.862776, loss_mean: 0.818621, loss_mean_cls: 0.044155, grad_norm: 0.372202
[[34m2025-10-04 12:41:38[0m] Step: 8823, Training Logs: loss_final: 0.883875, loss_mean: 0.841724, loss_mean_cls: 0.042151, grad_norm: 0.175646
[[34m2025-10-04 12:41:38[0m] Step: 8824, Training Logs: loss_final: 0.881367, loss_mean: 0.838451, loss_mean_cls: 0.042916, grad_norm: 0.239759
[[34m2025-10-04 12:41:38[0m] Step: 8825, Training Logs: loss_final: 0.856626, loss_mean: 0.812578, loss_mean_cls: 0.044048, grad_norm: 0.244936
[[34m2025-10-04 12:41:38[0m] Step: 8826, Training Logs: loss_final: 0.886371, loss_mean: 0.844625, loss_mean_cls: 0.041746, grad_norm: 0.305862
[[34m2025-10-04 12:41:39[0m] Step: 8827, Training Logs: loss_final: 0.885258, loss_mean: 0.841686, loss_mean_cls: 0.043572, grad_norm: 0.228464
[[34m2025-10-04 12:41:39[0m] Step: 8828, Training Logs: loss_final: 0.875389, loss_mean: 0.832039, loss_mean_cls: 0.043350, grad_norm: 0.170885
[[34m2025-10-04 12:41:39[0m] Step: 8829, Training Logs: loss_final: 0.864460, loss_mean: 0.822577, loss_mean_cls: 0.041882, grad_norm: 0.176134
[[34m2025-10-04 12:41:40[0m] Step: 8830, Training Logs: loss_final: 0.867965, loss_mean: 0.824639, loss_mean_cls: 0.043326, grad_norm: 0.220267
[[34m2025-10-04 12:41:40[0m] Step: 8831, Training Logs: loss_final: 0.876521, loss_mean: 0.832977, loss_mean_cls: 0.043545, grad_norm: 0.248338
[[34m2025-10-04 12:41:40[0m] Step: 8832, Training Logs: loss_final: 0.881197, loss_mean: 0.837446, loss_mean_cls: 0.043751, grad_norm: 0.199241
[[34m2025-10-04 12:41:40[0m] Step: 8833, Training Logs: loss_final: 0.875467, loss_mean: 0.831600, loss_mean_cls: 0.043867, grad_norm: 0.320266
[[34m2025-10-04 12:41:41[0m] Step: 8834, Training Logs: loss_final: 0.868975, loss_mean: 0.824987, loss_mean_cls: 0.043988, grad_norm: 0.318479
[[34m2025-10-04 12:41:41[0m] Step: 8835, Training Logs: loss_final: 0.868598, loss_mean: 0.825887, loss_mean_cls: 0.042711, grad_norm: 0.203785
[[34m2025-10-04 12:41:41[0m] Step: 8836, Training Logs: loss_final: 0.872657, loss_mean: 0.831124, loss_mean_cls: 0.041532, grad_norm: 0.306730
[[34m2025-10-04 12:41:42[0m] Step: 8837, Training Logs: loss_final: 0.860059, loss_mean: 0.817203, loss_mean_cls: 0.042856, grad_norm: 0.167417
[[34m2025-10-04 12:41:42[0m] Step: 8838, Training Logs: loss_final: 0.877832, loss_mean: 0.834985, loss_mean_cls: 0.042847, grad_norm: 0.336954
[[34m2025-10-04 12:41:42[0m] Step: 8839, Training Logs: loss_final: 0.862813, loss_mean: 0.821481, loss_mean_cls: 0.041332, grad_norm: 0.186582
[[34m2025-10-04 12:41:42[0m] Step: 8840, Training Logs: loss_final: 0.881642, loss_mean: 0.838590, loss_mean_cls: 0.043052, grad_norm: 0.238417
[[34m2025-10-04 12:41:43[0m] Step: 8841, Training Logs: loss_final: 0.873530, loss_mean: 0.831959, loss_mean_cls: 0.041571, grad_norm: 0.200605
[[34m2025-10-04 12:41:43[0m] Step: 8842, Training Logs: loss_final: 0.876951, loss_mean: 0.835175, loss_mean_cls: 0.041776, grad_norm: 0.200846
[[34m2025-10-04 12:41:43[0m] Step: 8843, Training Logs: loss_final: 0.868114, loss_mean: 0.825436, loss_mean_cls: 0.042678, grad_norm: 0.245368
[[34m2025-10-04 12:41:44[0m] Step: 8844, Training Logs: loss_final: 0.880189, loss_mean: 0.837351, loss_mean_cls: 0.042838, grad_norm: 0.241885
[[34m2025-10-04 12:41:44[0m] Step: 8845, Training Logs: loss_final: 0.865730, loss_mean: 0.823000, loss_mean_cls: 0.042730, grad_norm: 0.241970
[[34m2025-10-04 12:41:44[0m] Step: 8846, Training Logs: loss_final: 0.876636, loss_mean: 0.833204, loss_mean_cls: 0.043432, grad_norm: 0.327358
[[34m2025-10-04 12:41:45[0m] Step: 8847, Training Logs: loss_final: 0.848737, loss_mean: 0.805679, loss_mean_cls: 0.043058, grad_norm: 0.305182
[[34m2025-10-04 12:41:45[0m] Step: 8848, Training Logs: loss_final: 0.869462, loss_mean: 0.827045, loss_mean_cls: 0.042416, grad_norm: 0.209111
[[34m2025-10-04 12:41:45[0m] Step: 8849, Training Logs: loss_final: 0.852020, loss_mean: 0.809082, loss_mean_cls: 0.042938, grad_norm: 0.231944
[[34m2025-10-04 12:41:45[0m] Step: 8850, Training Logs: loss_final: 0.870969, loss_mean: 0.826521, loss_mean_cls: 0.044448, grad_norm: 0.245045
[[34m2025-10-04 12:41:46[0m] Step: 8851, Training Logs: loss_final: 0.879125, loss_mean: 0.835906, loss_mean_cls: 0.043218, grad_norm: 0.253349
[[34m2025-10-04 12:41:46[0m] Step: 8852, Training Logs: loss_final: 0.867850, loss_mean: 0.824307, loss_mean_cls: 0.043544, grad_norm: 0.219179
[[34m2025-10-04 12:41:46[0m] Step: 8853, Training Logs: loss_final: 0.869631, loss_mean: 0.825794, loss_mean_cls: 0.043838, grad_norm: 0.161115
[[34m2025-10-04 12:41:47[0m] Step: 8854, Training Logs: loss_final: 0.875155, loss_mean: 0.830484, loss_mean_cls: 0.044671, grad_norm: 0.262244
[[34m2025-10-04 12:41:47[0m] Step: 8855, Training Logs: loss_final: 0.883002, loss_mean: 0.838849, loss_mean_cls: 0.044153, grad_norm: 0.237411
[[34m2025-10-04 12:41:47[0m] Step: 8856, Training Logs: loss_final: 0.880806, loss_mean: 0.838125, loss_mean_cls: 0.042681, grad_norm: 0.262682
[[34m2025-10-04 12:41:48[0m] Step: 8857, Training Logs: loss_final: 0.855178, loss_mean: 0.810816, loss_mean_cls: 0.044362, grad_norm: 0.281602
[[34m2025-10-04 12:41:48[0m] Step: 8858, Training Logs: loss_final: 0.870900, loss_mean: 0.827207, loss_mean_cls: 0.043693, grad_norm: 0.339308
[[34m2025-10-04 12:41:48[0m] Step: 8859, Training Logs: loss_final: 0.862394, loss_mean: 0.818179, loss_mean_cls: 0.044215, grad_norm: 0.305582
[[34m2025-10-04 12:41:48[0m] Step: 8860, Training Logs: loss_final: 0.845151, loss_mean: 0.801454, loss_mean_cls: 0.043697, grad_norm: 0.211868
[[34m2025-10-04 12:41:49[0m] Step: 8861, Training Logs: loss_final: 0.869940, loss_mean: 0.825552, loss_mean_cls: 0.044388, grad_norm: 0.379478
[[34m2025-10-04 12:41:49[0m] Step: 8862, Training Logs: loss_final: 0.866030, loss_mean: 0.822333, loss_mean_cls: 0.043697, grad_norm: 0.251676
[[34m2025-10-04 12:41:49[0m] Step: 8863, Training Logs: loss_final: 0.881994, loss_mean: 0.838534, loss_mean_cls: 0.043461, grad_norm: 0.375956
[[34m2025-10-04 12:41:50[0m] Step: 8864, Training Logs: loss_final: 0.888755, loss_mean: 0.844946, loss_mean_cls: 0.043809, grad_norm: 0.214592
[[34m2025-10-04 12:41:50[0m] Step: 8865, Training Logs: loss_final: 0.867019, loss_mean: 0.824077, loss_mean_cls: 0.042942, grad_norm: 0.512706
[[34m2025-10-04 12:41:50[0m] Step: 8866, Training Logs: loss_final: 0.888823, loss_mean: 0.846946, loss_mean_cls: 0.041877, grad_norm: 0.391404
[[34m2025-10-04 12:41:51[0m] Step: 8867, Training Logs: loss_final: 0.877439, loss_mean: 0.835110, loss_mean_cls: 0.042329, grad_norm: 0.300686
[[34m2025-10-04 12:41:51[0m] Step: 8868, Training Logs: loss_final: 0.872454, loss_mean: 0.829350, loss_mean_cls: 0.043104, grad_norm: 0.400819
[[34m2025-10-04 12:41:51[0m] Step: 8869, Training Logs: loss_final: 0.868384, loss_mean: 0.825931, loss_mean_cls: 0.042453, grad_norm: 0.239319
[[34m2025-10-04 12:41:51[0m] Step: 8870, Training Logs: loss_final: 0.873325, loss_mean: 0.829584, loss_mean_cls: 0.043741, grad_norm: 0.310341
[[34m2025-10-04 12:41:52[0m] Step: 8871, Training Logs: loss_final: 0.877525, loss_mean: 0.833550, loss_mean_cls: 0.043975, grad_norm: 0.282735
[[34m2025-10-04 12:41:52[0m] Step: 8872, Training Logs: loss_final: 0.875122, loss_mean: 0.832168, loss_mean_cls: 0.042954, grad_norm: 0.373046
[[34m2025-10-04 12:41:52[0m] Step: 8873, Training Logs: loss_final: 0.874476, loss_mean: 0.831131, loss_mean_cls: 0.043345, grad_norm: 0.233708
[[34m2025-10-04 12:41:53[0m] Step: 8874, Training Logs: loss_final: 0.875552, loss_mean: 0.832983, loss_mean_cls: 0.042569, grad_norm: 0.407098
[[34m2025-10-04 12:41:53[0m] Step: 8875, Training Logs: loss_final: 0.862967, loss_mean: 0.819468, loss_mean_cls: 0.043500, grad_norm: 0.240646
[[34m2025-10-04 12:41:53[0m] Step: 8876, Training Logs: loss_final: 0.865476, loss_mean: 0.823053, loss_mean_cls: 0.042423, grad_norm: 0.403276
[[34m2025-10-04 12:41:53[0m] Step: 8877, Training Logs: loss_final: 0.868033, loss_mean: 0.823500, loss_mean_cls: 0.044533, grad_norm: 0.288122
[[34m2025-10-04 12:41:54[0m] Step: 8878, Training Logs: loss_final: 0.890104, loss_mean: 0.847406, loss_mean_cls: 0.042698, grad_norm: 0.370016
[[34m2025-10-04 12:41:54[0m] Step: 8879, Training Logs: loss_final: 0.869085, loss_mean: 0.827102, loss_mean_cls: 0.041983, grad_norm: 0.288350
[[34m2025-10-04 12:41:54[0m] Step: 8880, Training Logs: loss_final: 0.879910, loss_mean: 0.839085, loss_mean_cls: 0.040825, grad_norm: 0.322336
[[34m2025-10-04 12:41:55[0m] Step: 8881, Training Logs: loss_final: 0.875014, loss_mean: 0.832471, loss_mean_cls: 0.042543, grad_norm: 0.263953
[[34m2025-10-04 12:41:55[0m] Step: 8882, Training Logs: loss_final: 0.868546, loss_mean: 0.826065, loss_mean_cls: 0.042481, grad_norm: 0.359141
[[34m2025-10-04 12:41:55[0m] Step: 8883, Training Logs: loss_final: 0.870813, loss_mean: 0.827945, loss_mean_cls: 0.042868, grad_norm: 0.364764
[[34m2025-10-04 12:41:56[0m] Step: 8884, Training Logs: loss_final: 0.871450, loss_mean: 0.827076, loss_mean_cls: 0.044374, grad_norm: 0.289743
[[34m2025-10-04 12:41:56[0m] Step: 8885, Training Logs: loss_final: 0.887937, loss_mean: 0.845280, loss_mean_cls: 0.042657, grad_norm: 0.174284
[[34m2025-10-04 12:41:56[0m] Step: 8886, Training Logs: loss_final: 0.882215, loss_mean: 0.840291, loss_mean_cls: 0.041924, grad_norm: 0.346571
[[34m2025-10-04 12:41:56[0m] Step: 8887, Training Logs: loss_final: 0.857641, loss_mean: 0.813151, loss_mean_cls: 0.044490, grad_norm: 0.272920
[[34m2025-10-04 12:41:57[0m] Step: 8888, Training Logs: loss_final: 0.850699, loss_mean: 0.807887, loss_mean_cls: 0.042812, grad_norm: 0.309222
[[34m2025-10-04 12:41:57[0m] Step: 8889, Training Logs: loss_final: 0.864224, loss_mean: 0.820657, loss_mean_cls: 0.043568, grad_norm: 0.179726
[[34m2025-10-04 12:41:57[0m] Step: 8890, Training Logs: loss_final: 0.868620, loss_mean: 0.825319, loss_mean_cls: 0.043301, grad_norm: 0.236363
[[34m2025-10-04 12:41:58[0m] Step: 8891, Training Logs: loss_final: 0.855333, loss_mean: 0.810853, loss_mean_cls: 0.044480, grad_norm: 0.175081
[[34m2025-10-04 12:41:58[0m] Step: 8892, Training Logs: loss_final: 0.873325, loss_mean: 0.830147, loss_mean_cls: 0.043178, grad_norm: 0.233387
[[34m2025-10-04 12:41:58[0m] Step: 8893, Training Logs: loss_final: 0.870460, loss_mean: 0.828280, loss_mean_cls: 0.042180, grad_norm: 0.197937
[[34m2025-10-04 12:41:58[0m] Step: 8894, Training Logs: loss_final: 0.848083, loss_mean: 0.804649, loss_mean_cls: 0.043434, grad_norm: 0.175230
[[34m2025-10-04 12:41:59[0m] Step: 8895, Training Logs: loss_final: 0.879079, loss_mean: 0.836302, loss_mean_cls: 0.042777, grad_norm: 0.212082
[[34m2025-10-04 12:41:59[0m] Step: 8896, Training Logs: loss_final: 0.878681, loss_mean: 0.834523, loss_mean_cls: 0.044159, grad_norm: 0.279023
[[34m2025-10-04 12:41:59[0m] Step: 8897, Training Logs: loss_final: 0.845471, loss_mean: 0.800702, loss_mean_cls: 0.044769, grad_norm: 0.191649
[[34m2025-10-04 12:42:00[0m] Step: 8898, Training Logs: loss_final: 0.870144, loss_mean: 0.826937, loss_mean_cls: 0.043206, grad_norm: 0.244830
[[34m2025-10-04 12:42:00[0m] Step: 8899, Training Logs: loss_final: 0.877442, loss_mean: 0.834236, loss_mean_cls: 0.043206, grad_norm: 0.327046
[[34m2025-10-04 12:42:00[0m] Step: 8900, Training Logs: loss_final: 0.850762, loss_mean: 0.807547, loss_mean_cls: 0.043215, grad_norm: 0.192576
[[34m2025-10-04 12:42:01[0m] Step: 8901, Training Logs: loss_final: 0.854023, loss_mean: 0.809782, loss_mean_cls: 0.044241, grad_norm: 0.252469
[[34m2025-10-04 12:42:01[0m] Step: 8902, Training Logs: loss_final: 0.862138, loss_mean: 0.817573, loss_mean_cls: 0.044565, grad_norm: 0.392872
[[34m2025-10-04 12:42:01[0m] Step: 8903, Training Logs: loss_final: 0.883489, loss_mean: 0.840762, loss_mean_cls: 0.042727, grad_norm: 0.212708
[[34m2025-10-04 12:42:01[0m] Step: 8904, Training Logs: loss_final: 0.865773, loss_mean: 0.823304, loss_mean_cls: 0.042469, grad_norm: 0.263141
[[34m2025-10-04 12:42:02[0m] Step: 8905, Training Logs: loss_final: 0.867594, loss_mean: 0.824784, loss_mean_cls: 0.042809, grad_norm: 0.284157
[[34m2025-10-04 12:42:02[0m] Step: 8906, Training Logs: loss_final: 0.850509, loss_mean: 0.807518, loss_mean_cls: 0.042991, grad_norm: 0.174286
[[34m2025-10-04 12:42:02[0m] Step: 8907, Training Logs: loss_final: 0.884214, loss_mean: 0.841188, loss_mean_cls: 0.043026, grad_norm: 0.283105
[[34m2025-10-04 12:42:03[0m] Step: 8908, Training Logs: loss_final: 0.857663, loss_mean: 0.814559, loss_mean_cls: 0.043104, grad_norm: 0.243619
[[34m2025-10-04 12:42:03[0m] Step: 8909, Training Logs: loss_final: 0.890152, loss_mean: 0.847903, loss_mean_cls: 0.042249, grad_norm: 0.278304
[[34m2025-10-04 12:42:03[0m] Step: 8910, Training Logs: loss_final: 0.884228, loss_mean: 0.841325, loss_mean_cls: 0.042903, grad_norm: 0.172778
[[34m2025-10-04 12:42:04[0m] Step: 8911, Training Logs: loss_final: 0.870237, loss_mean: 0.827723, loss_mean_cls: 0.042514, grad_norm: 0.203953
[[34m2025-10-04 12:42:04[0m] Step: 8912, Training Logs: loss_final: 0.862890, loss_mean: 0.818335, loss_mean_cls: 0.044555, grad_norm: 0.235531
[[34m2025-10-04 12:42:04[0m] Step: 8913, Training Logs: loss_final: 0.879714, loss_mean: 0.837723, loss_mean_cls: 0.041991, grad_norm: 0.321213
[[34m2025-10-04 12:42:04[0m] Step: 8914, Training Logs: loss_final: 0.869987, loss_mean: 0.825557, loss_mean_cls: 0.044429, grad_norm: 0.404425
[[34m2025-10-04 12:42:05[0m] Step: 8915, Training Logs: loss_final: 0.885504, loss_mean: 0.842839, loss_mean_cls: 0.042666, grad_norm: 0.256302
[[34m2025-10-04 12:42:05[0m] Step: 8916, Training Logs: loss_final: 0.884435, loss_mean: 0.840719, loss_mean_cls: 0.043716, grad_norm: 0.315802
[[34m2025-10-04 12:42:05[0m] Step: 8917, Training Logs: loss_final: 0.888642, loss_mean: 0.845930, loss_mean_cls: 0.042712, grad_norm: 0.242815
[[34m2025-10-04 12:42:06[0m] Step: 8918, Training Logs: loss_final: 0.878058, loss_mean: 0.836015, loss_mean_cls: 0.042043, grad_norm: 0.495623
[[34m2025-10-04 12:42:06[0m] Step: 8919, Training Logs: loss_final: 0.869275, loss_mean: 0.825438, loss_mean_cls: 0.043838, grad_norm: 0.278177
[[34m2025-10-04 12:42:06[0m] Step: 8920, Training Logs: loss_final: 0.862832, loss_mean: 0.818931, loss_mean_cls: 0.043901, grad_norm: 0.337080
[[34m2025-10-04 12:42:06[0m] Step: 8921, Training Logs: loss_final: 0.877306, loss_mean: 0.834458, loss_mean_cls: 0.042848, grad_norm: 0.275698
[[34m2025-10-04 12:42:07[0m] Step: 8922, Training Logs: loss_final: 0.875025, loss_mean: 0.832015, loss_mean_cls: 0.043011, grad_norm: 0.402716
[[34m2025-10-04 12:42:07[0m] Step: 8923, Training Logs: loss_final: 0.855748, loss_mean: 0.811178, loss_mean_cls: 0.044570, grad_norm: 0.246249
[[34m2025-10-04 12:42:07[0m] Step: 8924, Training Logs: loss_final: 0.870758, loss_mean: 0.827103, loss_mean_cls: 0.043655, grad_norm: 0.394202
[[34m2025-10-04 12:42:08[0m] Step: 8925, Training Logs: loss_final: 0.887593, loss_mean: 0.844171, loss_mean_cls: 0.043423, grad_norm: 0.304473
[[34m2025-10-04 12:42:08[0m] Step: 8926, Training Logs: loss_final: 0.879949, loss_mean: 0.837925, loss_mean_cls: 0.042024, grad_norm: 0.357943
[[34m2025-10-04 12:42:08[0m] Step: 8927, Training Logs: loss_final: 0.867933, loss_mean: 0.823116, loss_mean_cls: 0.044817, grad_norm: 0.265752
[[34m2025-10-04 12:42:09[0m] Step: 8928, Training Logs: loss_final: 0.883871, loss_mean: 0.841629, loss_mean_cls: 0.042242, grad_norm: 0.476600
[[34m2025-10-04 12:42:09[0m] Step: 8929, Training Logs: loss_final: 0.875733, loss_mean: 0.833126, loss_mean_cls: 0.042606, grad_norm: 0.389181
[[34m2025-10-04 12:42:09[0m] Step: 8930, Training Logs: loss_final: 0.871173, loss_mean: 0.828862, loss_mean_cls: 0.042311, grad_norm: 0.271345
[[34m2025-10-04 12:42:09[0m] Step: 8931, Training Logs: loss_final: 0.857462, loss_mean: 0.814321, loss_mean_cls: 0.043141, grad_norm: 0.375391
[[34m2025-10-04 12:42:10[0m] Step: 8932, Training Logs: loss_final: 0.860538, loss_mean: 0.816319, loss_mean_cls: 0.044218, grad_norm: 0.224834
[[34m2025-10-04 12:42:10[0m] Step: 8933, Training Logs: loss_final: 0.852011, loss_mean: 0.807623, loss_mean_cls: 0.044388, grad_norm: 0.332327
[[34m2025-10-04 12:42:10[0m] Step: 8934, Training Logs: loss_final: 0.873473, loss_mean: 0.830441, loss_mean_cls: 0.043032, grad_norm: 0.302274
[[34m2025-10-04 12:42:11[0m] Step: 8935, Training Logs: loss_final: 0.846010, loss_mean: 0.802328, loss_mean_cls: 0.043682, grad_norm: 0.297418
[[34m2025-10-04 12:42:11[0m] Step: 8936, Training Logs: loss_final: 0.861217, loss_mean: 0.818364, loss_mean_cls: 0.042853, grad_norm: 0.238809
[[34m2025-10-04 12:42:11[0m] Step: 8937, Training Logs: loss_final: 0.887925, loss_mean: 0.843985, loss_mean_cls: 0.043941, grad_norm: 0.333234
[[34m2025-10-04 12:42:12[0m] Step: 8938, Training Logs: loss_final: 0.848893, loss_mean: 0.805785, loss_mean_cls: 0.043109, grad_norm: 0.267685
[[34m2025-10-04 12:42:12[0m] Step: 8939, Training Logs: loss_final: 0.886468, loss_mean: 0.842253, loss_mean_cls: 0.044215, grad_norm: 0.312085
[[34m2025-10-04 12:42:12[0m] Step: 8940, Training Logs: loss_final: 0.872735, loss_mean: 0.829209, loss_mean_cls: 0.043526, grad_norm: 0.300317
[[34m2025-10-04 12:42:12[0m] Step: 8941, Training Logs: loss_final: 0.864428, loss_mean: 0.821030, loss_mean_cls: 0.043398, grad_norm: 0.237682
[[34m2025-10-04 12:42:13[0m] Step: 8942, Training Logs: loss_final: 0.869938, loss_mean: 0.826433, loss_mean_cls: 0.043505, grad_norm: 0.275704
[[34m2025-10-04 12:42:13[0m] Step: 8943, Training Logs: loss_final: 0.856388, loss_mean: 0.813006, loss_mean_cls: 0.043383, grad_norm: 0.226138
[[34m2025-10-04 12:42:13[0m] Step: 8944, Training Logs: loss_final: 0.874751, loss_mean: 0.831097, loss_mean_cls: 0.043655, grad_norm: 0.299541
[[34m2025-10-04 12:42:14[0m] Step: 8945, Training Logs: loss_final: 0.888620, loss_mean: 0.845337, loss_mean_cls: 0.043284, grad_norm: 0.336618
[[34m2025-10-04 12:42:14[0m] Step: 8946, Training Logs: loss_final: 0.873054, loss_mean: 0.829350, loss_mean_cls: 0.043704, grad_norm: 0.244964
[[34m2025-10-04 12:42:14[0m] Step: 8947, Training Logs: loss_final: 0.860314, loss_mean: 0.816822, loss_mean_cls: 0.043493, grad_norm: 0.316657
[[34m2025-10-04 12:42:15[0m] Step: 8948, Training Logs: loss_final: 0.873391, loss_mean: 0.831417, loss_mean_cls: 0.041973, grad_norm: 0.236898
[[34m2025-10-04 12:42:15[0m] Step: 8949, Training Logs: loss_final: 0.853060, loss_mean: 0.808637, loss_mean_cls: 0.044424, grad_norm: 0.310797
[[34m2025-10-04 12:42:15[0m] Step: 8950, Training Logs: loss_final: 0.889292, loss_mean: 0.847231, loss_mean_cls: 0.042061, grad_norm: 0.240578
[[34m2025-10-04 12:42:15[0m] Step: 8951, Training Logs: loss_final: 0.864965, loss_mean: 0.820940, loss_mean_cls: 0.044025, grad_norm: 0.221126
[[34m2025-10-04 12:42:16[0m] Step: 8952, Training Logs: loss_final: 0.884113, loss_mean: 0.841282, loss_mean_cls: 0.042831, grad_norm: 0.219970
[[34m2025-10-04 12:42:16[0m] Step: 8953, Training Logs: loss_final: 0.889082, loss_mean: 0.846344, loss_mean_cls: 0.042738, grad_norm: 0.436800
[[34m2025-10-04 12:42:16[0m] Step: 8954, Training Logs: loss_final: 0.843575, loss_mean: 0.800384, loss_mean_cls: 0.043191, grad_norm: 0.225956
[[34m2025-10-04 12:42:17[0m] Step: 8955, Training Logs: loss_final: 0.885850, loss_mean: 0.841393, loss_mean_cls: 0.044458, grad_norm: 0.410242
[[34m2025-10-04 12:42:17[0m] Step: 8956, Training Logs: loss_final: 0.893234, loss_mean: 0.852311, loss_mean_cls: 0.040923, grad_norm: 0.187508
[[34m2025-10-04 12:42:17[0m] Step: 8957, Training Logs: loss_final: 0.873020, loss_mean: 0.831467, loss_mean_cls: 0.041553, grad_norm: 0.326361
[[34m2025-10-04 12:42:17[0m] Step: 8958, Training Logs: loss_final: 0.858932, loss_mean: 0.815303, loss_mean_cls: 0.043629, grad_norm: 0.220320
[[34m2025-10-04 12:42:18[0m] Step: 8959, Training Logs: loss_final: 0.873019, loss_mean: 0.829045, loss_mean_cls: 0.043974, grad_norm: 0.290788
[[34m2025-10-04 12:42:18[0m] Step: 8960, Training Logs: loss_final: 0.883354, loss_mean: 0.840429, loss_mean_cls: 0.042926, grad_norm: 0.237086
[[34m2025-10-04 12:42:18[0m] Step: 8961, Training Logs: loss_final: 0.874963, loss_mean: 0.831664, loss_mean_cls: 0.043300, grad_norm: 0.287374
[[34m2025-10-04 12:42:19[0m] Step: 8962, Training Logs: loss_final: 0.874975, loss_mean: 0.831484, loss_mean_cls: 0.043490, grad_norm: 0.161162
[[34m2025-10-04 12:42:19[0m] Step: 8963, Training Logs: loss_final: 0.883230, loss_mean: 0.841553, loss_mean_cls: 0.041677, grad_norm: 0.258926
[[34m2025-10-04 12:42:19[0m] Step: 8964, Training Logs: loss_final: 0.884611, loss_mean: 0.841096, loss_mean_cls: 0.043515, grad_norm: 0.247099
[[34m2025-10-04 12:42:20[0m] Step: 8965, Training Logs: loss_final: 0.872624, loss_mean: 0.829596, loss_mean_cls: 0.043028, grad_norm: 0.381823
[[34m2025-10-04 12:42:20[0m] Step: 8966, Training Logs: loss_final: 0.869874, loss_mean: 0.826204, loss_mean_cls: 0.043670, grad_norm: 0.225674
[[34m2025-10-04 12:42:20[0m] Step: 8967, Training Logs: loss_final: 0.884190, loss_mean: 0.840470, loss_mean_cls: 0.043720, grad_norm: 0.260963
[[34m2025-10-04 12:42:20[0m] Step: 8968, Training Logs: loss_final: 0.882260, loss_mean: 0.840528, loss_mean_cls: 0.041732, grad_norm: 0.249488
[[34m2025-10-04 12:42:21[0m] Step: 8969, Training Logs: loss_final: 0.882215, loss_mean: 0.840228, loss_mean_cls: 0.041987, grad_norm: 0.290522
[[34m2025-10-04 12:42:21[0m] Step: 8970, Training Logs: loss_final: 0.868006, loss_mean: 0.824095, loss_mean_cls: 0.043912, grad_norm: 0.279861
[[34m2025-10-04 12:42:21[0m] Step: 8971, Training Logs: loss_final: 0.886494, loss_mean: 0.844629, loss_mean_cls: 0.041866, grad_norm: 0.365649
[[34m2025-10-04 12:42:22[0m] Step: 8972, Training Logs: loss_final: 0.871343, loss_mean: 0.828308, loss_mean_cls: 0.043035, grad_norm: 0.294801
[[34m2025-10-04 12:42:22[0m] Step: 8973, Training Logs: loss_final: 0.870058, loss_mean: 0.827489, loss_mean_cls: 0.042569, grad_norm: 0.338308
[[34m2025-10-04 12:42:22[0m] Step: 8974, Training Logs: loss_final: 0.875176, loss_mean: 0.831887, loss_mean_cls: 0.043289, grad_norm: 0.293511
[[34m2025-10-04 12:42:23[0m] Step: 8975, Training Logs: loss_final: 0.884754, loss_mean: 0.842972, loss_mean_cls: 0.041783, grad_norm: 0.242020
[[34m2025-10-04 12:42:23[0m] Step: 8976, Training Logs: loss_final: 0.869551, loss_mean: 0.825883, loss_mean_cls: 0.043668, grad_norm: 0.285247
[[34m2025-10-04 12:42:23[0m] Step: 8977, Training Logs: loss_final: 0.854453, loss_mean: 0.810253, loss_mean_cls: 0.044200, grad_norm: 0.294621
[[34m2025-10-04 12:42:23[0m] Step: 8978, Training Logs: loss_final: 0.872240, loss_mean: 0.829666, loss_mean_cls: 0.042574, grad_norm: 0.445928
[[34m2025-10-04 12:42:24[0m] Step: 8979, Training Logs: loss_final: 0.875989, loss_mean: 0.833417, loss_mean_cls: 0.042572, grad_norm: 0.161798
[[34m2025-10-04 12:42:24[0m] Step: 8980, Training Logs: loss_final: 0.853083, loss_mean: 0.808941, loss_mean_cls: 0.044142, grad_norm: 0.294988
[[34m2025-10-04 12:42:24[0m] Step: 8981, Training Logs: loss_final: 0.869255, loss_mean: 0.826227, loss_mean_cls: 0.043028, grad_norm: 0.204391
[[34m2025-10-04 12:42:25[0m] Step: 8982, Training Logs: loss_final: 0.883280, loss_mean: 0.839754, loss_mean_cls: 0.043526, grad_norm: 0.226553
[[34m2025-10-04 12:42:25[0m] Step: 8983, Training Logs: loss_final: 0.869002, loss_mean: 0.825305, loss_mean_cls: 0.043697, grad_norm: 0.264912
[[34m2025-10-04 12:42:25[0m] Step: 8984, Training Logs: loss_final: 0.869287, loss_mean: 0.826622, loss_mean_cls: 0.042666, grad_norm: 0.227959
[[34m2025-10-04 12:42:26[0m] Step: 8985, Training Logs: loss_final: 0.867188, loss_mean: 0.825216, loss_mean_cls: 0.041971, grad_norm: 0.230650
[[34m2025-10-04 12:42:26[0m] Step: 8986, Training Logs: loss_final: 0.871282, loss_mean: 0.827341, loss_mean_cls: 0.043940, grad_norm: 0.187251
[[34m2025-10-04 12:42:26[0m] Step: 8987, Training Logs: loss_final: 0.867953, loss_mean: 0.823884, loss_mean_cls: 0.044069, grad_norm: 0.197378
[[34m2025-10-04 12:42:26[0m] Step: 8988, Training Logs: loss_final: 0.875674, loss_mean: 0.832874, loss_mean_cls: 0.042800, grad_norm: 0.249737
[[34m2025-10-04 12:42:27[0m] Step: 8989, Training Logs: loss_final: 0.881058, loss_mean: 0.837535, loss_mean_cls: 0.043522, grad_norm: 0.178085
[[34m2025-10-04 12:42:27[0m] Step: 8990, Training Logs: loss_final: 0.883144, loss_mean: 0.839867, loss_mean_cls: 0.043277, grad_norm: 0.189450
[[34m2025-10-04 12:42:27[0m] Step: 8991, Training Logs: loss_final: 0.872938, loss_mean: 0.829357, loss_mean_cls: 0.043581, grad_norm: 0.218524
[[34m2025-10-04 12:42:28[0m] Step: 8992, Training Logs: loss_final: 0.848300, loss_mean: 0.805109, loss_mean_cls: 0.043192, grad_norm: 0.206395
[[34m2025-10-04 12:42:28[0m] Step: 8993, Training Logs: loss_final: 0.869246, loss_mean: 0.826551, loss_mean_cls: 0.042694, grad_norm: 0.387557
[[34m2025-10-04 12:42:28[0m] Step: 8994, Training Logs: loss_final: 0.862556, loss_mean: 0.818327, loss_mean_cls: 0.044229, grad_norm: 0.196102
[[34m2025-10-04 12:42:28[0m] Step: 8995, Training Logs: loss_final: 0.878402, loss_mean: 0.836342, loss_mean_cls: 0.042060, grad_norm: 0.224922
[[34m2025-10-04 12:42:29[0m] Step: 8996, Training Logs: loss_final: 0.865626, loss_mean: 0.822262, loss_mean_cls: 0.043364, grad_norm: 0.232818
[[34m2025-10-04 12:42:29[0m] Step: 8997, Training Logs: loss_final: 0.872589, loss_mean: 0.829670, loss_mean_cls: 0.042918, grad_norm: 0.205528
[[34m2025-10-04 12:42:29[0m] Step: 8998, Training Logs: loss_final: 0.872234, loss_mean: 0.828802, loss_mean_cls: 0.043433, grad_norm: 0.281679
[[34m2025-10-04 12:42:30[0m] Step: 8999, Training Logs: loss_final: 0.847128, loss_mean: 0.802322, loss_mean_cls: 0.044806, grad_norm: 0.193705
[[34m2025-10-04 12:42:30[0m] Step: 9000, Training Logs: loss_final: 0.878697, loss_mean: 0.835912, loss_mean_cls: 0.042784, grad_norm: 0.308736
[[34m2025-10-04 12:42:30[0m] Step: 9001, Training Logs: loss_final: 0.880988, loss_mean: 0.839364, loss_mean_cls: 0.041623, grad_norm: 0.200344
[[34m2025-10-04 12:42:31[0m] Step: 9002, Training Logs: loss_final: 0.883368, loss_mean: 0.840071, loss_mean_cls: 0.043297, grad_norm: 0.189529
[[34m2025-10-04 12:42:31[0m] Step: 9003, Training Logs: loss_final: 0.874542, loss_mean: 0.831653, loss_mean_cls: 0.042889, grad_norm: 0.260400
[[34m2025-10-04 12:42:31[0m] Step: 9004, Training Logs: loss_final: 0.858908, loss_mean: 0.817148, loss_mean_cls: 0.041760, grad_norm: 0.239163
[[34m2025-10-04 12:42:31[0m] Step: 9005, Training Logs: loss_final: 0.845058, loss_mean: 0.801320, loss_mean_cls: 0.043738, grad_norm: 0.278840
[[34m2025-10-04 12:42:32[0m] Step: 9006, Training Logs: loss_final: 0.888243, loss_mean: 0.845464, loss_mean_cls: 0.042779, grad_norm: 0.211597
[[34m2025-10-04 12:42:32[0m] Step: 9007, Training Logs: loss_final: 0.872617, loss_mean: 0.830592, loss_mean_cls: 0.042025, grad_norm: 0.255264
[[34m2025-10-04 12:42:32[0m] Step: 9008, Training Logs: loss_final: 0.870166, loss_mean: 0.827524, loss_mean_cls: 0.042643, grad_norm: 0.269698
[[34m2025-10-04 12:42:33[0m] Step: 9009, Training Logs: loss_final: 0.880992, loss_mean: 0.837862, loss_mean_cls: 0.043130, grad_norm: 0.277662
[[34m2025-10-04 12:42:33[0m] Step: 9010, Training Logs: loss_final: 0.873654, loss_mean: 0.830482, loss_mean_cls: 0.043172, grad_norm: 0.267699
[[34m2025-10-04 12:42:33[0m] Step: 9011, Training Logs: loss_final: 0.877072, loss_mean: 0.834475, loss_mean_cls: 0.042597, grad_norm: 0.294731
[[34m2025-10-04 12:42:33[0m] Step: 9012, Training Logs: loss_final: 0.887056, loss_mean: 0.844031, loss_mean_cls: 0.043025, grad_norm: 0.305187
[[34m2025-10-04 12:42:34[0m] Step: 9013, Training Logs: loss_final: 0.862304, loss_mean: 0.819341, loss_mean_cls: 0.042963, grad_norm: 0.217777
[[34m2025-10-04 12:42:34[0m] Step: 9014, Training Logs: loss_final: 0.876632, loss_mean: 0.833433, loss_mean_cls: 0.043200, grad_norm: 0.180662
[[34m2025-10-04 12:42:34[0m] Step: 9015, Training Logs: loss_final: 0.881619, loss_mean: 0.838993, loss_mean_cls: 0.042626, grad_norm: 0.250641
[[34m2025-10-04 12:42:35[0m] Step: 9016, Training Logs: loss_final: 0.860124, loss_mean: 0.816355, loss_mean_cls: 0.043769, grad_norm: 0.256548
[[34m2025-10-04 12:42:35[0m] Step: 9017, Training Logs: loss_final: 0.862649, loss_mean: 0.819111, loss_mean_cls: 0.043538, grad_norm: 0.260572
[[34m2025-10-04 12:42:35[0m] Step: 9018, Training Logs: loss_final: 0.893370, loss_mean: 0.850657, loss_mean_cls: 0.042713, grad_norm: 0.216602
[[34m2025-10-04 12:42:36[0m] Step: 9019, Training Logs: loss_final: 0.875738, loss_mean: 0.832430, loss_mean_cls: 0.043307, grad_norm: 0.343442
[[34m2025-10-04 12:42:36[0m] Step: 9020, Training Logs: loss_final: 0.879554, loss_mean: 0.835828, loss_mean_cls: 0.043726, grad_norm: 0.284024
[[34m2025-10-04 12:42:36[0m] Step: 9021, Training Logs: loss_final: 0.878298, loss_mean: 0.837147, loss_mean_cls: 0.041150, grad_norm: 0.250996
[[34m2025-10-04 12:42:36[0m] Step: 9022, Training Logs: loss_final: 0.884086, loss_mean: 0.840463, loss_mean_cls: 0.043624, grad_norm: 0.247269
[[34m2025-10-04 12:42:37[0m] Step: 9023, Training Logs: loss_final: 0.894968, loss_mean: 0.852578, loss_mean_cls: 0.042390, grad_norm: 0.236177
[[34m2025-10-04 12:42:37[0m] Step: 9024, Training Logs: loss_final: 0.884708, loss_mean: 0.841235, loss_mean_cls: 0.043473, grad_norm: 0.432787
[[34m2025-10-04 12:42:37[0m] Step: 9025, Training Logs: loss_final: 0.880963, loss_mean: 0.838376, loss_mean_cls: 0.042586, grad_norm: 0.221744
[[34m2025-10-04 12:42:38[0m] Step: 9026, Training Logs: loss_final: 0.872285, loss_mean: 0.829532, loss_mean_cls: 0.042753, grad_norm: 0.291784
[[34m2025-10-04 12:42:38[0m] Step: 9027, Training Logs: loss_final: 0.875127, loss_mean: 0.832219, loss_mean_cls: 0.042908, grad_norm: 0.196407
[[34m2025-10-04 12:42:38[0m] Step: 9028, Training Logs: loss_final: 0.889894, loss_mean: 0.847778, loss_mean_cls: 0.042116, grad_norm: 0.230035
[[34m2025-10-04 12:42:38[0m] Step: 9029, Training Logs: loss_final: 0.860664, loss_mean: 0.816222, loss_mean_cls: 0.044442, grad_norm: 0.185253
[[34m2025-10-04 12:42:39[0m] Step: 9030, Training Logs: loss_final: 0.865865, loss_mean: 0.821505, loss_mean_cls: 0.044360, grad_norm: 0.201804
[[34m2025-10-04 12:42:39[0m] Step: 9031, Training Logs: loss_final: 0.870953, loss_mean: 0.826821, loss_mean_cls: 0.044132, grad_norm: 0.247205
[[34m2025-10-04 12:42:39[0m] Step: 9032, Training Logs: loss_final: 0.877679, loss_mean: 0.834238, loss_mean_cls: 0.043441, grad_norm: 0.204305
[[34m2025-10-04 12:42:40[0m] Step: 9033, Training Logs: loss_final: 0.894395, loss_mean: 0.850289, loss_mean_cls: 0.044107, grad_norm: 0.212651
[[34m2025-10-04 12:42:40[0m] Step: 9034, Training Logs: loss_final: 0.869326, loss_mean: 0.825943, loss_mean_cls: 0.043384, grad_norm: 0.335836
[[34m2025-10-04 12:42:40[0m] Step: 9035, Training Logs: loss_final: 0.878345, loss_mean: 0.835644, loss_mean_cls: 0.042701, grad_norm: 0.323570
[[34m2025-10-04 12:42:40[0m] Step: 9036, Training Logs: loss_final: 0.868518, loss_mean: 0.825219, loss_mean_cls: 0.043298, grad_norm: 0.242176
[[34m2025-10-04 12:42:41[0m] Step: 9037, Training Logs: loss_final: 0.888201, loss_mean: 0.846884, loss_mean_cls: 0.041317, grad_norm: 0.353047
[[34m2025-10-04 12:42:41[0m] Step: 9038, Training Logs: loss_final: 0.890087, loss_mean: 0.848172, loss_mean_cls: 0.041915, grad_norm: 0.228356
[[34m2025-10-04 12:42:41[0m] Step: 9039, Training Logs: loss_final: 0.878200, loss_mean: 0.834765, loss_mean_cls: 0.043435, grad_norm: 0.238088
[[34m2025-10-04 12:42:42[0m] Step: 9040, Training Logs: loss_final: 0.880586, loss_mean: 0.837603, loss_mean_cls: 0.042983, grad_norm: 0.192874
[[34m2025-10-04 12:42:42[0m] Step: 9041, Training Logs: loss_final: 0.866932, loss_mean: 0.823557, loss_mean_cls: 0.043375, grad_norm: 0.253828
[[34m2025-10-04 12:42:42[0m] Step: 9042, Training Logs: loss_final: 0.861921, loss_mean: 0.818449, loss_mean_cls: 0.043472, grad_norm: 0.221788
[[34m2025-10-04 12:42:43[0m] Step: 9043, Training Logs: loss_final: 0.874812, loss_mean: 0.832389, loss_mean_cls: 0.042423, grad_norm: 0.165681
[[34m2025-10-04 12:42:43[0m] Step: 9044, Training Logs: loss_final: 0.869453, loss_mean: 0.826799, loss_mean_cls: 0.042654, grad_norm: 0.211414
[[34m2025-10-04 12:42:43[0m] Step: 9045, Training Logs: loss_final: 0.891707, loss_mean: 0.847897, loss_mean_cls: 0.043809, grad_norm: 0.374137
[[34m2025-10-04 12:42:43[0m] Step: 9046, Training Logs: loss_final: 0.894466, loss_mean: 0.851543, loss_mean_cls: 0.042923, grad_norm: 0.292518
[[34m2025-10-04 12:42:44[0m] Step: 9047, Training Logs: loss_final: 0.876296, loss_mean: 0.833324, loss_mean_cls: 0.042972, grad_norm: 0.199523
[[34m2025-10-04 12:42:44[0m] Step: 9048, Training Logs: loss_final: 0.899082, loss_mean: 0.855769, loss_mean_cls: 0.043313, grad_norm: 0.333824
[[34m2025-10-04 12:42:44[0m] Step: 9049, Training Logs: loss_final: 0.874317, loss_mean: 0.831129, loss_mean_cls: 0.043188, grad_norm: 0.299951
[[34m2025-10-04 12:42:45[0m] Step: 9050, Training Logs: loss_final: 0.906190, loss_mean: 0.864160, loss_mean_cls: 0.042030, grad_norm: 0.239885
[[34m2025-10-04 12:42:45[0m] Step: 9051, Training Logs: loss_final: 0.869212, loss_mean: 0.824732, loss_mean_cls: 0.044479, grad_norm: 0.249656
[[34m2025-10-04 12:42:45[0m] Step: 9052, Training Logs: loss_final: 0.886049, loss_mean: 0.842737, loss_mean_cls: 0.043312, grad_norm: 0.199777
[[34m2025-10-04 12:42:46[0m] Step: 9053, Training Logs: loss_final: 0.883499, loss_mean: 0.840278, loss_mean_cls: 0.043221, grad_norm: 0.213279
[[34m2025-10-04 12:42:46[0m] Step: 9054, Training Logs: loss_final: 0.875362, loss_mean: 0.832670, loss_mean_cls: 0.042692, grad_norm: 0.228571
[[34m2025-10-04 12:42:46[0m] Step: 9055, Training Logs: loss_final: 0.867362, loss_mean: 0.823774, loss_mean_cls: 0.043588, grad_norm: 0.226915
[[34m2025-10-04 12:42:46[0m] Step: 9056, Training Logs: loss_final: 0.869906, loss_mean: 0.827947, loss_mean_cls: 0.041959, grad_norm: 0.226032
[[34m2025-10-04 12:42:47[0m] Step: 9057, Training Logs: loss_final: 0.886588, loss_mean: 0.844113, loss_mean_cls: 0.042475, grad_norm: 0.245201
[[34m2025-10-04 12:42:47[0m] Step: 9058, Training Logs: loss_final: 0.897068, loss_mean: 0.853899, loss_mean_cls: 0.043169, grad_norm: 0.193798
[[34m2025-10-04 12:42:47[0m] Step: 9059, Training Logs: loss_final: 0.852804, loss_mean: 0.809906, loss_mean_cls: 0.042898, grad_norm: 0.158490
[[34m2025-10-04 12:42:48[0m] Step: 9060, Training Logs: loss_final: 0.867288, loss_mean: 0.824659, loss_mean_cls: 0.042629, grad_norm: 0.225144
[[34m2025-10-04 12:42:48[0m] Step: 9061, Training Logs: loss_final: 0.865594, loss_mean: 0.821902, loss_mean_cls: 0.043692, grad_norm: 0.264465
[[34m2025-10-04 12:42:48[0m] Step: 9062, Training Logs: loss_final: 0.876614, loss_mean: 0.833013, loss_mean_cls: 0.043601, grad_norm: 0.217095
[[34m2025-10-04 12:42:48[0m] Step: 9063, Training Logs: loss_final: 0.865074, loss_mean: 0.821042, loss_mean_cls: 0.044032, grad_norm: 0.203099
[[34m2025-10-04 12:42:49[0m] Step: 9064, Training Logs: loss_final: 0.862724, loss_mean: 0.818571, loss_mean_cls: 0.044153, grad_norm: 0.367865
[[34m2025-10-04 12:42:49[0m] Step: 9065, Training Logs: loss_final: 0.880451, loss_mean: 0.837840, loss_mean_cls: 0.042611, grad_norm: 0.213201
[[34m2025-10-04 12:42:49[0m] Step: 9066, Training Logs: loss_final: 0.880345, loss_mean: 0.838408, loss_mean_cls: 0.041937, grad_norm: 0.334027
[[34m2025-10-04 12:42:50[0m] Step: 9067, Training Logs: loss_final: 0.873080, loss_mean: 0.829753, loss_mean_cls: 0.043327, grad_norm: 0.290389
[[34m2025-10-04 12:42:50[0m] Step: 9068, Training Logs: loss_final: 0.868926, loss_mean: 0.824024, loss_mean_cls: 0.044902, grad_norm: 0.228404
[[34m2025-10-04 12:42:50[0m] Step: 9069, Training Logs: loss_final: 0.863338, loss_mean: 0.820279, loss_mean_cls: 0.043058, grad_norm: 0.215303
[[34m2025-10-04 12:42:51[0m] Step: 9070, Training Logs: loss_final: 0.863179, loss_mean: 0.819596, loss_mean_cls: 0.043583, grad_norm: 0.201160
[[34m2025-10-04 12:42:51[0m] Step: 9071, Training Logs: loss_final: 0.861989, loss_mean: 0.818607, loss_mean_cls: 0.043382, grad_norm: 0.300790
[[34m2025-10-04 12:42:51[0m] Step: 9072, Training Logs: loss_final: 0.854643, loss_mean: 0.812156, loss_mean_cls: 0.042487, grad_norm: 0.231554
[[34m2025-10-04 12:42:51[0m] Step: 9073, Training Logs: loss_final: 0.850017, loss_mean: 0.807166, loss_mean_cls: 0.042851, grad_norm: 0.355260
[[34m2025-10-04 12:42:52[0m] Step: 9074, Training Logs: loss_final: 0.871243, loss_mean: 0.828984, loss_mean_cls: 0.042259, grad_norm: 0.219883
[[34m2025-10-04 12:42:52[0m] Step: 9075, Training Logs: loss_final: 0.863448, loss_mean: 0.820233, loss_mean_cls: 0.043214, grad_norm: 0.182784
[[34m2025-10-04 12:42:52[0m] Step: 9076, Training Logs: loss_final: 0.852673, loss_mean: 0.809585, loss_mean_cls: 0.043088, grad_norm: 0.230352
[[34m2025-10-04 12:42:53[0m] Step: 9077, Training Logs: loss_final: 0.857702, loss_mean: 0.814253, loss_mean_cls: 0.043449, grad_norm: 0.212377
[[34m2025-10-04 12:42:53[0m] Step: 9078, Training Logs: loss_final: 0.868696, loss_mean: 0.825509, loss_mean_cls: 0.043187, grad_norm: 0.271137
[[34m2025-10-04 12:42:53[0m] Step: 9079, Training Logs: loss_final: 0.857432, loss_mean: 0.813968, loss_mean_cls: 0.043463, grad_norm: 0.270564
[[34m2025-10-04 12:42:53[0m] Step: 9080, Training Logs: loss_final: 0.861429, loss_mean: 0.818706, loss_mean_cls: 0.042723, grad_norm: 0.236932
[[34m2025-10-04 12:42:54[0m] Step: 9081, Training Logs: loss_final: 0.869254, loss_mean: 0.825958, loss_mean_cls: 0.043296, grad_norm: 0.260792
[[34m2025-10-04 12:42:54[0m] Step: 9082, Training Logs: loss_final: 0.865183, loss_mean: 0.821839, loss_mean_cls: 0.043344, grad_norm: 0.255256
[[34m2025-10-04 12:42:54[0m] Step: 9083, Training Logs: loss_final: 0.874092, loss_mean: 0.830610, loss_mean_cls: 0.043482, grad_norm: 0.354310
[[34m2025-10-04 12:42:55[0m] Step: 9084, Training Logs: loss_final: 0.880874, loss_mean: 0.837614, loss_mean_cls: 0.043261, grad_norm: 0.216407
[[34m2025-10-04 12:42:55[0m] Step: 9085, Training Logs: loss_final: 0.866878, loss_mean: 0.824424, loss_mean_cls: 0.042454, grad_norm: 0.311748
[[34m2025-10-04 12:42:55[0m] Step: 9086, Training Logs: loss_final: 0.902609, loss_mean: 0.860053, loss_mean_cls: 0.042556, grad_norm: 0.296795
[[34m2025-10-04 12:42:56[0m] Step: 9087, Training Logs: loss_final: 0.862802, loss_mean: 0.819132, loss_mean_cls: 0.043670, grad_norm: 0.262982
[[34m2025-10-04 12:42:56[0m] Step: 9088, Training Logs: loss_final: 0.882111, loss_mean: 0.839357, loss_mean_cls: 0.042754, grad_norm: 0.187876
[[34m2025-10-04 12:42:56[0m] Step: 9089, Training Logs: loss_final: 0.859227, loss_mean: 0.815223, loss_mean_cls: 0.044005, grad_norm: 0.240070
[[34m2025-10-04 12:42:56[0m] Step: 9090, Training Logs: loss_final: 0.867648, loss_mean: 0.823351, loss_mean_cls: 0.044297, grad_norm: 0.250117
[[34m2025-10-04 12:42:57[0m] Step: 9091, Training Logs: loss_final: 0.895956, loss_mean: 0.853242, loss_mean_cls: 0.042714, grad_norm: 0.262178
[[34m2025-10-04 12:42:57[0m] Step: 9092, Training Logs: loss_final: 0.865893, loss_mean: 0.821947, loss_mean_cls: 0.043946, grad_norm: 0.175616
[[34m2025-10-04 12:42:57[0m] Step: 9093, Training Logs: loss_final: 0.872305, loss_mean: 0.829648, loss_mean_cls: 0.042657, grad_norm: 0.306539
[[34m2025-10-04 12:42:58[0m] Step: 9094, Training Logs: loss_final: 0.862783, loss_mean: 0.819173, loss_mean_cls: 0.043609, grad_norm: 0.235945
[[34m2025-10-04 12:42:58[0m] Step: 9095, Training Logs: loss_final: 0.866352, loss_mean: 0.823023, loss_mean_cls: 0.043329, grad_norm: 0.260350
[[34m2025-10-04 12:42:58[0m] Step: 9096, Training Logs: loss_final: 0.867858, loss_mean: 0.824668, loss_mean_cls: 0.043191, grad_norm: 0.184285
[[34m2025-10-04 12:42:58[0m] Step: 9097, Training Logs: loss_final: 0.869299, loss_mean: 0.826300, loss_mean_cls: 0.042999, grad_norm: 0.312868
[[34m2025-10-04 12:42:59[0m] Step: 9098, Training Logs: loss_final: 0.874146, loss_mean: 0.831249, loss_mean_cls: 0.042897, grad_norm: 0.196102
[[34m2025-10-04 12:42:59[0m] Step: 9099, Training Logs: loss_final: 0.864502, loss_mean: 0.822915, loss_mean_cls: 0.041587, grad_norm: 0.261164
[[34m2025-10-04 12:42:59[0m] Step: 9100, Training Logs: loss_final: 0.878657, loss_mean: 0.835919, loss_mean_cls: 0.042738, grad_norm: 0.242792
[[34m2025-10-04 12:43:00[0m] Step: 9101, Training Logs: loss_final: 0.866789, loss_mean: 0.823799, loss_mean_cls: 0.042990, grad_norm: 0.319382
[[34m2025-10-04 12:43:00[0m] Step: 9102, Training Logs: loss_final: 0.897456, loss_mean: 0.854436, loss_mean_cls: 0.043019, grad_norm: 0.197073
[[34m2025-10-04 12:43:00[0m] Step: 9103, Training Logs: loss_final: 0.861183, loss_mean: 0.818765, loss_mean_cls: 0.042418, grad_norm: 0.369757
[[34m2025-10-04 12:43:01[0m] Step: 9104, Training Logs: loss_final: 0.864758, loss_mean: 0.820939, loss_mean_cls: 0.043819, grad_norm: 0.176007
[[34m2025-10-04 12:43:01[0m] Step: 9105, Training Logs: loss_final: 0.878569, loss_mean: 0.835376, loss_mean_cls: 0.043193, grad_norm: 0.274980
[[34m2025-10-04 12:43:01[0m] Step: 9106, Training Logs: loss_final: 0.880877, loss_mean: 0.837203, loss_mean_cls: 0.043674, grad_norm: 0.243867
[[34m2025-10-04 12:43:01[0m] Step: 9107, Training Logs: loss_final: 0.876889, loss_mean: 0.835070, loss_mean_cls: 0.041819, grad_norm: 0.248062
[[34m2025-10-04 12:43:02[0m] Step: 9108, Training Logs: loss_final: 0.870317, loss_mean: 0.827421, loss_mean_cls: 0.042896, grad_norm: 0.309258
[[34m2025-10-04 12:43:02[0m] Step: 9109, Training Logs: loss_final: 0.873231, loss_mean: 0.830687, loss_mean_cls: 0.042544, grad_norm: 0.213402
[[34m2025-10-04 12:43:02[0m] Step: 9110, Training Logs: loss_final: 0.861638, loss_mean: 0.817416, loss_mean_cls: 0.044222, grad_norm: 0.295043
[[34m2025-10-04 12:43:03[0m] Step: 9111, Training Logs: loss_final: 0.872493, loss_mean: 0.828876, loss_mean_cls: 0.043617, grad_norm: 0.174548
[[34m2025-10-04 12:43:03[0m] Step: 9112, Training Logs: loss_final: 0.847862, loss_mean: 0.804343, loss_mean_cls: 0.043520, grad_norm: 0.297392
[[34m2025-10-04 12:43:03[0m] Step: 9113, Training Logs: loss_final: 0.855634, loss_mean: 0.811978, loss_mean_cls: 0.043657, grad_norm: 0.312112
[[34m2025-10-04 12:43:03[0m] Step: 9114, Training Logs: loss_final: 0.875199, loss_mean: 0.832883, loss_mean_cls: 0.042316, grad_norm: 0.289863
[[34m2025-10-04 12:43:04[0m] Step: 9115, Training Logs: loss_final: 0.884067, loss_mean: 0.841038, loss_mean_cls: 0.043029, grad_norm: 0.251231
[[34m2025-10-04 12:43:04[0m] Step: 9116, Training Logs: loss_final: 0.881993, loss_mean: 0.839375, loss_mean_cls: 0.042617, grad_norm: 0.345532
[[34m2025-10-04 12:43:04[0m] Step: 9117, Training Logs: loss_final: 0.846251, loss_mean: 0.802249, loss_mean_cls: 0.044002, grad_norm: 0.180538
[[34m2025-10-04 12:43:05[0m] Step: 9118, Training Logs: loss_final: 0.898020, loss_mean: 0.856514, loss_mean_cls: 0.041506, grad_norm: 0.295269
[[34m2025-10-04 12:43:05[0m] Step: 9119, Training Logs: loss_final: 0.872997, loss_mean: 0.829678, loss_mean_cls: 0.043319, grad_norm: 0.183379
[[34m2025-10-04 12:43:05[0m] Step: 9120, Training Logs: loss_final: 0.850231, loss_mean: 0.807239, loss_mean_cls: 0.042993, grad_norm: 0.271350
[[34m2025-10-04 12:43:05[0m] Step: 9121, Training Logs: loss_final: 0.869990, loss_mean: 0.826840, loss_mean_cls: 0.043150, grad_norm: 0.205092
[[34m2025-10-04 12:43:06[0m] Step: 9122, Training Logs: loss_final: 0.868124, loss_mean: 0.825058, loss_mean_cls: 0.043065, grad_norm: 0.332194
[[34m2025-10-04 12:43:06[0m] Step: 9123, Training Logs: loss_final: 0.872881, loss_mean: 0.830224, loss_mean_cls: 0.042657, grad_norm: 0.292921
[[34m2025-10-04 12:43:06[0m] Step: 9124, Training Logs: loss_final: 0.873106, loss_mean: 0.830991, loss_mean_cls: 0.042115, grad_norm: 0.189601
[[34m2025-10-04 12:43:07[0m] Step: 9125, Training Logs: loss_final: 0.867330, loss_mean: 0.825271, loss_mean_cls: 0.042059, grad_norm: 0.209609
[[34m2025-10-04 12:43:07[0m] Step: 9126, Training Logs: loss_final: 0.890195, loss_mean: 0.847530, loss_mean_cls: 0.042665, grad_norm: 0.194764
[[34m2025-10-04 12:43:07[0m] Step: 9127, Training Logs: loss_final: 0.864046, loss_mean: 0.820872, loss_mean_cls: 0.043175, grad_norm: 0.303291
[[34m2025-10-04 12:43:07[0m] Step: 9128, Training Logs: loss_final: 0.871836, loss_mean: 0.828244, loss_mean_cls: 0.043592, grad_norm: 0.233900
[[34m2025-10-04 12:43:08[0m] Step: 9129, Training Logs: loss_final: 0.868661, loss_mean: 0.826192, loss_mean_cls: 0.042469, grad_norm: 0.185704
[[34m2025-10-04 12:43:08[0m] Step: 9130, Training Logs: loss_final: 0.868145, loss_mean: 0.824922, loss_mean_cls: 0.043223, grad_norm: 0.213896
[[34m2025-10-04 12:43:08[0m] Step: 9131, Training Logs: loss_final: 0.866388, loss_mean: 0.823227, loss_mean_cls: 0.043162, grad_norm: 0.260175
[[34m2025-10-04 12:43:09[0m] Step: 9132, Training Logs: loss_final: 0.864775, loss_mean: 0.823869, loss_mean_cls: 0.040906, grad_norm: 0.300618
[[34m2025-10-04 12:43:09[0m] Step: 9133, Training Logs: loss_final: 0.885238, loss_mean: 0.842219, loss_mean_cls: 0.043019, grad_norm: 0.174260
[[34m2025-10-04 12:43:09[0m] Step: 9134, Training Logs: loss_final: 0.871828, loss_mean: 0.827823, loss_mean_cls: 0.044004, grad_norm: 0.208890
[[34m2025-10-04 12:43:09[0m] Step: 9135, Training Logs: loss_final: 0.887779, loss_mean: 0.845645, loss_mean_cls: 0.042134, grad_norm: 0.418800
[[34m2025-10-04 12:43:10[0m] Step: 9136, Training Logs: loss_final: 0.849601, loss_mean: 0.805580, loss_mean_cls: 0.044021, grad_norm: 0.306005
[[34m2025-10-04 12:43:10[0m] Step: 9137, Training Logs: loss_final: 0.866790, loss_mean: 0.824257, loss_mean_cls: 0.042534, grad_norm: 0.392506
[[34m2025-10-04 12:43:10[0m] Step: 9138, Training Logs: loss_final: 0.860996, loss_mean: 0.819015, loss_mean_cls: 0.041981, grad_norm: 0.469183
[[34m2025-10-04 12:43:11[0m] Step: 9139, Training Logs: loss_final: 0.875040, loss_mean: 0.832614, loss_mean_cls: 0.042426, grad_norm: 0.414933
[[34m2025-10-04 12:43:11[0m] Step: 9140, Training Logs: loss_final: 0.878097, loss_mean: 0.834670, loss_mean_cls: 0.043427, grad_norm: 0.503186
[[34m2025-10-04 12:43:11[0m] Step: 9141, Training Logs: loss_final: 0.860415, loss_mean: 0.817561, loss_mean_cls: 0.042853, grad_norm: 0.171074
[[34m2025-10-04 12:43:11[0m] Step: 9142, Training Logs: loss_final: 0.880920, loss_mean: 0.838188, loss_mean_cls: 0.042732, grad_norm: 0.272318
[[34m2025-10-04 12:43:12[0m] Step: 9143, Training Logs: loss_final: 0.866800, loss_mean: 0.822998, loss_mean_cls: 0.043802, grad_norm: 0.242811
[[34m2025-10-04 12:43:12[0m] Step: 9144, Training Logs: loss_final: 0.876159, loss_mean: 0.833106, loss_mean_cls: 0.043053, grad_norm: 0.224482
[[34m2025-10-04 12:43:12[0m] Step: 9145, Training Logs: loss_final: 0.876236, loss_mean: 0.834371, loss_mean_cls: 0.041865, grad_norm: 0.240529
[[34m2025-10-04 12:43:13[0m] Step: 9146, Training Logs: loss_final: 0.870121, loss_mean: 0.826332, loss_mean_cls: 0.043790, grad_norm: 0.276558
[[34m2025-10-04 12:43:13[0m] Step: 9147, Training Logs: loss_final: 0.862866, loss_mean: 0.819520, loss_mean_cls: 0.043346, grad_norm: 0.305074
[[34m2025-10-04 12:43:13[0m] Step: 9148, Training Logs: loss_final: 0.871986, loss_mean: 0.827926, loss_mean_cls: 0.044059, grad_norm: 0.322646
[[34m2025-10-04 12:43:13[0m] Step: 9149, Training Logs: loss_final: 0.872572, loss_mean: 0.830660, loss_mean_cls: 0.041912, grad_norm: 0.192815
[[34m2025-10-04 12:43:14[0m] Step: 9150, Training Logs: loss_final: 0.851608, loss_mean: 0.808492, loss_mean_cls: 0.043116, grad_norm: 0.321980
[[34m2025-10-04 12:43:14[0m] Step: 9151, Training Logs: loss_final: 0.867682, loss_mean: 0.824319, loss_mean_cls: 0.043363, grad_norm: 0.209219
[[34m2025-10-04 12:43:14[0m] Step: 9152, Training Logs: loss_final: 0.879901, loss_mean: 0.836756, loss_mean_cls: 0.043145, grad_norm: 0.256077
[[34m2025-10-04 12:43:15[0m] Step: 9153, Training Logs: loss_final: 0.849440, loss_mean: 0.807908, loss_mean_cls: 0.041532, grad_norm: 0.208409
[[34m2025-10-04 12:43:15[0m] Step: 9154, Training Logs: loss_final: 0.867619, loss_mean: 0.825079, loss_mean_cls: 0.042540, grad_norm: 0.311501
[[34m2025-10-04 12:43:15[0m] Step: 9155, Training Logs: loss_final: 0.858050, loss_mean: 0.814394, loss_mean_cls: 0.043656, grad_norm: 0.239226
[[34m2025-10-04 12:43:16[0m] Step: 9156, Training Logs: loss_final: 0.860306, loss_mean: 0.816516, loss_mean_cls: 0.043790, grad_norm: 0.238026
[[34m2025-10-04 12:43:16[0m] Step: 9157, Training Logs: loss_final: 0.883524, loss_mean: 0.840552, loss_mean_cls: 0.042971, grad_norm: 0.238007
[[34m2025-10-04 12:43:16[0m] Step: 9158, Training Logs: loss_final: 0.876676, loss_mean: 0.832604, loss_mean_cls: 0.044073, grad_norm: 0.271075
[[34m2025-10-04 12:43:16[0m] Step: 9159, Training Logs: loss_final: 0.870837, loss_mean: 0.825965, loss_mean_cls: 0.044872, grad_norm: 0.279718
[[34m2025-10-04 12:43:17[0m] Step: 9160, Training Logs: loss_final: 0.859160, loss_mean: 0.816226, loss_mean_cls: 0.042935, grad_norm: 0.268313
[[34m2025-10-04 12:43:17[0m] Step: 9161, Training Logs: loss_final: 0.858730, loss_mean: 0.816142, loss_mean_cls: 0.042588, grad_norm: 0.308337
[[34m2025-10-04 12:43:17[0m] Step: 9162, Training Logs: loss_final: 0.860081, loss_mean: 0.816016, loss_mean_cls: 0.044064, grad_norm: 0.295240
[[34m2025-10-04 12:43:18[0m] Step: 9163, Training Logs: loss_final: 0.861673, loss_mean: 0.817957, loss_mean_cls: 0.043716, grad_norm: 0.281596
[[34m2025-10-04 12:43:18[0m] Step: 9164, Training Logs: loss_final: 0.871038, loss_mean: 0.827443, loss_mean_cls: 0.043595, grad_norm: 0.212670
[[34m2025-10-04 12:43:18[0m] Step: 9165, Training Logs: loss_final: 0.875492, loss_mean: 0.831758, loss_mean_cls: 0.043734, grad_norm: 0.222803
[[34m2025-10-04 12:43:18[0m] Step: 9166, Training Logs: loss_final: 0.871729, loss_mean: 0.829659, loss_mean_cls: 0.042070, grad_norm: 0.245109
[[34m2025-10-04 12:43:19[0m] Step: 9167, Training Logs: loss_final: 0.884419, loss_mean: 0.841764, loss_mean_cls: 0.042655, grad_norm: 0.242725
[[34m2025-10-04 12:43:19[0m] Step: 9168, Training Logs: loss_final: 0.872526, loss_mean: 0.828703, loss_mean_cls: 0.043823, grad_norm: 0.225481
[[34m2025-10-04 12:43:19[0m] Step: 9169, Training Logs: loss_final: 0.863583, loss_mean: 0.820783, loss_mean_cls: 0.042800, grad_norm: 0.205393
[[34m2025-10-04 12:43:20[0m] Step: 9170, Training Logs: loss_final: 0.863943, loss_mean: 0.820660, loss_mean_cls: 0.043283, grad_norm: 0.239245
[[34m2025-10-04 12:43:20[0m] Step: 9171, Training Logs: loss_final: 0.893134, loss_mean: 0.850762, loss_mean_cls: 0.042372, grad_norm: 0.222586
[[34m2025-10-04 12:43:20[0m] Step: 9172, Training Logs: loss_final: 0.854753, loss_mean: 0.811306, loss_mean_cls: 0.043447, grad_norm: 0.176365
[[34m2025-10-04 12:43:20[0m] Step: 9173, Training Logs: loss_final: 0.862885, loss_mean: 0.819792, loss_mean_cls: 0.043094, grad_norm: 0.197840
[[34m2025-10-04 12:43:21[0m] Step: 9174, Training Logs: loss_final: 0.874581, loss_mean: 0.832092, loss_mean_cls: 0.042489, grad_norm: 0.255731
[[34m2025-10-04 12:43:21[0m] Step: 9175, Training Logs: loss_final: 0.864285, loss_mean: 0.821183, loss_mean_cls: 0.043102, grad_norm: 0.187734
[[34m2025-10-04 12:43:21[0m] Step: 9176, Training Logs: loss_final: 0.879436, loss_mean: 0.838450, loss_mean_cls: 0.040987, grad_norm: 0.216938
[[34m2025-10-04 12:43:22[0m] Step: 9177, Training Logs: loss_final: 0.866563, loss_mean: 0.824193, loss_mean_cls: 0.042371, grad_norm: 0.224246
[[34m2025-10-04 12:43:22[0m] Step: 9178, Training Logs: loss_final: 0.880993, loss_mean: 0.837365, loss_mean_cls: 0.043629, grad_norm: 0.222581
[[34m2025-10-04 12:43:22[0m] Step: 9179, Training Logs: loss_final: 0.884285, loss_mean: 0.842365, loss_mean_cls: 0.041921, grad_norm: 0.201956
[[34m2025-10-04 12:43:23[0m] Step: 9180, Training Logs: loss_final: 0.876667, loss_mean: 0.833273, loss_mean_cls: 0.043394, grad_norm: 0.227372
[[34m2025-10-04 12:43:23[0m] Step: 9181, Training Logs: loss_final: 0.882404, loss_mean: 0.840219, loss_mean_cls: 0.042184, grad_norm: 0.275912
[[34m2025-10-04 12:43:23[0m] Step: 9182, Training Logs: loss_final: 0.887110, loss_mean: 0.843500, loss_mean_cls: 0.043610, grad_norm: 0.197579
[[34m2025-10-04 12:43:23[0m] Step: 9183, Training Logs: loss_final: 0.865428, loss_mean: 0.823205, loss_mean_cls: 0.042223, grad_norm: 0.227506
[[34m2025-10-04 12:43:24[0m] Step: 9184, Training Logs: loss_final: 0.883530, loss_mean: 0.841188, loss_mean_cls: 0.042342, grad_norm: 0.233874
[[34m2025-10-04 12:43:24[0m] Step: 9185, Training Logs: loss_final: 0.860020, loss_mean: 0.816804, loss_mean_cls: 0.043216, grad_norm: 0.250633
[[34m2025-10-04 12:43:24[0m] Step: 9186, Training Logs: loss_final: 0.857277, loss_mean: 0.814670, loss_mean_cls: 0.042607, grad_norm: 0.215841
[[34m2025-10-04 12:43:25[0m] Step: 9187, Training Logs: loss_final: 0.844453, loss_mean: 0.801431, loss_mean_cls: 0.043022, grad_norm: 0.232079
[[34m2025-10-04 12:43:25[0m] Step: 9188, Training Logs: loss_final: 0.878211, loss_mean: 0.836059, loss_mean_cls: 0.042152, grad_norm: 0.219872
[[34m2025-10-04 12:43:25[0m] Step: 9189, Training Logs: loss_final: 0.864486, loss_mean: 0.821543, loss_mean_cls: 0.042943, grad_norm: 0.208708
[[34m2025-10-04 12:43:25[0m] Step: 9190, Training Logs: loss_final: 0.875099, loss_mean: 0.832070, loss_mean_cls: 0.043029, grad_norm: 0.191214
[[34m2025-10-04 12:43:26[0m] Step: 9191, Training Logs: loss_final: 0.850122, loss_mean: 0.806826, loss_mean_cls: 0.043296, grad_norm: 0.269784
[[34m2025-10-04 12:43:26[0m] Step: 9192, Training Logs: loss_final: 0.873541, loss_mean: 0.831043, loss_mean_cls: 0.042498, grad_norm: 0.190190
[[34m2025-10-04 12:43:26[0m] Step: 9193, Training Logs: loss_final: 0.871392, loss_mean: 0.828153, loss_mean_cls: 0.043239, grad_norm: 0.175134
[[34m2025-10-04 12:43:27[0m] Step: 9194, Training Logs: loss_final: 0.851821, loss_mean: 0.809707, loss_mean_cls: 0.042114, grad_norm: 0.288937
[[34m2025-10-04 12:43:27[0m] Step: 9195, Training Logs: loss_final: 0.858478, loss_mean: 0.814825, loss_mean_cls: 0.043653, grad_norm: 0.282493
[[34m2025-10-04 12:43:27[0m] Step: 9196, Training Logs: loss_final: 0.855076, loss_mean: 0.811895, loss_mean_cls: 0.043181, grad_norm: 0.253369
[[34m2025-10-04 12:43:27[0m] Step: 9197, Training Logs: loss_final: 0.885413, loss_mean: 0.842755, loss_mean_cls: 0.042658, grad_norm: 0.282167
[[34m2025-10-04 12:43:28[0m] Step: 9198, Training Logs: loss_final: 0.884707, loss_mean: 0.840305, loss_mean_cls: 0.044402, grad_norm: 0.263295
[[34m2025-10-04 12:43:28[0m] Step: 9199, Training Logs: loss_final: 0.860849, loss_mean: 0.818955, loss_mean_cls: 0.041893, grad_norm: 0.259622
[[34m2025-10-04 12:43:28[0m] Step: 9200, Training Logs: loss_final: 0.854944, loss_mean: 0.811093, loss_mean_cls: 0.043851, grad_norm: 0.235508
[[34m2025-10-04 12:43:29[0m] Step: 9201, Training Logs: loss_final: 0.879830, loss_mean: 0.836086, loss_mean_cls: 0.043744, grad_norm: 0.217059
[[34m2025-10-04 12:43:29[0m] Step: 9202, Training Logs: loss_final: 0.869203, loss_mean: 0.826356, loss_mean_cls: 0.042847, grad_norm: 0.186312
[[34m2025-10-04 12:43:29[0m] Step: 9203, Training Logs: loss_final: 0.855591, loss_mean: 0.813327, loss_mean_cls: 0.042264, grad_norm: 0.168616
[[34m2025-10-04 12:43:30[0m] Step: 9204, Training Logs: loss_final: 0.854142, loss_mean: 0.809892, loss_mean_cls: 0.044250, grad_norm: 0.194996
[[34m2025-10-04 12:43:30[0m] Step: 9205, Training Logs: loss_final: 0.889698, loss_mean: 0.847800, loss_mean_cls: 0.041898, grad_norm: 0.205817
[[34m2025-10-04 12:43:30[0m] Step: 9206, Training Logs: loss_final: 0.878179, loss_mean: 0.834750, loss_mean_cls: 0.043429, grad_norm: 0.179970
[[34m2025-10-04 12:43:30[0m] Step: 9207, Training Logs: loss_final: 0.890044, loss_mean: 0.847696, loss_mean_cls: 0.042348, grad_norm: 0.161236
[[34m2025-10-04 12:43:31[0m] Step: 9208, Training Logs: loss_final: 0.883002, loss_mean: 0.840608, loss_mean_cls: 0.042395, grad_norm: 0.236709
[[34m2025-10-04 12:43:31[0m] Step: 9209, Training Logs: loss_final: 0.882962, loss_mean: 0.839437, loss_mean_cls: 0.043525, grad_norm: 0.186781
[[34m2025-10-04 12:43:31[0m] Step: 9210, Training Logs: loss_final: 0.842155, loss_mean: 0.799469, loss_mean_cls: 0.042686, grad_norm: 0.222833
[[34m2025-10-04 12:43:32[0m] Step: 9211, Training Logs: loss_final: 0.882381, loss_mean: 0.839025, loss_mean_cls: 0.043356, grad_norm: 0.223077
[[34m2025-10-04 12:43:32[0m] Step: 9212, Training Logs: loss_final: 0.854477, loss_mean: 0.810810, loss_mean_cls: 0.043667, grad_norm: 0.266604
[[34m2025-10-04 12:43:32[0m] Step: 9213, Training Logs: loss_final: 0.853916, loss_mean: 0.810273, loss_mean_cls: 0.043643, grad_norm: 0.252712
[[34m2025-10-04 12:43:33[0m] Step: 9214, Training Logs: loss_final: 0.830154, loss_mean: 0.786159, loss_mean_cls: 0.043995, grad_norm: 0.270196
[[34m2025-10-04 12:43:33[0m] Step: 9215, Training Logs: loss_final: 0.885599, loss_mean: 0.843516, loss_mean_cls: 0.042083, grad_norm: 0.217685
[[34m2025-10-04 12:43:33[0m] Step: 9216, Training Logs: loss_final: 0.877648, loss_mean: 0.835446, loss_mean_cls: 0.042201, grad_norm: 0.361429
[[34m2025-10-04 12:43:33[0m] Step: 9217, Training Logs: loss_final: 0.873375, loss_mean: 0.830426, loss_mean_cls: 0.042949, grad_norm: 0.324572
[[34m2025-10-04 12:43:34[0m] Step: 9218, Training Logs: loss_final: 0.873276, loss_mean: 0.829504, loss_mean_cls: 0.043772, grad_norm: 0.360319
[[34m2025-10-04 12:43:34[0m] Step: 9219, Training Logs: loss_final: 0.873846, loss_mean: 0.830985, loss_mean_cls: 0.042861, grad_norm: 0.254445
[[34m2025-10-04 12:43:34[0m] Step: 9220, Training Logs: loss_final: 0.870299, loss_mean: 0.826840, loss_mean_cls: 0.043459, grad_norm: 0.334955
[[34m2025-10-04 12:43:35[0m] Step: 9221, Training Logs: loss_final: 0.877248, loss_mean: 0.834915, loss_mean_cls: 0.042333, grad_norm: 0.227204
[[34m2025-10-04 12:43:35[0m] Step: 9222, Training Logs: loss_final: 0.872285, loss_mean: 0.828300, loss_mean_cls: 0.043984, grad_norm: 0.533147
[[34m2025-10-04 12:43:35[0m] Step: 9223, Training Logs: loss_final: 0.886764, loss_mean: 0.844200, loss_mean_cls: 0.042564, grad_norm: 0.518682
[[34m2025-10-04 12:43:35[0m] Step: 9224, Training Logs: loss_final: 0.888409, loss_mean: 0.845105, loss_mean_cls: 0.043304, grad_norm: 0.270888
[[34m2025-10-04 12:43:36[0m] Step: 9225, Training Logs: loss_final: 0.875885, loss_mean: 0.833815, loss_mean_cls: 0.042070, grad_norm: 0.376281
[[34m2025-10-04 12:43:36[0m] Step: 9226, Training Logs: loss_final: 0.873623, loss_mean: 0.830721, loss_mean_cls: 0.042902, grad_norm: 0.306382
[[34m2025-10-04 12:43:36[0m] Step: 9227, Training Logs: loss_final: 0.887979, loss_mean: 0.846704, loss_mean_cls: 0.041275, grad_norm: 0.194004
[[34m2025-10-04 12:43:37[0m] Step: 9228, Training Logs: loss_final: 0.874822, loss_mean: 0.830836, loss_mean_cls: 0.043986, grad_norm: 0.252555
[[34m2025-10-04 12:43:37[0m] Step: 9229, Training Logs: loss_final: 0.864241, loss_mean: 0.820618, loss_mean_cls: 0.043623, grad_norm: 0.154751
[[34m2025-10-04 12:43:37[0m] Step: 9230, Training Logs: loss_final: 0.881790, loss_mean: 0.838893, loss_mean_cls: 0.042897, grad_norm: 0.258520
[[34m2025-10-04 12:43:38[0m] Step: 9231, Training Logs: loss_final: 0.873746, loss_mean: 0.830424, loss_mean_cls: 0.043322, grad_norm: 0.302015
[[34m2025-10-04 12:43:38[0m] Step: 9232, Training Logs: loss_final: 0.862829, loss_mean: 0.820153, loss_mean_cls: 0.042675, grad_norm: 0.172279
[[34m2025-10-04 12:43:38[0m] Step: 9233, Training Logs: loss_final: 0.857170, loss_mean: 0.813283, loss_mean_cls: 0.043887, grad_norm: 0.271876
[[34m2025-10-04 12:43:38[0m] Step: 9234, Training Logs: loss_final: 0.903167, loss_mean: 0.861574, loss_mean_cls: 0.041594, grad_norm: 0.224151
[[34m2025-10-04 12:43:39[0m] Step: 9235, Training Logs: loss_final: 0.891178, loss_mean: 0.849059, loss_mean_cls: 0.042120, grad_norm: 0.210379
[[34m2025-10-04 12:43:39[0m] Step: 9236, Training Logs: loss_final: 0.877131, loss_mean: 0.834934, loss_mean_cls: 0.042197, grad_norm: 0.206594
[[34m2025-10-04 12:43:39[0m] Step: 9237, Training Logs: loss_final: 0.878074, loss_mean: 0.834486, loss_mean_cls: 0.043589, grad_norm: 0.279870
[[34m2025-10-04 12:43:40[0m] Step: 9238, Training Logs: loss_final: 0.892879, loss_mean: 0.850724, loss_mean_cls: 0.042155, grad_norm: 0.281328
[[34m2025-10-04 12:43:40[0m] Step: 9239, Training Logs: loss_final: 0.859026, loss_mean: 0.815956, loss_mean_cls: 0.043070, grad_norm: 0.184922
[[34m2025-10-04 12:43:40[0m] Step: 9240, Training Logs: loss_final: 0.869546, loss_mean: 0.825966, loss_mean_cls: 0.043580, grad_norm: 0.168490
[[34m2025-10-04 12:43:40[0m] Step: 9241, Training Logs: loss_final: 0.857643, loss_mean: 0.815065, loss_mean_cls: 0.042577, grad_norm: 0.252509
[[34m2025-10-04 12:43:41[0m] Step: 9242, Training Logs: loss_final: 0.870143, loss_mean: 0.827565, loss_mean_cls: 0.042578, grad_norm: 0.256384
[[34m2025-10-04 12:43:41[0m] Step: 9243, Training Logs: loss_final: 0.884048, loss_mean: 0.841111, loss_mean_cls: 0.042937, grad_norm: 0.300163
[[34m2025-10-04 12:43:41[0m] Step: 9244, Training Logs: loss_final: 0.883261, loss_mean: 0.840123, loss_mean_cls: 0.043138, grad_norm: 0.282770
[[34m2025-10-04 12:43:42[0m] Step: 9245, Training Logs: loss_final: 0.873725, loss_mean: 0.830447, loss_mean_cls: 0.043278, grad_norm: 0.219256
[[34m2025-10-04 12:43:42[0m] Step: 9246, Training Logs: loss_final: 0.895724, loss_mean: 0.853816, loss_mean_cls: 0.041908, grad_norm: 0.190237
[[34m2025-10-04 12:43:42[0m] Step: 9247, Training Logs: loss_final: 0.850929, loss_mean: 0.807303, loss_mean_cls: 0.043626, grad_norm: 0.340625
[[34m2025-10-04 12:43:42[0m] Step: 9248, Training Logs: loss_final: 0.873171, loss_mean: 0.829894, loss_mean_cls: 0.043278, grad_norm: 0.352850
[[34m2025-10-04 12:43:43[0m] Step: 9249, Training Logs: loss_final: 0.864306, loss_mean: 0.821285, loss_mean_cls: 0.043021, grad_norm: 0.268535
[[34m2025-10-04 12:43:43[0m] Step: 9250, Training Logs: loss_final: 0.851564, loss_mean: 0.807894, loss_mean_cls: 0.043670, grad_norm: 0.355881
[[34m2025-10-04 12:43:43[0m] Step: 9251, Training Logs: loss_final: 0.845421, loss_mean: 0.801183, loss_mean_cls: 0.044238, grad_norm: 0.269580
[[34m2025-10-04 12:43:44[0m] Step: 9252, Training Logs: loss_final: 0.872048, loss_mean: 0.828504, loss_mean_cls: 0.043544, grad_norm: 0.379181
[[34m2025-10-04 12:43:44[0m] Step: 9253, Training Logs: loss_final: 0.877900, loss_mean: 0.834880, loss_mean_cls: 0.043021, grad_norm: 0.269500
[[34m2025-10-04 12:43:44[0m] Step: 9254, Training Logs: loss_final: 0.862068, loss_mean: 0.819812, loss_mean_cls: 0.042256, grad_norm: 0.183712
[[34m2025-10-04 12:43:44[0m] Step: 9255, Training Logs: loss_final: 0.868494, loss_mean: 0.825718, loss_mean_cls: 0.042776, grad_norm: 0.213126
[[34m2025-10-04 12:43:45[0m] Step: 9256, Training Logs: loss_final: 0.875153, loss_mean: 0.833550, loss_mean_cls: 0.041603, grad_norm: 0.280268
[[34m2025-10-04 12:43:45[0m] Step: 9257, Training Logs: loss_final: 0.874250, loss_mean: 0.832707, loss_mean_cls: 0.041544, grad_norm: 0.220534
[[34m2025-10-04 12:43:45[0m] Step: 9258, Training Logs: loss_final: 0.859674, loss_mean: 0.817023, loss_mean_cls: 0.042651, grad_norm: 0.266592
[[34m2025-10-04 12:43:46[0m] Step: 9259, Training Logs: loss_final: 0.867344, loss_mean: 0.824451, loss_mean_cls: 0.042893, grad_norm: 0.386856
[[34m2025-10-04 12:43:46[0m] Step: 9260, Training Logs: loss_final: 0.893535, loss_mean: 0.851913, loss_mean_cls: 0.041622, grad_norm: 0.271861
[[34m2025-10-04 12:43:46[0m] Step: 9261, Training Logs: loss_final: 0.888283, loss_mean: 0.844550, loss_mean_cls: 0.043732, grad_norm: 0.355366
[[34m2025-10-04 12:43:47[0m] Step: 9262, Training Logs: loss_final: 0.877564, loss_mean: 0.835511, loss_mean_cls: 0.042052, grad_norm: 0.180966
[[34m2025-10-04 12:43:47[0m] Step: 9263, Training Logs: loss_final: 0.871775, loss_mean: 0.827601, loss_mean_cls: 0.044174, grad_norm: 0.221580
[[34m2025-10-04 12:43:47[0m] Step: 9264, Training Logs: loss_final: 0.860868, loss_mean: 0.817573, loss_mean_cls: 0.043294, grad_norm: 0.350037
[[34m2025-10-04 12:43:47[0m] Step: 9265, Training Logs: loss_final: 0.865147, loss_mean: 0.820554, loss_mean_cls: 0.044593, grad_norm: 0.266428
[[34m2025-10-04 12:43:48[0m] Step: 9266, Training Logs: loss_final: 0.854619, loss_mean: 0.811213, loss_mean_cls: 0.043406, grad_norm: 0.253912
[[34m2025-10-04 12:43:48[0m] Step: 9267, Training Logs: loss_final: 0.872716, loss_mean: 0.830134, loss_mean_cls: 0.042582, grad_norm: 0.358170
[[34m2025-10-04 12:43:48[0m] Step: 9268, Training Logs: loss_final: 0.871059, loss_mean: 0.827690, loss_mean_cls: 0.043370, grad_norm: 0.180197
[[34m2025-10-04 12:43:49[0m] Step: 9269, Training Logs: loss_final: 0.873228, loss_mean: 0.830321, loss_mean_cls: 0.042907, grad_norm: 0.399026
[[34m2025-10-04 12:43:49[0m] Step: 9270, Training Logs: loss_final: 0.887157, loss_mean: 0.844888, loss_mean_cls: 0.042268, grad_norm: 0.238633
[[34m2025-10-04 12:43:49[0m] Step: 9271, Training Logs: loss_final: 0.864720, loss_mean: 0.821283, loss_mean_cls: 0.043438, grad_norm: 0.213049
[[34m2025-10-04 12:43:49[0m] Step: 9272, Training Logs: loss_final: 0.868686, loss_mean: 0.825250, loss_mean_cls: 0.043436, grad_norm: 0.243656
[[34m2025-10-04 12:43:50[0m] Step: 9273, Training Logs: loss_final: 0.872572, loss_mean: 0.830406, loss_mean_cls: 0.042166, grad_norm: 0.279960
[[34m2025-10-04 12:43:50[0m] Step: 9274, Training Logs: loss_final: 0.871262, loss_mean: 0.829017, loss_mean_cls: 0.042245, grad_norm: 0.246289
[[34m2025-10-04 12:43:50[0m] Step: 9275, Training Logs: loss_final: 0.880711, loss_mean: 0.837958, loss_mean_cls: 0.042753, grad_norm: 0.370951
[[34m2025-10-04 12:43:51[0m] Step: 9276, Training Logs: loss_final: 0.875133, loss_mean: 0.831663, loss_mean_cls: 0.043470, grad_norm: 0.211405
[[34m2025-10-04 12:43:51[0m] Step: 9277, Training Logs: loss_final: 0.874652, loss_mean: 0.830750, loss_mean_cls: 0.043902, grad_norm: 0.316624
[[34m2025-10-04 12:43:51[0m] Step: 9278, Training Logs: loss_final: 0.871019, loss_mean: 0.828845, loss_mean_cls: 0.042174, grad_norm: 0.289122
[[34m2025-10-04 12:43:51[0m] Step: 9279, Training Logs: loss_final: 0.863541, loss_mean: 0.820549, loss_mean_cls: 0.042991, grad_norm: 0.301080
[[34m2025-10-04 12:43:52[0m] Step: 9280, Training Logs: loss_final: 0.849519, loss_mean: 0.806112, loss_mean_cls: 0.043407, grad_norm: 0.262094
[[34m2025-10-04 12:43:52[0m] Step: 9281, Training Logs: loss_final: 0.878531, loss_mean: 0.835901, loss_mean_cls: 0.042630, grad_norm: 0.284709
[[34m2025-10-04 12:43:52[0m] Step: 9282, Training Logs: loss_final: 0.856760, loss_mean: 0.813474, loss_mean_cls: 0.043287, grad_norm: 0.235086
[[34m2025-10-04 12:43:53[0m] Step: 9283, Training Logs: loss_final: 0.886836, loss_mean: 0.844936, loss_mean_cls: 0.041900, grad_norm: 0.211646
[[34m2025-10-04 12:43:53[0m] Step: 9284, Training Logs: loss_final: 0.845362, loss_mean: 0.802976, loss_mean_cls: 0.042386, grad_norm: 0.187226
[[34m2025-10-04 12:43:53[0m] Step: 9285, Training Logs: loss_final: 0.845397, loss_mean: 0.802957, loss_mean_cls: 0.042440, grad_norm: 0.251220
[[34m2025-10-04 12:43:53[0m] Step: 9286, Training Logs: loss_final: 0.890711, loss_mean: 0.847746, loss_mean_cls: 0.042964, grad_norm: 0.230064
[[34m2025-10-04 12:43:54[0m] Step: 9287, Training Logs: loss_final: 0.873982, loss_mean: 0.831372, loss_mean_cls: 0.042610, grad_norm: 0.317711
[[34m2025-10-04 12:43:54[0m] Step: 9288, Training Logs: loss_final: 0.855173, loss_mean: 0.812402, loss_mean_cls: 0.042771, grad_norm: 0.345724
[[34m2025-10-04 12:43:54[0m] Step: 9289, Training Logs: loss_final: 0.856470, loss_mean: 0.812916, loss_mean_cls: 0.043554, grad_norm: 0.292231
[[34m2025-10-04 12:43:55[0m] Step: 9290, Training Logs: loss_final: 0.857060, loss_mean: 0.813806, loss_mean_cls: 0.043254, grad_norm: 0.246293
[[34m2025-10-04 12:43:55[0m] Step: 9291, Training Logs: loss_final: 0.871114, loss_mean: 0.828090, loss_mean_cls: 0.043024, grad_norm: 0.322356
[[34m2025-10-04 12:43:55[0m] Step: 9292, Training Logs: loss_final: 0.881988, loss_mean: 0.839213, loss_mean_cls: 0.042775, grad_norm: 0.257582
[[34m2025-10-04 12:43:55[0m] Step: 9293, Training Logs: loss_final: 0.857565, loss_mean: 0.815317, loss_mean_cls: 0.042248, grad_norm: 0.262771
[[34m2025-10-04 12:43:56[0m] Step: 9294, Training Logs: loss_final: 0.889668, loss_mean: 0.847273, loss_mean_cls: 0.042395, grad_norm: 0.261421
[[34m2025-10-04 12:43:56[0m] Step: 9295, Training Logs: loss_final: 0.876173, loss_mean: 0.832903, loss_mean_cls: 0.043270, grad_norm: 0.172914
[[34m2025-10-04 12:43:56[0m] Step: 9296, Training Logs: loss_final: 0.876564, loss_mean: 0.835288, loss_mean_cls: 0.041275, grad_norm: 0.214089
[[34m2025-10-04 12:43:57[0m] Step: 9297, Training Logs: loss_final: 0.890471, loss_mean: 0.847983, loss_mean_cls: 0.042488, grad_norm: 0.251530
[[34m2025-10-04 12:43:57[0m] Step: 9298, Training Logs: loss_final: 0.867663, loss_mean: 0.823555, loss_mean_cls: 0.044109, grad_norm: 0.210158
[[34m2025-10-04 12:43:57[0m] Step: 9299, Training Logs: loss_final: 0.855966, loss_mean: 0.812735, loss_mean_cls: 0.043231, grad_norm: 0.191612
[[34m2025-10-04 12:43:58[0m] Step: 9300, Training Logs: loss_final: 0.849943, loss_mean: 0.806311, loss_mean_cls: 0.043632, grad_norm: 0.254425
[[34m2025-10-04 12:43:58[0m] Step: 9301, Training Logs: loss_final: 0.864429, loss_mean: 0.822541, loss_mean_cls: 0.041888, grad_norm: 0.298455
[[34m2025-10-04 12:43:58[0m] Step: 9302, Training Logs: loss_final: 0.860449, loss_mean: 0.815557, loss_mean_cls: 0.044892, grad_norm: 0.386611
[[34m2025-10-04 12:43:58[0m] Step: 9303, Training Logs: loss_final: 0.885539, loss_mean: 0.842716, loss_mean_cls: 0.042822, grad_norm: 0.196607
[[34m2025-10-04 12:43:59[0m] Step: 9304, Training Logs: loss_final: 0.870142, loss_mean: 0.827499, loss_mean_cls: 0.042643, grad_norm: 0.327250
[[34m2025-10-04 12:43:59[0m] Step: 9305, Training Logs: loss_final: 0.885890, loss_mean: 0.843547, loss_mean_cls: 0.042343, grad_norm: 0.255315
[[34m2025-10-04 12:43:59[0m] Step: 9306, Training Logs: loss_final: 0.847067, loss_mean: 0.804164, loss_mean_cls: 0.042903, grad_norm: 0.349668
[[34m2025-10-04 12:44:00[0m] Step: 9307, Training Logs: loss_final: 0.887249, loss_mean: 0.844062, loss_mean_cls: 0.043187, grad_norm: 0.226601
[[34m2025-10-04 12:44:00[0m] Step: 9308, Training Logs: loss_final: 0.879230, loss_mean: 0.836013, loss_mean_cls: 0.043217, grad_norm: 0.335883
[[34m2025-10-04 12:44:00[0m] Step: 9309, Training Logs: loss_final: 0.868180, loss_mean: 0.825507, loss_mean_cls: 0.042673, grad_norm: 0.194491
[[34m2025-10-04 12:44:00[0m] Step: 9310, Training Logs: loss_final: 0.867098, loss_mean: 0.823629, loss_mean_cls: 0.043469, grad_norm: 0.463454
[[34m2025-10-04 12:44:01[0m] Step: 9311, Training Logs: loss_final: 0.856872, loss_mean: 0.814810, loss_mean_cls: 0.042063, grad_norm: 0.227961
[[34m2025-10-04 12:44:01[0m] Step: 9312, Training Logs: loss_final: 0.886961, loss_mean: 0.844816, loss_mean_cls: 0.042145, grad_norm: 0.282123
[[34m2025-10-04 12:44:01[0m] Step: 9313, Training Logs: loss_final: 0.853875, loss_mean: 0.810356, loss_mean_cls: 0.043519, grad_norm: 0.251329
[[34m2025-10-04 12:44:02[0m] Step: 9314, Training Logs: loss_final: 0.878425, loss_mean: 0.835998, loss_mean_cls: 0.042427, grad_norm: 0.202216
[[34m2025-10-04 12:44:02[0m] Step: 9315, Training Logs: loss_final: 0.868905, loss_mean: 0.825899, loss_mean_cls: 0.043006, grad_norm: 0.236070
[[34m2025-10-04 12:44:02[0m] Step: 9316, Training Logs: loss_final: 0.891021, loss_mean: 0.849319, loss_mean_cls: 0.041702, grad_norm: 0.236385
[[34m2025-10-04 12:44:02[0m] Step: 9317, Training Logs: loss_final: 0.863389, loss_mean: 0.820133, loss_mean_cls: 0.043256, grad_norm: 0.249452
[[34m2025-10-04 12:44:03[0m] Step: 9318, Training Logs: loss_final: 0.886268, loss_mean: 0.843679, loss_mean_cls: 0.042589, grad_norm: 0.186222
[[34m2025-10-04 12:44:03[0m] Step: 9319, Training Logs: loss_final: 0.859162, loss_mean: 0.816186, loss_mean_cls: 0.042976, grad_norm: 0.172472
[[34m2025-10-04 12:44:03[0m] Step: 9320, Training Logs: loss_final: 0.878794, loss_mean: 0.836825, loss_mean_cls: 0.041969, grad_norm: 0.283073
[[34m2025-10-04 12:44:04[0m] Step: 9321, Training Logs: loss_final: 0.886251, loss_mean: 0.844794, loss_mean_cls: 0.041457, grad_norm: 0.212316
[[34m2025-10-04 12:44:04[0m] Step: 9322, Training Logs: loss_final: 0.857860, loss_mean: 0.814044, loss_mean_cls: 0.043816, grad_norm: 0.174690
[[34m2025-10-04 12:44:04[0m] Step: 9323, Training Logs: loss_final: 0.875598, loss_mean: 0.832854, loss_mean_cls: 0.042743, grad_norm: 0.188442
[[34m2025-10-04 12:44:04[0m] Step: 9324, Training Logs: loss_final: 0.863100, loss_mean: 0.819966, loss_mean_cls: 0.043134, grad_norm: 0.174876
[[34m2025-10-04 12:44:05[0m] Step: 9325, Training Logs: loss_final: 0.882320, loss_mean: 0.840439, loss_mean_cls: 0.041881, grad_norm: 0.196773
[[34m2025-10-04 12:44:05[0m] Step: 9326, Training Logs: loss_final: 0.886119, loss_mean: 0.843647, loss_mean_cls: 0.042472, grad_norm: 0.267390
[[34m2025-10-04 12:44:05[0m] Step: 9327, Training Logs: loss_final: 0.859926, loss_mean: 0.817260, loss_mean_cls: 0.042666, grad_norm: 0.201155
[[34m2025-10-04 12:44:06[0m] Step: 9328, Training Logs: loss_final: 0.856058, loss_mean: 0.812850, loss_mean_cls: 0.043209, grad_norm: 0.237792
[[34m2025-10-04 12:44:06[0m] Step: 9329, Training Logs: loss_final: 0.873874, loss_mean: 0.832192, loss_mean_cls: 0.041681, grad_norm: 0.280097
[[34m2025-10-04 12:44:06[0m] Step: 9330, Training Logs: loss_final: 0.872037, loss_mean: 0.830548, loss_mean_cls: 0.041489, grad_norm: 0.242405
[[34m2025-10-04 12:44:06[0m] Step: 9331, Training Logs: loss_final: 0.869198, loss_mean: 0.826467, loss_mean_cls: 0.042731, grad_norm: 0.255922
[[34m2025-10-04 12:44:07[0m] Step: 9332, Training Logs: loss_final: 0.844349, loss_mean: 0.800196, loss_mean_cls: 0.044153, grad_norm: 0.315836
[[34m2025-10-04 12:44:07[0m] Step: 9333, Training Logs: loss_final: 0.877099, loss_mean: 0.833787, loss_mean_cls: 0.043312, grad_norm: 0.204170
[[34m2025-10-04 12:44:07[0m] Step: 9334, Training Logs: loss_final: 0.857659, loss_mean: 0.814623, loss_mean_cls: 0.043036, grad_norm: 0.417865
[[34m2025-10-04 12:44:08[0m] Step: 9335, Training Logs: loss_final: 0.881327, loss_mean: 0.838219, loss_mean_cls: 0.043107, grad_norm: 0.271076
[[34m2025-10-04 12:44:08[0m] Step: 9336, Training Logs: loss_final: 0.889529, loss_mean: 0.846768, loss_mean_cls: 0.042762, grad_norm: 0.373237
[[34m2025-10-04 12:44:08[0m] Step: 9337, Training Logs: loss_final: 0.866025, loss_mean: 0.822604, loss_mean_cls: 0.043422, grad_norm: 0.204042
[[34m2025-10-04 12:44:09[0m] Step: 9338, Training Logs: loss_final: 0.858651, loss_mean: 0.815141, loss_mean_cls: 0.043510, grad_norm: 0.437293
[[34m2025-10-04 12:44:09[0m] Step: 9339, Training Logs: loss_final: 0.875247, loss_mean: 0.832492, loss_mean_cls: 0.042754, grad_norm: 0.216969
[[34m2025-10-04 12:44:09[0m] Step: 9340, Training Logs: loss_final: 0.872495, loss_mean: 0.830718, loss_mean_cls: 0.041777, grad_norm: 0.436246
[[34m2025-10-04 12:44:09[0m] Step: 9341, Training Logs: loss_final: 0.871680, loss_mean: 0.828432, loss_mean_cls: 0.043247, grad_norm: 0.437051
[[34m2025-10-04 12:44:10[0m] Step: 9342, Training Logs: loss_final: 0.877052, loss_mean: 0.834179, loss_mean_cls: 0.042873, grad_norm: 0.482095
[[34m2025-10-04 12:44:10[0m] Step: 9343, Training Logs: loss_final: 0.903036, loss_mean: 0.860175, loss_mean_cls: 0.042861, grad_norm: 0.336026
[[34m2025-10-04 12:44:10[0m] Step: 9344, Training Logs: loss_final: 0.875032, loss_mean: 0.831562, loss_mean_cls: 0.043470, grad_norm: 0.276248
[[34m2025-10-04 12:44:11[0m] Step: 9345, Training Logs: loss_final: 0.873734, loss_mean: 0.830913, loss_mean_cls: 0.042821, grad_norm: 0.323810
[[34m2025-10-04 12:44:11[0m] Step: 9346, Training Logs: loss_final: 0.891564, loss_mean: 0.848402, loss_mean_cls: 0.043162, grad_norm: 0.233205
[[34m2025-10-04 12:44:11[0m] Step: 9347, Training Logs: loss_final: 0.875730, loss_mean: 0.833499, loss_mean_cls: 0.042231, grad_norm: 0.344708
[[34m2025-10-04 12:44:11[0m] Step: 9348, Training Logs: loss_final: 0.861426, loss_mean: 0.817885, loss_mean_cls: 0.043541, grad_norm: 0.298799
[[34m2025-10-04 12:44:12[0m] Step: 9349, Training Logs: loss_final: 0.842860, loss_mean: 0.799215, loss_mean_cls: 0.043645, grad_norm: 0.294398
[[34m2025-10-04 12:44:12[0m] Step: 9350, Training Logs: loss_final: 0.869822, loss_mean: 0.828187, loss_mean_cls: 0.041635, grad_norm: 0.434557
[[34m2025-10-04 12:44:12[0m] Step: 9351, Training Logs: loss_final: 0.877850, loss_mean: 0.835182, loss_mean_cls: 0.042668, grad_norm: 0.483611
[[34m2025-10-04 12:44:13[0m] Step: 9352, Training Logs: loss_final: 0.894397, loss_mean: 0.852330, loss_mean_cls: 0.042067, grad_norm: 0.349701
[[34m2025-10-04 12:44:13[0m] Step: 9353, Training Logs: loss_final: 0.854322, loss_mean: 0.811534, loss_mean_cls: 0.042788, grad_norm: 0.476090
[[34m2025-10-04 12:44:13[0m] Step: 9354, Training Logs: loss_final: 0.879967, loss_mean: 0.837970, loss_mean_cls: 0.041997, grad_norm: 0.334021
[[34m2025-10-04 12:44:13[0m] Step: 9355, Training Logs: loss_final: 0.879181, loss_mean: 0.835811, loss_mean_cls: 0.043370, grad_norm: 0.551453
[[34m2025-10-04 12:44:14[0m] Step: 9356, Training Logs: loss_final: 0.856320, loss_mean: 0.814540, loss_mean_cls: 0.041780, grad_norm: 0.295011
[[34m2025-10-04 12:44:14[0m] Step: 9357, Training Logs: loss_final: 0.883077, loss_mean: 0.841668, loss_mean_cls: 0.041409, grad_norm: 0.391132
[[34m2025-10-04 12:44:14[0m] Step: 9358, Training Logs: loss_final: 0.860811, loss_mean: 0.817920, loss_mean_cls: 0.042891, grad_norm: 0.301719
[[34m2025-10-04 12:44:15[0m] Step: 9359, Training Logs: loss_final: 0.861367, loss_mean: 0.818299, loss_mean_cls: 0.043067, grad_norm: 0.393236
[[34m2025-10-04 12:44:15[0m] Step: 9360, Training Logs: loss_final: 0.892400, loss_mean: 0.851166, loss_mean_cls: 0.041234, grad_norm: 0.292958
[[34m2025-10-04 12:44:15[0m] Step: 9361, Training Logs: loss_final: 0.861530, loss_mean: 0.817699, loss_mean_cls: 0.043831, grad_norm: 0.331846
[[34m2025-10-04 12:44:15[0m] Step: 9362, Training Logs: loss_final: 0.880878, loss_mean: 0.837623, loss_mean_cls: 0.043255, grad_norm: 0.323556
[[34m2025-10-04 12:44:16[0m] Step: 9363, Training Logs: loss_final: 0.864215, loss_mean: 0.821048, loss_mean_cls: 0.043167, grad_norm: 0.367526
[[34m2025-10-04 12:44:16[0m] Step: 9364, Training Logs: loss_final: 0.881721, loss_mean: 0.838466, loss_mean_cls: 0.043255, grad_norm: 0.326274
[[34m2025-10-04 12:44:16[0m] Step: 9365, Training Logs: loss_final: 0.867641, loss_mean: 0.824495, loss_mean_cls: 0.043146, grad_norm: 0.382358
[[34m2025-10-04 12:44:17[0m] Step: 9366, Training Logs: loss_final: 0.874907, loss_mean: 0.833310, loss_mean_cls: 0.041597, grad_norm: 0.298322
[[34m2025-10-04 12:44:17[0m] Step: 9367, Training Logs: loss_final: 0.866852, loss_mean: 0.824023, loss_mean_cls: 0.042829, grad_norm: 0.269575
[[34m2025-10-04 12:44:17[0m] Step: 9368, Training Logs: loss_final: 0.880431, loss_mean: 0.837274, loss_mean_cls: 0.043157, grad_norm: 0.255128
[[34m2025-10-04 12:44:18[0m] Step: 9369, Training Logs: loss_final: 0.868862, loss_mean: 0.824824, loss_mean_cls: 0.044037, grad_norm: 0.287031
[[34m2025-10-04 12:44:18[0m] Step: 9370, Training Logs: loss_final: 0.874182, loss_mean: 0.830850, loss_mean_cls: 0.043333, grad_norm: 0.380046
[[34m2025-10-04 12:44:18[0m] Step: 9371, Training Logs: loss_final: 0.865805, loss_mean: 0.822680, loss_mean_cls: 0.043125, grad_norm: 0.399369
[[34m2025-10-04 12:44:19[0m] Step: 9372, Training Logs: loss_final: 0.869929, loss_mean: 0.826593, loss_mean_cls: 0.043336, grad_norm: 0.362295
[[34m2025-10-04 12:44:19[0m] Step: 9373, Training Logs: loss_final: 0.869586, loss_mean: 0.825134, loss_mean_cls: 0.044453, grad_norm: 0.395819
[[34m2025-10-04 12:44:19[0m] Step: 9374, Training Logs: loss_final: 0.852427, loss_mean: 0.809860, loss_mean_cls: 0.042567, grad_norm: 0.321211
[[34m2025-10-04 12:44:19[0m] Step: 9375, Training Logs: loss_final: 0.873486, loss_mean: 0.830015, loss_mean_cls: 0.043471, grad_norm: 0.457654
[[34m2025-10-04 12:44:20[0m] Step: 9376, Training Logs: loss_final: 0.860525, loss_mean: 0.817467, loss_mean_cls: 0.043058, grad_norm: 0.406316
[[34m2025-10-04 12:44:20[0m] Step: 9377, Training Logs: loss_final: 0.878476, loss_mean: 0.835465, loss_mean_cls: 0.043011, grad_norm: 0.300852
[[34m2025-10-04 12:44:20[0m] Step: 9378, Training Logs: loss_final: 0.863684, loss_mean: 0.820397, loss_mean_cls: 0.043287, grad_norm: 0.302871
[[34m2025-10-04 12:44:21[0m] Step: 9379, Training Logs: loss_final: 0.859943, loss_mean: 0.816192, loss_mean_cls: 0.043750, grad_norm: 0.426024
[[34m2025-10-04 12:44:21[0m] Step: 9380, Training Logs: loss_final: 0.873291, loss_mean: 0.830557, loss_mean_cls: 0.042734, grad_norm: 0.325430
[[34m2025-10-04 12:44:21[0m] Step: 9381, Training Logs: loss_final: 0.858824, loss_mean: 0.815967, loss_mean_cls: 0.042857, grad_norm: 0.302606
[[34m2025-10-04 12:44:22[0m] Step: 9382, Training Logs: loss_final: 0.891240, loss_mean: 0.849964, loss_mean_cls: 0.041276, grad_norm: 0.466305
[[34m2025-10-04 12:44:22[0m] Step: 9383, Training Logs: loss_final: 0.883313, loss_mean: 0.840423, loss_mean_cls: 0.042889, grad_norm: 0.452959
[[34m2025-10-04 12:44:22[0m] Step: 9384, Training Logs: loss_final: 0.874386, loss_mean: 0.831544, loss_mean_cls: 0.042841, grad_norm: 0.377638
[[34m2025-10-04 12:44:22[0m] Step: 9385, Training Logs: loss_final: 0.878039, loss_mean: 0.833786, loss_mean_cls: 0.044253, grad_norm: 0.388236
[[34m2025-10-04 12:44:23[0m] Step: 9386, Training Logs: loss_final: 0.870790, loss_mean: 0.828527, loss_mean_cls: 0.042263, grad_norm: 0.223184
[[34m2025-10-04 12:44:23[0m] Step: 9387, Training Logs: loss_final: 0.874732, loss_mean: 0.831500, loss_mean_cls: 0.043232, grad_norm: 0.239998
[[34m2025-10-04 12:44:23[0m] Step: 9388, Training Logs: loss_final: 0.866868, loss_mean: 0.823579, loss_mean_cls: 0.043288, grad_norm: 0.417201
[[34m2025-10-04 12:44:24[0m] Step: 9389, Training Logs: loss_final: 0.866635, loss_mean: 0.823542, loss_mean_cls: 0.043094, grad_norm: 0.375437
[[34m2025-10-04 12:44:24[0m] Step: 9390, Training Logs: loss_final: 0.891976, loss_mean: 0.850081, loss_mean_cls: 0.041895, grad_norm: 0.445294
[[34m2025-10-04 12:44:24[0m] Step: 9391, Training Logs: loss_final: 0.873810, loss_mean: 0.831586, loss_mean_cls: 0.042224, grad_norm: 0.598686
[[34m2025-10-04 12:44:24[0m] Step: 9392, Training Logs: loss_final: 0.875795, loss_mean: 0.833659, loss_mean_cls: 0.042136, grad_norm: 0.222551
[[34m2025-10-04 12:44:25[0m] Step: 9393, Training Logs: loss_final: 0.869474, loss_mean: 0.827028, loss_mean_cls: 0.042446, grad_norm: 0.421276
[[34m2025-10-04 12:44:25[0m] Step: 9394, Training Logs: loss_final: 0.878913, loss_mean: 0.836878, loss_mean_cls: 0.042035, grad_norm: 0.190660
[[34m2025-10-04 12:44:25[0m] Step: 9395, Training Logs: loss_final: 0.892061, loss_mean: 0.849374, loss_mean_cls: 0.042688, grad_norm: 0.378096
[[34m2025-10-04 12:44:26[0m] Step: 9396, Training Logs: loss_final: 0.887047, loss_mean: 0.843601, loss_mean_cls: 0.043447, grad_norm: 0.209869
[[34m2025-10-04 12:44:26[0m] Step: 9397, Training Logs: loss_final: 0.885163, loss_mean: 0.842855, loss_mean_cls: 0.042308, grad_norm: 0.375900
[[34m2025-10-04 12:44:26[0m] Step: 9398, Training Logs: loss_final: 0.859549, loss_mean: 0.815138, loss_mean_cls: 0.044411, grad_norm: 0.348140
[[34m2025-10-04 12:44:26[0m] Step: 9399, Training Logs: loss_final: 0.878775, loss_mean: 0.836398, loss_mean_cls: 0.042377, grad_norm: 0.279380
[[34m2025-10-04 12:44:27[0m] Step: 9400, Training Logs: loss_final: 0.856271, loss_mean: 0.812963, loss_mean_cls: 0.043308, grad_norm: 0.335845
[[34m2025-10-04 12:44:27[0m] Step: 9401, Training Logs: loss_final: 0.861976, loss_mean: 0.818182, loss_mean_cls: 0.043794, grad_norm: 0.242830
[[34m2025-10-04 12:44:27[0m] Step: 9402, Training Logs: loss_final: 0.866484, loss_mean: 0.822751, loss_mean_cls: 0.043732, grad_norm: 0.206565
[[34m2025-10-04 12:44:28[0m] Step: 9403, Training Logs: loss_final: 0.892113, loss_mean: 0.849355, loss_mean_cls: 0.042758, grad_norm: 0.205542
[[34m2025-10-04 12:44:28[0m] Step: 9404, Training Logs: loss_final: 0.852908, loss_mean: 0.811353, loss_mean_cls: 0.041555, grad_norm: 0.253526
[[34m2025-10-04 12:44:28[0m] Step: 9405, Training Logs: loss_final: 0.856819, loss_mean: 0.813304, loss_mean_cls: 0.043516, grad_norm: 0.266050
[[34m2025-10-04 12:44:28[0m] Step: 9406, Training Logs: loss_final: 0.859122, loss_mean: 0.815221, loss_mean_cls: 0.043901, grad_norm: 0.198718
[[34m2025-10-04 12:44:29[0m] Step: 9407, Training Logs: loss_final: 0.884140, loss_mean: 0.841226, loss_mean_cls: 0.042914, grad_norm: 0.261322
[[34m2025-10-04 12:44:29[0m] Step: 9408, Training Logs: loss_final: 0.881601, loss_mean: 0.839030, loss_mean_cls: 0.042571, grad_norm: 0.181924
[[34m2025-10-04 12:44:29[0m] Step: 9409, Training Logs: loss_final: 0.878195, loss_mean: 0.835243, loss_mean_cls: 0.042952, grad_norm: 0.264530
[[34m2025-10-04 12:44:30[0m] Step: 9410, Training Logs: loss_final: 0.862452, loss_mean: 0.819590, loss_mean_cls: 0.042862, grad_norm: 0.192285
[[34m2025-10-04 12:44:30[0m] Step: 9411, Training Logs: loss_final: 0.862281, loss_mean: 0.818533, loss_mean_cls: 0.043748, grad_norm: 0.268299
[[34m2025-10-04 12:44:30[0m] Step: 9412, Training Logs: loss_final: 0.890193, loss_mean: 0.847963, loss_mean_cls: 0.042230, grad_norm: 0.207562
[[34m2025-10-04 12:44:31[0m] Step: 9413, Training Logs: loss_final: 0.877152, loss_mean: 0.833561, loss_mean_cls: 0.043591, grad_norm: 0.209840
[[34m2025-10-04 12:44:31[0m] Step: 9414, Training Logs: loss_final: 0.866694, loss_mean: 0.823135, loss_mean_cls: 0.043559, grad_norm: 0.239211
[[34m2025-10-04 12:44:31[0m] Step: 9415, Training Logs: loss_final: 0.881986, loss_mean: 0.840434, loss_mean_cls: 0.041552, grad_norm: 0.172415
[[34m2025-10-04 12:44:31[0m] Step: 9416, Training Logs: loss_final: 0.881770, loss_mean: 0.838860, loss_mean_cls: 0.042910, grad_norm: 0.242443
[[34m2025-10-04 12:44:32[0m] Step: 9417, Training Logs: loss_final: 0.868160, loss_mean: 0.825141, loss_mean_cls: 0.043018, grad_norm: 0.238952
[[34m2025-10-04 12:44:32[0m] Step: 9418, Training Logs: loss_final: 0.863612, loss_mean: 0.819789, loss_mean_cls: 0.043823, grad_norm: 0.261137
[[34m2025-10-04 12:44:32[0m] Step: 9419, Training Logs: loss_final: 0.871177, loss_mean: 0.827782, loss_mean_cls: 0.043395, grad_norm: 0.198727
[[34m2025-10-04 12:44:33[0m] Step: 9420, Training Logs: loss_final: 0.875260, loss_mean: 0.831893, loss_mean_cls: 0.043367, grad_norm: 0.360446
[[34m2025-10-04 12:44:33[0m] Step: 9421, Training Logs: loss_final: 0.855141, loss_mean: 0.812359, loss_mean_cls: 0.042782, grad_norm: 0.233048
[[34m2025-10-04 12:44:33[0m] Step: 9422, Training Logs: loss_final: 0.868462, loss_mean: 0.824402, loss_mean_cls: 0.044060, grad_norm: 0.313321
[[34m2025-10-04 12:44:33[0m] Step: 9423, Training Logs: loss_final: 0.867847, loss_mean: 0.826198, loss_mean_cls: 0.041649, grad_norm: 0.251133
[[34m2025-10-04 12:44:34[0m] Step: 9424, Training Logs: loss_final: 0.859030, loss_mean: 0.815935, loss_mean_cls: 0.043095, grad_norm: 0.332511
[[34m2025-10-04 12:44:34[0m] Step: 9425, Training Logs: loss_final: 0.881369, loss_mean: 0.838363, loss_mean_cls: 0.043006, grad_norm: 0.266957
[[34m2025-10-04 12:44:34[0m] Step: 9426, Training Logs: loss_final: 0.867059, loss_mean: 0.823795, loss_mean_cls: 0.043264, grad_norm: 0.235570
[[34m2025-10-04 12:44:35[0m] Step: 9427, Training Logs: loss_final: 0.879945, loss_mean: 0.836907, loss_mean_cls: 0.043038, grad_norm: 0.260129
[[34m2025-10-04 12:44:35[0m] Step: 9428, Training Logs: loss_final: 0.866927, loss_mean: 0.824528, loss_mean_cls: 0.042400, grad_norm: 0.441135
[[34m2025-10-04 12:44:35[0m] Step: 9429, Training Logs: loss_final: 0.861832, loss_mean: 0.819283, loss_mean_cls: 0.042549, grad_norm: 0.230302
[[34m2025-10-04 12:44:35[0m] Step: 9430, Training Logs: loss_final: 0.865893, loss_mean: 0.822647, loss_mean_cls: 0.043246, grad_norm: 0.394978
[[34m2025-10-04 12:44:36[0m] Step: 9431, Training Logs: loss_final: 0.869729, loss_mean: 0.827291, loss_mean_cls: 0.042438, grad_norm: 0.267122
[[34m2025-10-04 12:44:36[0m] Step: 9432, Training Logs: loss_final: 0.861007, loss_mean: 0.819309, loss_mean_cls: 0.041698, grad_norm: 0.255542
[[34m2025-10-04 12:44:36[0m] Step: 9433, Training Logs: loss_final: 0.860741, loss_mean: 0.817824, loss_mean_cls: 0.042916, grad_norm: 0.209615
[[34m2025-10-04 12:44:37[0m] Step: 9434, Training Logs: loss_final: 0.859598, loss_mean: 0.817706, loss_mean_cls: 0.041891, grad_norm: 0.163328
[[34m2025-10-04 12:44:37[0m] Step: 9435, Training Logs: loss_final: 0.878508, loss_mean: 0.835594, loss_mean_cls: 0.042914, grad_norm: 0.193744
[[34m2025-10-04 12:44:37[0m] Step: 9436, Training Logs: loss_final: 0.867220, loss_mean: 0.824170, loss_mean_cls: 0.043050, grad_norm: 0.182141
[[34m2025-10-04 12:44:37[0m] Step: 9437, Training Logs: loss_final: 0.885647, loss_mean: 0.842338, loss_mean_cls: 0.043309, grad_norm: 0.292786
[[34m2025-10-04 12:44:38[0m] Step: 9438, Training Logs: loss_final: 0.861440, loss_mean: 0.819321, loss_mean_cls: 0.042119, grad_norm: 0.210043
[[34m2025-10-04 12:44:38[0m] Step: 9439, Training Logs: loss_final: 0.879909, loss_mean: 0.837052, loss_mean_cls: 0.042857, grad_norm: 0.207333
[[34m2025-10-04 12:44:38[0m] Step: 9440, Training Logs: loss_final: 0.856053, loss_mean: 0.813291, loss_mean_cls: 0.042763, grad_norm: 0.192592
[[34m2025-10-04 12:44:39[0m] Step: 9441, Training Logs: loss_final: 0.888462, loss_mean: 0.846158, loss_mean_cls: 0.042304, grad_norm: 0.205419
[[34m2025-10-04 12:44:39[0m] Step: 9442, Training Logs: loss_final: 0.885647, loss_mean: 0.841806, loss_mean_cls: 0.043841, grad_norm: 0.267044
[[34m2025-10-04 12:44:39[0m] Step: 9443, Training Logs: loss_final: 0.865149, loss_mean: 0.821071, loss_mean_cls: 0.044078, grad_norm: 0.186091
[[34m2025-10-04 12:44:40[0m] Step: 9444, Training Logs: loss_final: 0.888584, loss_mean: 0.846665, loss_mean_cls: 0.041918, grad_norm: 0.300200
[[34m2025-10-04 12:44:40[0m] Step: 9445, Training Logs: loss_final: 0.868132, loss_mean: 0.824436, loss_mean_cls: 0.043696, grad_norm: 0.274744
[[34m2025-10-04 12:44:40[0m] Step: 9446, Training Logs: loss_final: 0.853752, loss_mean: 0.811560, loss_mean_cls: 0.042192, grad_norm: 0.200024
[[34m2025-10-04 12:44:40[0m] Step: 9447, Training Logs: loss_final: 0.871375, loss_mean: 0.828234, loss_mean_cls: 0.043141, grad_norm: 0.188930
[[34m2025-10-04 12:44:41[0m] Step: 9448, Training Logs: loss_final: 0.863277, loss_mean: 0.821085, loss_mean_cls: 0.042192, grad_norm: 0.222070
[[34m2025-10-04 12:44:41[0m] Step: 9449, Training Logs: loss_final: 0.869423, loss_mean: 0.826943, loss_mean_cls: 0.042480, grad_norm: 0.153345
[[34m2025-10-04 12:44:41[0m] Step: 9450, Training Logs: loss_final: 0.880178, loss_mean: 0.838193, loss_mean_cls: 0.041984, grad_norm: 0.268012
[[34m2025-10-04 12:44:42[0m] Step: 9451, Training Logs: loss_final: 0.877339, loss_mean: 0.833782, loss_mean_cls: 0.043557, grad_norm: 0.178038
[[34m2025-10-04 12:44:42[0m] Step: 9452, Training Logs: loss_final: 0.864437, loss_mean: 0.819177, loss_mean_cls: 0.045260, grad_norm: 0.306228
[[34m2025-10-04 12:44:42[0m] Step: 9453, Training Logs: loss_final: 0.865588, loss_mean: 0.822222, loss_mean_cls: 0.043366, grad_norm: 0.226025
[[34m2025-10-04 12:44:42[0m] Step: 9454, Training Logs: loss_final: 0.878777, loss_mean: 0.837660, loss_mean_cls: 0.041117, grad_norm: 0.312126
[[34m2025-10-04 12:44:43[0m] Step: 9455, Training Logs: loss_final: 0.871637, loss_mean: 0.828438, loss_mean_cls: 0.043199, grad_norm: 0.294980
[[34m2025-10-04 12:44:43[0m] Step: 9456, Training Logs: loss_final: 0.886386, loss_mean: 0.843200, loss_mean_cls: 0.043186, grad_norm: 0.186073
[[34m2025-10-04 12:44:43[0m] Step: 9457, Training Logs: loss_final: 0.856977, loss_mean: 0.813484, loss_mean_cls: 0.043493, grad_norm: 0.234615
[[34m2025-10-04 12:44:44[0m] Step: 9458, Training Logs: loss_final: 0.869226, loss_mean: 0.826746, loss_mean_cls: 0.042480, grad_norm: 0.196919
[[34m2025-10-04 12:44:44[0m] Step: 9459, Training Logs: loss_final: 0.861084, loss_mean: 0.817898, loss_mean_cls: 0.043186, grad_norm: 0.188897
[[34m2025-10-04 12:44:44[0m] Step: 9460, Training Logs: loss_final: 0.872915, loss_mean: 0.832076, loss_mean_cls: 0.040839, grad_norm: 0.199132
[[34m2025-10-04 12:44:45[0m] Step: 9461, Training Logs: loss_final: 0.867287, loss_mean: 0.824550, loss_mean_cls: 0.042737, grad_norm: 0.240807
[[34m2025-10-04 12:44:45[0m] Step: 9462, Training Logs: loss_final: 0.879279, loss_mean: 0.836523, loss_mean_cls: 0.042756, grad_norm: 0.173830
[[34m2025-10-04 12:44:45[0m] Step: 9463, Training Logs: loss_final: 0.887534, loss_mean: 0.844949, loss_mean_cls: 0.042584, grad_norm: 0.173702
[[34m2025-10-04 12:44:45[0m] Step: 9464, Training Logs: loss_final: 0.891024, loss_mean: 0.848451, loss_mean_cls: 0.042573, grad_norm: 0.235560
[[34m2025-10-04 12:44:46[0m] Step: 9465, Training Logs: loss_final: 0.869697, loss_mean: 0.827886, loss_mean_cls: 0.041812, grad_norm: 0.307228
[[34m2025-10-04 12:44:46[0m] Step: 9466, Training Logs: loss_final: 0.872383, loss_mean: 0.828772, loss_mean_cls: 0.043612, grad_norm: 0.183840
[[34m2025-10-04 12:44:46[0m] Step: 9467, Training Logs: loss_final: 0.871237, loss_mean: 0.827399, loss_mean_cls: 0.043838, grad_norm: 0.187549
[[34m2025-10-04 12:44:47[0m] Step: 9468, Training Logs: loss_final: 0.841324, loss_mean: 0.797749, loss_mean_cls: 0.043575, grad_norm: 0.323597
[[34m2025-10-04 12:44:47[0m] Step: 9469, Training Logs: loss_final: 0.864954, loss_mean: 0.821519, loss_mean_cls: 0.043436, grad_norm: 0.276704
[[34m2025-10-04 12:44:47[0m] Step: 9470, Training Logs: loss_final: 0.863039, loss_mean: 0.819908, loss_mean_cls: 0.043131, grad_norm: 0.256666
[[34m2025-10-04 12:44:48[0m] Step: 9471, Training Logs: loss_final: 0.872371, loss_mean: 0.830363, loss_mean_cls: 0.042009, grad_norm: 0.405319
[[34m2025-10-04 12:44:48[0m] Step: 9472, Training Logs: loss_final: 0.860600, loss_mean: 0.816873, loss_mean_cls: 0.043727, grad_norm: 0.168291
[[34m2025-10-04 12:44:48[0m] Step: 9473, Training Logs: loss_final: 0.880843, loss_mean: 0.838455, loss_mean_cls: 0.042387, grad_norm: 0.413554
[[34m2025-10-04 12:44:48[0m] Step: 9474, Training Logs: loss_final: 0.875961, loss_mean: 0.833463, loss_mean_cls: 0.042498, grad_norm: 0.213705
[[34m2025-10-04 12:44:49[0m] Step: 9475, Training Logs: loss_final: 0.877479, loss_mean: 0.835565, loss_mean_cls: 0.041914, grad_norm: 0.449725
[[34m2025-10-04 12:44:49[0m] Step: 9476, Training Logs: loss_final: 0.865757, loss_mean: 0.822333, loss_mean_cls: 0.043423, grad_norm: 0.203281
[[34m2025-10-04 12:44:49[0m] Step: 9477, Training Logs: loss_final: 0.868241, loss_mean: 0.825798, loss_mean_cls: 0.042443, grad_norm: 0.325931
[[34m2025-10-04 12:44:50[0m] Step: 9478, Training Logs: loss_final: 0.852856, loss_mean: 0.809795, loss_mean_cls: 0.043060, grad_norm: 0.272213
[[34m2025-10-04 12:44:50[0m] Step: 9479, Training Logs: loss_final: 0.865635, loss_mean: 0.823071, loss_mean_cls: 0.042564, grad_norm: 0.238798
[[34m2025-10-04 12:44:50[0m] Step: 9480, Training Logs: loss_final: 0.832564, loss_mean: 0.787671, loss_mean_cls: 0.044893, grad_norm: 0.238282
[[34m2025-10-04 12:44:50[0m] Step: 9481, Training Logs: loss_final: 0.860776, loss_mean: 0.817923, loss_mean_cls: 0.042852, grad_norm: 0.343982
[[34m2025-10-04 12:44:51[0m] Step: 9482, Training Logs: loss_final: 0.897162, loss_mean: 0.854487, loss_mean_cls: 0.042675, grad_norm: 0.268416
[[34m2025-10-04 12:44:51[0m] Step: 9483, Training Logs: loss_final: 0.872666, loss_mean: 0.830409, loss_mean_cls: 0.042257, grad_norm: 0.305873
[[34m2025-10-04 12:44:51[0m] Step: 9484, Training Logs: loss_final: 0.850772, loss_mean: 0.808528, loss_mean_cls: 0.042244, grad_norm: 0.178807
[[34m2025-10-04 12:44:52[0m] Step: 9485, Training Logs: loss_final: 0.876900, loss_mean: 0.835444, loss_mean_cls: 0.041457, grad_norm: 0.396961
[[34m2025-10-04 12:44:52[0m] Step: 9486, Training Logs: loss_final: 0.862732, loss_mean: 0.819732, loss_mean_cls: 0.043000, grad_norm: 0.199939
[[34m2025-10-04 12:44:52[0m] Step: 9487, Training Logs: loss_final: 0.868916, loss_mean: 0.826662, loss_mean_cls: 0.042254, grad_norm: 0.215685
[[34m2025-10-04 12:44:53[0m] Step: 9488, Training Logs: loss_final: 0.897061, loss_mean: 0.855170, loss_mean_cls: 0.041890, grad_norm: 0.244393
[[34m2025-10-04 12:44:53[0m] Step: 9489, Training Logs: loss_final: 0.879823, loss_mean: 0.836236, loss_mean_cls: 0.043587, grad_norm: 0.421911
[[34m2025-10-04 12:44:53[0m] Step: 9490, Training Logs: loss_final: 0.852740, loss_mean: 0.809168, loss_mean_cls: 0.043572, grad_norm: 0.353365
[[34m2025-10-04 12:44:53[0m] Step: 9491, Training Logs: loss_final: 0.873363, loss_mean: 0.829909, loss_mean_cls: 0.043454, grad_norm: 0.218484
[[34m2025-10-04 12:44:54[0m] Step: 9492, Training Logs: loss_final: 0.889889, loss_mean: 0.846126, loss_mean_cls: 0.043763, grad_norm: 0.436548
[[34m2025-10-04 12:44:54[0m] Step: 9493, Training Logs: loss_final: 0.854007, loss_mean: 0.811109, loss_mean_cls: 0.042899, grad_norm: 0.298662
[[34m2025-10-04 12:44:54[0m] Step: 9494, Training Logs: loss_final: 0.852822, loss_mean: 0.808720, loss_mean_cls: 0.044103, grad_norm: 0.318675
[[34m2025-10-04 12:44:55[0m] Step: 9495, Training Logs: loss_final: 0.872364, loss_mean: 0.829742, loss_mean_cls: 0.042622, grad_norm: 0.253372
[[34m2025-10-04 12:44:55[0m] Step: 9496, Training Logs: loss_final: 0.857640, loss_mean: 0.814944, loss_mean_cls: 0.042696, grad_norm: 0.230281
[[34m2025-10-04 12:44:55[0m] Step: 9497, Training Logs: loss_final: 0.860081, loss_mean: 0.817300, loss_mean_cls: 0.042781, grad_norm: 0.266700
[[34m2025-10-04 12:44:55[0m] Step: 9498, Training Logs: loss_final: 0.877926, loss_mean: 0.834105, loss_mean_cls: 0.043821, grad_norm: 0.292443
[[34m2025-10-04 12:44:56[0m] Step: 9499, Training Logs: loss_final: 0.885639, loss_mean: 0.843563, loss_mean_cls: 0.042075, grad_norm: 0.295779
[[34m2025-10-04 12:44:56[0m] Step: 9500, Training Logs: loss_final: 0.863560, loss_mean: 0.821199, loss_mean_cls: 0.042361, grad_norm: 0.184932
[[34m2025-10-04 12:44:56[0m] Step: 9501, Training Logs: loss_final: 0.860185, loss_mean: 0.816806, loss_mean_cls: 0.043378, grad_norm: 0.245488
[[34m2025-10-04 12:44:57[0m] Step: 9502, Training Logs: loss_final: 0.856200, loss_mean: 0.813519, loss_mean_cls: 0.042682, grad_norm: 0.352933
[[34m2025-10-04 12:44:57[0m] Step: 9503, Training Logs: loss_final: 0.859360, loss_mean: 0.815886, loss_mean_cls: 0.043474, grad_norm: 0.205018
[[34m2025-10-04 12:44:57[0m] Step: 9504, Training Logs: loss_final: 0.876968, loss_mean: 0.835120, loss_mean_cls: 0.041847, grad_norm: 0.491319
[[34m2025-10-04 12:44:58[0m] Step: 9505, Training Logs: loss_final: 0.870125, loss_mean: 0.827439, loss_mean_cls: 0.042686, grad_norm: 0.277605
[[34m2025-10-04 12:44:58[0m] Step: 9506, Training Logs: loss_final: 0.860435, loss_mean: 0.816020, loss_mean_cls: 0.044414, grad_norm: 0.440609
[[34m2025-10-04 12:44:58[0m] Step: 9507, Training Logs: loss_final: 0.881695, loss_mean: 0.839608, loss_mean_cls: 0.042087, grad_norm: 0.276951
[[34m2025-10-04 12:44:58[0m] Step: 9508, Training Logs: loss_final: 0.876778, loss_mean: 0.834254, loss_mean_cls: 0.042524, grad_norm: 0.355096
[[34m2025-10-04 12:44:59[0m] Step: 9509, Training Logs: loss_final: 0.864943, loss_mean: 0.821521, loss_mean_cls: 0.043421, grad_norm: 0.349460
[[34m2025-10-04 12:44:59[0m] Step: 9510, Training Logs: loss_final: 0.878727, loss_mean: 0.836468, loss_mean_cls: 0.042259, grad_norm: 0.313534
[[34m2025-10-04 12:44:59[0m] Step: 9511, Training Logs: loss_final: 0.871664, loss_mean: 0.828764, loss_mean_cls: 0.042899, grad_norm: 0.237222
[[34m2025-10-04 12:45:00[0m] Step: 9512, Training Logs: loss_final: 0.860363, loss_mean: 0.818845, loss_mean_cls: 0.041518, grad_norm: 0.241187
[[34m2025-10-04 12:45:00[0m] Step: 9513, Training Logs: loss_final: 0.888454, loss_mean: 0.845258, loss_mean_cls: 0.043196, grad_norm: 0.254167
[[34m2025-10-04 12:45:00[0m] Step: 9514, Training Logs: loss_final: 0.878808, loss_mean: 0.835902, loss_mean_cls: 0.042906, grad_norm: 0.338777
[[34m2025-10-04 12:45:00[0m] Step: 9515, Training Logs: loss_final: 0.878467, loss_mean: 0.836577, loss_mean_cls: 0.041890, grad_norm: 0.221923
[[34m2025-10-04 12:45:01[0m] Step: 9516, Training Logs: loss_final: 0.884308, loss_mean: 0.842284, loss_mean_cls: 0.042024, grad_norm: 0.367377
[[34m2025-10-04 12:45:01[0m] Step: 9517, Training Logs: loss_final: 0.862003, loss_mean: 0.818231, loss_mean_cls: 0.043773, grad_norm: 0.286467
[[34m2025-10-04 12:45:01[0m] Step: 9518, Training Logs: loss_final: 0.869064, loss_mean: 0.826071, loss_mean_cls: 0.042993, grad_norm: 0.294322
[[34m2025-10-04 12:45:02[0m] Step: 9519, Training Logs: loss_final: 0.868201, loss_mean: 0.824705, loss_mean_cls: 0.043496, grad_norm: 0.319520
[[34m2025-10-04 12:45:02[0m] Step: 9520, Training Logs: loss_final: 0.874933, loss_mean: 0.830987, loss_mean_cls: 0.043946, grad_norm: 0.268903
[[34m2025-10-04 12:45:02[0m] Step: 9521, Training Logs: loss_final: 0.883704, loss_mean: 0.840888, loss_mean_cls: 0.042816, grad_norm: 0.251511
[[34m2025-10-04 12:45:03[0m] Step: 9522, Training Logs: loss_final: 0.890871, loss_mean: 0.848150, loss_mean_cls: 0.042720, grad_norm: 0.225483
[[34m2025-10-04 12:45:03[0m] Step: 9523, Training Logs: loss_final: 0.842393, loss_mean: 0.798992, loss_mean_cls: 0.043401, grad_norm: 0.247672
[[34m2025-10-04 12:45:03[0m] Step: 9524, Training Logs: loss_final: 0.871072, loss_mean: 0.828206, loss_mean_cls: 0.042866, grad_norm: 0.268148
[[34m2025-10-04 12:45:03[0m] Step: 9525, Training Logs: loss_final: 0.859816, loss_mean: 0.817562, loss_mean_cls: 0.042254, grad_norm: 0.238962
[[34m2025-10-04 12:45:04[0m] Step: 9526, Training Logs: loss_final: 0.873294, loss_mean: 0.830464, loss_mean_cls: 0.042831, grad_norm: 0.366205
[[34m2025-10-04 12:45:04[0m] Step: 9527, Training Logs: loss_final: 0.865271, loss_mean: 0.822001, loss_mean_cls: 0.043270, grad_norm: 0.232309
[[34m2025-10-04 12:45:04[0m] Step: 9528, Training Logs: loss_final: 0.860783, loss_mean: 0.817732, loss_mean_cls: 0.043050, grad_norm: 0.214573
[[34m2025-10-04 12:45:05[0m] Step: 9529, Training Logs: loss_final: 0.906963, loss_mean: 0.865303, loss_mean_cls: 0.041660, grad_norm: 0.249894
[[34m2025-10-04 12:45:05[0m] Step: 9530, Training Logs: loss_final: 0.870481, loss_mean: 0.827372, loss_mean_cls: 0.043110, grad_norm: 0.300553
[[34m2025-10-04 12:45:05[0m] Step: 9531, Training Logs: loss_final: 0.875636, loss_mean: 0.832468, loss_mean_cls: 0.043168, grad_norm: 0.178602
[[34m2025-10-04 12:45:05[0m] Step: 9532, Training Logs: loss_final: 0.854442, loss_mean: 0.811527, loss_mean_cls: 0.042915, grad_norm: 0.163432
[[34m2025-10-04 12:45:06[0m] Step: 9533, Training Logs: loss_final: 0.875635, loss_mean: 0.833294, loss_mean_cls: 0.042341, grad_norm: 0.245845
[[34m2025-10-04 12:45:06[0m] Step: 9534, Training Logs: loss_final: 0.880931, loss_mean: 0.837556, loss_mean_cls: 0.043375, grad_norm: 0.207480
[[34m2025-10-04 12:45:06[0m] Step: 9535, Training Logs: loss_final: 0.863606, loss_mean: 0.820390, loss_mean_cls: 0.043215, grad_norm: 0.231994
[[34m2025-10-04 12:45:07[0m] Step: 9536, Training Logs: loss_final: 0.866708, loss_mean: 0.824450, loss_mean_cls: 0.042258, grad_norm: 0.247323
[[34m2025-10-04 12:45:07[0m] Step: 9537, Training Logs: loss_final: 0.875192, loss_mean: 0.831549, loss_mean_cls: 0.043643, grad_norm: 0.206317
[[34m2025-10-04 12:45:07[0m] Step: 9538, Training Logs: loss_final: 0.885128, loss_mean: 0.842367, loss_mean_cls: 0.042761, grad_norm: 0.213397
[[34m2025-10-04 12:45:08[0m] Step: 9539, Training Logs: loss_final: 0.867347, loss_mean: 0.824449, loss_mean_cls: 0.042898, grad_norm: 0.256619
[[34m2025-10-04 12:45:08[0m] Step: 9540, Training Logs: loss_final: 0.871965, loss_mean: 0.829781, loss_mean_cls: 0.042183, grad_norm: 0.160062
[[34m2025-10-04 12:45:08[0m] Step: 9541, Training Logs: loss_final: 0.881312, loss_mean: 0.838535, loss_mean_cls: 0.042777, grad_norm: 0.221064
[[34m2025-10-04 12:45:08[0m] Step: 9542, Training Logs: loss_final: 0.875835, loss_mean: 0.832572, loss_mean_cls: 0.043263, grad_norm: 0.182045
[[34m2025-10-04 12:45:09[0m] Step: 9543, Training Logs: loss_final: 0.846488, loss_mean: 0.804123, loss_mean_cls: 0.042365, grad_norm: 0.215243
[[34m2025-10-04 12:45:09[0m] Step: 9544, Training Logs: loss_final: 0.862781, loss_mean: 0.820195, loss_mean_cls: 0.042587, grad_norm: 0.227090
[[34m2025-10-04 12:45:09[0m] Step: 9545, Training Logs: loss_final: 0.871421, loss_mean: 0.828698, loss_mean_cls: 0.042723, grad_norm: 0.190860
[[34m2025-10-04 12:45:10[0m] Step: 9546, Training Logs: loss_final: 0.867886, loss_mean: 0.824462, loss_mean_cls: 0.043424, grad_norm: 0.165428
[[34m2025-10-04 12:45:10[0m] Step: 9547, Training Logs: loss_final: 0.890985, loss_mean: 0.848650, loss_mean_cls: 0.042335, grad_norm: 0.323154
[[34m2025-10-04 12:45:10[0m] Step: 9548, Training Logs: loss_final: 0.883384, loss_mean: 0.841156, loss_mean_cls: 0.042228, grad_norm: 0.326124
[[34m2025-10-04 12:45:10[0m] Step: 9549, Training Logs: loss_final: 0.865752, loss_mean: 0.822589, loss_mean_cls: 0.043164, grad_norm: 0.228231
[[34m2025-10-04 12:45:11[0m] Step: 9550, Training Logs: loss_final: 0.865901, loss_mean: 0.823078, loss_mean_cls: 0.042824, grad_norm: 0.236224
[[34m2025-10-04 12:45:11[0m] Step: 9551, Training Logs: loss_final: 0.892845, loss_mean: 0.850096, loss_mean_cls: 0.042748, grad_norm: 0.255123
[[34m2025-10-04 12:45:11[0m] Step: 9552, Training Logs: loss_final: 0.866035, loss_mean: 0.823490, loss_mean_cls: 0.042545, grad_norm: 0.235586
[[34m2025-10-04 12:45:12[0m] Step: 9553, Training Logs: loss_final: 0.875889, loss_mean: 0.834137, loss_mean_cls: 0.041752, grad_norm: 0.209314
[[34m2025-10-04 12:45:12[0m] Step: 9554, Training Logs: loss_final: 0.878857, loss_mean: 0.835270, loss_mean_cls: 0.043587, grad_norm: 0.195379
[[34m2025-10-04 12:45:12[0m] Step: 9555, Training Logs: loss_final: 0.850954, loss_mean: 0.806832, loss_mean_cls: 0.044122, grad_norm: 0.245971
[[34m2025-10-04 12:45:12[0m] Step: 9556, Training Logs: loss_final: 0.857492, loss_mean: 0.813413, loss_mean_cls: 0.044079, grad_norm: 0.283442
[[34m2025-10-04 12:45:13[0m] Step: 9557, Training Logs: loss_final: 0.881809, loss_mean: 0.840088, loss_mean_cls: 0.041721, grad_norm: 0.247688
[[34m2025-10-04 12:45:13[0m] Step: 9558, Training Logs: loss_final: 0.868614, loss_mean: 0.825300, loss_mean_cls: 0.043314, grad_norm: 0.213567
[[34m2025-10-04 12:45:13[0m] Step: 9559, Training Logs: loss_final: 0.864651, loss_mean: 0.820704, loss_mean_cls: 0.043948, grad_norm: 0.195151
[[34m2025-10-04 12:45:14[0m] Step: 9560, Training Logs: loss_final: 0.858450, loss_mean: 0.816028, loss_mean_cls: 0.042422, grad_norm: 0.373192
[[34m2025-10-04 12:45:14[0m] Step: 9561, Training Logs: loss_final: 0.867478, loss_mean: 0.824805, loss_mean_cls: 0.042673, grad_norm: 0.176393
[[34m2025-10-04 12:45:14[0m] Step: 9562, Training Logs: loss_final: 0.856147, loss_mean: 0.813234, loss_mean_cls: 0.042913, grad_norm: 0.272634
[[34m2025-10-04 12:45:14[0m] Step: 9563, Training Logs: loss_final: 0.881012, loss_mean: 0.838550, loss_mean_cls: 0.042462, grad_norm: 0.189603
[[34m2025-10-04 12:45:15[0m] Step: 9564, Training Logs: loss_final: 0.875100, loss_mean: 0.831433, loss_mean_cls: 0.043667, grad_norm: 0.253350
[[34m2025-10-04 12:45:15[0m] Step: 9565, Training Logs: loss_final: 0.883413, loss_mean: 0.840404, loss_mean_cls: 0.043009, grad_norm: 0.309973
[[34m2025-10-04 12:45:15[0m] Step: 9566, Training Logs: loss_final: 0.863179, loss_mean: 0.821418, loss_mean_cls: 0.041761, grad_norm: 0.454247
[[34m2025-10-04 12:45:16[0m] Step: 9567, Training Logs: loss_final: 0.847478, loss_mean: 0.805288, loss_mean_cls: 0.042190, grad_norm: 0.297664
[[34m2025-10-04 12:45:16[0m] Step: 9568, Training Logs: loss_final: 0.860961, loss_mean: 0.818200, loss_mean_cls: 0.042761, grad_norm: 0.201609
[[34m2025-10-04 12:45:16[0m] Step: 9569, Training Logs: loss_final: 0.866957, loss_mean: 0.823942, loss_mean_cls: 0.043015, grad_norm: 0.174688
[[34m2025-10-04 12:45:17[0m] Step: 9570, Training Logs: loss_final: 0.884729, loss_mean: 0.840818, loss_mean_cls: 0.043911, grad_norm: 0.489239
[[34m2025-10-04 12:45:17[0m] Step: 9571, Training Logs: loss_final: 0.863726, loss_mean: 0.821463, loss_mean_cls: 0.042263, grad_norm: 0.279111
[[34m2025-10-04 12:45:17[0m] Step: 9572, Training Logs: loss_final: 0.882889, loss_mean: 0.839734, loss_mean_cls: 0.043155, grad_norm: 0.262158
[[34m2025-10-04 12:45:17[0m] Step: 9573, Training Logs: loss_final: 0.837331, loss_mean: 0.793109, loss_mean_cls: 0.044222, grad_norm: 0.178024
[[34m2025-10-04 12:45:18[0m] Step: 9574, Training Logs: loss_final: 0.866944, loss_mean: 0.824453, loss_mean_cls: 0.042492, grad_norm: 0.313672
[[34m2025-10-04 12:45:18[0m] Step: 9575, Training Logs: loss_final: 0.859338, loss_mean: 0.815126, loss_mean_cls: 0.044212, grad_norm: 0.277632
[[34m2025-10-04 12:45:18[0m] Step: 9576, Training Logs: loss_final: 0.868790, loss_mean: 0.825855, loss_mean_cls: 0.042935, grad_norm: 0.340356
[[34m2025-10-04 12:45:19[0m] Step: 9577, Training Logs: loss_final: 0.885669, loss_mean: 0.844603, loss_mean_cls: 0.041066, grad_norm: 0.375360
[[34m2025-10-04 12:45:19[0m] Step: 9578, Training Logs: loss_final: 0.857224, loss_mean: 0.815584, loss_mean_cls: 0.041639, grad_norm: 0.191321
[[34m2025-10-04 12:45:19[0m] Step: 9579, Training Logs: loss_final: 0.857752, loss_mean: 0.813886, loss_mean_cls: 0.043866, grad_norm: 0.208498
[[34m2025-10-04 12:45:19[0m] Step: 9580, Training Logs: loss_final: 0.860880, loss_mean: 0.817748, loss_mean_cls: 0.043132, grad_norm: 0.212441
[[34m2025-10-04 12:45:20[0m] Step: 9581, Training Logs: loss_final: 0.866446, loss_mean: 0.822826, loss_mean_cls: 0.043621, grad_norm: 0.329504
[[34m2025-10-04 12:45:20[0m] Step: 9582, Training Logs: loss_final: 0.873508, loss_mean: 0.829174, loss_mean_cls: 0.044335, grad_norm: 0.194166
[[34m2025-10-04 12:45:20[0m] Step: 9583, Training Logs: loss_final: 0.867605, loss_mean: 0.826144, loss_mean_cls: 0.041461, grad_norm: 0.256737
[[34m2025-10-04 12:45:21[0m] Step: 9584, Training Logs: loss_final: 0.878957, loss_mean: 0.836809, loss_mean_cls: 0.042148, grad_norm: 0.196969
[[34m2025-10-04 12:45:21[0m] Step: 9585, Training Logs: loss_final: 0.861493, loss_mean: 0.818397, loss_mean_cls: 0.043096, grad_norm: 0.253161
[[34m2025-10-04 12:45:21[0m] Step: 9586, Training Logs: loss_final: 0.879037, loss_mean: 0.836672, loss_mean_cls: 0.042365, grad_norm: 0.267295
[[34m2025-10-04 12:45:21[0m] Step: 9587, Training Logs: loss_final: 0.885166, loss_mean: 0.843247, loss_mean_cls: 0.041919, grad_norm: 0.300100
[[34m2025-10-04 12:45:22[0m] Step: 9588, Training Logs: loss_final: 0.886936, loss_mean: 0.845059, loss_mean_cls: 0.041877, grad_norm: 0.252134
[[34m2025-10-04 12:45:22[0m] Step: 9589, Training Logs: loss_final: 0.861326, loss_mean: 0.817716, loss_mean_cls: 0.043609, grad_norm: 0.230300
[[34m2025-10-04 12:45:22[0m] Step: 9590, Training Logs: loss_final: 0.872954, loss_mean: 0.830307, loss_mean_cls: 0.042647, grad_norm: 0.196513
[[34m2025-10-04 12:45:23[0m] Step: 9591, Training Logs: loss_final: 0.868405, loss_mean: 0.826330, loss_mean_cls: 0.042075, grad_norm: 0.234400
[[34m2025-10-04 12:45:23[0m] Step: 9592, Training Logs: loss_final: 0.886479, loss_mean: 0.843769, loss_mean_cls: 0.042709, grad_norm: 0.300344
[[34m2025-10-04 12:45:23[0m] Step: 9593, Training Logs: loss_final: 0.879216, loss_mean: 0.835857, loss_mean_cls: 0.043359, grad_norm: 0.269001
[[34m2025-10-04 12:45:23[0m] Step: 9594, Training Logs: loss_final: 0.885907, loss_mean: 0.844110, loss_mean_cls: 0.041798, grad_norm: 0.200539
[[34m2025-10-04 12:45:24[0m] Step: 9595, Training Logs: loss_final: 0.874475, loss_mean: 0.831058, loss_mean_cls: 0.043417, grad_norm: 0.305926
[[34m2025-10-04 12:45:24[0m] Step: 9596, Training Logs: loss_final: 0.854871, loss_mean: 0.811507, loss_mean_cls: 0.043364, grad_norm: 0.237266
[[34m2025-10-04 12:45:24[0m] Step: 9597, Training Logs: loss_final: 0.877078, loss_mean: 0.834579, loss_mean_cls: 0.042499, grad_norm: 0.175222
[[34m2025-10-04 12:45:25[0m] Step: 9598, Training Logs: loss_final: 0.877266, loss_mean: 0.834582, loss_mean_cls: 0.042684, grad_norm: 0.199815
[[34m2025-10-04 12:45:25[0m] Step: 9599, Training Logs: loss_final: 0.854798, loss_mean: 0.811096, loss_mean_cls: 0.043702, grad_norm: 0.258048
[[34m2025-10-04 12:45:25[0m] Step: 9600, Training Logs: loss_final: 0.872631, loss_mean: 0.828660, loss_mean_cls: 0.043971, grad_norm: 0.248093
[[34m2025-10-04 12:45:25[0m] Step: 9601, Training Logs: loss_final: 0.884998, loss_mean: 0.843027, loss_mean_cls: 0.041971, grad_norm: 0.180601
[[34m2025-10-04 12:45:26[0m] Step: 9602, Training Logs: loss_final: 0.847648, loss_mean: 0.804390, loss_mean_cls: 0.043258, grad_norm: 0.211598
[[34m2025-10-04 12:45:26[0m] Step: 9603, Training Logs: loss_final: 0.874424, loss_mean: 0.831061, loss_mean_cls: 0.043362, grad_norm: 0.255552
[[34m2025-10-04 12:45:26[0m] Step: 9604, Training Logs: loss_final: 0.865664, loss_mean: 0.824078, loss_mean_cls: 0.041587, grad_norm: 0.300726
[[34m2025-10-04 12:45:27[0m] Step: 9605, Training Logs: loss_final: 0.869029, loss_mean: 0.826043, loss_mean_cls: 0.042985, grad_norm: 0.245305
[[34m2025-10-04 12:45:27[0m] Step: 9606, Training Logs: loss_final: 0.869053, loss_mean: 0.826779, loss_mean_cls: 0.042274, grad_norm: 0.255910
[[34m2025-10-04 12:45:27[0m] Step: 9607, Training Logs: loss_final: 0.862080, loss_mean: 0.819991, loss_mean_cls: 0.042089, grad_norm: 0.227760
[[34m2025-10-04 12:45:28[0m] Step: 9608, Training Logs: loss_final: 0.866045, loss_mean: 0.822074, loss_mean_cls: 0.043971, grad_norm: 0.254694
[[34m2025-10-04 12:45:28[0m] Step: 9609, Training Logs: loss_final: 0.858574, loss_mean: 0.816090, loss_mean_cls: 0.042483, grad_norm: 0.224930
[[34m2025-10-04 12:45:28[0m] Step: 9610, Training Logs: loss_final: 0.876521, loss_mean: 0.833423, loss_mean_cls: 0.043098, grad_norm: 0.244256
[[34m2025-10-04 12:45:28[0m] Step: 9611, Training Logs: loss_final: 0.871513, loss_mean: 0.829068, loss_mean_cls: 0.042445, grad_norm: 0.258446
[[34m2025-10-04 12:45:29[0m] Step: 9612, Training Logs: loss_final: 0.874596, loss_mean: 0.832369, loss_mean_cls: 0.042227, grad_norm: 0.257215
[[34m2025-10-04 12:45:29[0m] Step: 9613, Training Logs: loss_final: 0.886295, loss_mean: 0.842670, loss_mean_cls: 0.043625, grad_norm: 0.227435
[[34m2025-10-04 12:45:29[0m] Step: 9614, Training Logs: loss_final: 0.877596, loss_mean: 0.835320, loss_mean_cls: 0.042276, grad_norm: 0.215754
[[34m2025-10-04 12:45:30[0m] Step: 9615, Training Logs: loss_final: 0.873082, loss_mean: 0.830415, loss_mean_cls: 0.042666, grad_norm: 0.214604
[[34m2025-10-04 12:45:30[0m] Step: 9616, Training Logs: loss_final: 0.866509, loss_mean: 0.824888, loss_mean_cls: 0.041621, grad_norm: 0.305556
[[34m2025-10-04 12:45:30[0m] Step: 9617, Training Logs: loss_final: 0.867854, loss_mean: 0.825393, loss_mean_cls: 0.042461, grad_norm: 0.170854
