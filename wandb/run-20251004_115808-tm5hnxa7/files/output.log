
Steps:   0%|                                                                       | 1/1000000 [00:05<1607:40:57,  5.79s/it][[34m2025-10-04 11:58:22[39m] Generating EMA samples done.
[[34m2025-10-04 11:58:22[39m] Step: 1, Training Logs: loss_final: 1.795647, loss_mean: 1.689609, loss_mean_cls: 0.106038, grad_norm: 1.174266
Steps:   0%|   | 2/1000000 [00:06<710:33:35,  2.56s/it, grad_norm=1.17, loss_final=1.8, loss_mean=1.69, loss_mean_cls=0.106][[34m2025-10-04 11:58:22[39m] Step: 2, Training Logs: loss_final: 1.785069, loss_mean: 1.678836, loss_mean_cls: 0.106234, grad_norm: 0.901399
Steps:   0%| | 3/1000000 [00:06<421:15:18,  1.52s/it, grad_norm=0.901, loss_final=1.79, loss_mean=1.68, loss_mean_cls=0.106][[34m2025-10-04 11:58:23[39m] Step: 3, Training Logs: loss_final: 1.821233, loss_mean: 1.717687, loss_mean_cls: 0.103546, grad_norm: 0.796063
Steps:   0%| | 4/1000000 [00:06<287:52:39,  1.04s/it, grad_norm=0.796, loss_final=1.82, loss_mean=1.72, loss_mean_cls=0.104][[34m2025-10-04 11:58:23[39m] Step: 4, Training Logs: loss_final: 1.778000, loss_mean: 1.672744, loss_mean_cls: 0.105256, grad_norm: 0.684705
Steps:   0%| | 5/1000000 [00:06<211:38:01,  1.31it/s, grad_norm=0.685, loss_final=1.78, loss_mean=1.67, loss_mean_cls=0.105][[34m2025-10-04 11:58:23[39m] Step: 5, Training Logs: loss_final: 1.786037, loss_mean: 1.684995, loss_mean_cls: 0.101042, grad_norm: 0.812753
Steps:   0%| | 6/1000000 [00:07<168:15:26,  1.65it/s, grad_norm=0.813, loss_final=1.79, loss_mean=1.68, loss_mean_cls=0.101][[34m2025-10-04 11:58:23[39m] Step: 6, Training Logs: loss_final: 1.766339, loss_mean: 1.666939, loss_mean_cls: 0.099399, grad_norm: 0.754432
Steps:   0%| | 7/1000000 [00:07<138:33:25,  2.00it/s, grad_norm=0.754, loss_final=1.77, loss_mean=1.67, loss_mean_cls=0.0994[[34m2025-10-04 11:58:24[39m] Step: 7, Training Logs: loss_final: 1.754195, loss_mean: 1.651879, loss_mean_cls: 0.102316, grad_norm: 0.825753
Steps:   0%| | 8/1000000 [00:07<119:11:17,  2.33it/s, grad_norm=0.826, loss_final=1.75, loss_mean=1.65, loss_mean_cls=0.102][[34m2025-10-04 11:58:24[39m] Step: 8, Training Logs: loss_final: 1.751540, loss_mean: 1.652565, loss_mean_cls: 0.098975, grad_norm: 0.830367
Steps:   0%|  | 9/1000000 [00:08<106:16:59,  2.61it/s, grad_norm=0.83, loss_final=1.75, loss_mean=1.65, loss_mean_cls=0.099][[34m2025-10-04 11:58:24[39m] Step: 9, Training Logs: loss_final: 1.738420, loss_mean: 1.636014, loss_mean_cls: 0.102406, grad_norm: 0.858504
Steps:   0%| | 10/1000000 [00:08<99:25:41,  2.79it/s, grad_norm=0.859, loss_final=1.74, loss_mean=1.64, loss_mean_cls=0.102][[34m2025-10-04 11:58:25[39m] Step: 10, Training Logs: loss_final: 1.736533, loss_mean: 1.639352, loss_mean_cls: 0.097181, grad_norm: 1.043700
Steps:   0%| | 11/1000000 [00:08<92:51:50,  2.99it/s, grad_norm=1.04, loss_final=1.74, loss_mean=1.64, loss_mean_cls=0.0972][[34m2025-10-04 11:58:25[39m] Step: 11, Training Logs: loss_final: 1.709282, loss_mean: 1.608108, loss_mean_cls: 0.101174, grad_norm: 1.076937
Steps:   0%|  | 12/1000000 [00:08<88:32:05,  3.14it/s, grad_norm=1.08, loss_final=1.71, loss_mean=1.61, loss_mean_cls=0.101][[34m2025-10-04 11:58:25[39m] Step: 12, Training Logs: loss_final: 1.707039, loss_mean: 1.611309, loss_mean_cls: 0.095730, grad_norm: 1.105959
Steps:   0%| | 13/1000000 [00:09<86:58:14,  3.19it/s, grad_norm=1.11, loss_final=1.71, loss_mean=1.61, loss_mean_cls=0.0957][[34m2025-10-04 11:58:25[39m] Step: 13, Training Logs: loss_final: 1.671346, loss_mean: 1.574493, loss_mean_cls: 0.096853, grad_norm: 0.938799
Steps:   0%| | 14/1000000 [00:09<85:40:06,  3.24it/s, grad_norm=0.939, loss_final=1.67, loss_mean=1.57, loss_mean_cls=0.0969[[34m2025-10-04 11:58:26[39m] Step: 14, Training Logs: loss_final: 1.659532, loss_mean: 1.562734, loss_mean_cls: 0.096798, grad_norm: 0.746807
Steps:   0%| | 15/1000000 [00:09<83:46:26,  3.32it/s, grad_norm=0.747, loss_final=1.66, loss_mean=1.56, loss_mean_cls=0.0968[[34m2025-10-04 11:58:26[39m] Step: 15, Training Logs: loss_final: 1.653482, loss_mean: 1.557485, loss_mean_cls: 0.095997, grad_norm: 0.753487
Steps:   0%| | 16/1000000 [00:10<84:13:50,  3.30it/s, grad_norm=0.753, loss_final=1.65, loss_mean=1.56, loss_mean_cls=0.096][[34m2025-10-04 11:58:26[39m] Step: 16, Training Logs: loss_final: 1.668429, loss_mean: 1.573484, loss_mean_cls: 0.094945, grad_norm: 1.121265
Steps:   0%| | 17/1000000 [00:10<84:07:11,  3.30it/s, grad_norm=1.12, loss_final=1.67, loss_mean=1.57, loss_mean_cls=0.0949][[34m2025-10-04 11:58:27[39m] Step: 17, Training Logs: loss_final: 1.622318, loss_mean: 1.525880, loss_mean_cls: 0.096438, grad_norm: 0.825314
Steps:   0%| | 18/1000000 [00:10<82:43:53,  3.36it/s, grad_norm=0.825, loss_final=1.62, loss_mean=1.53, loss_mean_cls=0.0964[[34m2025-10-04 11:58:27[39m] Step: 18, Training Logs: loss_final: 1.619625, loss_mean: 1.526897, loss_mean_cls: 0.092728, grad_norm: 0.887005
Steps:   0%| | 19/1000000 [00:11<83:54:43,  3.31it/s, grad_norm=0.887, loss_final=1.62, loss_mean=1.53, loss_mean_cls=0.0927[[34m2025-10-04 11:58:27[39m] Step: 19, Training Logs: loss_final: 1.612384, loss_mean: 1.517632, loss_mean_cls: 0.094752, grad_norm: 0.850046
Steps:   0%| | 20/1000000 [00:11<82:24:55,  3.37it/s, grad_norm=0.85, loss_final=1.61, loss_mean=1.52, loss_mean_cls=0.0948][[34m2025-10-04 11:58:27[39m] Step: 20, Training Logs: loss_final: 1.615233, loss_mean: 1.523052, loss_mean_cls: 0.092182, grad_norm: 0.884069
Steps:   0%| | 21/1000000 [00:11<81:10:23,  3.42it/s, grad_norm=0.884, loss_final=1.62, loss_mean=1.52, loss_mean_cls=0.0922[[34m2025-10-04 11:58:28[39m] Step: 21, Training Logs: loss_final: 1.591437, loss_mean: 1.496822, loss_mean_cls: 0.094616, grad_norm: 0.907211
Steps:   0%| | 22/1000000 [00:11<80:22:03,  3.46it/s, grad_norm=0.907, loss_final=1.59, loss_mean=1.5, loss_mean_cls=0.0946][[34m2025-10-04 11:58:28[39m] Step: 22, Training Logs: loss_final: 1.577800, loss_mean: 1.487948, loss_mean_cls: 0.089852, grad_norm: 0.827453
Steps:   0%| | 23/1000000 [00:12<80:32:40,  3.45it/s, grad_norm=0.827, loss_final=1.58, loss_mean=1.49, loss_mean_cls=0.0899[[34m2025-10-04 11:58:28[39m] Step: 23, Training Logs: loss_final: 1.564300, loss_mean: 1.471926, loss_mean_cls: 0.092373, grad_norm: 0.895761
Steps:   0%| | 24/1000000 [00:12<79:48:34,  3.48it/s, grad_norm=0.896, loss_final=1.56, loss_mean=1.47, loss_mean_cls=0.0924[[34m2025-10-04 11:58:29[39m] Step: 24, Training Logs: loss_final: 1.544718, loss_mean: 1.452857, loss_mean_cls: 0.091861, grad_norm: 0.818182
Steps:   0%| | 25/1000000 [00:12<79:13:10,  3.51it/s, grad_norm=0.818, loss_final=1.54, loss_mean=1.45, loss_mean_cls=0.0919[[34m2025-10-04 11:58:29[39m] Step: 25, Training Logs: loss_final: 1.546056, loss_mean: 1.454019, loss_mean_cls: 0.092037, grad_norm: 0.739992
Steps:   0%|  | 26/1000000 [00:13<78:59:01,  3.52it/s, grad_norm=0.74, loss_final=1.55, loss_mean=1.45, loss_mean_cls=0.092][[34m2025-10-04 11:58:29[39m] Step: 26, Training Logs: loss_final: 1.531811, loss_mean: 1.439312, loss_mean_cls: 0.092500, grad_norm: 0.670495
Steps:   0%| | 27/1000000 [00:13<78:32:39,  3.54it/s, grad_norm=0.67, loss_final=1.53, loss_mean=1.44, loss_mean_cls=0.0925][[34m2025-10-04 11:58:29[39m] Step: 27, Training Logs: loss_final: 1.507269, loss_mean: 1.414880, loss_mean_cls: 0.092389, grad_norm: 0.758085
Steps:   0%| | 28/1000000 [00:13<78:57:43,  3.52it/s, grad_norm=0.758, loss_final=1.51, loss_mean=1.41, loss_mean_cls=0.0924[[34m2025-10-04 11:58:30[39m] Step: 28, Training Logs: loss_final: 1.519138, loss_mean: 1.429240, loss_mean_cls: 0.089898, grad_norm: 0.610955
Steps:   0%| | 29/1000000 [00:13<78:34:40,  3.53it/s, grad_norm=0.611, loss_final=1.52, loss_mean=1.43, loss_mean_cls=0.0899[[34m2025-10-04 11:58:30[39m] Step: 29, Training Logs: loss_final: 1.499355, loss_mean: 1.407855, loss_mean_cls: 0.091500, grad_norm: 0.716972
Steps:   0%| | 30/1000000 [00:14<78:30:41,  3.54it/s, grad_norm=0.717, loss_final=1.5, loss_mean=1.41, loss_mean_cls=0.0915][[34m2025-10-04 11:58:30[39m] Step: 30, Training Logs: loss_final: 1.483597, loss_mean: 1.393857, loss_mean_cls: 0.089740, grad_norm: 0.649487
Steps:   0%| | 31/1000000 [00:14<78:16:53,  3.55it/s, grad_norm=0.649, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0897[[34m2025-10-04 11:58:31[39m] Step: 31, Training Logs: loss_final: 1.477716, loss_mean: 1.388294, loss_mean_cls: 0.089422, grad_norm: 0.829367
Steps:   0%| | 32/1000000 [00:14<80:02:40,  3.47it/s, grad_norm=0.829, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0894[[34m2025-10-04 11:58:31[39m] Step: 32, Training Logs: loss_final: 1.488828, loss_mean: 1.401187, loss_mean_cls: 0.087641, grad_norm: 0.938168
Steps:   0%| | 33/1000000 [00:15<79:40:16,  3.49it/s, grad_norm=0.938, loss_final=1.49, loss_mean=1.4, loss_mean_cls=0.0876][[34m2025-10-04 11:58:31[39m] Step: 33, Training Logs: loss_final: 1.479227, loss_mean: 1.392007, loss_mean_cls: 0.087220, grad_norm: 0.871349
Steps:   0%| | 34/1000000 [00:15<79:21:57,  3.50it/s, grad_norm=0.871, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0872[[34m2025-10-04 11:58:31[39m] Step: 34, Training Logs: loss_final: 1.478032, loss_mean: 1.389406, loss_mean_cls: 0.088627, grad_norm: 0.667966
Steps:   0%| | 35/1000000 [00:15<79:13:55,  3.51it/s, grad_norm=0.668, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0886[[34m2025-10-04 11:58:32[39m] Step: 35, Training Logs: loss_final: 1.466846, loss_mean: 1.379078, loss_mean_cls: 0.087767, grad_norm: 0.999040
Steps:   0%| | 36/1000000 [00:15<81:18:39,  3.42it/s, grad_norm=0.999, loss_final=1.47, loss_mean=1.38, loss_mean_cls=0.0878[[34m2025-10-04 11:58:32[39m] Step: 36, Training Logs: loss_final: 1.428661, loss_mean: 1.341122, loss_mean_cls: 0.087538, grad_norm: 0.792584
Steps:   0%| | 37/1000000 [00:16<80:21:24,  3.46it/s, grad_norm=0.793, loss_final=1.43, loss_mean=1.34, loss_mean_cls=0.0875[[34m2025-10-04 11:58:32[39m] Step: 37, Training Logs: loss_final: 1.445827, loss_mean: 1.359899, loss_mean_cls: 0.085929, grad_norm: 1.028947
Steps:   0%| | 38/1000000 [00:16<79:48:37,  3.48it/s, grad_norm=1.03, loss_final=1.45, loss_mean=1.36, loss_mean_cls=0.0859][[34m2025-10-04 11:58:33[39m] Step: 38, Training Logs: loss_final: 1.443309, loss_mean: 1.355692, loss_mean_cls: 0.087617, grad_norm: 0.735793
Steps:   0%| | 39/1000000 [00:16<79:18:33,  3.50it/s, grad_norm=0.736, loss_final=1.44, loss_mean=1.36, loss_mean_cls=0.0876[[34m2025-10-04 11:58:33[39m] Step: 39, Training Logs: loss_final: 1.421322, loss_mean: 1.335917, loss_mean_cls: 0.085405, grad_norm: 0.781432
Steps:   0%| | 40/1000000 [00:17<79:26:18,  3.50it/s, grad_norm=0.781, loss_final=1.42, loss_mean=1.34, loss_mean_cls=0.0854[[34m2025-10-04 11:58:33[39m] Step: 40, Training Logs: loss_final: 1.428990, loss_mean: 1.345265, loss_mean_cls: 0.083725, grad_norm: 0.614151
Steps:   0%| | 41/1000000 [00:17<79:02:39,  3.51it/s, grad_norm=0.614, loss_final=1.43, loss_mean=1.35, loss_mean_cls=0.0837[[34m2025-10-04 11:58:33[39m] Step: 41, Training Logs: loss_final: 1.413831, loss_mean: 1.326741, loss_mean_cls: 0.087090, grad_norm: 0.728774
Steps:   0%| | 42/1000000 [00:17<80:17:16,  3.46it/s, grad_norm=0.729, loss_final=1.41, loss_mean=1.33, loss_mean_cls=0.0871[[34m2025-10-04 11:58:34[39m] Step: 42, Training Logs: loss_final: 1.416155, loss_mean: 1.330340, loss_mean_cls: 0.085814, grad_norm: 0.865924
Steps:   0%| | 43/1000000 [00:17<79:26:48,  3.50it/s, grad_norm=0.866, loss_final=1.42, loss_mean=1.33, loss_mean_cls=0.0858[[34m2025-10-04 11:58:34[39m] Step: 43, Training Logs: loss_final: 1.426980, loss_mean: 1.342608, loss_mean_cls: 0.084372, grad_norm: 1.213044
Steps:   0%| | 44/1000000 [00:18<78:45:24,  3.53it/s, grad_norm=1.21, loss_final=1.43, loss_mean=1.34, loss_mean_cls=0.0844][[34m2025-10-04 11:58:34[39m] Step: 44, Training Logs: loss_final: 1.417303, loss_mean: 1.335616, loss_mean_cls: 0.081687, grad_norm: 1.148594
Steps:   0%| | 45/1000000 [00:18<79:23:08,  3.50it/s, grad_norm=1.15, loss_final=1.42, loss_mean=1.34, loss_mean_cls=0.0817][[34m2025-10-04 11:58:35[39m] Step: 45, Training Logs: loss_final: 1.374272, loss_mean: 1.291312, loss_mean_cls: 0.082960, grad_norm: 0.669817
Steps:   0%|  | 46/1000000 [00:18<78:51:08,  3.52it/s, grad_norm=0.67, loss_final=1.37, loss_mean=1.29, loss_mean_cls=0.083][[34m2025-10-04 11:58:35[39m] Step: 46, Training Logs: loss_final: 1.377121, loss_mean: 1.294297, loss_mean_cls: 0.082824, grad_norm: 0.975531
Steps:   0%| | 47/1000000 [00:19<79:11:18,  3.51it/s, grad_norm=0.976, loss_final=1.38, loss_mean=1.29, loss_mean_cls=0.0828[[34m2025-10-04 11:58:35[39m] Step: 47, Training Logs: loss_final: 1.380542, loss_mean: 1.298003, loss_mean_cls: 0.082539, grad_norm: 1.359640
Steps:   0%|  | 48/1000000 [00:19<80:15:23,  3.46it/s, grad_norm=1.36, loss_final=1.38, loss_mean=1.3, loss_mean_cls=0.0825][[34m2025-10-04 11:58:35[39m] Step: 48, Training Logs: loss_final: 1.373809, loss_mean: 1.291792, loss_mean_cls: 0.082016, grad_norm: 1.120516
Steps:   0%|  | 49/1000000 [00:19<79:30:23,  3.49it/s, grad_norm=1.12, loss_final=1.37, loss_mean=1.29, loss_mean_cls=0.082][[34m2025-10-04 11:58:36[39m] Step: 49, Training Logs: loss_final: 1.355756, loss_mean: 1.274424, loss_mean_cls: 0.081331, grad_norm: 1.070781
Steps:   0%| | 50/1000000 [00:19<79:08:07,  3.51it/s, grad_norm=1.07, loss_final=1.36, loss_mean=1.27, loss_mean_cls=0.0813][[34m2025-10-04 11:58:36[39m] Step: 50, Training Logs: loss_final: 1.365003, loss_mean: 1.282681, loss_mean_cls: 0.082322, grad_norm: 0.959041
Steps:   0%| | 51/1000000 [00:20<78:48:58,  3.52it/s, grad_norm=0.959, loss_final=1.37, loss_mean=1.28, loss_mean_cls=0.0823[[34m2025-10-04 11:58:36[39m] Step: 51, Training Logs: loss_final: 1.352158, loss_mean: 1.271270, loss_mean_cls: 0.080888, grad_norm: 1.033821
Steps:   0%| | 52/1000000 [00:20<79:41:13,  3.49it/s, grad_norm=1.03, loss_final=1.35, loss_mean=1.27, loss_mean_cls=0.0809][[34m2025-10-04 11:58:37[39m] Step: 52, Training Logs: loss_final: 1.309095, loss_mean: 1.229182, loss_mean_cls: 0.079913, grad_norm: 1.091182
Steps:   0%| | 53/1000000 [00:20<81:13:02,  3.42it/s, grad_norm=1.09, loss_final=1.31, loss_mean=1.23, loss_mean_cls=0.0799][[34m2025-10-04 11:58:37[39m] Step: 53, Training Logs: loss_final: 1.342832, loss_mean: 1.261409, loss_mean_cls: 0.081422, grad_norm: 0.836610
Steps:   0%| | 54/1000000 [00:21<82:19:07,  3.37it/s, grad_norm=0.837, loss_final=1.34, loss_mean=1.26, loss_mean_cls=0.0814[[34m2025-10-04 11:58:37[39m] Step: 54, Training Logs: loss_final: 1.308012, loss_mean: 1.231543, loss_mean_cls: 0.076469, grad_norm: 1.063546
Steps:   0%| | 55/1000000 [00:21<82:03:06,  3.39it/s, grad_norm=1.06, loss_final=1.31, loss_mean=1.23, loss_mean_cls=0.0765][[34m2025-10-04 11:58:38[39m] Step: 55, Training Logs: loss_final: 1.315091, loss_mean: 1.236589, loss_mean_cls: 0.078502, grad_norm: 2.240750
Steps:   0%| | 56/1000000 [00:21<80:48:59,  3.44it/s, grad_norm=2.24, loss_final=1.32, loss_mean=1.24, loss_mean_cls=0.0785][[34m2025-10-04 11:58:38[39m] Step: 56, Training Logs: loss_final: 1.286936, loss_mean: 1.208036, loss_mean_cls: 0.078900, grad_norm: 0.838871
Steps:   0%| | 57/1000000 [00:21<82:02:25,  3.39it/s, grad_norm=0.839, loss_final=1.29, loss_mean=1.21, loss_mean_cls=0.0789[[34m2025-10-04 11:58:38[39m] Step: 57, Training Logs: loss_final: 1.272975, loss_mean: 1.193654, loss_mean_cls: 0.079321, grad_norm: 1.586586
Steps:   0%| | 58/1000000 [00:22<81:01:06,  3.43it/s, grad_norm=1.59, loss_final=1.27, loss_mean=1.19, loss_mean_cls=0.0793][[34m2025-10-04 11:58:38[39m] Step: 58, Training Logs: loss_final: 1.325151, loss_mean: 1.246340, loss_mean_cls: 0.078811, grad_norm: 1.656412
Steps:   0%| | 59/1000000 [00:22<80:42:51,  3.44it/s, grad_norm=1.66, loss_final=1.33, loss_mean=1.25, loss_mean_cls=0.0788][[34m2025-10-04 11:58:39[39m] Step: 59, Training Logs: loss_final: 1.270622, loss_mean: 1.193167, loss_mean_cls: 0.077456, grad_norm: 1.205681
Steps:   0%| | 60/1000000 [00:22<80:04:34,  3.47it/s, grad_norm=1.21, loss_final=1.27, loss_mean=1.19, loss_mean_cls=0.0775][[34m2025-10-04 11:58:39[39m] Step: 60, Training Logs: loss_final: 1.283479, loss_mean: 1.205036, loss_mean_cls: 0.078444, grad_norm: 1.382442
Steps:   0%| | 61/1000000 [00:23<81:24:30,  3.41it/s, grad_norm=1.38, loss_final=1.28, loss_mean=1.21, loss_mean_cls=0.0784][[34m2025-10-04 11:58:39[39m] Step: 61, Training Logs: loss_final: 1.255970, loss_mean: 1.178587, loss_mean_cls: 0.077383, grad_norm: 1.058620
Steps:   0%| | 62/1000000 [00:23<82:03:45,  3.38it/s, grad_norm=1.06, loss_final=1.26, loss_mean=1.18, loss_mean_cls=0.0774][[34m2025-10-04 11:58:40[39m] Step: 62, Training Logs: loss_final: 1.257552, loss_mean: 1.182739, loss_mean_cls: 0.074813, grad_norm: 0.873871
Steps:   0%| | 63/1000000 [00:23<81:18:41,  3.42it/s, grad_norm=0.874, loss_final=1.26, loss_mean=1.18, loss_mean_cls=0.0748[[34m2025-10-04 11:58:40[39m] Step: 63, Training Logs: loss_final: 1.253753, loss_mean: 1.176965, loss_mean_cls: 0.076789, grad_norm: 1.645415
Steps:   0%| | 64/1000000 [00:23<81:40:50,  3.40it/s, grad_norm=1.65, loss_final=1.25, loss_mean=1.18, loss_mean_cls=0.0768][[34m2025-10-04 11:58:40[39m] Step: 64, Training Logs: loss_final: 1.215299, loss_mean: 1.139653, loss_mean_cls: 0.075647, grad_norm: 1.055909
Steps:   0%| | 65/1000000 [00:24<81:21:13,  3.41it/s, grad_norm=1.06, loss_final=1.22, loss_mean=1.14, loss_mean_cls=0.0756][[34m2025-10-04 11:58:40[39m] Step: 65, Training Logs: loss_final: 1.263942, loss_mean: 1.187037, loss_mean_cls: 0.076905, grad_norm: 1.124856
Steps:   0%| | 66/1000000 [00:24<81:40:32,  3.40it/s, grad_norm=1.12, loss_final=1.26, loss_mean=1.19, loss_mean_cls=0.0769][[34m2025-10-04 11:58:41[39m] Step: 66, Training Logs: loss_final: 1.239204, loss_mean: 1.164142, loss_mean_cls: 0.075062, grad_norm: 0.871376
Steps:   0%| | 67/1000000 [00:24<80:34:28,  3.45it/s, grad_norm=0.871, loss_final=1.24, loss_mean=1.16, loss_mean_cls=0.0751[[34m2025-10-04 11:58:41[39m] Step: 67, Training Logs: loss_final: 1.210094, loss_mean: 1.134537, loss_mean_cls: 0.075557, grad_norm: 0.926383
Steps:   0%| | 68/1000000 [00:25<80:28:56,  3.45it/s, grad_norm=0.926, loss_final=1.21, loss_mean=1.13, loss_mean_cls=0.0756[[34m2025-10-04 11:58:41[39m] Step: 68, Training Logs: loss_final: 1.245885, loss_mean: 1.172380, loss_mean_cls: 0.073505, grad_norm: 0.998089
Steps:   0%| | 69/1000000 [00:25<79:48:57,  3.48it/s, grad_norm=0.998, loss_final=1.25, loss_mean=1.17, loss_mean_cls=0.0735[[34m2025-10-04 11:58:42[39m] Step: 69, Training Logs: loss_final: 1.222483, loss_mean: 1.148157, loss_mean_cls: 0.074326, grad_norm: 0.835955
Steps:   0%| | 70/1000000 [00:25<81:39:54,  3.40it/s, grad_norm=0.836, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.0743[[34m2025-10-04 11:58:42[39m] Step: 70, Training Logs: loss_final: 1.218089, loss_mean: 1.146074, loss_mean_cls: 0.072015, grad_norm: 0.929099
Steps:   0%| | 71/1000000 [00:26<81:40:50,  3.40it/s, grad_norm=0.929, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.072][[34m2025-10-04 11:58:42[39m] Step: 71, Training Logs: loss_final: 1.244082, loss_mean: 1.170653, loss_mean_cls: 0.073430, grad_norm: 1.489280
Steps:   0%| | 72/1000000 [00:26<81:03:57,  3.43it/s, grad_norm=1.49, loss_final=1.24, loss_mean=1.17, loss_mean_cls=0.0734][[34m2025-10-04 11:58:42[39m] Step: 72, Training Logs: loss_final: 1.207598, loss_mean: 1.134306, loss_mean_cls: 0.073292, grad_norm: 0.932517
Steps:   0%| | 73/1000000 [00:26<81:01:37,  3.43it/s, grad_norm=0.933, loss_final=1.21, loss_mean=1.13, loss_mean_cls=0.0733[[34m2025-10-04 11:58:43[39m] Step: 73, Training Logs: loss_final: 1.181768, loss_mean: 1.107891, loss_mean_cls: 0.073878, grad_norm: 1.283269
Steps:   0%| | 74/1000000 [00:26<80:29:14,  3.45it/s, grad_norm=1.28, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.0739][[34m2025-10-04 11:58:43[39m] Step: 74, Training Logs: loss_final: 1.209371, loss_mean: 1.137290, loss_mean_cls: 0.072081, grad_norm: 1.301948
Steps:   0%|  | 75/1000000 [00:27<79:44:35,  3.48it/s, grad_norm=1.3, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0721][[34m2025-10-04 11:58:43[39m] Step: 75, Training Logs: loss_final: 1.220604, loss_mean: 1.149832, loss_mean_cls: 0.070772, grad_norm: 0.997420
Steps:   0%| | 76/1000000 [00:27<80:43:54,  3.44it/s, grad_norm=0.997, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.0708[[34m2025-10-04 11:58:44[39m] Step: 76, Training Logs: loss_final: 1.209661, loss_mean: 1.136858, loss_mean_cls: 0.072803, grad_norm: 1.404692
Steps:   0%|  | 77/1000000 [00:27<80:09:10,  3.47it/s, grad_norm=1.4, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0728][[34m2025-10-04 11:58:44[39m] Step: 77, Training Logs: loss_final: 1.208287, loss_mean: 1.137162, loss_mean_cls: 0.071125, grad_norm: 0.815032
Steps:   0%| | 78/1000000 [00:28<79:43:41,  3.48it/s, grad_norm=0.815, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0711[[34m2025-10-04 11:58:44[39m] Step: 78, Training Logs: loss_final: 1.212492, loss_mean: 1.141473, loss_mean_cls: 0.071019, grad_norm: 1.342152
Steps:   0%|  | 79/1000000 [00:28<79:20:47,  3.50it/s, grad_norm=1.34, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.071][[34m2025-10-04 11:58:44[39m] Step: 79, Training Logs: loss_final: 1.216359, loss_mean: 1.146272, loss_mean_cls: 0.070087, grad_norm: 1.453270
Steps:   0%| | 80/1000000 [00:28<80:41:59,  3.44it/s, grad_norm=1.45, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.0701][[34m2025-10-04 11:58:45[39m] Step: 80, Training Logs: loss_final: 1.179088, loss_mean: 1.110210, loss_mean_cls: 0.068878, grad_norm: 1.049633
Steps:   0%| | 81/1000000 [00:28<79:54:49,  3.48it/s, grad_norm=1.05, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.0689][[34m2025-10-04 11:58:45[39m] Step: 81, Training Logs: loss_final: 1.225874, loss_mean: 1.155485, loss_mean_cls: 0.070389, grad_norm: 2.606600
Steps:   0%| | 82/1000000 [00:29<82:37:42,  3.36it/s, grad_norm=2.61, loss_final=1.23, loss_mean=1.16, loss_mean_cls=0.0704][[34m2025-10-04 11:58:45[39m] Step: 82, Training Logs: loss_final: 1.217092, loss_mean: 1.147198, loss_mean_cls: 0.069894, grad_norm: 1.769640
Steps:   0%| | 83/1000000 [00:29<81:43:35,  3.40it/s, grad_norm=1.77, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.0699][[34m2025-10-04 11:58:46[39m] Step: 83, Training Logs: loss_final: 1.186351, loss_mean: 1.116974, loss_mean_cls: 0.069377, grad_norm: 1.784245
Steps:   0%| | 84/1000000 [00:29<83:07:06,  3.34it/s, grad_norm=1.78, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.0694][[34m2025-10-04 11:58:46[39m] Step: 84, Training Logs: loss_final: 1.189900, loss_mean: 1.118911, loss_mean_cls: 0.070989, grad_norm: 1.635399
Steps:   0%|  | 85/1000000 [00:30<82:15:09,  3.38it/s, grad_norm=1.64, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.071][[34m2025-10-04 11:58:46[39m] Step: 85, Training Logs: loss_final: 1.185666, loss_mean: 1.116685, loss_mean_cls: 0.068981, grad_norm: 1.403618
Steps:   0%|   | 86/1000000 [00:30<81:25:53,  3.41it/s, grad_norm=1.4, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.069][[34m2025-10-04 11:58:47[39m] Step: 86, Training Logs: loss_final: 1.146943, loss_mean: 1.077051, loss_mean_cls: 0.069892, grad_norm: 2.097867
Steps:   0%|  | 87/1000000 [00:30<82:56:54,  3.35it/s, grad_norm=2.1, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.0699][[34m2025-10-04 11:58:47[39m] Step: 87, Training Logs: loss_final: 1.193006, loss_mean: 1.123862, loss_mean_cls: 0.069145, grad_norm: 1.299907
Steps:   0%|  | 88/1000000 [00:31<82:51:55,  3.35it/s, grad_norm=1.3, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.0691][[34m2025-10-04 11:58:47[39m] Step: 88, Training Logs: loss_final: 1.156297, loss_mean: 1.087725, loss_mean_cls: 0.068572, grad_norm: 1.627457
Steps:   0%| | 89/1000000 [00:31<84:07:26,  3.30it/s, grad_norm=1.63, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0686][[34m2025-10-04 11:58:47[39m] Step: 89, Training Logs: loss_final: 1.200314, loss_mean: 1.132090, loss_mean_cls: 0.068224, grad_norm: 1.380630
Steps:   0%|  | 90/1000000 [00:31<82:21:21,  3.37it/s, grad_norm=1.38, loss_final=1.2, loss_mean=1.13, loss_mean_cls=0.0682][[34m2025-10-04 11:58:48[39m] Step: 90, Training Logs: loss_final: 1.155200, loss_mean: 1.087793, loss_mean_cls: 0.067407, grad_norm: 1.678809
Steps:   0%| | 91/1000000 [00:31<80:58:25,  3.43it/s, grad_norm=1.68, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0674][[34m2025-10-04 11:58:48[39m] Step: 91, Training Logs: loss_final: 1.147881, loss_mean: 1.078900, loss_mean_cls: 0.068982, grad_norm: 0.911546
Steps:   0%| | 92/1000000 [00:32<81:34:13,  3.41it/s, grad_norm=0.912, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.069][[34m2025-10-04 11:58:48[39m] Step: 92, Training Logs: loss_final: 1.157635, loss_mean: 1.089134, loss_mean_cls: 0.068502, grad_norm: 1.804483
Steps:   0%|  | 93/1000000 [00:32<82:27:00,  3.37it/s, grad_norm=1.8, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0685][[34m2025-10-04 11:58:49[39m] Step: 93, Training Logs: loss_final: 1.185192, loss_mean: 1.116255, loss_mean_cls: 0.068937, grad_norm: 0.748166
Steps:   0%| | 94/1000000 [00:32<81:04:46,  3.43it/s, grad_norm=0.748, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.0689[[34m2025-10-04 11:58:49[39m] Step: 94, Training Logs: loss_final: 1.173838, loss_mean: 1.104753, loss_mean_cls: 0.069085, grad_norm: 1.239238
Steps:   0%|  | 95/1000000 [00:33<80:08:38,  3.47it/s, grad_norm=1.24, loss_final=1.17, loss_mean=1.1, loss_mean_cls=0.0691][[34m2025-10-04 11:58:49[39m] Step: 95, Training Logs: loss_final: 1.161858, loss_mean: 1.093089, loss_mean_cls: 0.068769, grad_norm: 0.744298
Steps:   0%| | 96/1000000 [00:33<82:59:00,  3.35it/s, grad_norm=0.744, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0688[[34m2025-10-04 11:58:50[39m] Step: 96, Training Logs: loss_final: 1.178396, loss_mean: 1.111730, loss_mean_cls: 0.066667, grad_norm: 0.897914
Steps:   0%| | 97/1000000 [00:33<81:41:34,  3.40it/s, grad_norm=0.898, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.0667[[34m2025-10-04 11:58:50[39m] Step: 97, Training Logs: loss_final: 1.162250, loss_mean: 1.094257, loss_mean_cls: 0.067993, grad_norm: 0.709594
Steps:   0%|  | 98/1000000 [00:33<80:43:09,  3.44it/s, grad_norm=0.71, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.068][[34m2025-10-04 11:58:50[39m] Step: 98, Training Logs: loss_final: 1.137692, loss_mean: 1.071057, loss_mean_cls: 0.066635, grad_norm: 1.172933
Steps:   0%| | 99/1000000 [00:34<80:09:26,  3.47it/s, grad_norm=1.17, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.0666][[34m2025-10-04 11:58:50[39m] Step: 99, Training Logs: loss_final: 1.138081, loss_mean: 1.069899, loss_mean_cls: 0.068183, grad_norm: 0.774389
Steps:   0%| | 101/1000000 [00:34<80:45:08,  3.44it/s, grad_norm=1.09, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.0672[[34m2025-10-04 11:58:51[39m] Step: 101, Training Logs: loss_final: 1.159549, loss_mean: 1.094029, loss_mean_cls: 0.065520, grad_norm: 1.553042
Steps:   0%| | 101/1000000 [00:34<80:45:08,  3.44it/s, grad_norm=1.09, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.0672[[34m2025-10-04 11:58:51[39m] Step: 101, Training Logs: loss_final: 1.159549, loss_mean: 1.094029, loss_mean_cls: 0.065520, grad_norm: 1.553042
Steps:   0%| | 102/1000000 [00:35<79:53:19,  3.48it/s, grad_norm=1.55, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0655[[34m2025-10-04 11:58:51[39m] Step: 102, Training Logs: loss_final: 1.160604, loss_mean: 1.092967, loss_mean_cls: 0.067636, grad_norm: 1.162378
Steps:   0%| | 103/1000000 [00:35<79:19:35,  3.50it/s, grad_norm=1.16, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0676[[34m2025-10-04 11:58:52[39m] Step: 103, Training Logs: loss_final: 1.144908, loss_mean: 1.077739, loss_mean_cls: 0.067169, grad_norm: 1.119771
Steps:   0%| | 104/1000000 [00:35<79:04:11,  3.51it/s, grad_norm=1.12, loss_final=1.14, loss_mean=1.08, loss_mean_cls=0.0672[[34m2025-10-04 11:58:52[39m] Step: 104, Training Logs: loss_final: 1.125550, loss_mean: 1.060457, loss_mean_cls: 0.065093, grad_norm: 1.471360
Steps:   0%| | 105/1000000 [00:35<79:51:54,  3.48it/s, grad_norm=1.47, loss_final=1.13, loss_mean=1.06, loss_mean_cls=0.0651[[34m2025-10-04 11:58:52[39m] Step: 105, Training Logs: loss_final: 1.159310, loss_mean: 1.093179, loss_mean_cls: 0.066131, grad_norm: 1.545331
Steps:   0%| | 106/1000000 [00:36<81:03:04,  3.43it/s, grad_norm=1.55, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0661[[34m2025-10-04 11:58:52[39m] Step: 106, Training Logs: loss_final: 1.158241, loss_mean: 1.092270, loss_mean_cls: 0.065971, grad_norm: 0.889555
Steps:   0%| | 113/1000000 [00:38<80:59:30,  3.43it/s, grad_norm=0.805, loss_final=1.12, loss_mean=1.05, loss_mean_cls=0.066[[34m2025-10-04 11:58:53[39m] Step: 107, Training Logs: loss_final: 1.137724, loss_mean: 1.071675, loss_mean_cls: 0.066049, grad_norm: 0.914894
Steps:   0%| | 108/1000000 [00:36<80:19:48,  3.46it/s, grad_norm=0.915, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.066[[34m2025-10-04 11:58:53[39m] Step: 108, Training Logs: loss_final: 1.164804, loss_mean: 1.099778, loss_mean_cls: 0.065027, grad_norm: 0.929008
Steps:   0%| | 109/1000000 [00:37<80:06:01,  3.47it/s, grad_norm=0.929, loss_final=1.16, loss_mean=1.1, loss_mean_cls=0.065][[34m2025-10-04 11:58:53[39m] Step: 109, Training Logs: loss_final: 1.143355, loss_mean: 1.076445, loss_mean_cls: 0.066910, grad_norm: 1.209500
Steps:   0%| | 110/1000000 [00:37<79:39:09,  3.49it/s, grad_norm=1.21, loss_final=1.14, loss_mean=1.08, loss_mean_cls=0.0669[[34m2025-10-04 11:58:54[39m] Step: 110, Training Logs: loss_final: 1.143968, loss_mean: 1.077758, loss_mean_cls: 0.066210, grad_norm: 0.751902
Steps:   0%| | 111/1000000 [00:37<82:33:18,  3.36it/s, grad_norm=0.752, loss_final=1.14, loss_mean=1.08, loss_mean_cls=0.066[[34m2025-10-04 11:58:54[39m] Step: 111, Training Logs: loss_final: 1.110958, loss_mean: 1.043878, loss_mean_cls: 0.067080, grad_norm: 0.783049
Steps:   0%| | 112/1000000 [00:37<81:23:10,  3.41it/s, grad_norm=0.783, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.067[[34m2025-10-04 11:58:54[39m] Step: 112, Training Logs: loss_final: 1.130311, loss_mean: 1.063896, loss_mean_cls: 0.066415, grad_norm: 0.821997
Steps:   0%| | 113/1000000 [00:38<80:59:30,  3.43it/s, grad_norm=0.822, loss_final=1.13, loss_mean=1.06, loss_mean_cls=0.066[[34m2025-10-04 11:58:54[39m] Step: 113, Training Logs: loss_final: 1.115051, loss_mean: 1.048683, loss_mean_cls: 0.066368, grad_norm: 0.805253
Steps:   0%| | 120/1000000 [00:40<80:32:18,  3.45it/s, grad_norm=0.976, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.064[[34m2025-10-04 11:58:55[39m] Step: 114, Training Logs: loss_final: 1.148364, loss_mean: 1.083573, loss_mean_cls: 0.064791, grad_norm: 1.915238
Steps:   0%| | 115/1000000 [00:38<79:49:29,  3.48it/s, grad_norm=1.92, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.0648[[34m2025-10-04 11:58:55[39m] Step: 115, Training Logs: loss_final: 1.127321, loss_mean: 1.061635, loss_mean_cls: 0.065686, grad_norm: 1.188713
Steps:   0%| | 116/1000000 [00:39<79:37:57,  3.49it/s, grad_norm=1.19, loss_final=1.13, loss_mean=1.06, loss_mean_cls=0.0657[[34m2025-10-04 11:58:55[39m] Step: 116, Training Logs: loss_final: 1.157699, loss_mean: 1.092663, loss_mean_cls: 0.065036, grad_norm: 2.045234
Steps:   0%| | 117/1000000 [00:39<79:15:57,  3.50it/s, grad_norm=2.05, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.065][[34m2025-10-04 11:58:56[39m] Step: 117, Training Logs: loss_final: 1.161479, loss_mean: 1.095012, loss_mean_cls: 0.066467, grad_norm: 1.294130
Steps:   0%| | 118/1000000 [00:39<79:06:39,  3.51it/s, grad_norm=1.29, loss_final=1.16, loss_mean=1.1, loss_mean_cls=0.0665][[34m2025-10-04 11:58:56[39m] Step: 118, Training Logs: loss_final: 1.112753, loss_mean: 1.046154, loss_mean_cls: 0.066600, grad_norm: 1.279721
Steps:   0%| | 119/1000000 [00:39<79:05:38,  3.51it/s, grad_norm=1.28, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0666[[34m2025-10-04 11:58:56[39m] Step: 119, Training Logs: loss_final: 1.135281, loss_mean: 1.070652, loss_mean_cls: 0.064629, grad_norm: 1.446000
Steps:   0%| | 120/1000000 [00:40<80:32:18,  3.45it/s, grad_norm=1.45, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.0646[[34m2025-10-04 11:58:56[39m] Step: 120, Training Logs: loss_final: 1.124041, loss_mean: 1.059114, loss_mean_cls: 0.064927, grad_norm: 0.976336
Steps:   0%| | 127/1000000 [00:42<79:03:03,  3.51it/s, grad_norm=0.713, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.063[[34m2025-10-04 11:58:57[39m] Step: 121, Training Logs: loss_final: 1.151919, loss_mean: 1.085931, loss_mean_cls: 0.065988, grad_norm: 1.263715
Steps:   0%| | 122/1000000 [00:40<79:25:03,  3.50it/s, grad_norm=1.26, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.066][[34m2025-10-04 11:58:57[39m] Step: 122, Training Logs: loss_final: 1.104317, loss_mean: 1.039116, loss_mean_cls: 0.065201, grad_norm: 0.845004
Steps:   0%| | 123/1000000 [00:41<78:57:56,  3.52it/s, grad_norm=0.845, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0652[[34m2025-10-04 11:58:57[39m] Step: 123, Training Logs: loss_final: 1.152336, loss_mean: 1.088170, loss_mean_cls: 0.064166, grad_norm: 1.076913
Steps:   0%| | 124/1000000 [00:41<78:32:33,  3.54it/s, grad_norm=1.08, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.0642[[34m2025-10-04 11:58:58[39m] Step: 124, Training Logs: loss_final: 1.139302, loss_mean: 1.074941, loss_mean_cls: 0.064361, grad_norm: 0.802603
Steps:   0%| | 125/1000000 [00:41<80:11:26,  3.46it/s, grad_norm=0.803, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.064[[34m2025-10-04 11:58:58[39m] Step: 125, Training Logs: loss_final: 1.117040, loss_mean: 1.052078, loss_mean_cls: 0.064962, grad_norm: 1.045692
Steps:   0%| | 126/1000000 [00:41<79:27:44,  3.50it/s, grad_norm=1.05, loss_final=1.12, loss_mean=1.05, loss_mean_cls=0.065][[34m2025-10-04 11:58:58[39m] Step: 126, Training Logs: loss_final: 1.110948, loss_mean: 1.047124, loss_mean_cls: 0.063824, grad_norm: 1.023891
Steps:   0%| | 127/1000000 [00:42<79:03:03,  3.51it/s, grad_norm=1.02, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0638[[34m2025-10-04 11:58:58[39m] Step: 127, Training Logs: loss_final: 1.105299, loss_mean: 1.041788, loss_mean_cls: 0.063510, grad_norm: 0.713374
Steps:   0%| | 134/1000000 [00:44<79:39:21,  3.49it/s, grad_norm=0.563, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0632[[34m2025-10-04 11:58:59[39m] Step: 128, Training Logs: loss_final: 1.130095, loss_mean: 1.065627, loss_mean_cls: 0.064469, grad_norm: 1.065424
Steps:   0%| | 129/1000000 [00:42<80:00:41,  3.47it/s, grad_norm=1.07, loss_final=1.13, loss_mean=1.07, loss_mean_cls=0.0645[[34m2025-10-04 11:58:59[39m] Step: 129, Training Logs: loss_final: 1.146949, loss_mean: 1.083305, loss_mean_cls: 0.063643, grad_norm: 0.794002
Steps:   0%| | 130/1000000 [00:43<80:20:05,  3.46it/s, grad_norm=0.794, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.063[[34m2025-10-04 11:58:59[39m] Step: 130, Training Logs: loss_final: 1.136735, loss_mean: 1.072915, loss_mean_cls: 0.063820, grad_norm: 0.932061
Steps:   0%| | 131/1000000 [00:43<79:37:59,  3.49it/s, grad_norm=0.932, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.063[[34m2025-10-04 11:59:00[39m] Step: 131, Training Logs: loss_final: 1.133647, loss_mean: 1.069658, loss_mean_cls: 0.063989, grad_norm: 0.600076
Steps:   0%|  | 132/1000000 [00:43<80:56:42,  3.43it/s, grad_norm=0.6, loss_final=1.13, loss_mean=1.07, loss_mean_cls=0.064][[34m2025-10-04 11:59:00[39m] Step: 132, Training Logs: loss_final: 1.080663, loss_mean: 1.016690, loss_mean_cls: 0.063973, grad_norm: 0.629335
Steps:   0%| | 133/1000000 [00:44<80:23:43,  3.45it/s, grad_norm=0.629, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.064[[34m2025-10-04 11:59:00[39m] Step: 133, Training Logs: loss_final: 1.105346, loss_mean: 1.039862, loss_mean_cls: 0.065484, grad_norm: 0.643454
Steps:   0%| | 134/1000000 [00:44<79:39:21,  3.49it/s, grad_norm=0.643, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.065[[34m2025-10-04 11:59:00[39m] Step: 134, Training Logs: loss_final: 1.100369, loss_mean: 1.037168, loss_mean_cls: 0.063201, grad_norm: 0.563212
Steps:   0%| | 141/1000000 [00:46<81:17:37,  3.42it/s, grad_norm=1.23, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.0642[[34m2025-10-04 11:59:01[39m] Step: 135, Training Logs: loss_final: 1.105932, loss_mean: 1.041672, loss_mean_cls: 0.064261, grad_norm: 0.622289
Steps:   0%| | 136/1000000 [00:44<78:53:47,  3.52it/s, grad_norm=0.622, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.064[[34m2025-10-04 11:59:01[39m] Step: 136, Training Logs: loss_final: 1.110836, loss_mean: 1.046662, loss_mean_cls: 0.064174, grad_norm: 0.698803
Steps:   0%| | 137/1000000 [00:45<80:47:29,  3.44it/s, grad_norm=0.699, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.064[[34m2025-10-04 11:59:01[39m] Step: 137, Training Logs: loss_final: 1.076723, loss_mean: 1.011783, loss_mean_cls: 0.064940, grad_norm: 0.707218
Steps:   0%| | 138/1000000 [00:45<80:00:36,  3.47it/s, grad_norm=0.707, loss_final=1.08, loss_mean=1.01, loss_mean_cls=0.064[[34m2025-10-04 11:59:02[39m] Step: 138, Training Logs: loss_final: 1.120642, loss_mean: 1.055791, loss_mean_cls: 0.064851, grad_norm: 0.994787
Steps:   0%| | 139/1000000 [00:45<79:50:05,  3.48it/s, grad_norm=0.995, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.064[[34m2025-10-04 11:59:02[39m] Step: 139, Training Logs: loss_final: 1.087007, loss_mean: 1.022182, loss_mean_cls: 0.064826, grad_norm: 1.046595
Steps:   0%| | 140/1000000 [00:46<81:47:56,  3.40it/s, grad_norm=1.05, loss_final=1.09, loss_mean=1.02, loss_mean_cls=0.0648[[34m2025-10-04 11:59:02[39m] Step: 140, Training Logs: loss_final: 1.093595, loss_mean: 1.029660, loss_mean_cls: 0.063935, grad_norm: 1.089518
Steps:   0%| | 141/1000000 [00:46<81:17:37,  3.42it/s, grad_norm=1.09, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0639[[34m2025-10-04 11:59:03[39m] Step: 141, Training Logs: loss_final: 1.123184, loss_mean: 1.058955, loss_mean_cls: 0.064229, grad_norm: 1.226431
Steps:   0%| | 148/1000000 [00:48<79:54:11,  3.48it/s, grad_norm=1.17, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0637[[34m2025-10-04 11:59:03[39m] Step: 142, Training Logs: loss_final: 1.099040, loss_mean: 1.036157, loss_mean_cls: 0.062882, grad_norm: 1.219716
Steps:   0%| | 143/1000000 [00:46<80:18:14,  3.46it/s, grad_norm=1.22, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0629][[34m2025-10-04 11:59:03[39m] Step: 143, Training Logs: loss_final: 1.055038, loss_mean: 0.990889, loss_mean_cls: 0.064149, grad_norm: 0.605489
Steps:   0%| | 144/1000000 [00:47<81:21:39,  3.41it/s, grad_norm=0.605, loss_final=1.06, loss_mean=0.991, loss_mean_cls=0.06[[34m2025-10-04 11:59:03[39m] Step: 144, Training Logs: loss_final: 1.091070, loss_mean: 1.026933, loss_mean_cls: 0.064137, grad_norm: 1.270204
Steps:   0%| | 145/1000000 [00:47<80:28:19,  3.45it/s, grad_norm=1.27, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0641[[34m2025-10-04 11:59:04[39m] Step: 145, Training Logs: loss_final: 1.068309, loss_mean: 1.003605, loss_mean_cls: 0.064705, grad_norm: 1.111048
Steps:   0%|   | 146/1000000 [00:47<81:03:11,  3.43it/s, grad_norm=1.11, loss_final=1.07, loss_mean=1, loss_mean_cls=0.0647][[34m2025-10-04 11:59:04[39m] Step: 146, Training Logs: loss_final: 1.097403, loss_mean: 1.033930, loss_mean_cls: 0.063474, grad_norm: 0.946707
Steps:   0%| | 147/1000000 [00:48<80:08:23,  3.47it/s, grad_norm=0.947, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0635[[34m2025-10-04 11:59:04[39m] Step: 147, Training Logs: loss_final: 1.095557, loss_mean: 1.031377, loss_mean_cls: 0.064180, grad_norm: 1.267769
Steps:   0%| | 148/1000000 [00:48<79:54:11,  3.48it/s, grad_norm=1.27, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0642][[34m2025-10-04 11:59:05[39m] Step: 148, Training Logs: loss_final: 1.109137, loss_mean: 1.045402, loss_mean_cls: 0.063735, grad_norm: 1.166205
Steps:   0%| | 155/1000000 [00:50<80:02:46,  3.47it/s, grad_norm=1.21, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0641][[34m2025-10-04 11:59:05[39m] Step: 149, Training Logs: loss_final: 1.110693, loss_mean: 1.047938, loss_mean_cls: 0.062756, grad_norm: 1.223423
Steps:   0%| | 150/1000000 [00:48<79:26:25,  3.50it/s, grad_norm=1.22, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0628[[34m2025-10-04 11:59:05[39m] Step: 150, Training Logs: loss_final: 1.108818, loss_mean: 1.046710, loss_mean_cls: 0.062108, grad_norm: 0.936960
Steps:   0%| | 151/1000000 [00:49<78:52:40,  3.52it/s, grad_norm=0.937, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.062[[34m2025-10-04 11:59:05[39m] Step: 151, Training Logs: loss_final: 1.059399, loss_mean: 0.994788, loss_mean_cls: 0.064611, grad_norm: 0.846941
Steps:   0%| | 152/1000000 [00:49<80:12:43,  3.46it/s, grad_norm=0.847, loss_final=1.06, loss_mean=0.995, loss_mean_cls=0.06[[34m2025-10-04 11:59:06[39m] Step: 152, Training Logs: loss_final: 1.121889, loss_mean: 1.058429, loss_mean_cls: 0.063460, grad_norm: 1.020130
Steps:   0%| | 153/1000000 [00:49<79:42:46,  3.48it/s, grad_norm=1.02, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.0635[[34m2025-10-04 11:59:06[39m] Step: 153, Training Logs: loss_final: 1.095939, loss_mean: 1.034368, loss_mean_cls: 0.061571, grad_norm: 1.148290
Steps:   0%| | 154/1000000 [00:50<80:57:13,  3.43it/s, grad_norm=1.15, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0616][[34m2025-10-04 11:59:06[39m] Step: 154, Training Logs: loss_final: 1.094089, loss_mean: 1.031305, loss_mean_cls: 0.062784, grad_norm: 0.649710
Steps:   0%| | 155/1000000 [00:50<80:02:46,  3.47it/s, grad_norm=0.65, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0628[[34m2025-10-04 11:59:07[39m] Step: 155, Training Logs: loss_final: 1.098525, loss_mean: 1.034432, loss_mean_cls: 0.064093, grad_norm: 1.210493
Steps:   0%| | 162/1000000 [00:52<79:37:00,  3.49it/s, grad_norm=1.32, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0626[[34m2025-10-04 11:59:07[39m] Step: 156, Training Logs: loss_final: 1.091479, loss_mean: 1.028702, loss_mean_cls: 0.062777, grad_norm: 1.007981
Steps:   0%| | 157/1000000 [00:50<80:05:37,  3.47it/s, grad_norm=1.01, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0628[[34m2025-10-04 11:59:07[39m] Step: 157, Training Logs: loss_final: 1.082099, loss_mean: 1.020590, loss_mean_cls: 0.061508, grad_norm: 1.185494
Steps:   0%| | 158/1000000 [00:51<79:30:16,  3.49it/s, grad_norm=1.19, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0615[[34m2025-10-04 11:59:07[39m] Step: 158, Training Logs: loss_final: 1.100116, loss_mean: 1.036934, loss_mean_cls: 0.063181, grad_norm: 1.049936
Steps:   0%| | 159/1000000 [00:51<79:08:14,  3.51it/s, grad_norm=1.05, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0632][[34m2025-10-04 11:59:08[39m] Step: 159, Training Logs: loss_final: 1.080206, loss_mean: 1.018823, loss_mean_cls: 0.061383, grad_norm: 0.724169
Steps:   0%| | 160/1000000 [00:51<81:10:35,  3.42it/s, grad_norm=0.724, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.061[[34m2025-10-04 11:59:08[39m] Step: 160, Training Logs: loss_final: 1.082649, loss_mean: 1.018734, loss_mean_cls: 0.063915, grad_norm: 1.187783
Steps:   0%| | 161/1000000 [00:52<80:14:11,  3.46it/s, grad_norm=1.19, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0639[[34m2025-10-04 11:59:08[39m] Step: 161, Training Logs: loss_final: 1.101871, loss_mean: 1.037806, loss_mean_cls: 0.064065, grad_norm: 0.812157
Steps:   0%| | 162/1000000 [00:52<79:37:00,  3.49it/s, grad_norm=0.812, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0641[[34m2025-10-04 11:59:09[39m] Step: 162, Training Logs: loss_final: 1.090401, loss_mean: 1.027790, loss_mean_cls: 0.062612, grad_norm: 1.317237
Steps:   0%| | 169/1000000 [00:54<80:49:06,  3.44it/s, grad_norm=0.92, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0629[[34m2025-10-04 11:59:09[39m] Step: 163, Training Logs: loss_final: 1.092381, loss_mean: 1.029764, loss_mean_cls: 0.062617, grad_norm: 1.294814
Steps:   0%| | 164/1000000 [00:52<78:52:44,  3.52it/s, grad_norm=1.29, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0626[[34m2025-10-04 11:59:09[39m] Step: 164, Training Logs: loss_final: 1.088589, loss_mean: 1.025306, loss_mean_cls: 0.063283, grad_norm: 0.899731
Steps:   0%| | 165/1000000 [00:53<78:46:03,  3.53it/s, grad_norm=0.9, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0633][[34m2025-10-04 11:59:09[39m] Step: 165, Training Logs: loss_final: 1.089273, loss_mean: 1.027073, loss_mean_cls: 0.062200, grad_norm: 1.093026
Steps:   0%| | 166/1000000 [00:53<78:35:54,  3.53it/s, grad_norm=1.09, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0622[[34m2025-10-04 11:59:10[39m] Step: 166, Training Logs: loss_final: 1.053399, loss_mean: 0.989408, loss_mean_cls: 0.063992, grad_norm: 0.855233
Steps:   0%| | 167/1000000 [00:53<78:26:04,  3.54it/s, grad_norm=0.855, loss_final=1.05, loss_mean=0.989, loss_mean_cls=0.06[[34m2025-10-04 11:59:10[39m] Step: 167, Training Logs: loss_final: 1.110872, loss_mean: 1.046482, loss_mean_cls: 0.064389, grad_norm: 1.277748
Steps:   0%| | 168/1000000 [00:54<81:27:43,  3.41it/s, grad_norm=1.28, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0644[[34m2025-10-04 11:59:10[39m] Step: 168, Training Logs: loss_final: 1.088963, loss_mean: 1.025599, loss_mean_cls: 0.063364, grad_norm: 0.880995
Steps:   0%| | 169/1000000 [00:54<80:49:06,  3.44it/s, grad_norm=0.881, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.063[[34m2025-10-04 11:59:11[39m] Step: 169, Training Logs: loss_final: 1.070628, loss_mean: 1.007739, loss_mean_cls: 0.062889, grad_norm: 0.920139
Steps:   0%| | 176/1000000 [00:56<80:11:10,  3.46it/s, grad_norm=0.856, loss_final=1.08, loss_mean=1.01, loss_mean_cls=0.064[[34m2025-10-04 11:59:11[39m] Step: 170, Training Logs: loss_final: 1.082061, loss_mean: 1.019171, loss_mean_cls: 0.062890, grad_norm: 0.868008
Steps:   0%| | 171/1000000 [00:54<79:36:22,  3.49it/s, grad_norm=0.868, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.062[[34m2025-10-04 11:59:11[39m] Step: 171, Training Logs: loss_final: 1.097022, loss_mean: 1.034947, loss_mean_cls: 0.062075, grad_norm: 1.069641
Steps:   0%| | 172/1000000 [00:55<79:31:44,  3.49it/s, grad_norm=1.07, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0621][[34m2025-10-04 11:59:11[39m] Step: 172, Training Logs: loss_final: 1.063415, loss_mean: 1.001423, loss_mean_cls: 0.061992, grad_norm: 0.822438
Steps:   0%|   | 173/1000000 [00:55<80:50:30,  3.44it/s, grad_norm=0.822, loss_final=1.06, loss_mean=1, loss_mean_cls=0.062][[34m2025-10-04 11:59:12[39m] Step: 173, Training Logs: loss_final: 1.045943, loss_mean: 0.982353, loss_mean_cls: 0.063590, grad_norm: 0.881901
Steps:   0%| | 174/1000000 [00:55<80:12:16,  3.46it/s, grad_norm=0.882, loss_final=1.05, loss_mean=0.982, loss_mean_cls=0.06[[34m2025-10-04 11:59:12[39m] Step: 174, Training Logs: loss_final: 1.078992, loss_mean: 1.017144, loss_mean_cls: 0.061848, grad_norm: 0.923457
Steps:   0%| | 175/1000000 [00:56<79:49:24,  3.48it/s, grad_norm=0.923, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.061[[34m2025-10-04 11:59:12[39m] Step: 175, Training Logs: loss_final: 1.045215, loss_mean: 0.982871, loss_mean_cls: 0.062343, grad_norm: 1.179002
Steps:   0%| | 176/1000000 [00:56<80:11:10,  3.46it/s, grad_norm=1.18, loss_final=1.05, loss_mean=0.983, loss_mean_cls=0.062[[34m2025-10-04 11:59:13[39m] Step: 176, Training Logs: loss_final: 1.075766, loss_mean: 1.011802, loss_mean_cls: 0.063964, grad_norm: 0.855685
Steps:   0%| | 183/1000000 [00:58<79:41:15,  3.49it/s, grad_norm=1.23, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0619[[34m2025-10-04 11:59:13[39m] Step: 177, Training Logs: loss_final: 1.050661, loss_mean: 0.987994, loss_mean_cls: 0.062667, grad_norm: 1.013143
Steps:   0%| | 178/1000000 [00:56<80:27:52,  3.45it/s, grad_norm=1.01, loss_final=1.05, loss_mean=0.988, loss_mean_cls=0.062[[34m2025-10-04 11:59:13[39m] Step: 178, Training Logs: loss_final: 1.070028, loss_mean: 1.007090, loss_mean_cls: 0.062938, grad_norm: 0.863631
Steps:   0%| | 179/1000000 [00:57<83:22:55,  3.33it/s, grad_norm=0.864, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.062[[34m2025-10-04 11:59:13[39m] Step: 179, Training Logs: loss_final: 1.078369, loss_mean: 1.015977, loss_mean_cls: 0.062392, grad_norm: 0.799556
Steps:   0%| | 180/1000000 [00:57<82:49:45,  3.35it/s, grad_norm=0.8, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0624][[34m2025-10-04 11:59:14[39m] Step: 180, Training Logs: loss_final: 1.044156, loss_mean: 0.982728, loss_mean_cls: 0.061428, grad_norm: 0.979666
Steps:   0%| | 181/1000000 [00:57<81:23:14,  3.41it/s, grad_norm=0.98, loss_final=1.04, loss_mean=0.983, loss_mean_cls=0.061[[34m2025-10-04 11:59:14[39m] Step: 181, Training Logs: loss_final: 1.084331, loss_mean: 1.022099, loss_mean_cls: 0.062231, grad_norm: 1.378204
Steps:   0%| | 182/1000000 [00:58<80:24:56,  3.45it/s, grad_norm=1.38, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0622[[34m2025-10-04 11:59:14[39m] Step: 182, Training Logs: loss_final: 1.050454, loss_mean: 0.987956, loss_mean_cls: 0.062498, grad_norm: 0.845689
Steps:   0%| | 183/1000000 [00:58<79:41:15,  3.49it/s, grad_norm=0.846, loss_final=1.05, loss_mean=0.988, loss_mean_cls=0.06[[34m2025-10-04 11:59:15[39m] Step: 183, Training Logs: loss_final: 1.080031, loss_mean: 1.018179, loss_mean_cls: 0.061852, grad_norm: 1.231650
Steps:   0%| | 190/1000000 [01:00<79:55:07,  3.48it/s, grad_norm=1.12, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.062][[34m2025-10-04 11:59:15[39m] Step: 184, Training Logs: loss_final: 1.059551, loss_mean: 0.997448, loss_mean_cls: 0.062104, grad_norm: 1.116566
Steps:   0%| | 185/1000000 [00:59<79:22:09,  3.50it/s, grad_norm=1.12, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.062[[34m2025-10-04 11:59:15[39m] Step: 185, Training Logs: loss_final: 1.064238, loss_mean: 1.002420, loss_mean_cls: 0.061818, grad_norm: 0.871099
Steps:   0%|  | 186/1000000 [00:59<78:55:25,  3.52it/s, grad_norm=0.871, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0618][[34m2025-10-04 11:59:15[39m] Step: 186, Training Logs: loss_final: 1.061370, loss_mean: 1.000179, loss_mean_cls: 0.061191, grad_norm: 1.114910
Steps:   0%|   | 187/1000000 [00:59<78:39:37,  3.53it/s, grad_norm=1.11, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0612][[34m2025-10-04 11:59:16[39m] Step: 187, Training Logs: loss_final: 1.055591, loss_mean: 0.993270, loss_mean_cls: 0.062321, grad_norm: 0.863193
Steps:   0%| | 188/1000000 [00:59<81:25:51,  3.41it/s, grad_norm=0.863, loss_final=1.06, loss_mean=0.993, loss_mean_cls=0.06[[34m2025-10-04 11:59:16[39m] Step: 188, Training Logs: loss_final: 1.102974, loss_mean: 1.041552, loss_mean_cls: 0.061422, grad_norm: 1.363935
Steps:   0%| | 189/1000000 [01:00<80:27:22,  3.45it/s, grad_norm=1.36, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0614][[34m2025-10-04 11:59:16[39m] Step: 189, Training Logs: loss_final: 1.044736, loss_mean: 0.981661, loss_mean_cls: 0.063075, grad_norm: 0.957310
Steps:   0%| | 190/1000000 [01:00<79:55:07,  3.48it/s, grad_norm=0.957, loss_final=1.04, loss_mean=0.982, loss_mean_cls=0.06[[34m2025-10-04 11:59:17[39m] Step: 190, Training Logs: loss_final: 1.091679, loss_mean: 1.029676, loss_mean_cls: 0.062002, grad_norm: 1.115942
Steps:   0%| | 197/1000000 [01:02<82:05:31,  3.38it/s, grad_norm=1.01, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0603[[34m2025-10-04 11:59:17[39m] Step: 191, Training Logs: loss_final: 1.040212, loss_mean: 0.978650, loss_mean_cls: 0.061562, grad_norm: 0.846537
Steps:   0%| | 192/1000000 [01:01<79:33:12,  3.49it/s, grad_norm=0.847, loss_final=1.04, loss_mean=0.979, loss_mean_cls=0.06[[34m2025-10-04 11:59:17[39m] Step: 192, Training Logs: loss_final: 1.047045, loss_mean: 0.984966, loss_mean_cls: 0.062079, grad_norm: 0.881012
Steps:   0%| | 193/1000000 [01:01<79:21:25,  3.50it/s, grad_norm=0.881, loss_final=1.05, loss_mean=0.985, loss_mean_cls=0.06[[34m2025-10-04 11:59:17[39m] Step: 193, Training Logs: loss_final: 1.063240, loss_mean: 1.002117, loss_mean_cls: 0.061123, grad_norm: 1.449064
Steps:   0%|   | 194/1000000 [01:01<79:30:09,  3.49it/s, grad_norm=1.45, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0611][[34m2025-10-04 11:59:18[39m] Step: 194, Training Logs: loss_final: 1.068559, loss_mean: 1.006875, loss_mean_cls: 0.061684, grad_norm: 0.821755
Steps:   0%| | 195/1000000 [01:01<79:57:57,  3.47it/s, grad_norm=0.822, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.061[[34m2025-10-04 11:59:18[39m] Step: 195, Training Logs: loss_final: 1.077656, loss_mean: 1.015350, loss_mean_cls: 0.062305, grad_norm: 1.585069
Steps:   0%| | 196/1000000 [01:02<83:23:59,  3.33it/s, grad_norm=1.59, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0623[[34m2025-10-04 11:59:18[39m] Step: 196, Training Logs: loss_final: 1.041453, loss_mean: 0.977241, loss_mean_cls: 0.064212, grad_norm: 0.797770
Steps:   0%| | 197/1000000 [01:02<82:05:31,  3.38it/s, grad_norm=0.798, loss_final=1.04, loss_mean=0.977, loss_mean_cls=0.06[[34m2025-10-04 11:59:19[39m] Step: 197, Training Logs: loss_final: 1.094154, loss_mean: 1.033823, loss_mean_cls: 0.060330, grad_norm: 1.014875
Steps:   0%| | 198/1000000 [01:02<81:56:14,  3.39it/s, grad_norm=1.01, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0603[[34m2025-10-04 11:59:19[39m] Step: 198, Training Logs: loss_final: 1.058999, loss_mean: 0.997704, loss_mean_cls: 0.061295, grad_norm: 1.202015
Steps:   0%| | 199/1000000 [01:03<84:04:49,  3.30it/s, grad_norm=1.2, loss_final=1.06, loss_mean=0.998, loss_mean_cls=0.0613[[34m2025-10-04 11:59:19[39m] Step: 199, Training Logs: loss_final: 1.073846, loss_mean: 1.013786, loss_mean_cls: 0.060060, grad_norm: 1.062728
Steps:   0%| | 200/1000000 [01:03<83:36:45,  3.32it/s, grad_norm=1.06, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0601[[34m2025-10-04 11:59:20[39m] Step: 200, Training Logs: loss_final: 1.068950, loss_mean: 1.007000, loss_mean_cls: 0.061949, grad_norm: 1.038326
Steps:   0%| | 201/1000000 [01:03<82:20:26,  3.37it/s, grad_norm=1.04, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0619[[34m2025-10-04 11:59:20[39m] Step: 201, Training Logs: loss_final: 1.039293, loss_mean: 0.978058, loss_mean_cls: 0.061235, grad_norm: 0.692390
Steps:   0%| | 202/1000000 [01:04<83:44:05,  3.32it/s, grad_norm=0.692, loss_final=1.04, loss_mean=0.978, loss_mean_cls=0.06[[34m2025-10-04 11:59:20[39m] Step: 202, Training Logs: loss_final: 1.054545, loss_mean: 0.993963, loss_mean_cls: 0.060582, grad_norm: 0.979375
Steps:   0%| | 203/1000000 [01:04<82:58:09,  3.35it/s, grad_norm=0.979, loss_final=1.05, loss_mean=0.994, loss_mean_cls=0.06[[34m2025-10-04 11:59:20[39m] Step: 203, Training Logs: loss_final: 1.063964, loss_mean: 1.002774, loss_mean_cls: 0.061190, grad_norm: 1.226263
Steps:   0%|   | 204/1000000 [01:04<81:48:35,  3.39it/s, grad_norm=1.23, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0612]
Steps:   0%| | 205/1000000 [01:04<82:52:12,  3.35it/s, grad_norm=1.01, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0617[[34m2025-10-04 11:59:21[39m] Step: 205, Training Logs: loss_final: 1.075126, loss_mean: 1.013581, loss_mean_cls: 0.061545, grad_norm: 0.868994
Steps:   0%| | 206/1000000 [01:05<81:45:29,  3.40it/s, grad_norm=0.869, loss_final=1.08, loss_mean=1.01, loss_mean_cls=0.061[[34m2025-10-04 11:59:21[39m] Step: 206, Training Logs: loss_final: 1.074482, loss_mean: 1.014164, loss_mean_cls: 0.060319, grad_norm: 1.324093
Steps:   0%| | 207/1000000 [01:05<81:12:17,  3.42it/s, grad_norm=1.32, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0603[[34m2025-10-04 11:59:22[39m] Step: 207, Training Logs: loss_final: 1.057914, loss_mean: 0.997108, loss_mean_cls: 0.060806, grad_norm: 1.008509
Steps:   0%| | 208/1000000 [01:05<82:58:11,  3.35it/s, grad_norm=1.01, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.060[[34m2025-10-04 11:59:22[39m] Step: 208, Training Logs: loss_final: 1.044524, loss_mean: 0.982995, loss_mean_cls: 0.061529, grad_norm: 0.756854
Steps:   0%| | 209/1000000 [01:06<84:02:36,  3.30it/s, grad_norm=0.757, loss_final=1.04, loss_mean=0.983, loss_mean_cls=0.06[[34m2025-10-04 11:59:22[39m] Step: 209, Training Logs: loss_final: 1.058371, loss_mean: 0.997310, loss_mean_cls: 0.061061, grad_norm: 1.001887
Steps:   0%|  | 210/1000000 [01:06<83:16:03,  3.34it/s, grad_norm=1, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.0611][[34m2025-10-04 11:59:23[39m] Step: 210, Training Logs: loss_final: 1.063412, loss_mean: 1.003053, loss_mean_cls: 0.060359, grad_norm: 0.786596
Steps:   0%|  | 210/1000000 [01:06<83:16:03,  3.34it/s, grad_norm=0.787, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0604]
Steps:   0%| | 212/1000000 [01:07<84:52:59,  3.27it/s, grad_norm=0.711, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.06[[34m2025-10-04 11:59:23[39m] Step: 212, Training Logs: loss_final: 1.056001, loss_mean: 0.996604, loss_mean_cls: 0.059398, grad_norm: 1.461260
Steps:   0%| | 213/1000000 [01:07<83:49:34,  3.31it/s, grad_norm=1.46, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.059[[34m2025-10-04 11:59:23[39m] Step: 213, Training Logs: loss_final: 1.073707, loss_mean: 1.013813, loss_mean_cls: 0.059894, grad_norm: 0.708135
Steps:   0%| | 214/1000000 [01:07<83:22:25,  3.33it/s, grad_norm=0.708, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.059[[34m2025-10-04 11:59:24[39m] Step: 214, Training Logs: loss_final: 1.072841, loss_mean: 1.012409, loss_mean_cls: 0.060433, grad_norm: 1.319640
Steps:   0%| | 215/1000000 [01:07<82:49:18,  3.35it/s, grad_norm=1.32, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0604[[34m2025-10-04 11:59:24[39m] Step: 215, Training Logs: loss_final: 1.057741, loss_mean: 0.996811, loss_mean_cls: 0.060930, grad_norm: 0.979530
Steps:   0%| | 216/1000000 [01:08<85:13:18,  3.26it/s, grad_norm=0.98, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.060[[34m2025-10-04 11:59:24[39m] Step: 216, Training Logs: loss_final: 1.070524, loss_mean: 1.009519, loss_mean_cls: 0.061005, grad_norm: 0.869351
Steps:   0%| | 217/1000000 [01:08<83:57:52,  3.31it/s, grad_norm=0.869, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.061[[34m2025-10-04 11:59:25[39m] Step: 217, Training Logs: loss_final: 1.060650, loss_mean: 1.000460, loss_mean_cls: 0.060190, grad_norm: 1.262853
Steps:   0%|   | 217/1000000 [01:08<83:57:52,  3.31it/s, grad_norm=1.26, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0602]
Steps:   0%|  | 219/1000000 [01:09<82:51:52,  3.35it/s, grad_norm=1, loss_final=1.03, loss_mean=0.965, loss_mean_cls=0.0615][[34m2025-10-04 11:59:25[39m] Step: 219, Training Logs: loss_final: 1.049094, loss_mean: 0.987929, loss_mean_cls: 0.061165, grad_norm: 1.374824
Steps:   0%| | 220/1000000 [01:09<83:44:44,  3.32it/s, grad_norm=1.37, loss_final=1.05, loss_mean=0.988, loss_mean_cls=0.061[[34m2025-10-04 11:59:26[39m] Step: 220, Training Logs: loss_final: 1.036040, loss_mean: 0.975466, loss_mean_cls: 0.060574, grad_norm: 1.094213
Steps:   0%| | 221/1000000 [01:09<82:31:08,  3.37it/s, grad_norm=1.09, loss_final=1.04, loss_mean=0.975, loss_mean_cls=0.060[[34m2025-10-04 11:59:26[39m] Step: 221, Training Logs: loss_final: 1.086760, loss_mean: 1.026719, loss_mean_cls: 0.060041, grad_norm: 1.093327
Steps:   0%|  | 222/1000000 [01:10<83:36:48,  3.32it/s, grad_norm=1.09, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.06][[34m2025-10-04 11:59:26[39m] Step: 222, Training Logs: loss_final: 1.038916, loss_mean: 0.978318, loss_mean_cls: 0.060598, grad_norm: 1.068303
Steps:   0%| | 223/1000000 [01:10<83:01:53,  3.34it/s, grad_norm=1.07, loss_final=1.04, loss_mean=0.978, loss_mean_cls=0.060[[34m2025-10-04 11:59:26[39m] Step: 223, Training Logs: loss_final: 1.057428, loss_mean: 0.997229, loss_mean_cls: 0.060198, grad_norm: 1.065031
Steps:   0%| | 224/1000000 [01:10<82:43:37,  3.36it/s, grad_norm=1.07, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.060[[34m2025-10-04 11:59:27[39m] Step: 224, Training Logs: loss_final: 1.067340, loss_mean: 1.008277, loss_mean_cls: 0.059063, grad_norm: 1.163527
Steps:   0%| | 224/1000000 [01:10<82:43:37,  3.36it/s, grad_norm=1.16, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0591
Steps:   0%|   | 226/1000000 [01:11<84:36:20,  3.28it/s, grad_norm=1.06, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0605][[34m2025-10-04 11:59:27[39m] Step: 226, Training Logs: loss_final: 1.045886, loss_mean: 0.986143, loss_mean_cls: 0.059743, grad_norm: 1.153386
Steps:   0%| | 227/1000000 [01:11<82:41:16,  3.36it/s, grad_norm=1.15, loss_final=1.05, loss_mean=0.986, loss_mean_cls=0.059[[34m2025-10-04 11:59:28[39m] Step: 227, Training Logs: loss_final: 1.034164, loss_mean: 0.973530, loss_mean_cls: 0.060634, grad_norm: 0.976899
Steps:   0%| | 228/1000000 [01:11<81:27:11,  3.41it/s, grad_norm=0.977, loss_final=1.03, loss_mean=0.974, loss_mean_cls=0.06[[34m2025-10-04 11:59:28[39m] Step: 228, Training Logs: loss_final: 1.059143, loss_mean: 0.998691, loss_mean_cls: 0.060452, grad_norm: 1.149375
Steps:   0%| | 229/1000000 [01:12<80:51:50,  3.43it/s, grad_norm=1.15, loss_final=1.06, loss_mean=0.999, loss_mean_cls=0.060[[34m2025-10-04 11:59:28[39m] Step: 229, Training Logs: loss_final: 1.029826, loss_mean: 0.970765, loss_mean_cls: 0.059061, grad_norm: 1.116368
Steps:   0%| | 230/1000000 [01:12<82:14:30,  3.38it/s, grad_norm=1.12, loss_final=1.03, loss_mean=0.971, loss_mean_cls=0.059[[34m2025-10-04 11:59:29[39m] Step: 230, Training Logs: loss_final: 1.025412, loss_mean: 0.964530, loss_mean_cls: 0.060883, grad_norm: 1.519690
Steps:   0%| | 231/1000000 [01:12<81:05:27,  3.42it/s, grad_norm=1.52, loss_final=1.03, loss_mean=0.965, loss_mean_cls=0.060[[34m2025-10-04 11:59:29[39m] Step: 231, Training Logs: loss_final: 1.060463, loss_mean: 1.000738, loss_mean_cls: 0.059725, grad_norm: 1.156793
Steps:   0%|   | 231/1000000 [01:12<81:05:27,  3.42it/s, grad_norm=1.16, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0597]
Steps:   0%| | 233/1000000 [01:13<81:26:58,  3.41it/s, grad_norm=1.08, loss_final=1.05, loss_mean=0.993, loss_mean_cls=0.059[[34m2025-10-04 11:59:29[39m] Step: 233, Training Logs: loss_final: 1.049033, loss_mean: 0.990299, loss_mean_cls: 0.058734, grad_norm: 1.123685
Steps:   0%| | 234/1000000 [01:13<83:25:54,  3.33it/s, grad_norm=1.12, loss_final=1.05, loss_mean=0.99, loss_mean_cls=0.0587[[34m2025-10-04 11:59:30[39m] Step: 234, Training Logs: loss_final: 1.063928, loss_mean: 1.004589, loss_mean_cls: 0.059339, grad_norm: 1.342039
Steps:   0%|   | 235/1000000 [01:13<81:56:10,  3.39it/s, grad_norm=1.34, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0593][[34m2025-10-04 11:59:30[39m] Step: 235, Training Logs: loss_final: 1.038271, loss_mean: 0.977799, loss_mean_cls: 0.060472, grad_norm: 0.908680
Steps:   0%| | 236/1000000 [01:14<81:02:04,  3.43it/s, grad_norm=0.909, loss_final=1.04, loss_mean=0.978, loss_mean_cls=0.06[[34m2025-10-04 11:59:30[39m] Step: 236, Training Logs: loss_final: 1.047474, loss_mean: 0.987739, loss_mean_cls: 0.059735, grad_norm: 1.005320
Steps:   0%| | 237/1000000 [01:14<80:29:58,  3.45it/s, grad_norm=1.01, loss_final=1.05, loss_mean=0.988, loss_mean_cls=0.059[[34m2025-10-04 11:59:31[39m] Step: 237, Training Logs: loss_final: 1.023801, loss_mean: 0.963113, loss_mean_cls: 0.060688, grad_norm: 1.016588
Steps:   0%| | 237/1000000 [01:14<80:29:58,  3.45it/s, grad_norm=1.02, loss_final=1.02, loss_mean=0.963, loss_mean_cls=0.060
Steps:   0%| | 240/1000000 [01:15<81:28:14,  3.41it/s, grad_norm=0.858, loss_final=1.04, loss_mean=0.977, loss_mean_cls=0.06[[34m2025-10-04 11:59:31[39m] Step: 240, Training Logs: loss_final: 1.022371, loss_mean: 0.961824, loss_mean_cls: 0.060547, grad_norm: 1.372671
Steps:   0%| | 241/1000000 [01:15<81:06:17,  3.42it/s, grad_norm=1.37, loss_final=1.02, loss_mean=0.962, loss_mean_cls=0.060[[34m2025-10-04 11:59:32[39m] Step: 241, Training Logs: loss_final: 1.035283, loss_mean: 0.976663, loss_mean_cls: 0.058621, grad_norm: 1.023927
Steps:   0%| | 242/1000000 [01:15<83:33:01,  3.32it/s, grad_norm=1.02, loss_final=1.04, loss_mean=0.977, loss_mean_cls=0.058[[34m2025-10-04 11:59:32[39m] Step: 242, Training Logs: loss_final: 1.033531, loss_mean: 0.973893, loss_mean_cls: 0.059638, grad_norm: 0.840458
Steps:   0%| | 243/1000000 [01:16<84:17:03,  3.29it/s, grad_norm=0.84, loss_final=1.03, loss_mean=0.974, loss_mean_cls=0.059[[34m2025-10-04 11:59:32[39m] Step: 243, Training Logs: loss_final: 1.056961, loss_mean: 0.996270, loss_mean_cls: 0.060691, grad_norm: 1.204095
Steps:   0%| | 244/1000000 [01:16<84:37:28,  3.28it/s, grad_norm=1.2, loss_final=1.06, loss_mean=0.996, loss_mean_cls=0.0607[[34m2025-10-04 11:59:33[39m] Step: 244, Training Logs: loss_final: 1.031632, loss_mean: 0.971930, loss_mean_cls: 0.059702, grad_norm: 0.793920
Steps:   0%| | 244/1000000 [01:16<84:37:28,  3.28it/s, grad_norm=0.794, loss_final=1.03, loss_mean=0.972, loss_mean_cls=0.05
Steps:   0%| | 247/1000000 [01:17<82:35:32,  3.36it/s, grad_norm=0.919, loss_final=1.06, loss_mean=0.998, loss_mean_cls=0.05[[34m2025-10-04 11:59:34[39m] Step: 247, Training Logs: loss_final: 1.026028, loss_mean: 0.967065, loss_mean_cls: 0.058962, grad_norm: 0.866246
Steps:   0%| | 248/1000000 [01:17<82:36:16,  3.36it/s, grad_norm=0.866, loss_final=1.03, loss_mean=0.967, loss_mean_cls=0.05[[34m2025-10-04 11:59:34[39m] Step: 248, Training Logs: loss_final: 1.066033, loss_mean: 1.007041, loss_mean_cls: 0.058992, grad_norm: 0.883244
Steps:   0%| | 249/1000000 [01:18<84:23:11,  3.29it/s, grad_norm=0.883, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.059[[34m2025-10-04 11:59:34[39m] Step: 249, Training Logs: loss_final: 1.040200, loss_mean: 0.980144, loss_mean_cls: 0.060056, grad_norm: 1.024802
Steps:   0%| | 250/1000000 [01:18<83:45:28,  3.32it/s, grad_norm=1.02, loss_final=1.04, loss_mean=0.98, loss_mean_cls=0.0601[[34m2025-10-04 11:59:35[39m] Step: 250, Training Logs: loss_final: 1.028762, loss_mean: 0.969286, loss_mean_cls: 0.059477, grad_norm: 0.768439
Steps:   0%| | 251/1000000 [01:18<83:06:04,  3.34it/s, grad_norm=0.768, loss_final=1.03, loss_mean=0.969, loss_mean_cls=0.05[[34m2025-10-04 11:59:35[39m] Step: 251, Training Logs: loss_final: 1.023879, loss_mean: 0.965186, loss_mean_cls: 0.058693, grad_norm: 0.993195
Steps:   0%| | 251/1000000 [01:18<83:06:04,  3.34it/s, grad_norm=0.993, loss_final=1.02, loss_mean=0.965, loss_mean_cls=0.05
Steps:   0%| | 254/1000000 [01:19<80:42:57,  3.44it/s, grad_norm=0.896, loss_final=1.02, loss_mean=0.959, loss_mean_cls=0.05[[34m2025-10-04 11:59:36[39m] Step: 254, Training Logs: loss_final: 1.033269, loss_mean: 0.972717, loss_mean_cls: 0.060552, grad_norm: 1.572865
Steps:   0%| | 255/1000000 [01:19<82:56:32,  3.35it/s, grad_norm=1.57, loss_final=1.03, loss_mean=0.973, loss_mean_cls=0.060[[34m2025-10-04 11:59:36[39m] Step: 255, Training Logs: loss_final: 1.052704, loss_mean: 0.994115, loss_mean_cls: 0.058589, grad_norm: 1.067278
Steps:   0%| | 256/1000000 [01:20<81:53:50,  3.39it/s, grad_norm=1.07, loss_final=1.05, loss_mean=0.994, loss_mean_cls=0.058[[34m2025-10-04 11:59:36[39m] Step: 256, Training Logs: loss_final: 1.033268, loss_mean: 0.974695, loss_mean_cls: 0.058573, grad_norm: 1.141338
Steps:   0%| | 257/1000000 [01:20<81:14:36,  3.42it/s, grad_norm=1.14, loss_final=1.03, loss_mean=0.975, loss_mean_cls=0.058[[34m2025-10-04 11:59:37[39m] Step: 257, Training Logs: loss_final: 1.020905, loss_mean: 0.962658, loss_mean_cls: 0.058247, grad_norm: 0.801576
Steps:   0%| | 258/1000000 [01:20<80:49:13,  3.44it/s, grad_norm=0.802, loss_final=1.02, loss_mean=0.963, loss_mean_cls=0.05[[34m2025-10-04 11:59:37[39m] Step: 258, Training Logs: loss_final: 1.069934, loss_mean: 1.011987, loss_mean_cls: 0.057947, grad_norm: 1.034377
Steps:   0%| | 258/1000000 [01:20<80:49:13,  3.44it/s, grad_norm=1.03, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0579
Steps:   0%| | 261/1000000 [01:21<81:11:15,  3.42it/s, grad_norm=0.862, loss_final=1.04, loss_mean=0.986, loss_mean_cls=0.05[[34m2025-10-04 11:59:38[39m] Step: 261, Training Logs: loss_final: 1.056149, loss_mean: 0.997148, loss_mean_cls: 0.059001, grad_norm: 1.018608
Steps:   0%| | 262/1000000 [01:21<80:14:44,  3.46it/s, grad_norm=1.02, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.059[[34m2025-10-04 11:59:38[39m] Step: 262, Training Logs: loss_final: 1.019789, loss_mean: 0.961388, loss_mean_cls: 0.058400, grad_norm: 1.054557
Steps:   0%| | 263/1000000 [01:22<80:37:48,  3.44it/s, grad_norm=1.05, loss_final=1.02, loss_mean=0.961, loss_mean_cls=0.058[[34m2025-10-04 11:59:38[39m] Step: 263, Training Logs: loss_final: 1.049937, loss_mean: 0.991498, loss_mean_cls: 0.058439, grad_norm: 0.757994
Steps:   0%| | 264/1000000 [01:22<80:26:36,  3.45it/s, grad_norm=0.758, loss_final=1.05, loss_mean=0.991, loss_mean_cls=0.05[[34m2025-10-04 11:59:39[39m] Step: 264, Training Logs: loss_final: 1.047902, loss_mean: 0.989353, loss_mean_cls: 0.058549, grad_norm: 1.401295
Steps:   0%| | 265/1000000 [01:22<79:44:52,  3.48it/s, grad_norm=1.4, loss_final=1.05, loss_mean=0.989, loss_mean_cls=0.0585[[34m2025-10-04 11:59:39[39m] Step: 265, Training Logs: loss_final: 1.017465, loss_mean: 0.958300, loss_mean_cls: 0.059164, grad_norm: 0.842247
Steps:   0%| | 265/1000000 [01:22<79:44:52,  3.48it/s, grad_norm=0.842, loss_final=1.02, loss_mean=0.958, loss_mean_cls=0.05
Steps:   0%| | 268/1000000 [01:23<79:52:41,  3.48it/s, grad_norm=1.02, loss_final=1.03, loss_mean=0.977, loss_mean_cls=0.057[[34m2025-10-04 11:59:40[39m] Step: 268, Training Logs: loss_final: 1.019988, loss_mean: 0.961462, loss_mean_cls: 0.058526, grad_norm: 0.888695
Steps:   0%| | 269/1000000 [01:23<79:23:06,  3.50it/s, grad_norm=0.889, loss_final=1.02, loss_mean=0.961, loss_mean_cls=0.05[[34m2025-10-04 11:59:40[39m] Step: 269, Training Logs: loss_final: 1.040647, loss_mean: 0.981753, loss_mean_cls: 0.058894, grad_norm: 0.903536
Steps:   0%| | 270/1000000 [01:24<79:01:53,  3.51it/s, grad_norm=0.904, loss_final=1.04, loss_mean=0.982, loss_mean_cls=0.05[[34m2025-10-04 11:59:40[39m] Step: 270, Training Logs: loss_final: 1.027176, loss_mean: 0.967842, loss_mean_cls: 0.059333, grad_norm: 0.874025
Steps:   0%| | 271/1000000 [01:24<78:56:25,  3.52it/s, grad_norm=0.874, loss_final=1.03, loss_mean=0.968, loss_mean_cls=0.05[[34m2025-10-04 11:59:41[39m] Step: 271, Training Logs: loss_final: 1.048959, loss_mean: 0.989350, loss_mean_cls: 0.059609, grad_norm: 0.803761
Steps:   0%| | 272/1000000 [01:24<79:54:40,  3.48it/s, grad_norm=0.804, loss_final=1.05, loss_mean=0.989, loss_mean_cls=0.05[[34m2025-10-04 11:59:41[39m] Step: 272, Training Logs: loss_final: 1.015719, loss_mean: 0.958181, loss_mean_cls: 0.057537, grad_norm: 1.248416
Steps:   0%| | 272/1000000 [01:24<79:54:40,  3.48it/s, grad_norm=1.25, loss_final=1.02, loss_mean=0.958, loss_mean_cls=0.057
Steps:   0%| | 275/1000000 [01:25<79:03:19,  3.51it/s, grad_norm=0.967, loss_final=1.02, loss_mean=0.966, loss_mean_cls=0.05[[34m2025-10-04 11:59:42[39m] Step: 275, Training Logs: loss_final: 1.032919, loss_mean: 0.974401, loss_mean_cls: 0.058517, grad_norm: 0.843899
Steps:   0%| | 276/1000000 [01:25<78:55:19,  3.52it/s, grad_norm=0.844, loss_final=1.03, loss_mean=0.974, loss_mean_cls=0.05[[34m2025-10-04 11:59:42[39m] Step: 276, Training Logs: loss_final: 1.033738, loss_mean: 0.975844, loss_mean_cls: 0.057894, grad_norm: 0.904493
Steps:   0%| | 277/1000000 [01:26<78:41:17,  3.53it/s, grad_norm=0.904, loss_final=1.03, loss_mean=0.976, loss_mean_cls=0.05[[34m2025-10-04 11:59:42[39m] Step: 277, Training Logs: loss_final: 1.022608, loss_mean: 0.964248, loss_mean_cls: 0.058360, grad_norm: 0.866951
Steps:   0%| | 278/1000000 [01:26<78:39:46,  3.53it/s, grad_norm=0.867, loss_final=1.02, loss_mean=0.964, loss_mean_cls=0.05[[34m2025-10-04 11:59:43[39m] Step: 278, Training Logs: loss_final: 1.036414, loss_mean: 0.978575, loss_mean_cls: 0.057839, grad_norm: 0.707303
Steps:   0%| | 279/1000000 [01:26<79:09:12,  3.51it/s, grad_norm=0.707, loss_final=1.04, loss_mean=0.979, loss_mean_cls=0.05[[34m2025-10-04 11:59:43[39m] Step: 279, Training Logs: loss_final: 1.028003, loss_mean: 0.970761, loss_mean_cls: 0.057243, grad_norm: 0.905739
Steps:   0%| | 279/1000000 [01:26<79:09:12,  3.51it/s, grad_norm=0.906, loss_final=1.03, loss_mean=0.971, loss_mean_cls=0.05
Steps:   0%| | 282/1000000 [01:27<79:46:08,  3.48it/s, grad_norm=0.821, loss_final=1.04, loss_mean=0.981, loss_mean_cls=0.05[[34m2025-10-04 11:59:44[39m] Step: 282, Training Logs: loss_final: 1.011075, loss_mean: 0.952894, loss_mean_cls: 0.058181, grad_norm: 0.671294
Steps:   0%| | 283/1000000 [01:27<79:43:28,  3.48it/s, grad_norm=0.671, loss_final=1.01, loss_mean=0.953, loss_mean_cls=0.05[[34m2025-10-04 11:59:44[39m] Step: 283, Training Logs: loss_final: 1.019086, loss_mean: 0.961182, loss_mean_cls: 0.057904, grad_norm: 1.114947
Steps:   0%| | 284/1000000 [01:28<79:32:05,  3.49it/s, grad_norm=1.11, loss_final=1.02, loss_mean=0.961, loss_mean_cls=0.057[[34m2025-10-04 11:59:44[39m] Step: 284, Training Logs: loss_final: 1.029109, loss_mean: 0.970442, loss_mean_cls: 0.058667, grad_norm: 0.666842
Steps:   0%| | 285/1000000 [01:28<79:15:45,  3.50it/s, grad_norm=0.667, loss_final=1.03, loss_mean=0.97, loss_mean_cls=0.058[[34m2025-10-04 11:59:45[39m] Step: 285, Training Logs: loss_final: 1.041883, loss_mean: 0.983726, loss_mean_cls: 0.058158, grad_norm: 1.000216
Steps:   0%|  | 286/1000000 [01:28<79:15:46,  3.50it/s, grad_norm=1, loss_final=1.04, loss_mean=0.984, loss_mean_cls=0.0582][[34m2025-10-04 11:59:45[39m] Step: 286, Training Logs: loss_final: 1.011892, loss_mean: 0.954671, loss_mean_cls: 0.057221, grad_norm: 0.742658
Steps:   0%| | 286/1000000 [01:28<79:15:46,  3.50it/s, grad_norm=0.743, loss_final=1.01, loss_mean=0.955, loss_mean_cls=0.05
Steps:   0%| | 289/1000000 [01:29<80:11:21,  3.46it/s, grad_norm=0.743, loss_final=1.02, loss_mean=0.962, loss_mean_cls=0.05[[34m2025-10-04 11:59:46[39m] Step: 289, Training Logs: loss_final: 1.017251, loss_mean: 0.959054, loss_mean_cls: 0.058197, grad_norm: 0.871899
Steps:   0%| | 290/1000000 [01:29<81:28:09,  3.41it/s, grad_norm=0.872, loss_final=1.02, loss_mean=0.959, loss_mean_cls=0.05[[34m2025-10-04 11:59:46[39m] Step: 290, Training Logs: loss_final: 1.014231, loss_mean: 0.956765, loss_mean_cls: 0.057466, grad_norm: 1.069149
Steps:   0%| | 291/1000000 [01:30<80:48:21,  3.44it/s, grad_norm=1.07, loss_final=1.01, loss_mean=0.957, loss_mean_cls=0.057[[34m2025-10-04 11:59:46[39m] Step: 291, Training Logs: loss_final: 1.012971, loss_mean: 0.955729, loss_mean_cls: 0.057241, grad_norm: 0.948060
Steps:   0%| | 292/1000000 [01:30<80:15:51,  3.46it/s, grad_norm=0.948, loss_final=1.01, loss_mean=0.956, loss_mean_cls=0.05[[34m2025-10-04 11:59:47[39m] Step: 292, Training Logs: loss_final: 0.988343, loss_mean: 0.930338, loss_mean_cls: 0.058006, grad_norm: 0.654610
Steps:   0%| | 293/1000000 [01:30<80:31:40,  3.45it/s, grad_norm=0.655, loss_final=0.988, loss_mean=0.93, loss_mean_cls=0.05[[34m2025-10-04 11:59:47[39m] Step: 293, Training Logs: loss_final: 1.008259, loss_mean: 0.951601, loss_mean_cls: 0.056658, grad_norm: 0.772584
Steps:   0%| | 293/1000000 [01:30<80:31:40,  3.45it/s, grad_norm=0.773, loss_final=1.01, loss_mean=0.952, loss_mean_cls=0.05
Steps:   0%| | 296/1000000 [01:31<82:44:49,  3.36it/s, grad_norm=0.79, loss_final=1.01, loss_mean=0.947, loss_mean_cls=0.058[[34m2025-10-04 11:59:48[39m] Step: 296, Training Logs: loss_final: 0.995307, loss_mean: 0.937201, loss_mean_cls: 0.058106, grad_norm: 0.773530
Steps:   0%| | 297/1000000 [01:31<81:53:19,  3.39it/s, grad_norm=0.774, loss_final=0.995, loss_mean=0.937, loss_mean_cls=0.0[[34m2025-10-04 11:59:48[39m] Step: 297, Training Logs: loss_final: 1.018198, loss_mean: 0.960209, loss_mean_cls: 0.057988, grad_norm: 0.778248
Steps:   0%| | 298/1000000 [01:32<81:14:56,  3.42it/s, grad_norm=0.778, loss_final=1.02, loss_mean=0.96, loss_mean_cls=0.058[[34m2025-10-04 11:59:48[39m] Step: 298, Training Logs: loss_final: 1.017756, loss_mean: 0.959511, loss_mean_cls: 0.058246, grad_norm: 0.963301
Steps:   0%| | 299/1000000 [01:32<81:49:56,  3.39it/s, grad_norm=0.963, loss_final=1.02, loss_mean=0.96, loss_mean_cls=0.058[[34m2025-10-04 11:59:49[39m] Step: 299, Training Logs: loss_final: 1.029318, loss_mean: 0.970967, loss_mean_cls: 0.058351, grad_norm: 0.937274
Steps:   0%| | 300/1000000 [01:32<82:25:05,  3.37it/s, grad_norm=0.937, loss_final=1.03, loss_mean=0.971, loss_mean_cls=0.05[[34m2025-10-04 11:59:49[39m] Step: 300, Training Logs: loss_final: 1.007811, loss_mean: 0.949522, loss_mean_cls: 0.058289, grad_norm: 0.630913
Steps:   0%| | 300/1000000 [01:32<82:25:05,  3.37it/s, grad_norm=0.631, loss_final=1.01, loss_mean=0.95, loss_mean_cls=0.058
Steps:   0%| | 303/1000000 [01:33<83:51:55,  3.31it/s, grad_norm=1.55, loss_final=1.03, loss_mean=0.974, loss_mean_cls=0.056[[34m2025-10-04 11:59:50[39m] Step: 303, Training Logs: loss_final: 1.024089, loss_mean: 0.966090, loss_mean_cls: 0.057998, grad_norm: 0.793828
Steps:   0%| | 304/1000000 [01:33<82:16:56,  3.37it/s, grad_norm=0.794, loss_final=1.02, loss_mean=0.966, loss_mean_cls=0.05[[34m2025-10-04 11:59:50[39m] Step: 304, Training Logs: loss_final: 1.026446, loss_mean: 0.968071, loss_mean_cls: 0.058375, grad_norm: 0.844596
Steps:   0%| | 305/1000000 [01:34<82:13:21,  3.38it/s, grad_norm=0.845, loss_final=1.03, loss_mean=0.968, loss_mean_cls=0.05[[34m2025-10-04 11:59:50[39m] Step: 305, Training Logs: loss_final: 1.004808, loss_mean: 0.947714, loss_mean_cls: 0.057094, grad_norm: 0.770304
Steps:   0%|  | 306/1000000 [01:34<81:08:18,  3.42it/s, grad_norm=0.77, loss_final=1, loss_mean=0.948, loss_mean_cls=0.0571][[34m2025-10-04 11:59:51[39m] Step: 306, Training Logs: loss_final: 1.006524, loss_mean: 0.948498, loss_mean_cls: 0.058026, grad_norm: 0.978765
Steps:   0%| | 307/1000000 [01:34<80:11:54,  3.46it/s, grad_norm=0.979, loss_final=1.01, loss_mean=0.948, loss_mean_cls=0.05[[34m2025-10-04 11:59:51[39m] Step: 307, Training Logs: loss_final: 1.023667, loss_mean: 0.966769, loss_mean_cls: 0.056897, grad_norm: 1.071971
Steps:   0%| | 307/1000000 [01:34<80:11:54,  3.46it/s, grad_norm=1.07, loss_final=1.02, loss_mean=0.967, loss_mean_cls=0.056
Steps:   0%| | 310/1000000 [01:35<81:25:42,  3.41it/s, grad_norm=1.16, loss_final=1.03, loss_mean=0.968, loss_mean_cls=0.057[[34m2025-10-04 11:59:52[39m] Step: 310, Training Logs: loss_final: 1.006072, loss_mean: 0.947475, loss_mean_cls: 0.058597, grad_norm: 1.213351
Steps:   0%| | 311/1000000 [01:36<81:49:43,  3.39it/s, grad_norm=1.21, loss_final=1.01, loss_mean=0.947, loss_mean_cls=0.058[[34m2025-10-04 11:59:52[39m] Step: 311, Training Logs: loss_final: 1.026765, loss_mean: 0.969551, loss_mean_cls: 0.057215, grad_norm: 1.922248
Steps:   0%| | 312/1000000 [01:36<81:35:24,  3.40it/s, grad_norm=1.92, loss_final=1.03, loss_mean=0.97, loss_mean_cls=0.0572[[34m2025-10-04 11:59:52[39m] Step: 312, Training Logs: loss_final: 1.014698, loss_mean: 0.957144, loss_mean_cls: 0.057554, grad_norm: 1.485842
Steps:   0%| | 313/1000000 [01:36<81:08:25,  3.42it/s, grad_norm=1.49, loss_final=1.01, loss_mean=0.957, loss_mean_cls=0.057[[34m2025-10-04 11:59:53[39m] Step: 313, Training Logs: loss_final: 1.014127, loss_mean: 0.956727, loss_mean_cls: 0.057400, grad_norm: 1.422783
Steps:   0%| | 314/1000000 [01:36<81:06:04,  3.42it/s, grad_norm=1.42, loss_final=1.01, loss_mean=0.957, loss_mean_cls=0.057[[34m2025-10-04 11:59:53[39m] Step: 314, Training Logs: loss_final: 1.001512, loss_mean: 0.943434, loss_mean_cls: 0.058078, grad_norm: 1.408204
Steps:   0%|  | 314/1000000 [01:36<81:06:04,  3.42it/s, grad_norm=1.41, loss_final=1, loss_mean=0.943, loss_mean_cls=0.0581]
Steps:   0%| | 317/1000000 [01:37<81:15:44,  3.42it/s, grad_norm=1.03, loss_final=1.01, loss_mean=0.947, loss_mean_cls=0.058[[34m2025-10-04 11:59:54[39m] Step: 317, Training Logs: loss_final: 1.048406, loss_mean: 0.992274, loss_mean_cls: 0.056133, grad_norm: 1.178803
Steps:   0%| | 318/1000000 [01:38<81:29:31,  3.41it/s, grad_norm=1.18, loss_final=1.05, loss_mean=0.992, loss_mean_cls=0.056[[34m2025-10-04 11:59:54[39m] Step: 318, Training Logs: loss_final: 1.015677, loss_mean: 0.958566, loss_mean_cls: 0.057111, grad_norm: 1.145043
Steps:   0%| | 319/1000000 [01:38<82:21:12,  3.37it/s, grad_norm=1.15, loss_final=1.02, loss_mean=0.959, loss_mean_cls=0.057[[34m2025-10-04 11:59:55[39m] Step: 319, Training Logs: loss_final: 1.015932, loss_mean: 0.959430, loss_mean_cls: 0.056502, grad_norm: 1.110621
Steps:   0%| | 320/1000000 [01:38<81:12:49,  3.42it/s, grad_norm=1.11, loss_final=1.02, loss_mean=0.959, loss_mean_cls=0.056[[34m2025-10-04 11:59:55[39m] Step: 320, Training Logs: loss_final: 1.018037, loss_mean: 0.960450, loss_mean_cls: 0.057587, grad_norm: 1.310447
Steps:   0%| | 321/1000000 [01:38<80:37:44,  3.44it/s, grad_norm=1.31, loss_final=1.02, loss_mean=0.96, loss_mean_cls=0.0576[[34m2025-10-04 11:59:55[39m] Step: 321, Training Logs: loss_final: 1.017647, loss_mean: 0.959925, loss_mean_cls: 0.057721, grad_norm: 0.899477
Steps:   0%| | 321/1000000 [01:38<80:37:44,  3.44it/s, grad_norm=0.899, loss_final=1.02, loss_mean=0.96, loss_mean_cls=0.057
Steps:   0%| | 324/1000000 [01:39<80:16:21,  3.46it/s, grad_norm=1.17, loss_final=1.04, loss_mean=0.982, loss_mean_cls=0.056[[34m2025-10-04 11:59:56[39m] Step: 324, Training Logs: loss_final: 1.026602, loss_mean: 0.968773, loss_mean_cls: 0.057829, grad_norm: 1.161447
Steps:   0%| | 325/1000000 [01:40<80:46:25,  3.44it/s, grad_norm=1.16, loss_final=1.03, loss_mean=0.969, loss_mean_cls=0.057[[34m2025-10-04 11:59:56[39m] Step: 325, Training Logs: loss_final: 1.005603, loss_mean: 0.948553, loss_mean_cls: 0.057050, grad_norm: 1.094988
Steps:   0%| | 326/1000000 [01:40<79:59:58,  3.47it/s, grad_norm=1.09, loss_final=1.01, loss_mean=0.949, loss_mean_cls=0.057[[34m2025-10-04 11:59:57[39m] Step: 326, Training Logs: loss_final: 1.014215, loss_mean: 0.957177, loss_mean_cls: 0.057039, grad_norm: 1.100006
Steps:   0%| | 327/1000000 [01:40<81:42:39,  3.40it/s, grad_norm=1.1, loss_final=1.01, loss_mean=0.957, loss_mean_cls=0.057][[34m2025-10-04 11:59:57[39m] Step: 327, Training Logs: loss_final: 1.019311, loss_mean: 0.961141, loss_mean_cls: 0.058170, grad_norm: 0.816990
Steps:   0%| | 328/1000000 [01:40<80:58:56,  3.43it/s, grad_norm=0.817, loss_final=1.02, loss_mean=0.961, loss_mean_cls=0.05[[34m2025-10-04 11:59:57[39m] Step: 328, Training Logs: loss_final: 1.006825, loss_mean: 0.949626, loss_mean_cls: 0.057199, grad_norm: 1.118074
Steps:   0%| | 328/1000000 [01:40<80:58:56,  3.43it/s, grad_norm=1.12, loss_final=1.01, loss_mean=0.95, loss_mean_cls=0.0572
Steps:   0%| | 331/1000000 [01:41<79:32:45,  3.49it/s, grad_norm=1.07, loss_final=1.02, loss_mean=0.962, loss_mean_cls=0.055[[34m2025-10-04 11:59:58[39m] Step: 331, Training Logs: loss_final: 1.028385, loss_mean: 0.971684, loss_mean_cls: 0.056701, grad_norm: 1.124968
Steps:   0%| | 332/1000000 [01:42<80:41:46,  3.44it/s, grad_norm=1.12, loss_final=1.03, loss_mean=0.972, loss_mean_cls=0.056[[34m2025-10-04 11:59:58[39m] Step: 332, Training Logs: loss_final: 1.017041, loss_mean: 0.960833, loss_mean_cls: 0.056208, grad_norm: 0.727091
Steps:   0%| | 333/1000000 [01:42<80:42:45,  3.44it/s, grad_norm=0.727, loss_final=1.02, loss_mean=0.961, loss_mean_cls=0.05[[34m2025-10-04 11:59:59[39m] Step: 333, Training Logs: loss_final: 1.017956, loss_mean: 0.961653, loss_mean_cls: 0.056303, grad_norm: 0.962668
Steps:   0%| | 334/1000000 [01:42<81:08:01,  3.42it/s, grad_norm=0.963, loss_final=1.02, loss_mean=0.962, loss_mean_cls=0.05[[34m2025-10-04 11:59:59[39m] Step: 334, Training Logs: loss_final: 1.012183, loss_mean: 0.955372, loss_mean_cls: 0.056812, grad_norm: 0.781692
Steps:   0%| | 335/1000000 [01:43<80:30:21,  3.45it/s, grad_norm=0.782, loss_final=1.01, loss_mean=0.955, loss_mean_cls=0.05[[34m2025-10-04 11:59:59[39m] Step: 335, Training Logs: loss_final: 1.020072, loss_mean: 0.962697, loss_mean_cls: 0.057375, grad_norm: 0.895319
Steps:   0%| | 338/1000000 [01:43<81:23:35,  3.41it/s, grad_norm=0.989, loss_final=1.01, loss_mean=0.949, loss_mean_cls=0.05[[34m2025-10-04 12:00:00[39m] Step: 338, Training Logs: loss_final: 0.997454, loss_mean: 0.940297, loss_mean_cls: 0.057157, grad_norm: 0.714587
Steps:   0%| | 339/1000000 [01:44<81:59:54,  3.39it/s, grad_norm=0.715, loss_final=0.997, loss_mean=0.94, loss_mean_cls=0.05[[34m2025-10-04 12:00:00[39m] Step: 339, Training Logs: loss_final: 1.042962, loss_mean: 0.986831, loss_mean_cls: 0.056131, grad_norm: 0.853947
Steps:   0%| | 340/1000000 [01:44<81:12:53,  3.42it/s, grad_norm=0.854, loss_final=1.04, loss_mean=0.987, loss_mean_cls=0.05[[34m2025-10-04 12:00:01[39m] Step: 340, Training Logs: loss_final: 1.012410, loss_mean: 0.955630, loss_mean_cls: 0.056780, grad_norm: 1.113705
Steps:   0%| | 341/1000000 [01:44<80:31:34,  3.45it/s, grad_norm=1.11, loss_final=1.01, loss_mean=0.956, loss_mean_cls=0.056[[34m2025-10-04 12:00:01[39m] Step: 341, Training Logs: loss_final: 1.002821, loss_mean: 0.946296, loss_mean_cls: 0.056525, grad_norm: 0.670107
Steps:   0%|  | 341/1000000 [01:44<80:31:34,  3.45it/s, grad_norm=0.67, loss_final=1, loss_mean=0.946, loss_mean_cls=0.0565]
Steps:   0%| | 345/1000000 [01:45<79:28:14,  3.49it/s, grad_norm=0.71, loss_final=1.01, loss_mean=0.95, loss_mean_cls=0.0564[[34m2025-10-04 12:00:02[39m] Step: 345, Training Logs: loss_final: 0.992759, loss_mean: 0.936511, loss_mean_cls: 0.056248, grad_norm: 0.668432
Steps:   0%| | 346/1000000 [01:46<81:38:05,  3.40it/s, grad_norm=0.668, loss_final=0.993, loss_mean=0.937, loss_mean_cls=0.0[[34m2025-10-04 12:00:02[39m] Step: 346, Training Logs: loss_final: 1.021784, loss_mean: 0.965745, loss_mean_cls: 0.056039, grad_norm: 0.674241
Steps:   0%| | 347/1000000 [01:46<80:43:05,  3.44it/s, grad_norm=0.674, loss_final=1.02, loss_mean=0.966, loss_mean_cls=0.05[[34m2025-10-04 12:00:03[39m] Step: 347, Training Logs: loss_final: 1.013461, loss_mean: 0.957676, loss_mean_cls: 0.055785, grad_norm: 0.870181
Steps:   0%| | 348/1000000 [01:46<81:42:31,  3.40it/s, grad_norm=0.87, loss_final=1.01, loss_mean=0.958, loss_mean_cls=0.055[[34m2025-10-04 12:00:03[39m] Step: 348, Training Logs: loss_final: 1.003865, loss_mean: 0.947640, loss_mean_cls: 0.056225, grad_norm: 0.593953
Steps:   0%| | 348/1000000 [01:46<81:42:31,  3.40it/s, grad_norm=0.594, loss_final=1, loss_mean=0.948, loss_mean_cls=0.0562]
Steps:   0%| | 352/1000000 [01:47<80:28:49,  3.45it/s, grad_norm=0.95, loss_final=1.02, loss_mean=0.967, loss_mean_cls=0.056[[34m2025-10-04 12:00:04[39m] Step: 352, Training Logs: loss_final: 1.030451, loss_mean: 0.973910, loss_mean_cls: 0.056542, grad_norm: 0.939556
Steps:   0%| | 353/1000000 [01:48<80:02:01,  3.47it/s, grad_norm=0.94, loss_final=1.03, loss_mean=0.974, loss_mean_cls=0.056[[34m2025-10-04 12:00:04[39m] Step: 353, Training Logs: loss_final: 1.003234, loss_mean: 0.947138, loss_mean_cls: 0.056095, grad_norm: 0.647791
Steps:   0%| | 354/1000000 [01:48<79:34:57,  3.49it/s, grad_norm=0.648, loss_final=1, loss_mean=0.947, loss_mean_cls=0.0561][[34m2025-10-04 12:00:05[39m] Step: 354, Training Logs: loss_final: 1.027248, loss_mean: 0.971350, loss_mean_cls: 0.055899, grad_norm: 0.809754
Steps:   0%| | 355/1000000 [01:48<79:16:17,  3.50it/s, grad_norm=0.81, loss_final=1.03, loss_mean=0.971, loss_mean_cls=0.055[[34m2025-10-04 12:00:05[39m] Step: 355, Training Logs: loss_final: 1.030000, loss_mean: 0.974122, loss_mean_cls: 0.055878, grad_norm: 0.868476
Steps:   0%| | 355/1000000 [01:48<79:16:17,  3.50it/s, grad_norm=0.868, loss_final=1.03, loss_mean=0.974, loss_mean_cls=0.05
Steps:   0%| | 359/1000000 [01:49<82:08:02,  3.38it/s, grad_norm=0.917, loss_final=1.04, loss_mean=0.986, loss_mean_cls=0.05[[34m2025-10-04 12:00:06[39m] Step: 359, Training Logs: loss_final: 1.009598, loss_mean: 0.954062, loss_mean_cls: 0.055536, grad_norm: 1.072015
Steps:   0%| | 360/1000000 [01:50<83:22:12,  3.33it/s, grad_norm=1.07, loss_final=1.01, loss_mean=0.954, loss_mean_cls=0.055[[34m2025-10-04 12:00:06[39m] Step: 360, Training Logs: loss_final: 1.000474, loss_mean: 0.943805, loss_mean_cls: 0.056670, grad_norm: 1.079769
Steps:   0%|  | 361/1000000 [01:50<84:03:40,  3.30it/s, grad_norm=1.08, loss_final=1, loss_mean=0.944, loss_mean_cls=0.0567][[34m2025-10-04 12:00:07[39m] Step: 361, Training Logs: loss_final: 1.001710, loss_mean: 0.945982, loss_mean_cls: 0.055729, grad_norm: 1.026698
Steps:   0%|  | 362/1000000 [01:50<84:05:15,  3.30it/s, grad_norm=1.03, loss_final=1, loss_mean=0.946, loss_mean_cls=0.0557][[34m2025-10-04 12:00:07[39m] Step: 362, Training Logs: loss_final: 1.019222, loss_mean: 0.962967, loss_mean_cls: 0.056255, grad_norm: 0.823947
Steps:   0%| | 362/1000000 [01:50<84:05:15,  3.30it/s, grad_norm=0.824, loss_final=1.02, loss_mean=0.963, loss_mean_cls=0.05
Steps:   0%| | 366/1000000 [01:52<82:05:30,  3.38it/s, grad_norm=0.916, loss_final=1.03, loss_mean=0.971, loss_mean_cls=0.05[[34m2025-10-04 12:00:08[39m] Step: 366, Training Logs: loss_final: 0.996688, loss_mean: 0.940544, loss_mean_cls: 0.056145, grad_norm: 0.895093
Steps:   0%| | 367/1000000 [01:52<81:06:51,  3.42it/s, grad_norm=0.895, loss_final=0.997, loss_mean=0.941, loss_mean_cls=0.0[[34m2025-10-04 12:00:09[39m] Step: 367, Training Logs: loss_final: 1.000296, loss_mean: 0.942490, loss_mean_cls: 0.057806, grad_norm: 0.804379
Steps:   0%| | 367/1000000 [01:52<81:06:51,  3.42it/s, grad_norm=0.804, loss_final=1, loss_mean=0.942, loss_mean_cls=0.0578]
Steps:   0%| | 373/1000000 [01:54<82:30:25,  3.37it/s, grad_norm=1.31, loss_final=1.01, loss_mean=0.955, loss_mean_cls=0.055[[34m2025-10-04 12:00:10[39m] Step: 373, Training Logs: loss_final: 1.006794, loss_mean: 0.949628, loss_mean_cls: 0.057166, grad_norm: 1.350664
Steps:   0%| | 374/1000000 [01:54<83:10:12,  3.34it/s, grad_norm=1.35, loss_final=1.01, loss_mean=0.95, loss_mean_cls=0.0572[[34m2025-10-04 12:00:11[39m] Step: 374, Training Logs: loss_final: 1.010429, loss_mean: 0.954434, loss_mean_cls: 0.055994, grad_norm: 0.737399
Steps:   0%| | 374/1000000 [01:54<83:10:12,  3.34it/s, grad_norm=0.737, loss_final=1.01, loss_mean=0.954, loss_mean_cls=0.05
Steps:   0%| | 380/1000000 [01:56<82:42:12,  3.36it/s, grad_norm=0.957, loss_final=0.998, loss_mean=0.942, loss_mean_cls=0.0[[34m2025-10-04 12:00:12[39m] Step: 380, Training Logs: loss_final: 1.000465, loss_mean: 0.944448, loss_mean_cls: 0.056017, grad_norm: 1.133845
Steps:   0%|   | 381/1000000 [01:56<81:37:33,  3.40it/s, grad_norm=1.13, loss_final=1, loss_mean=0.944, loss_mean_cls=0.056][[34m2025-10-04 12:00:13[39m] Step: 381, Training Logs: loss_final: 1.003357, loss_mean: 0.947993, loss_mean_cls: 0.055364, grad_norm: 0.632612
Steps:   0%| | 381/1000000 [01:56<81:37:33,  3.40it/s, grad_norm=0.633, loss_final=1, loss_mean=0.948, loss_mean_cls=0.0554]
Steps:   0%| | 387/1000000 [01:58<79:55:52,  3.47it/s, grad_norm=0.88, loss_final=1.02, loss_mean=0.96, loss_mean_cls=0.0561[[34m2025-10-04 12:00:14[39m] Step: 387, Training Logs: loss_final: 1.024759, loss_mean: 0.968850, loss_mean_cls: 0.055908, grad_norm: 0.870078
Steps:   0%| | 388/1000000 [01:58<81:23:26,  3.41it/s, grad_norm=0.87, loss_final=1.02, loss_mean=0.969, loss_mean_cls=0.055[[34m2025-10-04 12:00:15[39m] Step: 388, Training Logs: loss_final: 1.033858, loss_mean: 0.979204, loss_mean_cls: 0.054654, grad_norm: 0.843862
Steps:   0%| | 388/1000000 [01:58<81:23:26,  3.41it/s, grad_norm=0.844, loss_final=1.03, loss_mean=0.979, loss_mean_cls=0.05
Steps:   0%| | 394/1000000 [02:00<83:53:59,  3.31it/s, grad_norm=1.22, loss_final=1.03, loss_mean=0.972, loss_mean_cls=0.056[[34m2025-10-04 12:00:17[39m] Step: 394, Training Logs: loss_final: 1.005784, loss_mean: 0.950423, loss_mean_cls: 0.055361, grad_norm: 1.221492
Steps:   0%| | 395/1000000 [02:00<83:12:03,  3.34it/s, grad_norm=1.22, loss_final=1.01, loss_mean=0.95, loss_mean_cls=0.0554[[34m2025-10-04 12:00:17[39m] Step: 395, Training Logs: loss_final: 1.020835, loss_mean: 0.965560, loss_mean_cls: 0.055275, grad_norm: 1.180369
Steps:   0%| | 395/1000000 [02:00<83:12:03,  3.34it/s, grad_norm=1.18, loss_final=1.02, loss_mean=0.966, loss_mean_cls=0.055
Steps:   0%| | 401/1000000 [02:02<82:17:44,  3.37it/s, grad_norm=0.963, loss_final=1.01, loss_mean=0.958, loss_mean_cls=0.05[[34m2025-10-04 12:00:19[39m] Step: 401, Training Logs: loss_final: 1.019100, loss_mean: 0.963040, loss_mean_cls: 0.056060, grad_norm: 0.862213
Steps:   0%| | 401/1000000 [02:02<82:17:44,  3.37it/s, grad_norm=0.862, loss_final=1.02, loss_mean=0.963, loss_mean_cls=0.05
Steps:   0%| | 408/1000000 [02:04<81:56:46,  3.39it/s, grad_norm=0.804, loss_final=1, loss_mean=0.945, loss_mean_cls=0.0561][[34m2025-10-04 12:00:21[39m] Step: 408, Training Logs: loss_final: 1.002301, loss_mean: 0.947588, loss_mean_cls: 0.054714, grad_norm: 0.955987
Steps:   0%| | 408/1000000 [02:04<81:56:46,  3.39it/s, grad_norm=0.956, loss_final=1, loss_mean=0.948, loss_mean_cls=0.0547]
Steps:   0%| | 415/1000000 [02:06<81:09:30,  3.42it/s, grad_norm=0.879, loss_final=1.02, loss_mean=0.968, loss_mean_cls=0.05[[34m2025-10-04 12:00:23[39m] Step: 415, Training Logs: loss_final: 1.025498, loss_mean: 0.970613, loss_mean_cls: 0.054885, grad_norm: 1.148207
Steps:   0%| | 415/1000000 [02:06<81:09:30,  3.42it/s, grad_norm=1.15, loss_final=1.03, loss_mean=0.971, loss_mean_cls=0.054
Steps:   0%| | 422/1000000 [02:08<79:53:55,  3.48it/s, grad_norm=1.01, loss_final=0.965, loss_mean=0.909, loss_mean_cls=0.05[[34m2025-10-04 12:00:25[39m] Step: 422, Training Logs: loss_final: 0.995844, loss_mean: 0.941701, loss_mean_cls: 0.054143, grad_norm: 0.725723
Steps:   0%| | 422/1000000 [02:08<79:53:55,  3.48it/s, grad_norm=0.726, loss_final=0.996, loss_mean=0.942, loss_mean_cls=0.0
Steps:   0%| | 429/1000000 [02:10<78:54:17,  3.52it/s, grad_norm=1.11, loss_final=0.98, loss_mean=0.925, loss_mean_cls=0.055[[34m2025-10-04 12:00:27[39m] Step: 429, Training Logs: loss_final: 0.993751, loss_mean: 0.938265, loss_mean_cls: 0.055485, grad_norm: 0.924058
Steps:   0%| | 429/1000000 [02:10<78:54:17,  3.52it/s, grad_norm=0.924, loss_final=0.994, loss_mean=0.938, loss_mean_cls=0.0
Steps:   0%| | 436/1000000 [02:12<80:53:31,  3.43it/s, grad_norm=0.755, loss_final=1.01, loss_mean=0.951, loss_mean_cls=0.05[[34m2025-10-04 12:00:29[39m] Step: 436, Training Logs: loss_final: 0.986392, loss_mean: 0.929166, loss_mean_cls: 0.057226, grad_norm: 0.959395
Steps:   0%| | 436/1000000 [02:12<80:53:31,  3.43it/s, grad_norm=0.959, loss_final=0.986, loss_mean=0.929, loss_mean_cls=0.0
Steps:   0%| | 443/1000000 [02:14<81:29:32,  3.41it/s, grad_norm=0.99, loss_final=0.995, loss_mean=0.939, loss_mean_cls=0.05[[34m2025-10-04 12:00:31[39m] Step: 443, Training Logs: loss_final: 0.989937, loss_mean: 0.934574, loss_mean_cls: 0.055363, grad_norm: 1.057715
Steps:   0%| | 443/1000000 [02:14<81:29:32,  3.41it/s, grad_norm=1.06, loss_final=0.99, loss_mean=0.935, loss_mean_cls=0.055
Steps:   0%| | 450/1000000 [02:16<80:09:54,  3.46it/s, grad_norm=0.906, loss_final=1, loss_mean=0.948, loss_mean_cls=0.0547][[34m2025-10-04 12:00:33[39m] Step: 450, Training Logs: loss_final: 1.004995, loss_mean: 0.951234, loss_mean_cls: 0.053762, grad_norm: 0.885046
Steps:   0%| | 450/1000000 [02:16<80:09:54,  3.46it/s, grad_norm=0.885, loss_final=1, loss_mean=0.951, loss_mean_cls=0.0538]
Steps:   0%| | 457/1000000 [02:18<80:34:06,  3.45it/s, grad_norm=0.866, loss_final=1, loss_mean=0.949, loss_mean_cls=0.0546][[34m2025-10-04 12:00:35[39m] Step: 457, Training Logs: loss_final: 1.014030, loss_mean: 0.959732, loss_mean_cls: 0.054298, grad_norm: 0.984038
Steps:   0%| | 457/1000000 [02:18<80:34:06,  3.45it/s, grad_norm=0.984, loss_final=1.01, loss_mean=0.96, loss_mean_cls=0.054
Steps:   0%| | 464/1000000 [02:20<84:58:36,  3.27it/s, grad_norm=1.21, loss_final=0.996, loss_mean=0.942, loss_mean_cls=0.05[[34m2025-10-04 12:00:37[39m] Step: 464, Training Logs: loss_final: 0.979170, loss_mean: 0.923971, loss_mean_cls: 0.055199, grad_norm: 0.871957
Steps:   0%| | 464/1000000 [02:20<84:58:36,  3.27it/s, grad_norm=0.872, loss_final=0.979, loss_mean=0.924, loss_mean_cls=0.0
Steps:   0%| | 471/1000000 [02:22<80:33:07,  3.45it/s, grad_norm=0.655, loss_final=0.991, loss_mean=0.935, loss_mean_cls=0.0[[34m2025-10-04 12:00:39[39m] Step: 471, Training Logs: loss_final: 1.000215, loss_mean: 0.944981, loss_mean_cls: 0.055233, grad_norm: 1.080070
Steps:   0%|  | 471/1000000 [02:22<80:33:07,  3.45it/s, grad_norm=1.08, loss_final=1, loss_mean=0.945, loss_mean_cls=0.0552]
Steps:   0%| | 478/1000000 [02:24<81:29:08,  3.41it/s, grad_norm=0.912, loss_final=0.996, loss_mean=0.942, loss_mean_cls=0.0[[34m2025-10-04 12:00:41[39m] Step: 478, Training Logs: loss_final: 1.003468, loss_mean: 0.949648, loss_mean_cls: 0.053819, grad_norm: 0.948200
Steps:   0%|  | 478/1000000 [02:24<81:29:08,  3.41it/s, grad_norm=0.948, loss_final=1, loss_mean=0.95, loss_mean_cls=0.0538]
Steps:   0%| | 484/1000000 [02:26<82:40:05,  3.36it/s, grad_norm=1.59, loss_final=1.02, loss_mean=0.967, loss_mean_cls=0.055[[34m2025-10-04 12:00:41[39m] Step: 478, Training Logs: loss_final: 1.003468, loss_mean: 0.949648, loss_mean_cls: 0.053819, grad_norm: 0.948200
Steps:   0%| | 491/1000000 [02:28<82:30:08,  3.37it/s, grad_norm=0.886, loss_final=0.97, loss_mean=0.915, loss_mean_cls=0.05[[34m2025-10-04 12:00:43[39m] Step: 485, Training Logs: loss_final: 1.011734, loss_mean: 0.957149, loss_mean_cls: 0.054585, grad_norm: 1.363770
Steps:   0%| | 498/1000000 [02:30<79:49:00,  3.48it/s, grad_norm=0.85, loss_final=0.981, loss_mean=0.926, loss_mean_cls=0.05[[34m2025-10-04 12:00:45[39m] Step: 492, Training Logs: loss_final: 0.996066, loss_mean: 0.942019, loss_mean_cls: 0.054047, grad_norm: 1.317507
Steps:   0%| | 505/1000000 [02:32<80:50:40,  3.43it/s, grad_norm=0.747, loss_final=0.968, loss_mean=0.914, loss_mean_cls=0.0[[34m2025-10-04 12:00:47[39m] Step: 499, Training Logs: loss_final: 0.974514, loss_mean: 0.919706, loss_mean_cls: 0.054809, grad_norm: 0.895420
Steps:   0%| | 512/1000000 [02:34<81:57:41,  3.39it/s, grad_norm=0.851, loss_final=0.97, loss_mean=0.916, loss_mean_cls=0.05[[34m2025-10-04 12:00:49[39m] Step: 506, Training Logs: loss_final: 1.001376, loss_mean: 0.945980, loss_mean_cls: 0.055396, grad_norm: 0.689536
Steps:   0%|  | 519/1000000 [02:36<81:28:51,  3.41it/s, grad_norm=1.04, loss_final=1, loss_mean=0.948, loss_mean_cls=0.0528][[34m2025-10-04 12:00:51[39m] Step: 513, Training Logs: loss_final: 1.008229, loss_mean: 0.954650, loss_mean_cls: 0.053579, grad_norm: 0.926745
Steps:   0%| | 526/1000000 [02:39<82:01:26,  3.38it/s, grad_norm=1.08, loss_final=0.959, loss_mean=0.906, loss_mean_cls=0.05[[34m2025-10-04 12:00:53[39m] Step: 520, Training Logs: loss_final: 0.997998, loss_mean: 0.944368, loss_mean_cls: 0.053630, grad_norm: 0.745950
Steps:   0%| | 533/1000000 [02:41<82:22:49,  3.37it/s, grad_norm=1.11, loss_final=1.01, loss_mean=0.952, loss_mean_cls=0.054[[34m2025-10-04 12:00:55[39m] Step: 527, Training Logs: loss_final: 1.002514, loss_mean: 0.949213, loss_mean_cls: 0.053302, grad_norm: 0.868751
Steps:   0%| | 540/1000000 [02:43<80:14:59,  3.46it/s, grad_norm=0.911, loss_final=0.99, loss_mean=0.936, loss_mean_cls=0.05[[34m2025-10-04 12:00:57[39m] Step: 534, Training Logs: loss_final: 0.980358, loss_mean: 0.925976, loss_mean_cls: 0.054382, grad_norm: 0.796904
Steps:   0%| | 541/1000000 [02:43<79:53:46,  3.47it/s, grad_norm=0.911, loss_final=0.99, loss_mean=0.936, loss_mean_cls=0.05[[34m2025-10-04 12:01:00[39m] Step: 541, Training Logs: loss_final: 0.975565, loss_mean: 0.921534, loss_mean_cls: 0.054031, grad_norm: 0.913433
Steps:   0%| | 548/1000000 [02:45<83:48:21,  3.31it/s, grad_norm=0.862, loss_final=0.973, loss_mean=0.92, loss_mean_cls=0.05[[34m2025-10-04 12:01:02[39m] Step: 548, Training Logs: loss_final: 0.964863, loss_mean: 0.910865, loss_mean_cls: 0.053997, grad_norm: 0.881582
Steps:   0%| | 555/1000000 [02:47<83:06:28,  3.34it/s, grad_norm=1.13, loss_final=0.981, loss_mean=0.927, loss_mean_cls=0.05[[34m2025-10-04 12:01:04[39m] Step: 555, Training Logs: loss_final: 0.977680, loss_mean: 0.923543, loss_mean_cls: 0.054138, grad_norm: 0.985705
Steps:   0%| | 562/1000000 [02:49<80:22:05,  3.45it/s, grad_norm=0.74, loss_final=0.986, loss_mean=0.932, loss_mean_cls=0.05[[34m2025-10-04 12:01:06[39m] Step: 562, Training Logs: loss_final: 0.969387, loss_mean: 0.915793, loss_mean_cls: 0.053594, grad_norm: 0.885723
Steps:   0%| | 569/1000000 [02:51<81:25:21,  3.41it/s, grad_norm=1.03, loss_final=0.992, loss_mean=0.94, loss_mean_cls=0.052[[34m2025-10-04 12:01:08[39m] Step: 569, Training Logs: loss_final: 0.984886, loss_mean: 0.931311, loss_mean_cls: 0.053576, grad_norm: 0.769391
Steps:   0%| | 576/1000000 [02:53<79:42:52,  3.48it/s, grad_norm=0.902, loss_final=0.979, loss_mean=0.925, loss_mean_cls=0.0[[34m2025-10-04 12:01:10[39m] Step: 576, Training Logs: loss_final: 0.996702, loss_mean: 0.943603, loss_mean_cls: 0.053099, grad_norm: 1.328864
Steps:   0%| | 583/1000000 [02:55<83:22:33,  3.33it/s, grad_norm=0.909, loss_final=0.99, loss_mean=0.937, loss_mean_cls=0.05[[34m2025-10-04 12:01:12[39m] Step: 583, Training Logs: loss_final: 0.986052, loss_mean: 0.932556, loss_mean_cls: 0.053496, grad_norm: 0.986844
Steps:   0%| | 590/1000000 [02:57<81:27:08,  3.41it/s, grad_norm=0.872, loss_final=0.989, loss_mean=0.937, loss_mean_cls=0.0[[34m2025-10-04 12:01:14[39m] Step: 590, Training Logs: loss_final: 0.973206, loss_mean: 0.919281, loss_mean_cls: 0.053925, grad_norm: 0.866129
Steps:   0%| | 597/1000000 [02:59<82:49:00,  3.35it/s, grad_norm=0.702, loss_final=0.986, loss_mean=0.935, loss_mean_cls=0.0[[34m2025-10-04 12:01:16[39m] Step: 597, Training Logs: loss_final: 0.988200, loss_mean: 0.934375, loss_mean_cls: 0.053825, grad_norm: 1.376437
Steps:   0%| | 604/1000000 [03:02<85:34:12,  3.24it/s, grad_norm=0.905, loss_final=0.978, loss_mean=0.925, loss_mean_cls=0.0[[34m2025-10-04 12:01:18[39m] Step: 604, Training Logs: loss_final: 1.013389, loss_mean: 0.961599, loss_mean_cls: 0.051789, grad_norm: 1.067720
Steps:   0%| | 611/1000000 [03:04<80:55:44,  3.43it/s, grad_norm=0.658, loss_final=0.981, loss_mean=0.927, loss_mean_cls=0.0[[34m2025-10-04 12:01:20[39m] Step: 611, Training Logs: loss_final: 0.980346, loss_mean: 0.927160, loss_mean_cls: 0.053186, grad_norm: 0.809386
Steps:   0%| | 618/1000000 [03:06<78:56:53,  3.52it/s, grad_norm=0.815, loss_final=0.995, loss_mean=0.942, loss_mean_cls=0.0[[34m2025-10-04 12:01:22[39m] Step: 618, Training Logs: loss_final: 0.984749, loss_mean: 0.931723, loss_mean_cls: 0.053026, grad_norm: 1.010564
Steps:   0%| | 625/1000000 [03:08<79:33:21,  3.49it/s, grad_norm=1.03, loss_final=0.961, loss_mean=0.91, loss_mean_cls=0.051[[34m2025-10-04 12:01:24[39m] Step: 625, Training Logs: loss_final: 0.979538, loss_mean: 0.925779, loss_mean_cls: 0.053759, grad_norm: 0.885046
Steps:   0%| | 632/1000000 [03:10<81:39:34,  3.40it/s, grad_norm=0.842, loss_final=0.971, loss_mean=0.916, loss_mean_cls=0.0[[34m2025-10-04 12:01:26[39m] Step: 632, Training Logs: loss_final: 0.985116, loss_mean: 0.931824, loss_mean_cls: 0.053293, grad_norm: 0.794634
Steps:   0%| | 639/1000000 [03:12<80:23:08,  3.45it/s, grad_norm=0.834, loss_final=0.978, loss_mean=0.925, loss_mean_cls=0.0[[34m2025-10-04 12:01:28[39m] Step: 639, Training Logs: loss_final: 0.974248, loss_mean: 0.921593, loss_mean_cls: 0.052655, grad_norm: 0.815015
Steps:   0%| | 646/1000000 [03:14<78:58:41,  3.51it/s, grad_norm=0.493, loss_final=0.985, loss_mean=0.933, loss_mean_cls=0.0[[34m2025-10-04 12:01:30[39m] Step: 646, Training Logs: loss_final: 1.007385, loss_mean: 0.954281, loss_mean_cls: 0.053104, grad_norm: 0.913238
Steps:   0%| | 653/1000000 [03:16<84:10:03,  3.30it/s, grad_norm=1.03, loss_final=0.988, loss_mean=0.934, loss_mean_cls=0.05[[34m2025-10-04 12:01:32[39m] Step: 653, Training Logs: loss_final: 0.985123, loss_mean: 0.932587, loss_mean_cls: 0.052537, grad_norm: 0.995581
Steps:   0%| | 660/1000000 [03:18<79:52:32,  3.48it/s, grad_norm=1.2, loss_final=0.988, loss_mean=0.934, loss_mean_cls=0.054[[34m2025-10-04 12:01:34[39m] Step: 660, Training Logs: loss_final: 0.998024, loss_mean: 0.944459, loss_mean_cls: 0.053565, grad_norm: 1.263652
Steps:   0%| | 667/1000000 [03:20<83:55:37,  3.31it/s, grad_norm=0.713, loss_final=0.986, loss_mean=0.933, loss_mean_cls=0.0[[34m2025-10-04 12:01:37[39m] Step: 667, Training Logs: loss_final: 0.964570, loss_mean: 0.911145, loss_mean_cls: 0.053425, grad_norm: 0.823347
Steps:   0%| | 674/1000000 [03:22<82:25:01,  3.37it/s, grad_norm=1.09, loss_final=1.01, loss_mean=0.957, loss_mean_cls=0.052[[34m2025-10-04 12:01:39[39m] Step: 674, Training Logs: loss_final: 0.967671, loss_mean: 0.914629, loss_mean_cls: 0.053042, grad_norm: 0.789310
Steps:   0%| | 681/1000000 [03:24<81:06:31,  3.42it/s, grad_norm=1, loss_final=0.964, loss_mean=0.912, loss_mean_cls=0.0526][[34m2025-10-04 12:01:41[39m] Step: 681, Training Logs: loss_final: 0.986372, loss_mean: 0.934655, loss_mean_cls: 0.051717, grad_norm: 0.762239
Steps:   0%| | 688/1000000 [03:26<83:32:29,  3.32it/s, grad_norm=0.84, loss_final=0.965, loss_mean=0.911, loss_mean_cls=0.05[[34m2025-10-04 12:01:43[39m] Step: 688, Training Logs: loss_final: 0.966884, loss_mean: 0.914379, loss_mean_cls: 0.052505, grad_norm: 1.038725
Steps:   0%| | 695/1000000 [03:28<81:34:33,  3.40it/s, grad_norm=0.672, loss_final=0.986, loss_mean=0.935, loss_mean_cls=0.0[[34m2025-10-04 12:01:45[39m] Step: 695, Training Logs: loss_final: 0.969130, loss_mean: 0.917048, loss_mean_cls: 0.052083, grad_norm: 0.755802
Steps:   0%| | 702/1000000 [03:30<82:47:44,  3.35it/s, grad_norm=1.12, loss_final=0.975, loss_mean=0.922, loss_mean_cls=0.05[[34m2025-10-04 12:01:47[39m] Step: 702, Training Logs: loss_final: 0.972694, loss_mean: 0.919622, loss_mean_cls: 0.053072, grad_norm: 0.489744
Steps:   0%| | 709/1000000 [03:32<83:09:21,  3.34it/s, grad_norm=1.02, loss_final=0.979, loss_mean=0.927, loss_mean_cls=0.05[[34m2025-10-04 12:01:49[39m] Step: 709, Training Logs: loss_final: 0.955855, loss_mean: 0.902375, loss_mean_cls: 0.053479, grad_norm: 0.786072
Steps:   0%| | 716/1000000 [03:34<83:15:56,  3.33it/s, grad_norm=0.689, loss_final=0.954, loss_mean=0.901, loss_mean_cls=0.0[[34m2025-10-04 12:01:51[39m] Step: 716, Training Logs: loss_final: 0.971372, loss_mean: 0.920224, loss_mean_cls: 0.051148, grad_norm: 0.584641
Steps:   0%| | 723/1000000 [03:36<80:31:22,  3.45it/s, grad_norm=0.82, loss_final=0.962, loss_mean=0.91, loss_mean_cls=0.052[[34m2025-10-04 12:01:53[39m] Step: 723, Training Logs: loss_final: 0.966274, loss_mean: 0.914726, loss_mean_cls: 0.051548, grad_norm: 0.634492
Steps:   0%| | 730/1000000 [03:39<83:29:48,  3.32it/s, grad_norm=0.843, loss_final=0.965, loss_mean=0.912, loss_mean_cls=0.0[[34m2025-10-04 12:01:55[39m] Step: 730, Training Logs: loss_final: 0.981535, loss_mean: 0.929549, loss_mean_cls: 0.051987, grad_norm: 0.851333
Steps:   0%| | 737/1000000 [03:41<82:41:07,  3.36it/s, grad_norm=0.785, loss_final=0.967, loss_mean=0.914, loss_mean_cls=0.0[[34m2025-10-04 12:01:57[39m] Step: 737, Training Logs: loss_final: 0.973060, loss_mean: 0.920788, loss_mean_cls: 0.052272, grad_norm: 0.907380
Steps:   0%| | 744/1000000 [03:43<81:15:31,  3.42it/s, grad_norm=0.694, loss_final=0.967, loss_mean=0.914, loss_mean_cls=0.0[[34m2025-10-04 12:01:59[39m] Step: 744, Training Logs: loss_final: 0.970917, loss_mean: 0.917698, loss_mean_cls: 0.053219, grad_norm: 0.980308
Steps:   0%| | 751/1000000 [03:45<82:19:28,  3.37it/s, grad_norm=0.82, loss_final=0.973, loss_mean=0.921, loss_mean_cls=0.05[[34m2025-10-04 12:02:01[39m] Step: 751, Training Logs: loss_final: 0.983364, loss_mean: 0.931575, loss_mean_cls: 0.051789, grad_norm: 0.923502
Steps:   0%| | 758/1000000 [03:47<81:22:18,  3.41it/s, grad_norm=1.3, loss_final=0.978, loss_mean=0.927, loss_mean_cls=0.051[[34m2025-10-04 12:02:03[39m] Step: 758, Training Logs: loss_final: 0.981828, loss_mean: 0.929368, loss_mean_cls: 0.052460, grad_norm: 0.848423
Steps:   0%| | 765/1000000 [03:49<80:34:41,  3.44it/s, grad_norm=0.734, loss_final=0.948, loss_mean=0.895, loss_mean_cls=0.0[[34m2025-10-04 12:02:05[39m] Step: 765, Training Logs: loss_final: 0.964411, loss_mean: 0.912737, loss_mean_cls: 0.051674, grad_norm: 0.670622
Steps:   0%| | 771/1000000 [03:51<82:55:59,  3.35it/s, grad_norm=0.879, loss_final=0.98, loss_mean=0.929, loss_mean_cls=0.05[[34m2025-10-04 12:02:05[39m] Step: 765, Training Logs: loss_final: 0.964411, loss_mean: 0.912737, loss_mean_cls: 0.051674, grad_norm: 0.670622
Steps:   0%| | 779/1000000 [03:53<79:40:26,  3.48it/s, grad_norm=0.887, loss_final=0.979, loss_mean=0.927, loss_mean_cls=0.0[[34m2025-10-04 12:02:10[39m] Step: 779, Training Logs: loss_final: 0.969450, loss_mean: 0.917784, loss_mean_cls: 0.051666, grad_norm: 1.045190
Steps:   0%| | 785/1000000 [03:55<81:51:11,  3.39it/s, grad_norm=0.656, loss_final=0.967, loss_mean=0.914, loss_mean_cls=0.0[[34m2025-10-04 12:02:10[39m] Step: 779, Training Logs: loss_final: 0.969450, loss_mean: 0.917784, loss_mean_cls: 0.051666, grad_norm: 1.045190
Steps:   0%| | 792/1000000 [03:57<81:21:46,  3.41it/s, grad_norm=0.549, loss_final=0.952, loss_mean=0.901, loss_mean_cls=0.0[[34m2025-10-04 12:02:12[39m] Step: 786, Training Logs: loss_final: 0.965095, loss_mean: 0.913066, loss_mean_cls: 0.052029, grad_norm: 0.743107
Steps:   0%| | 799/1000000 [03:59<80:27:59,  3.45it/s, grad_norm=0.591, loss_final=0.954, loss_mean=0.902, loss_mean_cls=0.0[[34m2025-10-04 12:02:14[39m] Step: 793, Training Logs: loss_final: 0.948666, loss_mean: 0.896760, loss_mean_cls: 0.051906, grad_norm: 0.537744
Steps:   0%| | 806/1000000 [04:01<81:58:53,  3.39it/s, grad_norm=0.707, loss_final=0.986, loss_mean=0.935, loss_mean_cls=0.0[[34m2025-10-04 12:02:16[39m] Step: 800, Training Logs: loss_final: 0.956422, loss_mean: 0.903338, loss_mean_cls: 0.053084, grad_norm: 0.649889
Steps:   0%| | 813/1000000 [04:03<81:04:18,  3.42it/s, grad_norm=0.824, loss_final=0.957, loss_mean=0.904, loss_mean_cls=0.0[[34m2025-10-04 12:02:18[39m] Step: 807, Training Logs: loss_final: 0.986348, loss_mean: 0.934520, loss_mean_cls: 0.051828, grad_norm: 0.540067
Steps:   0%| | 820/1000000 [04:05<81:34:52,  3.40it/s, grad_norm=0.864, loss_final=0.974, loss_mean=0.922, loss_mean_cls=0.0[[34m2025-10-04 12:02:20[39m] Step: 814, Training Logs: loss_final: 0.941201, loss_mean: 0.888102, loss_mean_cls: 0.053099, grad_norm: 0.758180
Steps:   0%| | 821/1000000 [04:05<80:52:14,  3.43it/s, grad_norm=0.864, loss_final=0.974, loss_mean=0.922, loss_mean_cls=0.0[[34m2025-10-04 12:02:22[39m] Step: 821, Training Logs: loss_final: 0.950138, loss_mean: 0.899773, loss_mean_cls: 0.050366, grad_norm: 0.957282
Steps:   0%| | 828/1000000 [04:07<83:19:58,  3.33it/s, grad_norm=0.763, loss_final=0.979, loss_mean=0.928, loss_mean_cls=0.0[[34m2025-10-04 12:02:24[39m] Step: 828, Training Logs: loss_final: 0.988403, loss_mean: 0.936990, loss_mean_cls: 0.051413, grad_norm: 0.865914
Steps:   0%| | 835/1000000 [04:09<84:19:41,  3.29it/s, grad_norm=0.762, loss_final=0.971, loss_mean=0.92, loss_mean_cls=0.05[[34m2025-10-04 12:02:26[39m] Step: 835, Training Logs: loss_final: 0.958720, loss_mean: 0.906325, loss_mean_cls: 0.052395, grad_norm: 0.592898
Steps:   0%| | 842/1000000 [04:11<81:10:03,  3.42it/s, grad_norm=0.698, loss_final=0.944, loss_mean=0.891, loss_mean_cls=0.0[[34m2025-10-04 12:02:28[39m] Step: 842, Training Logs: loss_final: 0.939499, loss_mean: 0.887457, loss_mean_cls: 0.052042, grad_norm: 0.763151
Steps:   0%| | 849/1000000 [04:14<80:13:54,  3.46it/s, grad_norm=0.727, loss_final=0.974, loss_mean=0.921, loss_mean_cls=0.0[[34m2025-10-04 12:02:30[39m] Step: 849, Training Logs: loss_final: 0.959059, loss_mean: 0.905394, loss_mean_cls: 0.053665, grad_norm: 0.753257
Steps:   0%| | 856/1000000 [04:16<81:50:34,  3.39it/s, grad_norm=0.71, loss_final=0.971, loss_mean=0.919, loss_mean_cls=0.05[[34m2025-10-04 12:02:32[39m] Step: 856, Training Logs: loss_final: 0.985502, loss_mean: 0.933915, loss_mean_cls: 0.051587, grad_norm: 0.687994
Steps:   0%| | 863/1000000 [04:18<79:48:39,  3.48it/s, grad_norm=0.757, loss_final=0.951, loss_mean=0.901, loss_mean_cls=0.0[[34m2025-10-04 12:02:34[39m] Step: 863, Training Logs: loss_final: 0.959825, loss_mean: 0.908946, loss_mean_cls: 0.050879, grad_norm: 0.839385
Steps:   0%| | 870/1000000 [04:20<80:34:54,  3.44it/s, grad_norm=0.845, loss_final=0.954, loss_mean=0.902, loss_mean_cls=0.0[[34m2025-10-04 12:02:36[39m] Step: 870, Training Logs: loss_final: 0.957401, loss_mean: 0.904898, loss_mean_cls: 0.052503, grad_norm: 0.790943
Steps:   0%| | 877/1000000 [04:22<79:58:41,  3.47it/s, grad_norm=1.19, loss_final=0.972, loss_mean=0.92, loss_mean_cls=0.052[[34m2025-10-04 12:02:38[39m] Step: 877, Training Logs: loss_final: 0.974316, loss_mean: 0.923972, loss_mean_cls: 0.050344, grad_norm: 1.125610
Steps:   0%| | 884/1000000 [04:24<83:29:57,  3.32it/s, grad_norm=0.889, loss_final=0.957, loss_mean=0.906, loss_mean_cls=0.0[[34m2025-10-04 12:02:40[39m] Step: 884, Training Logs: loss_final: 0.958963, loss_mean: 0.908810, loss_mean_cls: 0.050153, grad_norm: 1.017892
Steps:   0%| | 891/1000000 [04:26<81:04:48,  3.42it/s, grad_norm=0.826, loss_final=0.972, loss_mean=0.921, loss_mean_cls=0.0[[34m2025-10-04 12:02:42[39m] Step: 891, Training Logs: loss_final: 0.975344, loss_mean: 0.924409, loss_mean_cls: 0.050935, grad_norm: 0.740681
Steps:   0%| | 898/1000000 [04:28<83:22:46,  3.33it/s, grad_norm=1.04, loss_final=0.959, loss_mean=0.907, loss_mean_cls=0.05[[34m2025-10-04 12:02:45[39m] Step: 898, Training Logs: loss_final: 0.948075, loss_mean: 0.896484, loss_mean_cls: 0.051591, grad_norm: 0.634547
Steps:   0%| | 905/1000000 [04:30<83:04:58,  3.34it/s, grad_norm=0.679, loss_final=0.965, loss_mean=0.914, loss_mean_cls=0.0[[34m2025-10-04 12:02:47[39m] Step: 905, Training Logs: loss_final: 0.950107, loss_mean: 0.897349, loss_mean_cls: 0.052758, grad_norm: 0.730740
Steps:   0%| | 912/1000000 [04:32<83:46:37,  3.31it/s, grad_norm=0.768, loss_final=0.942, loss_mean=0.891, loss_mean_cls=0.0[[34m2025-10-04 12:02:49[39m] Step: 912, Training Logs: loss_final: 0.926007, loss_mean: 0.873813, loss_mean_cls: 0.052194, grad_norm: 0.552234
Steps:   0%| | 919/1000000 [04:34<81:51:56,  3.39it/s, grad_norm=0.995, loss_final=0.936, loss_mean=0.884, loss_mean_cls=0.0[[34m2025-10-04 12:02:51[39m] Step: 919, Training Logs: loss_final: 0.956432, loss_mean: 0.904088, loss_mean_cls: 0.052343, grad_norm: 0.867237
Steps:   0%| | 926/1000000 [04:36<81:44:48,  3.39it/s, grad_norm=0.731, loss_final=0.97, loss_mean=0.921, loss_mean_cls=0.04[[34m2025-10-04 12:02:53[39m] Step: 926, Training Logs: loss_final: 0.962555, loss_mean: 0.910895, loss_mean_cls: 0.051660, grad_norm: 0.897457
Steps:   0%| | 933/1000000 [04:38<82:48:17,  3.35it/s, grad_norm=1.05, loss_final=0.965, loss_mean=0.913, loss_mean_cls=0.05[[34m2025-10-04 12:02:55[39m] Step: 933, Training Logs: loss_final: 0.952504, loss_mean: 0.900798, loss_mean_cls: 0.051706, grad_norm: 0.575406
Steps:   0%| | 940/1000000 [04:40<84:18:14,  3.29it/s, grad_norm=0.701, loss_final=0.953, loss_mean=0.9, loss_mean_cls=0.052[[34m2025-10-04 12:02:57[39m] Step: 940, Training Logs: loss_final: 0.975514, loss_mean: 0.923533, loss_mean_cls: 0.051981, grad_norm: 0.798944
Steps:   0%| | 947/1000000 [04:43<82:54:56,  3.35it/s, grad_norm=0.544, loss_final=0.947, loss_mean=0.895, loss_mean_cls=0.0[[34m2025-10-04 12:02:59[39m] Step: 947, Training Logs: loss_final: 0.966249, loss_mean: 0.915801, loss_mean_cls: 0.050448, grad_norm: 0.515732
Steps:   0%| | 954/1000000 [04:45<83:35:09,  3.32it/s, grad_norm=0.652, loss_final=0.949, loss_mean=0.897, loss_mean_cls=0.0[[34m2025-10-04 12:03:01[39m] Step: 954, Training Logs: loss_final: 0.952755, loss_mean: 0.902519, loss_mean_cls: 0.050236, grad_norm: 0.587048
Steps:   0%| | 961/1000000 [04:47<82:48:44,  3.35it/s, grad_norm=0.824, loss_final=0.956, loss_mean=0.906, loss_mean_cls=0.0[[34m2025-10-04 12:03:03[39m] Step: 961, Training Logs: loss_final: 0.967080, loss_mean: 0.916643, loss_mean_cls: 0.050436, grad_norm: 0.690140
Steps:   0%| | 968/1000000 [04:49<85:05:05,  3.26it/s, grad_norm=0.656, loss_final=0.947, loss_mean=0.896, loss_mean_cls=0.0[[34m2025-10-04 12:03:06[39m] Step: 968, Training Logs: loss_final: 0.947897, loss_mean: 0.896510, loss_mean_cls: 0.051387, grad_norm: 0.903539
Steps:   0%| | 974/1000000 [04:51<82:41:21,  3.36it/s, grad_norm=0.861, loss_final=0.953, loss_mean=0.902, loss_mean_cls=0.0[[34m2025-10-04 12:03:06[39m] Step: 968, Training Logs: loss_final: 0.947897, loss_mean: 0.896510, loss_mean_cls: 0.051387, grad_norm: 0.903539
Steps:   0%| | 981/1000000 [04:53<82:41:40,  3.36it/s, grad_norm=0.779, loss_final=0.95, loss_mean=0.898, loss_mean_cls=0.05[[34m2025-10-04 12:03:08[39m] Step: 975, Training Logs: loss_final: 0.966277, loss_mean: 0.915110, loss_mean_cls: 0.051167, grad_norm: 0.811094
Steps:   0%| | 988/1000000 [04:55<82:34:48,  3.36it/s, grad_norm=0.572, loss_final=0.959, loss_mean=0.908, loss_mean_cls=0.0[[34m2025-10-04 12:03:10[39m] Step: 982, Training Logs: loss_final: 0.971003, loss_mean: 0.921161, loss_mean_cls: 0.049843, grad_norm: 0.582156
Steps:   0%| | 995/1000000 [04:57<81:56:45,  3.39it/s, grad_norm=0.549, loss_final=0.945, loss_mean=0.894, loss_mean_cls=0.0[[34m2025-10-04 12:03:12[39m] Step: 989, Training Logs: loss_final: 0.952984, loss_mean: 0.901795, loss_mean_cls: 0.051188, grad_norm: 0.724804
Steps:   0%| | 1002/1000000 [04:59<82:05:03,  3.38it/s, grad_norm=0.49, loss_final=0.973, loss_mean=0.921, loss_mean_cls=0.0[[34m2025-10-04 12:03:14[39m] Step: 996, Training Logs: loss_final: 0.956657, loss_mean: 0.906065, loss_mean_cls: 0.050592, grad_norm: 0.580647
Steps:   0%| | 1003/1000000 [04:59<81:20:15,  3.41it/s, grad_norm=0.49, loss_final=0.973, loss_mean=0.921, loss_mean_cls=0.0[[34m2025-10-04 12:03:16[39m] Step: 1003, Training Logs: loss_final: 0.974727, loss_mean: 0.923548, loss_mean_cls: 0.051180, grad_norm: 0.695354
Steps:   0%| | 1010/1000000 [05:01<81:51:35,  3.39it/s, grad_norm=0.809, loss_final=0.933, loss_mean=0.881, loss_mean_cls=0.[[34m2025-10-04 12:03:18[39m] Step: 1010, Training Logs: loss_final: 0.958435, loss_mean: 0.907058, loss_mean_cls: 0.051377, grad_norm: 0.797081
Steps:   0%| | 1017/1000000 [05:03<79:50:21,  3.48it/s, grad_norm=0.684, loss_final=0.938, loss_mean=0.887, loss_mean_cls=0.[[34m2025-10-04 12:03:20[39m] Step: 1017, Training Logs: loss_final: 0.981372, loss_mean: 0.932410, loss_mean_cls: 0.048962, grad_norm: 0.628584
Steps:   0%| | 1024/1000000 [05:05<80:57:22,  3.43it/s, grad_norm=0.773, loss_final=0.956, loss_mean=0.905, loss_mean_cls=0.[[34m2025-10-04 12:03:22[39m] Step: 1024, Training Logs: loss_final: 0.970575, loss_mean: 0.920898, loss_mean_cls: 0.049676, grad_norm: 1.014128
Steps:   0%| | 1031/1000000 [05:07<82:19:31,  3.37it/s, grad_norm=1.09, loss_final=0.937, loss_mean=0.886, loss_mean_cls=0.0[[34m2025-10-04 12:03:24[39m] Step: 1031, Training Logs: loss_final: 0.944851, loss_mean: 0.894446, loss_mean_cls: 0.050405, grad_norm: 0.887662
Steps:   0%| | 1038/1000000 [05:10<81:34:42,  3.40it/s, grad_norm=1.04, loss_final=0.971, loss_mean=0.92, loss_mean_cls=0.05[[34m2025-10-04 12:03:26[39m] Step: 1038, Training Logs: loss_final: 0.930452, loss_mean: 0.879061, loss_mean_cls: 0.051391, grad_norm: 0.602857
Steps:   0%| | 1045/1000000 [05:12<82:07:13,  3.38it/s, grad_norm=0.768, loss_final=0.953, loss_mean=0.901, loss_mean_cls=0.[[34m2025-10-04 12:03:28[39m] Step: 1045, Training Logs: loss_final: 0.958428, loss_mean: 0.908099, loss_mean_cls: 0.050329, grad_norm: 0.863592
Steps:   0%| | 1052/1000000 [05:14<82:30:24,  3.36it/s, grad_norm=0.54, loss_final=0.953, loss_mean=0.902, loss_mean_cls=0.0[[34m2025-10-04 12:03:30[39m] Step: 1052, Training Logs: loss_final: 0.950767, loss_mean: 0.899219, loss_mean_cls: 0.051548, grad_norm: 0.617250
Steps:   0%| | 1059/1000000 [05:16<81:41:20,  3.40it/s, grad_norm=0.589, loss_final=0.935, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:03:32[39m] Step: 1059, Training Logs: loss_final: 0.954998, loss_mean: 0.905816, loss_mean_cls: 0.049182, grad_norm: 0.768030
Steps:   0%| | 1066/1000000 [05:18<81:17:37,  3.41it/s, grad_norm=0.924, loss_final=0.943, loss_mean=0.892, loss_mean_cls=0.[[34m2025-10-04 12:03:34[39m] Step: 1066, Training Logs: loss_final: 0.960737, loss_mean: 0.911094, loss_mean_cls: 0.049643, grad_norm: 0.590939
Steps:   0%| | 1073/1000000 [05:20<81:03:19,  3.42it/s, grad_norm=0.813, loss_final=0.947, loss_mean=0.896, loss_mean_cls=0.[[34m2025-10-04 12:03:37[39m] Step: 1073, Training Logs: loss_final: 0.936257, loss_mean: 0.885997, loss_mean_cls: 0.050260, grad_norm: 0.647214
Steps:   0%| | 1080/1000000 [05:22<82:42:15,  3.36it/s, grad_norm=0.94, loss_final=0.966, loss_mean=0.915, loss_mean_cls=0.0[[34m2025-10-04 12:03:39[39m] Step: 1080, Training Logs: loss_final: 0.950898, loss_mean: 0.898914, loss_mean_cls: 0.051984, grad_norm: 0.707301
Steps:   0%| | 1087/1000000 [05:24<81:48:56,  3.39it/s, grad_norm=0.637, loss_final=0.96, loss_mean=0.91, loss_mean_cls=0.05[[34m2025-10-04 12:03:41[39m] Step: 1087, Training Logs: loss_final: 0.945655, loss_mean: 0.893947, loss_mean_cls: 0.051708, grad_norm: 0.619389
Steps:   0%| | 1094/1000000 [05:26<82:26:34,  3.37it/s, grad_norm=0.623, loss_final=0.94, loss_mean=0.889, loss_mean_cls=0.0[[34m2025-10-04 12:03:43[39m] Step: 1094, Training Logs: loss_final: 0.986497, loss_mean: 0.936936, loss_mean_cls: 0.049562, grad_norm: 0.611199
Steps:   0%| | 1101/1000000 [05:28<85:23:15,  3.25it/s, grad_norm=0.922, loss_final=0.944, loss_mean=0.894, loss_mean_cls=0.[[34m2025-10-04 12:03:45[39m] Step: 1101, Training Logs: loss_final: 0.961436, loss_mean: 0.910047, loss_mean_cls: 0.051389, grad_norm: 0.873941
Steps:   0%| | 1108/1000000 [05:30<86:22:52,  3.21it/s, grad_norm=0.753, loss_final=0.94, loss_mean=0.889, loss_mean_cls=0.0[[34m2025-10-04 12:03:47[39m] Step: 1108, Training Logs: loss_final: 0.937176, loss_mean: 0.887312, loss_mean_cls: 0.049864, grad_norm: 0.622921
Steps:   0%| | 1115/1000000 [05:33<84:10:11,  3.30it/s, grad_norm=0.781, loss_final=0.953, loss_mean=0.902, loss_mean_cls=0.[[34m2025-10-04 12:03:49[39m] Step: 1115, Training Logs: loss_final: 0.934864, loss_mean: 0.884750, loss_mean_cls: 0.050113, grad_norm: 0.677099
Steps:   0%| | 1122/1000000 [05:35<82:25:53,  3.37it/s, grad_norm=0.803, loss_final=0.95, loss_mean=0.899, loss_mean_cls=0.0[[34m2025-10-04 12:03:51[39m] Step: 1122, Training Logs: loss_final: 0.916945, loss_mean: 0.865475, loss_mean_cls: 0.051470, grad_norm: 0.702925
Steps:   0%| | 1129/1000000 [05:37<83:01:08,  3.34it/s, grad_norm=0.836, loss_final=0.943, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:03:53[39m] Step: 1129, Training Logs: loss_final: 0.968875, loss_mean: 0.919708, loss_mean_cls: 0.049167, grad_norm: 0.696867
Steps:   0%| | 1136/1000000 [05:39<82:17:47,  3.37it/s, grad_norm=0.77, loss_final=0.954, loss_mean=0.902, loss_mean_cls=0.0[[34m2025-10-04 12:03:55[39m] Step: 1136, Training Logs: loss_final: 0.969302, loss_mean: 0.919790, loss_mean_cls: 0.049512, grad_norm: 0.593940
Steps:   0%| | 1143/1000000 [05:41<82:32:57,  3.36it/s, grad_norm=0.797, loss_final=0.932, loss_mean=0.881, loss_mean_cls=0.[[34m2025-10-04 12:03:57[39m] Step: 1143, Training Logs: loss_final: 0.950294, loss_mean: 0.900922, loss_mean_cls: 0.049372, grad_norm: 0.667473
Steps:   0%| | 1150/1000000 [05:43<80:17:04,  3.46it/s, grad_norm=0.907, loss_final=0.952, loss_mean=0.901, loss_mean_cls=0.[[34m2025-10-04 12:04:00[39m] Step: 1150, Training Logs: loss_final: 0.958362, loss_mean: 0.908353, loss_mean_cls: 0.050010, grad_norm: 0.883741
Steps:   0%| | 1157/1000000 [05:45<82:34:38,  3.36it/s, grad_norm=0.597, loss_final=0.951, loss_mean=0.9, loss_mean_cls=0.05[[34m2025-10-04 12:04:02[39m] Step: 1157, Training Logs: loss_final: 0.921881, loss_mean: 0.871285, loss_mean_cls: 0.050596, grad_norm: 0.976959
Steps:   0%| | 1164/1000000 [05:47<80:04:10,  3.47it/s, grad_norm=0.648, loss_final=0.954, loss_mean=0.903, loss_mean_cls=0.[[34m2025-10-04 12:04:04[39m] Step: 1164, Training Logs: loss_final: 0.955525, loss_mean: 0.904259, loss_mean_cls: 0.051266, grad_norm: 0.800640
Steps:   0%| | 1171/1000000 [05:49<80:59:21,  3.43it/s, grad_norm=0.619, loss_final=0.942, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:04:06[39m] Step: 1171, Training Logs: loss_final: 0.943429, loss_mean: 0.893253, loss_mean_cls: 0.050176, grad_norm: 0.606713
Steps:   0%| | 1178/1000000 [05:51<80:17:22,  3.46it/s, grad_norm=1.07, loss_final=0.94, loss_mean=0.891, loss_mean_cls=0.04[[34m2025-10-04 12:04:08[39m] Step: 1178, Training Logs: loss_final: 0.944105, loss_mean: 0.893581, loss_mean_cls: 0.050524, grad_norm: 0.627879
Steps:   0%| | 1184/1000000 [05:53<82:00:21,  3.38it/s, grad_norm=0.576, loss_final=0.94, loss_mean=0.889, loss_mean_cls=0.0[[34m2025-10-04 12:04:08[39m] Step: 1178, Training Logs: loss_final: 0.944105, loss_mean: 0.893581, loss_mean_cls: 0.050524, grad_norm: 0.627879
Steps:   0%| | 1191/1000000 [05:55<80:30:02,  3.45it/s, grad_norm=0.593, loss_final=0.954, loss_mean=0.903, loss_mean_cls=0.[[34m2025-10-04 12:04:10[39m] Step: 1185, Training Logs: loss_final: 0.933534, loss_mean: 0.883848, loss_mean_cls: 0.049685, grad_norm: 0.719522
Steps:   0%| | 1198/1000000 [05:57<80:40:04,  3.44it/s, grad_norm=0.583, loss_final=0.953, loss_mean=0.905, loss_mean_cls=0.[[34m2025-10-04 12:04:12[39m] Step: 1192, Training Logs: loss_final: 0.953782, loss_mean: 0.903609, loss_mean_cls: 0.050173, grad_norm: 0.560370
Steps:   0%| | 1205/1000000 [05:59<81:05:39,  3.42it/s, grad_norm=0.689, loss_final=0.957, loss_mean=0.907, loss_mean_cls=0.[[34m2025-10-04 12:04:14[39m] Step: 1199, Training Logs: loss_final: 0.959341, loss_mean: 0.910439, loss_mean_cls: 0.048903, grad_norm: 0.634731
Steps:   0%| | 1212/1000000 [06:01<81:56:55,  3.39it/s, grad_norm=0.549, loss_final=0.976, loss_mean=0.926, loss_mean_cls=0.[[34m2025-10-04 12:04:16[39m] Step: 1206, Training Logs: loss_final: 0.954116, loss_mean: 0.904397, loss_mean_cls: 0.049719, grad_norm: 0.773914
Steps:   0%| | 1219/1000000 [06:03<82:35:14,  3.36it/s, grad_norm=0.695, loss_final=0.953, loss_mean=0.902, loss_mean_cls=0.[[34m2025-10-04 12:04:18[39m] Step: 1213, Training Logs: loss_final: 0.948424, loss_mean: 0.898173, loss_mean_cls: 0.050251, grad_norm: 0.694920
Steps:   0%| | 1226/1000000 [06:05<81:33:12,  3.40it/s, grad_norm=0.669, loss_final=0.946, loss_mean=0.895, loss_mean_cls=0.[[34m2025-10-04 12:04:20[39m] Step: 1220, Training Logs: loss_final: 0.950413, loss_mean: 0.900566, loss_mean_cls: 0.049847, grad_norm: 0.806125
Steps:   0%| | 1227/1000000 [06:06<84:07:12,  3.30it/s, grad_norm=0.669, loss_final=0.946, loss_mean=0.895, loss_mean_cls=0.[[34m2025-10-04 12:04:22[39m] Step: 1227, Training Logs: loss_final: 0.931463, loss_mean: 0.881237, loss_mean_cls: 0.050227, grad_norm: 0.650539
Steps:   0%| | 1234/1000000 [06:08<81:34:52,  3.40it/s, grad_norm=0.487, loss_final=0.958, loss_mean=0.908, loss_mean_cls=0.[[34m2025-10-04 12:04:24[39m] Step: 1234, Training Logs: loss_final: 0.939569, loss_mean: 0.888938, loss_mean_cls: 0.050631, grad_norm: 0.510845
Steps:   0%| | 1241/1000000 [06:10<80:33:34,  3.44it/s, grad_norm=0.665, loss_final=0.912, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:04:26[39m] Step: 1241, Training Logs: loss_final: 0.951518, loss_mean: 0.901124, loss_mean_cls: 0.050394, grad_norm: 0.681365
Steps:   0%| | 1248/1000000 [06:12<81:43:43,  3.39it/s, grad_norm=0.481, loss_final=0.934, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:04:28[39m] Step: 1248, Training Logs: loss_final: 0.937002, loss_mean: 0.886653, loss_mean_cls: 0.050349, grad_norm: 0.722150
Steps:   0%| | 1255/1000000 [06:14<84:45:44,  3.27it/s, grad_norm=0.569, loss_final=0.941, loss_mean=0.89, loss_mean_cls=0.0[[34m2025-10-04 12:04:30[39m] Step: 1255, Training Logs: loss_final: 0.943070, loss_mean: 0.893645, loss_mean_cls: 0.049426, grad_norm: 0.748168
Steps:   0%| | 1262/1000000 [06:16<83:03:48,  3.34it/s, grad_norm=0.775, loss_final=0.942, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:04:33[39m] Step: 1262, Training Logs: loss_final: 0.954274, loss_mean: 0.904013, loss_mean_cls: 0.050261, grad_norm: 0.755843
Steps:   0%| | 1269/1000000 [06:18<82:39:00,  3.36it/s, grad_norm=0.641, loss_final=0.924, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:04:35[39m] Step: 1269, Training Logs: loss_final: 0.969118, loss_mean: 0.920007, loss_mean_cls: 0.049111, grad_norm: 0.696603
Steps:   0%| | 1276/1000000 [06:20<81:33:38,  3.40it/s, grad_norm=0.62, loss_final=0.936, loss_mean=0.886, loss_mean_cls=0.0[[34m2025-10-04 12:04:37[39m] Step: 1276, Training Logs: loss_final: 0.938788, loss_mean: 0.889219, loss_mean_cls: 0.049569, grad_norm: 0.547533
Steps:   0%| | 1283/1000000 [06:22<81:17:20,  3.41it/s, grad_norm=1.19, loss_final=0.932, loss_mean=0.881, loss_mean_cls=0.0[[34m2025-10-04 12:04:39[39m] Step: 1283, Training Logs: loss_final: 0.954392, loss_mean: 0.905621, loss_mean_cls: 0.048771, grad_norm: 0.737599
Steps:   0%| | 1290/1000000 [06:24<83:33:11,  3.32it/s, grad_norm=0.466, loss_final=0.932, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:04:41[39m] Step: 1290, Training Logs: loss_final: 0.941135, loss_mean: 0.892697, loss_mean_cls: 0.048438, grad_norm: 0.820447
Steps:   0%| | 1297/1000000 [06:26<80:59:57,  3.42it/s, grad_norm=0.473, loss_final=0.932, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:04:43[39m] Step: 1297, Training Logs: loss_final: 0.950276, loss_mean: 0.900751, loss_mean_cls: 0.049525, grad_norm: 0.681269
Steps:   0%| | 1304/1000000 [06:28<82:12:09,  3.37it/s, grad_norm=0.616, loss_final=0.934, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:04:45[39m] Step: 1304, Training Logs: loss_final: 0.932867, loss_mean: 0.882917, loss_mean_cls: 0.049950, grad_norm: 0.685475
Steps:   0%| | 1311/1000000 [06:30<80:01:23,  3.47it/s, grad_norm=0.764, loss_final=0.927, loss_mean=0.876, loss_mean_cls=0.[[34m2025-10-04 12:04:47[39m] Step: 1311, Training Logs: loss_final: 0.935544, loss_mean: 0.885270, loss_mean_cls: 0.050274, grad_norm: 0.886894
Steps:   0%| | 1318/1000000 [06:32<79:51:28,  3.47it/s, grad_norm=0.833, loss_final=0.938, loss_mean=0.888, loss_mean_cls=0.[[34m2025-10-04 12:04:49[39m] Step: 1318, Training Logs: loss_final: 0.955767, loss_mean: 0.905226, loss_mean_cls: 0.050541, grad_norm: 0.674722
Steps:   0%| | 1325/1000000 [06:34<80:24:27,  3.45it/s, grad_norm=0.659, loss_final=0.92, loss_mean=0.871, loss_mean_cls=0.0[[34m2025-10-04 12:04:51[39m] Step: 1325, Training Logs: loss_final: 0.933106, loss_mean: 0.883432, loss_mean_cls: 0.049673, grad_norm: 0.737376
Steps:   0%| | 1332/1000000 [06:36<82:10:27,  3.38it/s, grad_norm=0.516, loss_final=0.937, loss_mean=0.887, loss_mean_cls=0.[[34m2025-10-04 12:04:53[39m] Step: 1332, Training Logs: loss_final: 0.962921, loss_mean: 0.912861, loss_mean_cls: 0.050060, grad_norm: 0.603721
Steps:   0%| | 1339/1000000 [06:38<81:43:11,  3.39it/s, grad_norm=0.6, loss_final=0.95, loss_mean=0.9, loss_mean_cls=0.0492][[34m2025-10-04 12:04:55[39m] Step: 1339, Training Logs: loss_final: 0.922996, loss_mean: 0.872377, loss_mean_cls: 0.050618, grad_norm: 0.646036
Steps:   0%| | 1346/1000000 [06:41<81:07:19,  3.42it/s, grad_norm=0.601, loss_final=0.928, loss_mean=0.878, loss_mean_cls=0.[[34m2025-10-04 12:04:57[39m] Step: 1346, Training Logs: loss_final: 0.932443, loss_mean: 0.881399, loss_mean_cls: 0.051044, grad_norm: 0.604157
Steps:   0%| | 1353/1000000 [06:43<81:05:53,  3.42it/s, grad_norm=0.598, loss_final=0.928, loss_mean=0.878, loss_mean_cls=0.[[34m2025-10-04 12:04:59[39m] Step: 1353, Training Logs: loss_final: 0.944965, loss_mean: 0.896047, loss_mean_cls: 0.048918, grad_norm: 0.659782
Steps:   0%| | 1360/1000000 [06:45<82:19:03,  3.37it/s, grad_norm=0.663, loss_final=0.942, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:05:01[39m] Step: 1360, Training Logs: loss_final: 0.911683, loss_mean: 0.861411, loss_mean_cls: 0.050271, grad_norm: 0.884249
Steps:   0%| | 1367/1000000 [06:47<81:53:16,  3.39it/s, grad_norm=0.643, loss_final=0.932, loss_mean=0.882, loss_mean_cls=0.[[34m2025-10-04 12:05:03[39m] Step: 1367, Training Logs: loss_final: 0.939185, loss_mean: 0.890402, loss_mean_cls: 0.048782, grad_norm: 0.507424
Steps:   0%| | 1374/1000000 [06:49<81:26:42,  3.41it/s, grad_norm=0.67, loss_final=0.945, loss_mean=0.895, loss_mean_cls=0.0[[34m2025-10-04 12:05:05[39m] Step: 1374, Training Logs: loss_final: 0.930553, loss_mean: 0.880536, loss_mean_cls: 0.050017, grad_norm: 0.591071
Steps:   0%| | 1381/1000000 [06:51<81:25:37,  3.41it/s, grad_norm=0.777, loss_final=0.944, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:05:07[39m] Step: 1381, Training Logs: loss_final: 0.936401, loss_mean: 0.887522, loss_mean_cls: 0.048879, grad_norm: 0.766453
Steps:   0%| | 1388/1000000 [06:53<83:09:42,  3.34it/s, grad_norm=0.778, loss_final=0.937, loss_mean=0.887, loss_mean_cls=0.[[34m2025-10-04 12:05:10[39m] Step: 1388, Training Logs: loss_final: 0.930414, loss_mean: 0.880772, loss_mean_cls: 0.049642, grad_norm: 0.575547
Steps:   0%| | 1395/1000000 [06:55<85:37:06,  3.24it/s, grad_norm=0.647, loss_final=0.927, loss_mean=0.876, loss_mean_cls=0.[[34m2025-10-04 12:05:12[39m] Step: 1395, Training Logs: loss_final: 0.938744, loss_mean: 0.890163, loss_mean_cls: 0.048581, grad_norm: 0.795740
Steps:   0%| | 1402/1000000 [06:57<80:34:59,  3.44it/s, grad_norm=0.54, loss_final=0.908, loss_mean=0.858, loss_mean_cls=0.0[[34m2025-10-04 12:05:14[39m] Step: 1402, Training Logs: loss_final: 0.943707, loss_mean: 0.893633, loss_mean_cls: 0.050075, grad_norm: 0.630611
Steps:   0%| | 1409/1000000 [06:59<80:19:05,  3.45it/s, grad_norm=0.746, loss_final=0.944, loss_mean=0.894, loss_mean_cls=0.[[34m2025-10-04 12:05:16[39m] Step: 1409, Training Logs: loss_final: 0.916533, loss_mean: 0.866766, loss_mean_cls: 0.049768, grad_norm: 0.628580
Steps:   0%| | 1416/1000000 [07:01<83:06:53,  3.34it/s, grad_norm=0.663, loss_final=0.949, loss_mean=0.899, loss_mean_cls=0.[[34m2025-10-04 12:05:18[39m] Step: 1416, Training Logs: loss_final: 0.945041, loss_mean: 0.894768, loss_mean_cls: 0.050274, grad_norm: 0.510112
Steps:   0%| | 1423/1000000 [07:03<82:28:38,  3.36it/s, grad_norm=0.569, loss_final=0.942, loss_mean=0.892, loss_mean_cls=0.[[34m2025-10-04 12:05:20[39m] Step: 1423, Training Logs: loss_final: 0.936972, loss_mean: 0.887737, loss_mean_cls: 0.049236, grad_norm: 0.575231
Steps:   0%| | 1430/1000000 [07:05<81:23:11,  3.41it/s, grad_norm=1.01, loss_final=0.948, loss_mean=0.899, loss_mean_cls=0.0[[34m2025-10-04 12:05:20[39m] Step: 1423, Training Logs: loss_final: 0.936972, loss_mean: 0.887737, loss_mean_cls: 0.049236, grad_norm: 0.575231
Steps:   0%| | 1436/1000000 [07:07<81:43:27,  3.39it/s, grad_norm=0.623, loss_final=0.918, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:05:22[39m] Step: 1430, Training Logs: loss_final: 0.941317, loss_mean: 0.892106, loss_mean_cls: 0.049212, grad_norm: 0.485819
Steps:   0%| | 1443/1000000 [07:09<81:22:32,  3.41it/s, grad_norm=0.85, loss_final=0.957, loss_mean=0.907, loss_mean_cls=0.0[[34m2025-10-04 12:05:24[39m] Step: 1437, Training Logs: loss_final: 0.931018, loss_mean: 0.881859, loss_mean_cls: 0.049159, grad_norm: 0.824170
Steps:   0%| | 1450/1000000 [07:11<80:53:09,  3.43it/s, grad_norm=0.868, loss_final=0.932, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:05:26[39m] Step: 1444, Training Logs: loss_final: 0.955271, loss_mean: 0.907534, loss_mean_cls: 0.047737, grad_norm: 0.633021
Steps:   0%| | 1457/1000000 [07:13<80:03:45,  3.46it/s, grad_norm=0.544, loss_final=0.929, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:05:28[39m] Step: 1451, Training Logs: loss_final: 0.932172, loss_mean: 0.881737, loss_mean_cls: 0.050435, grad_norm: 0.807126
Steps:   0%| | 1464/1000000 [07:15<83:38:34,  3.32it/s, grad_norm=0.563, loss_final=0.925, loss_mean=0.876, loss_mean_cls=0.[[34m2025-10-04 12:05:30[39m] Step: 1458, Training Logs: loss_final: 0.925972, loss_mean: 0.876141, loss_mean_cls: 0.049831, grad_norm: 0.711204
Steps:   0%| | 1471/1000000 [07:17<83:57:41,  3.30it/s, grad_norm=0.525, loss_final=0.954, loss_mean=0.904, loss_mean_cls=0.[[34m2025-10-04 12:05:32[39m] Step: 1465, Training Logs: loss_final: 0.948587, loss_mean: 0.899959, loss_mean_cls: 0.048628, grad_norm: 0.723803
Steps:   0%| | 1478/1000000 [07:19<81:31:48,  3.40it/s, grad_norm=0.54, loss_final=0.915, loss_mean=0.865, loss_mean_cls=0.0[[34m2025-10-04 12:05:34[39m] Step: 1472, Training Logs: loss_final: 0.922510, loss_mean: 0.873296, loss_mean_cls: 0.049213, grad_norm: 0.804908
Steps:   0%| | 1485/1000000 [07:21<80:08:10,  3.46it/s, grad_norm=0.927, loss_final=0.922, loss_mean=0.873, loss_mean_cls=0.[[34m2025-10-04 12:05:36[39m] Step: 1479, Training Logs: loss_final: 0.940730, loss_mean: 0.891731, loss_mean_cls: 0.048999, grad_norm: 0.801809
Steps:   0%| | 1486/1000000 [07:22<79:42:17,  3.48it/s, grad_norm=0.927, loss_final=0.922, loss_mean=0.873, loss_mean_cls=0.[[34m2025-10-04 12:05:38[39m] Step: 1486, Training Logs: loss_final: 0.940029, loss_mean: 0.889982, loss_mean_cls: 0.050047, grad_norm: 0.717228
Steps:   0%| | 1493/1000000 [07:24<80:39:03,  3.44it/s, grad_norm=0.484, loss_final=0.946, loss_mean=0.898, loss_mean_cls=0.[[34m2025-10-04 12:05:40[39m] Step: 1493, Training Logs: loss_final: 0.952820, loss_mean: 0.903674, loss_mean_cls: 0.049147, grad_norm: 0.574489
Steps:   0%| | 1500/1000000 [07:26<81:53:17,  3.39it/s, grad_norm=1.04, loss_final=0.968, loss_mean=0.92, loss_mean_cls=0.04[[34m2025-10-04 12:05:43[39m] Step: 1500, Training Logs: loss_final: 0.923985, loss_mean: 0.874298, loss_mean_cls: 0.049687, grad_norm: 0.650879
Steps:   0%| | 1507/1000000 [07:28<82:05:57,  3.38it/s, grad_norm=0.856, loss_final=0.917, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:05:45[39m] Step: 1507, Training Logs: loss_final: 0.939299, loss_mean: 0.889798, loss_mean_cls: 0.049501, grad_norm: 0.452440
Steps:   0%| | 1514/1000000 [07:30<82:17:16,  3.37it/s, grad_norm=0.541, loss_final=0.937, loss_mean=0.888, loss_mean_cls=0.[[34m2025-10-04 12:05:47[39m] Step: 1514, Training Logs: loss_final: 0.941484, loss_mean: 0.891910, loss_mean_cls: 0.049574, grad_norm: 0.539435
Steps:   0%| | 1521/1000000 [07:32<81:57:32,  3.38it/s, grad_norm=0.578, loss_final=0.937, loss_mean=0.888, loss_mean_cls=0.[[34m2025-10-04 12:05:49[39m] Step: 1521, Training Logs: loss_final: 0.953734, loss_mean: 0.903818, loss_mean_cls: 0.049916, grad_norm: 0.555303
Steps:   0%| | 1528/1000000 [07:34<82:55:47,  3.34it/s, grad_norm=0.892, loss_final=0.937, loss_mean=0.887, loss_mean_cls=0.[[34m2025-10-04 12:05:51[39m] Step: 1528, Training Logs: loss_final: 0.925843, loss_mean: 0.875643, loss_mean_cls: 0.050200, grad_norm: 0.833768
Steps:   0%| | 1535/1000000 [07:36<81:31:32,  3.40it/s, grad_norm=0.855, loss_final=0.939, loss_mean=0.889, loss_mean_cls=0.[[34m2025-10-04 12:05:53[39m] Step: 1535, Training Logs: loss_final: 0.930085, loss_mean: 0.880174, loss_mean_cls: 0.049911, grad_norm: 0.819596
Steps:   0%| | 1542/1000000 [07:38<82:25:29,  3.36it/s, grad_norm=0.755, loss_final=0.943, loss_mean=0.895, loss_mean_cls=0.[[34m2025-10-04 12:05:55[39m] Step: 1542, Training Logs: loss_final: 0.946644, loss_mean: 0.897339, loss_mean_cls: 0.049306, grad_norm: 0.672827
Steps:   0%| | 1549/1000000 [07:40<80:08:19,  3.46it/s, grad_norm=0.657, loss_final=0.931, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:05:57[39m] Step: 1549, Training Logs: loss_final: 0.940679, loss_mean: 0.891122, loss_mean_cls: 0.049557, grad_norm: 0.873539
Steps:   0%| | 1556/1000000 [07:42<80:21:17,  3.45it/s, grad_norm=0.611, loss_final=0.933, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:05:59[39m] Step: 1556, Training Logs: loss_final: 0.902060, loss_mean: 0.851893, loss_mean_cls: 0.050166, grad_norm: 0.691355
Steps:   0%| | 1563/1000000 [07:44<81:01:01,  3.42it/s, grad_norm=0.554, loss_final=0.925, loss_mean=0.876, loss_mean_cls=0.[[34m2025-10-04 12:06:01[39m] Step: 1563, Training Logs: loss_final: 0.930394, loss_mean: 0.881878, loss_mean_cls: 0.048517, grad_norm: 0.769462
Steps:   0%| | 1570/1000000 [07:46<81:34:51,  3.40it/s, grad_norm=0.711, loss_final=0.942, loss_mean=0.894, loss_mean_cls=0.[[34m2025-10-04 12:06:03[39m] Step: 1570, Training Logs: loss_final: 0.928354, loss_mean: 0.878811, loss_mean_cls: 0.049542, grad_norm: 0.767763
Steps:   0%| | 1577/1000000 [07:49<84:32:44,  3.28it/s, grad_norm=0.683, loss_final=0.941, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:06:05[39m] Step: 1577, Training Logs: loss_final: 0.940979, loss_mean: 0.892401, loss_mean_cls: 0.048578, grad_norm: 0.596110
Steps:   0%| | 1584/1000000 [07:51<82:22:53,  3.37it/s, grad_norm=0.664, loss_final=0.934, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:06:07[39m] Step: 1584, Training Logs: loss_final: 0.946588, loss_mean: 0.897867, loss_mean_cls: 0.048722, grad_norm: 0.390517
Steps:   0%| | 1591/1000000 [07:53<83:07:28,  3.34it/s, grad_norm=0.464, loss_final=0.914, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:06:09[39m] Step: 1591, Training Logs: loss_final: 0.909087, loss_mean: 0.860725, loss_mean_cls: 0.048361, grad_norm: 0.525915
Steps:   0%| | 1598/1000000 [07:55<82:22:28,  3.37it/s, grad_norm=0.513, loss_final=0.922, loss_mean=0.873, loss_mean_cls=0.[[34m2025-10-04 12:06:11[39m] Step: 1598, Training Logs: loss_final: 0.931629, loss_mean: 0.882999, loss_mean_cls: 0.048630, grad_norm: 0.513705
Steps:   0%| | 1605/1000000 [07:57<80:24:45,  3.45it/s, grad_norm=0.744, loss_final=0.936, loss_mean=0.887, loss_mean_cls=0.[[34m2025-10-04 12:06:13[39m] Step: 1605, Training Logs: loss_final: 0.938572, loss_mean: 0.890635, loss_mean_cls: 0.047937, grad_norm: 0.626302
Steps:   0%| | 1612/1000000 [07:59<81:11:18,  3.42it/s, grad_norm=0.606, loss_final=0.936, loss_mean=0.886, loss_mean_cls=0.[[34m2025-10-04 12:06:16[39m] Step: 1612, Training Logs: loss_final: 0.938708, loss_mean: 0.889657, loss_mean_cls: 0.049051, grad_norm: 0.693483
Steps:   0%| | 1619/1000000 [08:01<84:42:58,  3.27it/s, grad_norm=0.503, loss_final=0.91, loss_mean=0.862, loss_mean_cls=0.0[[34m2025-10-04 12:06:18[39m] Step: 1619, Training Logs: loss_final: 0.935611, loss_mean: 0.886767, loss_mean_cls: 0.048845, grad_norm: 0.502299
Steps:   0%| | 1626/1000000 [08:03<80:52:51,  3.43it/s, grad_norm=0.841, loss_final=0.935, loss_mean=0.887, loss_mean_cls=0.[[34m2025-10-04 12:06:20[39m] Step: 1626, Training Logs: loss_final: 0.926058, loss_mean: 0.877926, loss_mean_cls: 0.048132, grad_norm: 0.487894
Steps:   0%| | 1633/1000000 [08:05<80:07:45,  3.46it/s, grad_norm=0.602, loss_final=0.9, loss_mean=0.85, loss_mean_cls=0.05][[34m2025-10-04 12:06:22[39m] Step: 1633, Training Logs: loss_final: 0.921590, loss_mean: 0.871947, loss_mean_cls: 0.049643, grad_norm: 0.452672
Steps:   0%| | 1640/1000000 [08:07<81:04:23,  3.42it/s, grad_norm=0.579, loss_final=0.923, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:06:24[39m] Step: 1640, Training Logs: loss_final: 0.926232, loss_mean: 0.877487, loss_mean_cls: 0.048745, grad_norm: 0.710371
Steps:   0%| | 1647/1000000 [08:09<80:53:15,  3.43it/s, grad_norm=0.692, loss_final=0.951, loss_mean=0.903, loss_mean_cls=0.[[34m2025-10-04 12:06:26[39m] Step: 1647, Training Logs: loss_final: 0.959074, loss_mean: 0.910759, loss_mean_cls: 0.048315, grad_norm: 0.655858
Steps:   0%| | 1654/1000000 [08:11<82:28:52,  3.36it/s, grad_norm=0.74, loss_final=0.928, loss_mean=0.88, loss_mean_cls=0.04[[34m2025-10-04 12:06:28[39m] Step: 1654, Training Logs: loss_final: 0.929976, loss_mean: 0.880914, loss_mean_cls: 0.049062, grad_norm: 0.417254
Steps:   0%| | 1661/1000000 [08:13<83:58:41,  3.30it/s, grad_norm=0.619, loss_final=0.942, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:06:30[39m] Step: 1661, Training Logs: loss_final: 0.927086, loss_mean: 0.879406, loss_mean_cls: 0.047680, grad_norm: 0.600885
Steps:   0%| | 1668/1000000 [08:15<82:07:48,  3.38it/s, grad_norm=0.471, loss_final=0.917, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:06:32[39m] Step: 1668, Training Logs: loss_final: 0.912766, loss_mean: 0.864027, loss_mean_cls: 0.048739, grad_norm: 0.567845
Steps:   0%| | 1675/1000000 [08:18<87:20:12,  3.18it/s, grad_norm=0.601, loss_final=0.933, loss_mean=0.885, loss_mean_cls=0.[[34m2025-10-04 12:06:34[39m] Step: 1675, Training Logs: loss_final: 0.921495, loss_mean: 0.872968, loss_mean_cls: 0.048527, grad_norm: 0.750491
Steps:   0%| | 1681/1000000 [08:19<82:45:52,  3.35it/s, grad_norm=0.907, loss_final=0.923, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:06:34[39m] Step: 1675, Training Logs: loss_final: 0.921495, loss_mean: 0.872968, loss_mean_cls: 0.048527, grad_norm: 0.750491
Steps:   0%| | 1688/1000000 [08:21<81:50:32,  3.39it/s, grad_norm=0.69, loss_final=0.949, loss_mean=0.9, loss_mean_cls=0.048[[34m2025-10-04 12:06:36[39m] Step: 1682, Training Logs: loss_final: 0.912600, loss_mean: 0.862887, loss_mean_cls: 0.049713, grad_norm: 0.616278
Steps:   0%| | 1695/1000000 [08:23<81:40:03,  3.40it/s, grad_norm=0.906, loss_final=0.943, loss_mean=0.894, loss_mean_cls=0.[[34m2025-10-04 12:06:38[39m] Step: 1689, Training Logs: loss_final: 0.938304, loss_mean: 0.890327, loss_mean_cls: 0.047978, grad_norm: 0.658175
Steps:   0%| | 1702/1000000 [08:26<82:46:34,  3.35it/s, grad_norm=0.746, loss_final=0.927, loss_mean=0.878, loss_mean_cls=0.[[34m2025-10-04 12:06:40[39m] Step: 1696, Training Logs: loss_final: 0.925871, loss_mean: 0.877894, loss_mean_cls: 0.047977, grad_norm: 0.509867
Steps:   0%| | 1709/1000000 [08:28<81:46:53,  3.39it/s, grad_norm=0.513, loss_final=0.932, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:06:43[39m] Step: 1703, Training Logs: loss_final: 0.907098, loss_mean: 0.858751, loss_mean_cls: 0.048347, grad_norm: 0.617001
Steps:   0%| | 1710/1000000 [08:28<81:01:57,  3.42it/s, grad_norm=0.513, loss_final=0.932, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:06:45[39m] Step: 1710, Training Logs: loss_final: 0.933579, loss_mean: 0.884524, loss_mean_cls: 0.049055, grad_norm: 0.683517
Steps:   0%| | 1717/1000000 [08:30<82:22:20,  3.37it/s, grad_norm=0.46, loss_final=0.94, loss_mean=0.892, loss_mean_cls=0.04[[34m2025-10-04 12:06:47[39m] Step: 1717, Training Logs: loss_final: 0.941818, loss_mean: 0.892701, loss_mean_cls: 0.049117, grad_norm: 0.583116
Steps:   0%| | 1724/1000000 [08:32<80:39:03,  3.44it/s, grad_norm=0.558, loss_final=0.931, loss_mean=0.882, loss_mean_cls=0.[[34m2025-10-04 12:06:49[39m] Step: 1724, Training Logs: loss_final: 0.901466, loss_mean: 0.852192, loss_mean_cls: 0.049274, grad_norm: 0.611038
Steps:   0%| | 1731/1000000 [08:34<80:47:17,  3.43it/s, grad_norm=0.71, loss_final=0.913, loss_mean=0.864, loss_mean_cls=0.0[[34m2025-10-04 12:06:51[39m] Step: 1731, Training Logs: loss_final: 0.920988, loss_mean: 0.872800, loss_mean_cls: 0.048188, grad_norm: 0.625760
Steps:   0%| | 1738/1000000 [08:36<81:39:37,  3.40it/s, grad_norm=0.507, loss_final=0.913, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:06:53[39m] Step: 1738, Training Logs: loss_final: 0.917053, loss_mean: 0.867045, loss_mean_cls: 0.050008, grad_norm: 0.674838
Steps:   0%| | 1745/1000000 [08:38<80:32:28,  3.44it/s, grad_norm=0.577, loss_final=0.914, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:06:55[39m] Step: 1745, Training Logs: loss_final: 0.967714, loss_mean: 0.919966, loss_mean_cls: 0.047748, grad_norm: 0.720260
Steps:   0%| | 1752/1000000 [08:40<82:08:35,  3.38it/s, grad_norm=0.52, loss_final=0.913, loss_mean=0.863, loss_mean_cls=0.0[[34m2025-10-04 12:06:57[39m] Step: 1752, Training Logs: loss_final: 0.916490, loss_mean: 0.867412, loss_mean_cls: 0.049078, grad_norm: 0.802745
Steps:   0%| | 1759/1000000 [08:42<81:44:30,  3.39it/s, grad_norm=0.538, loss_final=0.919, loss_mean=0.871, loss_mean_cls=0.[[34m2025-10-04 12:06:59[39m] Step: 1759, Training Logs: loss_final: 0.923584, loss_mean: 0.873891, loss_mean_cls: 0.049693, grad_norm: 0.684602
Steps:   0%| | 1766/1000000 [08:44<80:56:29,  3.43it/s, grad_norm=0.742, loss_final=0.925, loss_mean=0.875, loss_mean_cls=0.[[34m2025-10-04 12:07:01[39m] Step: 1766, Training Logs: loss_final: 0.924839, loss_mean: 0.876186, loss_mean_cls: 0.048653, grad_norm: 0.742093
Steps:   0%| | 1773/1000000 [08:46<83:33:57,  3.32it/s, grad_norm=0.68, loss_final=0.945, loss_mean=0.896, loss_mean_cls=0.0[[34m2025-10-04 12:07:03[39m] Step: 1773, Training Logs: loss_final: 0.931325, loss_mean: 0.883163, loss_mean_cls: 0.048163, grad_norm: 0.619377
Steps:   0%| | 1780/1000000 [08:49<83:21:51,  3.33it/s, grad_norm=0.753, loss_final=0.916, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:07:05[39m] Step: 1780, Training Logs: loss_final: 0.940517, loss_mean: 0.891950, loss_mean_cls: 0.048568, grad_norm: 0.656629
Steps:   0%| | 1787/1000000 [08:51<82:12:53,  3.37it/s, grad_norm=0.835, loss_final=0.925, loss_mean=0.877, loss_mean_cls=0.[[34m2025-10-04 12:07:07[39m] Step: 1787, Training Logs: loss_final: 0.906445, loss_mean: 0.856577, loss_mean_cls: 0.049868, grad_norm: 0.870349
Steps:   0%| | 1794/1000000 [08:53<81:06:38,  3.42it/s, grad_norm=0.64, loss_final=0.91, loss_mean=0.859, loss_mean_cls=0.05[[34m2025-10-04 12:07:09[39m] Step: 1794, Training Logs: loss_final: 0.914624, loss_mean: 0.865493, loss_mean_cls: 0.049131, grad_norm: 0.509170
Steps:   0%| | 1801/1000000 [08:55<81:41:38,  3.39it/s, grad_norm=0.528, loss_final=0.928, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:07:11[39m] Step: 1801, Training Logs: loss_final: 0.903664, loss_mean: 0.855153, loss_mean_cls: 0.048511, grad_norm: 0.467789
Steps:   0%| | 1808/1000000 [08:57<85:44:37,  3.23it/s, grad_norm=0.673, loss_final=0.917, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:07:14[39m] Step: 1808, Training Logs: loss_final: 0.920869, loss_mean: 0.872050, loss_mean_cls: 0.048819, grad_norm: 0.648561
Steps:   0%| | 1815/1000000 [08:59<82:54:02,  3.34it/s, grad_norm=0.716, loss_final=0.908, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:07:16[39m] Step: 1815, Training Logs: loss_final: 0.925567, loss_mean: 0.878212, loss_mean_cls: 0.047354, grad_norm: 0.826720
Steps:   0%| | 1822/1000000 [09:01<81:18:47,  3.41it/s, grad_norm=0.631, loss_final=0.915, loss_mean=0.866, loss_mean_cls=0.[[34m2025-10-04 12:07:18[39m] Step: 1822, Training Logs: loss_final: 0.926869, loss_mean: 0.877479, loss_mean_cls: 0.049390, grad_norm: 0.569030
Steps:   0%| | 1829/1000000 [09:03<81:32:50,  3.40it/s, grad_norm=0.648, loss_final=0.916, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:07:20[39m] Step: 1829, Training Logs: loss_final: 0.934200, loss_mean: 0.885906, loss_mean_cls: 0.048294, grad_norm: 0.575318
Steps:   0%| | 1836/1000000 [09:05<81:29:36,  3.40it/s, grad_norm=0.575, loss_final=0.914, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:07:22[39m] Step: 1836, Training Logs: loss_final: 0.915317, loss_mean: 0.867646, loss_mean_cls: 0.047670, grad_norm: 0.448906
Steps:   0%| | 1843/1000000 [09:07<80:30:18,  3.44it/s, grad_norm=0.768, loss_final=0.908, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:07:24[39m] Step: 1843, Training Logs: loss_final: 0.942818, loss_mean: 0.894675, loss_mean_cls: 0.048143, grad_norm: 0.539701
Steps:   0%| | 1850/1000000 [09:09<79:34:28,  3.48it/s, grad_norm=0.655, loss_final=0.934, loss_mean=0.886, loss_mean_cls=0.[[34m2025-10-04 12:07:26[39m] Step: 1850, Training Logs: loss_final: 0.928110, loss_mean: 0.880283, loss_mean_cls: 0.047826, grad_norm: 0.742539
Steps:   0%| | 1857/1000000 [09:11<81:09:11,  3.42it/s, grad_norm=0.644, loss_final=0.937, loss_mean=0.889, loss_mean_cls=0.[[34m2025-10-04 12:07:28[39m] Step: 1857, Training Logs: loss_final: 0.939635, loss_mean: 0.891049, loss_mean_cls: 0.048585, grad_norm: 0.558581
Steps:   0%| | 1864/1000000 [09:13<81:39:35,  3.40it/s, grad_norm=0.61, loss_final=0.919, loss_mean=0.87, loss_mean_cls=0.04[[34m2025-10-04 12:07:30[39m] Step: 1864, Training Logs: loss_final: 0.936826, loss_mean: 0.888500, loss_mean_cls: 0.048326, grad_norm: 0.706288
Steps:   0%| | 1871/1000000 [09:15<80:37:51,  3.44it/s, grad_norm=0.495, loss_final=0.921, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:07:32[39m] Step: 1871, Training Logs: loss_final: 0.914428, loss_mean: 0.865476, loss_mean_cls: 0.048952, grad_norm: 0.631423
Steps:   0%| | 1878/1000000 [09:17<82:25:35,  3.36it/s, grad_norm=0.536, loss_final=0.929, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:07:34[39m] Step: 1878, Training Logs: loss_final: 0.912486, loss_mean: 0.864854, loss_mean_cls: 0.047631, grad_norm: 0.428328
Steps:   0%| | 1885/1000000 [09:19<80:25:02,  3.45it/s, grad_norm=0.545, loss_final=0.915, loss_mean=0.866, loss_mean_cls=0.[[34m2025-10-04 12:07:36[39m] Step: 1885, Training Logs: loss_final: 0.940014, loss_mean: 0.891398, loss_mean_cls: 0.048616, grad_norm: 0.634957
Steps:   0%| | 1892/1000000 [09:21<81:47:59,  3.39it/s, grad_norm=0.699, loss_final=0.917, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:07:38[39m] Step: 1892, Training Logs: loss_final: 0.926095, loss_mean: 0.878559, loss_mean_cls: 0.047536, grad_norm: 0.788595
Steps:   0%| | 1899/1000000 [09:24<82:50:08,  3.35it/s, grad_norm=0.477, loss_final=0.936, loss_mean=0.888, loss_mean_cls=0.[[34m2025-10-04 12:07:40[39m] Step: 1899, Training Logs: loss_final: 0.925084, loss_mean: 0.877757, loss_mean_cls: 0.047327, grad_norm: 0.711573
Steps:   0%| | 1906/1000000 [09:26<81:38:15,  3.40it/s, grad_norm=0.488, loss_final=0.917, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:07:42[39m] Step: 1906, Training Logs: loss_final: 0.930167, loss_mean: 0.882354, loss_mean_cls: 0.047813, grad_norm: 0.578291
Steps:   0%| | 1913/1000000 [09:28<83:15:20,  3.33it/s, grad_norm=0.76, loss_final=0.9, loss_mean=0.85, loss_mean_cls=0.0502[[34m2025-10-04 12:07:44[39m] Step: 1913, Training Logs: loss_final: 0.901914, loss_mean: 0.852147, loss_mean_cls: 0.049766, grad_norm: 0.461910
Steps:   0%| | 1919/1000000 [09:30<83:05:50,  3.34it/s, grad_norm=0.761, loss_final=0.941, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:07:44[39m] Step: 1913, Training Logs: loss_final: 0.901914, loss_mean: 0.852147, loss_mean_cls: 0.049766, grad_norm: 0.461910
Steps:   0%| | 1926/1000000 [09:32<81:41:15,  3.39it/s, grad_norm=0.702, loss_final=0.929, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:07:46[39m] Step: 1920, Training Logs: loss_final: 0.918239, loss_mean: 0.870176, loss_mean_cls: 0.048063, grad_norm: 0.634731
Steps:   0%| | 1933/1000000 [09:34<79:51:09,  3.47it/s, grad_norm=0.463, loss_final=0.936, loss_mean=0.888, loss_mean_cls=0.[[34m2025-10-04 12:07:49[39m] Step: 1927, Training Logs: loss_final: 0.948689, loss_mean: 0.901496, loss_mean_cls: 0.047193, grad_norm: 0.475425
Steps:   0%| | 1940/1000000 [09:36<80:22:38,  3.45it/s, grad_norm=0.527, loss_final=0.932, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:07:51[39m] Step: 1934, Training Logs: loss_final: 0.925243, loss_mean: 0.876900, loss_mean_cls: 0.048343, grad_norm: 0.522299
Steps:   0%| | 1947/1000000 [09:38<80:26:30,  3.45it/s, grad_norm=0.551, loss_final=0.911, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:07:53[39m] Step: 1941, Training Logs: loss_final: 0.922711, loss_mean: 0.875968, loss_mean_cls: 0.046743, grad_norm: 0.383985
Steps:   0%| | 1954/1000000 [09:40<81:56:32,  3.38it/s, grad_norm=0.775, loss_final=0.921, loss_mean=0.872, loss_mean_cls=0.[[34m2025-10-04 12:07:55[39m] Step: 1948, Training Logs: loss_final: 0.911895, loss_mean: 0.863390, loss_mean_cls: 0.048504, grad_norm: 0.518361
Steps:   0%| | 1961/1000000 [09:42<81:59:12,  3.38it/s, grad_norm=0.816, loss_final=0.911, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:07:57[39m] Step: 1955, Training Logs: loss_final: 0.937173, loss_mean: 0.889164, loss_mean_cls: 0.048009, grad_norm: 0.538185
Steps:   0%| | 1962/1000000 [09:42<82:12:33,  3.37it/s, grad_norm=0.816, loss_final=0.911, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:07:59[39m] Step: 1962, Training Logs: loss_final: 0.954606, loss_mean: 0.906654, loss_mean_cls: 0.047952, grad_norm: 0.456784
Steps:   0%| | 1969/1000000 [09:44<82:04:25,  3.38it/s, grad_norm=0.786, loss_final=0.951, loss_mean=0.903, loss_mean_cls=0.[[34m2025-10-04 12:08:01[39m] Step: 1969, Training Logs: loss_final: 0.902217, loss_mean: 0.852614, loss_mean_cls: 0.049603, grad_norm: 0.499659
Steps:   0%| | 1976/1000000 [09:46<81:15:30,  3.41it/s, grad_norm=0.51, loss_final=0.916, loss_mean=0.867, loss_mean_cls=0.0[[34m2025-10-04 12:08:03[39m] Step: 1976, Training Logs: loss_final: 0.925884, loss_mean: 0.877327, loss_mean_cls: 0.048556, grad_norm: 0.623448
Steps:   0%| | 1983/1000000 [09:48<81:12:07,  3.41it/s, grad_norm=0.54, loss_final=0.924, loss_mean=0.876, loss_mean_cls=0.0[[34m2025-10-04 12:08:05[39m] Step: 1983, Training Logs: loss_final: 0.906237, loss_mean: 0.857170, loss_mean_cls: 0.049067, grad_norm: 0.529959
Steps:   0%| | 1990/1000000 [09:50<79:39:52,  3.48it/s, grad_norm=0.464, loss_final=0.935, loss_mean=0.887, loss_mean_cls=0.[[34m2025-10-04 12:08:07[39m] Step: 1990, Training Logs: loss_final: 0.914600, loss_mean: 0.867722, loss_mean_cls: 0.046878, grad_norm: 0.587508
Steps:   0%| | 1997/1000000 [09:52<80:32:17,  3.44it/s, grad_norm=0.485, loss_final=0.927, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:08:09[39m] Step: 1997, Training Logs: loss_final: 0.922534, loss_mean: 0.875768, loss_mean_cls: 0.046766, grad_norm: 0.434701
Steps:   0%| | 2004/1000000 [09:54<81:34:50,  3.40it/s, grad_norm=0.861, loss_final=0.917, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:08:11[39m] Step: 2004, Training Logs: loss_final: 0.900248, loss_mean: 0.851403, loss_mean_cls: 0.048845, grad_norm: 0.518439
Steps:   0%| | 2011/1000000 [09:56<80:30:05,  3.44it/s, grad_norm=0.548, loss_final=0.927, loss_mean=0.879, loss_mean_cls=0.[[34m2025-10-04 12:08:13[39m] Step: 2011, Training Logs: loss_final: 0.911355, loss_mean: 0.862981, loss_mean_cls: 0.048374, grad_norm: 0.970159
Steps:   0%| | 2018/1000000 [09:58<80:49:52,  3.43it/s, grad_norm=0.917, loss_final=0.922, loss_mean=0.876, loss_mean_cls=0.[[34m2025-10-04 12:08:15[39m] Step: 2018, Training Logs: loss_final: 0.926681, loss_mean: 0.878424, loss_mean_cls: 0.048257, grad_norm: 0.515672
Steps:   0%| | 2025/1000000 [10:01<80:24:50,  3.45it/s, grad_norm=0.373, loss_final=0.906, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:08:17[39m] Step: 2025, Training Logs: loss_final: 0.925594, loss_mean: 0.877617, loss_mean_cls: 0.047977, grad_norm: 0.723344
Steps:   0%| | 2032/1000000 [10:03<81:37:40,  3.40it/s, grad_norm=0.616, loss_final=0.931, loss_mean=0.882, loss_mean_cls=0.[[34m2025-10-04 12:08:19[39m] Step: 2032, Training Logs: loss_final: 0.920071, loss_mean: 0.870905, loss_mean_cls: 0.049166, grad_norm: 0.557851
Steps:   0%| | 2039/1000000 [10:05<82:35:22,  3.36it/s, grad_norm=0.611, loss_final=0.94, loss_mean=0.892, loss_mean_cls=0.0[[34m2025-10-04 12:08:21[39m] Step: 2039, Training Logs: loss_final: 0.930997, loss_mean: 0.883773, loss_mean_cls: 0.047224, grad_norm: 0.538569
Steps:   0%| | 2046/1000000 [10:07<83:30:34,  3.32it/s, grad_norm=0.484, loss_final=0.913, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:08:23[39m] Step: 2046, Training Logs: loss_final: 0.928655, loss_mean: 0.879643, loss_mean_cls: 0.049012, grad_norm: 0.558907
Steps:   0%| | 2053/1000000 [10:09<81:31:18,  3.40it/s, grad_norm=0.69, loss_final=0.904, loss_mean=0.856, loss_mean_cls=0.0[[34m2025-10-04 12:08:26[39m] Step: 2053, Training Logs: loss_final: 0.932684, loss_mean: 0.884796, loss_mean_cls: 0.047888, grad_norm: 0.871220
Steps:   0%| | 2060/1000000 [10:11<81:54:20,  3.38it/s, grad_norm=0.726, loss_final=0.919, loss_mean=0.871, loss_mean_cls=0.[[34m2025-10-04 12:08:28[39m] Step: 2060, Training Logs: loss_final: 0.915410, loss_mean: 0.866704, loss_mean_cls: 0.048706, grad_norm: 0.581479
Steps:   0%| | 2067/1000000 [10:13<81:28:38,  3.40it/s, grad_norm=0.596, loss_final=0.916, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:08:30[39m] Step: 2067, Training Logs: loss_final: 0.901847, loss_mean: 0.852327, loss_mean_cls: 0.049519, grad_norm: 0.590867
Steps:   0%| | 2074/1000000 [10:15<82:09:25,  3.37it/s, grad_norm=0.512, loss_final=0.915, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:08:32[39m] Step: 2074, Training Logs: loss_final: 0.932167, loss_mean: 0.884383, loss_mean_cls: 0.047784, grad_norm: 0.384129
Steps:   0%| | 2081/1000000 [10:17<81:19:17,  3.41it/s, grad_norm=0.752, loss_final=0.951, loss_mean=0.904, loss_mean_cls=0.[[34m2025-10-04 12:08:34[39m] Step: 2081, Training Logs: loss_final: 0.896865, loss_mean: 0.849419, loss_mean_cls: 0.047446, grad_norm: 0.536668
Steps:   0%| | 2088/1000000 [10:19<81:09:43,  3.42it/s, grad_norm=0.552, loss_final=0.903, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:08:36[39m] Step: 2088, Training Logs: loss_final: 0.895202, loss_mean: 0.847222, loss_mean_cls: 0.047979, grad_norm: 0.668978
Steps:   0%| | 2095/1000000 [10:21<84:41:26,  3.27it/s, grad_norm=0.572, loss_final=0.943, loss_mean=0.896, loss_mean_cls=0.[[34m2025-10-04 12:08:38[39m] Step: 2095, Training Logs: loss_final: 0.930794, loss_mean: 0.882998, loss_mean_cls: 0.047797, grad_norm: 0.419804
Steps:   0%| | 2102/1000000 [10:23<84:17:57,  3.29it/s, grad_norm=0.484, loss_final=0.932, loss_mean=0.886, loss_mean_cls=0.[[34m2025-10-04 12:08:40[39m] Step: 2102, Training Logs: loss_final: 0.939945, loss_mean: 0.892390, loss_mean_cls: 0.047555, grad_norm: 0.449443
Steps:   0%| | 2109/1000000 [10:25<80:12:36,  3.46it/s, grad_norm=0.493, loss_final=0.913, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:08:42[39m] Step: 2109, Training Logs: loss_final: 0.939175, loss_mean: 0.892470, loss_mean_cls: 0.046705, grad_norm: 0.386666
Steps:   0%| | 2116/1000000 [10:28<81:03:20,  3.42it/s, grad_norm=0.49, loss_final=0.9, loss_mean=0.851, loss_mean_cls=0.048[[34m2025-10-04 12:08:44[39m] Step: 2116, Training Logs: loss_final: 0.940450, loss_mean: 0.892813, loss_mean_cls: 0.047637, grad_norm: 0.383223
Steps:   0%| | 2123/1000000 [10:30<81:12:28,  3.41it/s, grad_norm=0.971, loss_final=0.931, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:08:46[39m] Step: 2123, Training Logs: loss_final: 0.914727, loss_mean: 0.866759, loss_mean_cls: 0.047968, grad_norm: 0.843016
Steps:   0%| | 2130/1000000 [10:32<82:08:40,  3.37it/s, grad_norm=0.644, loss_final=0.905, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:08:48[39m] Step: 2130, Training Logs: loss_final: 0.932156, loss_mean: 0.883693, loss_mean_cls: 0.048462, grad_norm: 0.627530
Steps:   0%| | 2137/1000000 [10:34<80:30:00,  3.44it/s, grad_norm=0.491, loss_final=0.931, loss_mean=0.882, loss_mean_cls=0.[[34m2025-10-04 12:08:50[39m] Step: 2137, Training Logs: loss_final: 0.920709, loss_mean: 0.873721, loss_mean_cls: 0.046988, grad_norm: 0.556416
Steps:   0%| | 2144/1000000 [10:36<81:21:14,  3.41it/s, grad_norm=0.589, loss_final=0.929, loss_mean=0.881, loss_mean_cls=0.[[34m2025-10-04 12:08:52[39m] Step: 2144, Training Logs: loss_final: 0.930866, loss_mean: 0.883090, loss_mean_cls: 0.047777, grad_norm: 0.666529
Steps:   0%| | 2151/1000000 [10:38<82:17:02,  3.37it/s, grad_norm=0.718, loss_final=0.941, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:08:54[39m] Step: 2151, Training Logs: loss_final: 0.920994, loss_mean: 0.873136, loss_mean_cls: 0.047858, grad_norm: 0.619629
Steps:   0%| | 2158/1000000 [10:40<81:49:37,  3.39it/s, grad_norm=0.431, loss_final=0.907, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:08:57[39m] Step: 2158, Training Logs: loss_final: 0.928470, loss_mean: 0.882020, loss_mean_cls: 0.046451, grad_norm: 0.892120
Steps:   0%| | 2165/1000000 [10:42<81:36:12,  3.40it/s, grad_norm=0.862, loss_final=0.931, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:08:59[39m] Step: 2165, Training Logs: loss_final: 0.924347, loss_mean: 0.877421, loss_mean_cls: 0.046925, grad_norm: 0.464690
Steps:   0%| | 2172/1000000 [10:44<80:26:20,  3.45it/s, grad_norm=0.442, loss_final=0.929, loss_mean=0.881, loss_mean_cls=0.[[34m2025-10-04 12:09:01[39m] Step: 2172, Training Logs: loss_final: 0.919839, loss_mean: 0.872372, loss_mean_cls: 0.047467, grad_norm: 0.543263
Steps:   0%| | 2179/1000000 [10:46<79:37:43,  3.48it/s, grad_norm=0.469, loss_final=0.905, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:09:03[39m] Step: 2179, Training Logs: loss_final: 0.907007, loss_mean: 0.858903, loss_mean_cls: 0.048104, grad_norm: 0.549835
Steps:   0%| | 2186/1000000 [10:48<82:19:32,  3.37it/s, grad_norm=0.517, loss_final=0.916, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:09:05[39m] Step: 2186, Training Logs: loss_final: 0.910051, loss_mean: 0.862396, loss_mean_cls: 0.047655, grad_norm: 0.442548
Steps:   0%| | 2192/1000000 [10:50<81:45:02,  3.39it/s, grad_norm=0.66, loss_final=0.922, loss_mean=0.875, loss_mean_cls=0.0[[34m2025-10-04 12:09:05[39m] Step: 2186, Training Logs: loss_final: 0.910051, loss_mean: 0.862396, loss_mean_cls: 0.047655, grad_norm: 0.442548
Steps:   0%| | 2199/1000000 [10:52<80:04:42,  3.46it/s, grad_norm=0.562, loss_final=0.932, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:09:07[39m] Step: 2193, Training Logs: loss_final: 0.936279, loss_mean: 0.888166, loss_mean_cls: 0.048113, grad_norm: 0.526481
Steps:   0%| | 2206/1000000 [10:54<82:47:57,  3.35it/s, grad_norm=0.595, loss_final=0.905, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:09:09[39m] Step: 2200, Training Logs: loss_final: 0.919048, loss_mean: 0.872266, loss_mean_cls: 0.046782, grad_norm: 0.730284
Steps:   0%| | 2213/1000000 [10:56<81:29:27,  3.40it/s, grad_norm=0.663, loss_final=0.919, loss_mean=0.872, loss_mean_cls=0.[[34m2025-10-04 12:09:11[39m] Step: 2207, Training Logs: loss_final: 0.912366, loss_mean: 0.864651, loss_mean_cls: 0.047715, grad_norm: 0.599858
Steps:   0%| | 2220/1000000 [10:58<81:32:40,  3.40it/s, grad_norm=0.436, loss_final=0.929, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:09:13[39m] Step: 2214, Training Logs: loss_final: 0.900987, loss_mean: 0.852988, loss_mean_cls: 0.047998, grad_norm: 0.635788
Steps:   0%| | 2227/1000000 [11:00<81:03:16,  3.42it/s, grad_norm=0.496, loss_final=0.907, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:09:15[39m] Step: 2221, Training Logs: loss_final: 0.913267, loss_mean: 0.864747, loss_mean_cls: 0.048520, grad_norm: 0.874886
Steps:   0%| | 2234/1000000 [11:02<83:13:35,  3.33it/s, grad_norm=0.606, loss_final=0.93, loss_mean=0.883, loss_mean_cls=0.0[[34m2025-10-04 12:09:17[39m] Step: 2228, Training Logs: loss_final: 0.936231, loss_mean: 0.889469, loss_mean_cls: 0.046762, grad_norm: 0.515601
Steps:   0%| | 2235/1000000 [11:02<83:31:58,  3.32it/s, grad_norm=0.606, loss_final=0.93, loss_mean=0.883, loss_mean_cls=0.0[[34m2025-10-04 12:09:19[39m] Step: 2235, Training Logs: loss_final: 0.905272, loss_mean: 0.856700, loss_mean_cls: 0.048572, grad_norm: 0.662140
Steps:   0%| | 2242/1000000 [11:05<82:46:41,  3.35it/s, grad_norm=1.04, loss_final=0.921, loss_mean=0.873, loss_mean_cls=0.0[[34m2025-10-04 12:09:21[39m] Step: 2242, Training Logs: loss_final: 0.935221, loss_mean: 0.888695, loss_mean_cls: 0.046527, grad_norm: 0.587009
Steps:   0%| | 2249/1000000 [11:07<82:07:39,  3.37it/s, grad_norm=0.471, loss_final=0.927, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:09:23[39m] Step: 2249, Training Logs: loss_final: 0.929962, loss_mean: 0.882749, loss_mean_cls: 0.047213, grad_norm: 0.669764
Steps:   0%| | 2256/1000000 [11:09<82:17:10,  3.37it/s, grad_norm=0.637, loss_final=0.916, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:09:25[39m] Step: 2256, Training Logs: loss_final: 0.919851, loss_mean: 0.873583, loss_mean_cls: 0.046268, grad_norm: 0.501091
Steps:   0%| | 2263/1000000 [11:11<83:01:50,  3.34it/s, grad_norm=0.597, loss_final=0.915, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:09:27[39m] Step: 2263, Training Logs: loss_final: 0.903978, loss_mean: 0.857014, loss_mean_cls: 0.046964, grad_norm: 0.649804
Steps:   0%| | 2270/1000000 [11:13<84:51:41,  3.27it/s, grad_norm=0.562, loss_final=0.918, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:09:30[39m] Step: 2270, Training Logs: loss_final: 0.932797, loss_mean: 0.884842, loss_mean_cls: 0.047955, grad_norm: 0.597644
Steps:   0%| | 2277/1000000 [11:15<80:50:58,  3.43it/s, grad_norm=0.764, loss_final=0.917, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:09:32[39m] Step: 2277, Training Logs: loss_final: 0.925555, loss_mean: 0.878449, loss_mean_cls: 0.047106, grad_norm: 0.752626
Steps:   0%| | 2284/1000000 [11:17<81:08:51,  3.42it/s, grad_norm=0.737, loss_final=0.922, loss_mean=0.875, loss_mean_cls=0.[[34m2025-10-04 12:09:34[39m] Step: 2284, Training Logs: loss_final: 0.931376, loss_mean: 0.883867, loss_mean_cls: 0.047510, grad_norm: 0.518524
Steps:   0%| | 2291/1000000 [11:19<81:52:23,  3.39it/s, grad_norm=0.521, loss_final=0.911, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:09:36[39m] Step: 2291, Training Logs: loss_final: 0.916007, loss_mean: 0.868287, loss_mean_cls: 0.047720, grad_norm: 0.473616
Steps:   0%| | 2298/1000000 [11:21<79:42:14,  3.48it/s, grad_norm=0.462, loss_final=0.909, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:09:38[39m] Step: 2298, Training Logs: loss_final: 0.909222, loss_mean: 0.861534, loss_mean_cls: 0.047688, grad_norm: 0.680112
Steps:   0%| | 2305/1000000 [11:23<80:31:22,  3.44it/s, grad_norm=0.527, loss_final=0.919, loss_mean=0.872, loss_mean_cls=0.[[34m2025-10-04 12:09:40[39m] Step: 2305, Training Logs: loss_final: 0.912186, loss_mean: 0.864975, loss_mean_cls: 0.047211, grad_norm: 0.610371
Steps:   0%| | 2312/1000000 [11:25<80:33:08,  3.44it/s, grad_norm=0.566, loss_final=0.94, loss_mean=0.894, loss_mean_cls=0.0[[34m2025-10-04 12:09:42[39m] Step: 2312, Training Logs: loss_final: 0.914997, loss_mean: 0.867637, loss_mean_cls: 0.047360, grad_norm: 0.658710
Steps:   0%| | 2319/1000000 [11:27<80:59:26,  3.42it/s, grad_norm=0.493, loss_final=0.903, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:09:44[39m] Step: 2319, Training Logs: loss_final: 0.896004, loss_mean: 0.848849, loss_mean_cls: 0.047155, grad_norm: 0.459355
Steps:   0%| | 2326/1000000 [11:29<79:53:23,  3.47it/s, grad_norm=0.573, loss_final=0.921, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:09:46[39m] Step: 2326, Training Logs: loss_final: 0.903352, loss_mean: 0.855650, loss_mean_cls: 0.047702, grad_norm: 0.431115
Steps:   0%| | 2333/1000000 [11:31<79:45:21,  3.47it/s, grad_norm=0.549, loss_final=0.931, loss_mean=0.884, loss_mean_cls=0.[[34m2025-10-04 12:09:48[39m] Step: 2333, Training Logs: loss_final: 0.902588, loss_mean: 0.854835, loss_mean_cls: 0.047753, grad_norm: 0.693367
Steps:   0%| | 2340/1000000 [11:33<80:45:26,  3.43it/s, grad_norm=0.432, loss_final=0.908, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:09:50[39m] Step: 2340, Training Logs: loss_final: 0.897291, loss_mean: 0.849042, loss_mean_cls: 0.048249, grad_norm: 0.807821
Steps:   0%| | 2347/1000000 [11:35<82:40:43,  3.35it/s, grad_norm=0.603, loss_final=0.929, loss_mean=0.881, loss_mean_cls=0.[[34m2025-10-04 12:09:52[39m] Step: 2347, Training Logs: loss_final: 0.906325, loss_mean: 0.858497, loss_mean_cls: 0.047828, grad_norm: 0.581205
Steps:   0%| | 2354/1000000 [11:37<82:52:02,  3.34it/s, grad_norm=0.354, loss_final=0.91, loss_mean=0.863, loss_mean_cls=0.0[[34m2025-10-04 12:09:54[39m] Step: 2354, Training Logs: loss_final: 0.925424, loss_mean: 0.878536, loss_mean_cls: 0.046888, grad_norm: 0.510733
Steps:   0%| | 2361/1000000 [11:40<82:45:54,  3.35it/s, grad_norm=0.387, loss_final=0.883, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:09:56[39m] Step: 2361, Training Logs: loss_final: 0.898125, loss_mean: 0.849879, loss_mean_cls: 0.048245, grad_norm: 0.499225
Steps:   0%| | 2368/1000000 [11:42<81:57:20,  3.38it/s, grad_norm=0.658, loss_final=0.913, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:09:58[39m] Step: 2368, Training Logs: loss_final: 0.914773, loss_mean: 0.868115, loss_mean_cls: 0.046658, grad_norm: 0.524421
Steps:   0%| | 2375/1000000 [11:44<79:51:56,  3.47it/s, grad_norm=0.773, loss_final=0.886, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:10:00[39m] Step: 2375, Training Logs: loss_final: 0.894126, loss_mean: 0.845115, loss_mean_cls: 0.049011, grad_norm: 0.784118
Steps:   0%| | 2382/1000000 [11:46<80:25:57,  3.45it/s, grad_norm=0.467, loss_final=0.901, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:10:02[39m] Step: 2382, Training Logs: loss_final: 0.898204, loss_mean: 0.850051, loss_mean_cls: 0.048153, grad_norm: 0.738020
Steps:   0%| | 2389/1000000 [11:48<81:58:27,  3.38it/s, grad_norm=0.656, loss_final=0.911, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:10:04[39m] Step: 2389, Training Logs: loss_final: 0.914569, loss_mean: 0.866151, loss_mean_cls: 0.048419, grad_norm: 0.633480
Steps:   0%| | 2396/1000000 [11:50<82:31:29,  3.36it/s, grad_norm=0.454, loss_final=0.93, loss_mean=0.883, loss_mean_cls=0.0[[34m2025-10-04 12:10:07[39m] Step: 2396, Training Logs: loss_final: 0.895569, loss_mean: 0.847315, loss_mean_cls: 0.048253, grad_norm: 0.372630
Steps:   0%| | 2403/1000000 [11:52<83:07:44,  3.33it/s, grad_norm=0.732, loss_final=0.911, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:10:09[39m] Step: 2403, Training Logs: loss_final: 0.902695, loss_mean: 0.854352, loss_mean_cls: 0.048343, grad_norm: 0.793794
Steps:   0%| | 2410/1000000 [11:54<80:10:23,  3.46it/s, grad_norm=0.565, loss_final=0.918, loss_mean=0.87, loss_mean_cls=0.0[[34m2025-10-04 12:10:11[39m] Step: 2410, Training Logs: loss_final: 0.908467, loss_mean: 0.860546, loss_mean_cls: 0.047921, grad_norm: 0.499554
Steps:   0%| | 2417/1000000 [11:56<82:46:59,  3.35it/s, grad_norm=0.52, loss_final=0.918, loss_mean=0.87, loss_mean_cls=0.04[[34m2025-10-04 12:10:13[39m] Step: 2417, Training Logs: loss_final: 0.926308, loss_mean: 0.879028, loss_mean_cls: 0.047279, grad_norm: 0.599058
Steps:   0%| | 2424/1000000 [11:58<80:01:41,  3.46it/s, grad_norm=0.531, loss_final=0.921, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:10:15[39m] Step: 2424, Training Logs: loss_final: 0.918481, loss_mean: 0.871001, loss_mean_cls: 0.047480, grad_norm: 0.467982
Steps:   0%| | 2431/1000000 [12:00<84:18:55,  3.29it/s, grad_norm=0.625, loss_final=0.923, loss_mean=0.877, loss_mean_cls=0.[[34m2025-10-04 12:10:17[39m] Step: 2431, Training Logs: loss_final: 0.943578, loss_mean: 0.896073, loss_mean_cls: 0.047505, grad_norm: 0.577380
Steps:   0%| | 2438/1000000 [12:02<80:22:22,  3.45it/s, grad_norm=0.383, loss_final=0.921, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:10:19[39m] Step: 2438, Training Logs: loss_final: 0.910412, loss_mean: 0.863003, loss_mean_cls: 0.047408, grad_norm: 0.557349
Steps:   0%| | 2445/1000000 [12:04<82:55:20,  3.34it/s, grad_norm=0.476, loss_final=0.937, loss_mean=0.89, loss_mean_cls=0.0[[34m2025-10-04 12:10:21[39m] Step: 2445, Training Logs: loss_final: 0.912043, loss_mean: 0.865018, loss_mean_cls: 0.047025, grad_norm: 0.813201
Steps:   0%| | 2452/1000000 [12:06<80:38:31,  3.44it/s, grad_norm=0.559, loss_final=0.913, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:10:23[39m] Step: 2452, Training Logs: loss_final: 0.926538, loss_mean: 0.878969, loss_mean_cls: 0.047569, grad_norm: 0.524422
Steps:   0%| | 2459/1000000 [12:08<80:48:51,  3.43it/s, grad_norm=0.494, loss_final=0.906, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:10:25[39m] Step: 2459, Training Logs: loss_final: 0.929616, loss_mean: 0.883271, loss_mean_cls: 0.046345, grad_norm: 0.517317
Steps:   0%| | 2465/1000000 [12:10<81:43:13,  3.39it/s, grad_norm=0.519, loss_final=0.917, loss_mean=0.87, loss_mean_cls=0.0[[34m2025-10-04 12:10:25[39m] Step: 2459, Training Logs: loss_final: 0.929616, loss_mean: 0.883271, loss_mean_cls: 0.046345, grad_norm: 0.517317
Steps:   0%| | 2472/1000000 [12:12<80:37:48,  3.44it/s, grad_norm=0.622, loss_final=0.916, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:10:27[39m] Step: 2466, Training Logs: loss_final: 0.902077, loss_mean: 0.854445, loss_mean_cls: 0.047633, grad_norm: 0.406275
Steps:   0%| | 2479/1000000 [12:14<82:11:28,  3.37it/s, grad_norm=0.334, loss_final=0.925, loss_mean=0.878, loss_mean_cls=0.[[34m2025-10-04 12:10:29[39m] Step: 2473, Training Logs: loss_final: 0.932438, loss_mean: 0.884897, loss_mean_cls: 0.047541, grad_norm: 0.476383
Steps:   0%| | 2486/1000000 [12:16<79:45:06,  3.47it/s, grad_norm=0.412, loss_final=0.913, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:10:31[39m] Step: 2480, Training Logs: loss_final: 0.901837, loss_mean: 0.855433, loss_mean_cls: 0.046404, grad_norm: 0.582115
Steps:   0%| | 2493/1000000 [12:18<80:02:26,  3.46it/s, grad_norm=0.46, loss_final=0.884, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:10:33[39m] Step: 2487, Training Logs: loss_final: 0.907689, loss_mean: 0.860397, loss_mean_cls: 0.047292, grad_norm: 0.367232
Steps:   0%| | 2500/1000000 [12:20<80:50:45,  3.43it/s, grad_norm=0.443, loss_final=0.899, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:10:35[39m] Step: 2494, Training Logs: loss_final: 0.927060, loss_mean: 0.880025, loss_mean_cls: 0.047035, grad_norm: 0.687401
Steps:   0%| | 2507/1000000 [12:22<82:25:56,  3.36it/s, grad_norm=0.64, loss_final=0.891, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:10:37[39m] Step: 2501, Training Logs: loss_final: 0.907392, loss_mean: 0.860770, loss_mean_cls: 0.046622, grad_norm: 0.451832
Steps:   0%| | 2508/1000000 [12:23<81:33:02,  3.40it/s, grad_norm=0.64, loss_final=0.891, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:10:39[39m] Step: 2508, Training Logs: loss_final: 0.905409, loss_mean: 0.857993, loss_mean_cls: 0.047417, grad_norm: 0.376881
Steps:   0%| | 2515/1000000 [12:25<82:24:04,  3.36it/s, grad_norm=0.753, loss_final=0.925, loss_mean=0.877, loss_mean_cls=0.[[34m2025-10-04 12:10:42[39m] Step: 2515, Training Logs: loss_final: 0.916108, loss_mean: 0.869158, loss_mean_cls: 0.046949, grad_norm: 0.484389
Steps:   0%| | 2522/1000000 [12:27<83:27:02,  3.32it/s, grad_norm=0.471, loss_final=0.897, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:10:44[39m] Step: 2522, Training Logs: loss_final: 0.908727, loss_mean: 0.861520, loss_mean_cls: 0.047207, grad_norm: 0.502137
Steps:   0%| | 2529/1000000 [12:29<81:41:29,  3.39it/s, grad_norm=0.568, loss_final=0.904, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:10:46[39m] Step: 2529, Training Logs: loss_final: 0.912635, loss_mean: 0.865890, loss_mean_cls: 0.046746, grad_norm: 0.645022
Steps:   0%| | 2536/1000000 [12:31<85:52:18,  3.23it/s, grad_norm=0.607, loss_final=0.929, loss_mean=0.883, loss_mean_cls=0.[[34m2025-10-04 12:10:48[39m] Step: 2536, Training Logs: loss_final: 0.902758, loss_mean: 0.854640, loss_mean_cls: 0.048118, grad_norm: 0.607874
Steps:   0%| | 2543/1000000 [12:33<82:48:02,  3.35it/s, grad_norm=0.58, loss_final=0.927, loss_mean=0.88, loss_mean_cls=0.04[[34m2025-10-04 12:10:50[39m] Step: 2543, Training Logs: loss_final: 0.910357, loss_mean: 0.863331, loss_mean_cls: 0.047026, grad_norm: 0.499332
Steps:   0%| | 2550/1000000 [12:35<83:23:39,  3.32it/s, grad_norm=0.45, loss_final=0.931, loss_mean=0.885, loss_mean_cls=0.0[[34m2025-10-04 12:10:52[39m] Step: 2550, Training Logs: loss_final: 0.901204, loss_mean: 0.854023, loss_mean_cls: 0.047181, grad_norm: 0.648731
Steps:   0%| | 2557/1000000 [12:37<82:32:48,  3.36it/s, grad_norm=0.448, loss_final=0.902, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:10:54[39m] Step: 2557, Training Logs: loss_final: 0.925208, loss_mean: 0.878528, loss_mean_cls: 0.046680, grad_norm: 0.430752
Steps:   0%| | 2564/1000000 [12:39<82:11:07,  3.37it/s, grad_norm=0.734, loss_final=0.903, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:10:56[39m] Step: 2564, Training Logs: loss_final: 0.912655, loss_mean: 0.865592, loss_mean_cls: 0.047063, grad_norm: 0.702418
Steps:   0%| | 2571/1000000 [12:41<80:49:47,  3.43it/s, grad_norm=0.513, loss_final=0.909, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:10:58[39m] Step: 2571, Training Logs: loss_final: 0.916714, loss_mean: 0.869605, loss_mean_cls: 0.047109, grad_norm: 0.702428
Steps:   0%| | 2578/1000000 [12:44<80:22:23,  3.45it/s, grad_norm=0.45, loss_final=0.905, loss_mean=0.859, loss_mean_cls=0.0[[34m2025-10-04 12:11:00[39m] Step: 2578, Training Logs: loss_final: 0.918747, loss_mean: 0.872312, loss_mean_cls: 0.046435, grad_norm: 0.586184
Steps:   0%| | 2585/1000000 [12:46<80:02:42,  3.46it/s, grad_norm=0.624, loss_final=0.914, loss_mean=0.866, loss_mean_cls=0.[[34m2025-10-04 12:11:02[39m] Step: 2585, Training Logs: loss_final: 0.921886, loss_mean: 0.875154, loss_mean_cls: 0.046732, grad_norm: 0.523325
Steps:   0%| | 2592/1000000 [12:48<80:44:31,  3.43it/s, grad_norm=0.691, loss_final=0.909, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:11:04[39m] Step: 2592, Training Logs: loss_final: 0.914956, loss_mean: 0.867956, loss_mean_cls: 0.047000, grad_norm: 0.592182
Steps:   0%| | 2599/1000000 [12:50<82:00:23,  3.38it/s, grad_norm=0.618, loss_final=0.923, loss_mean=0.876, loss_mean_cls=0.[[34m2025-10-04 12:11:06[39m] Step: 2599, Training Logs: loss_final: 0.926021, loss_mean: 0.879127, loss_mean_cls: 0.046894, grad_norm: 0.898088
Steps:   0%| | 2606/1000000 [12:52<83:44:17,  3.31it/s, grad_norm=0.673, loss_final=0.903, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:11:08[39m] Step: 2606, Training Logs: loss_final: 0.932670, loss_mean: 0.885370, loss_mean_cls: 0.047300, grad_norm: 0.373134
Steps:   0%| | 2613/1000000 [12:54<81:14:18,  3.41it/s, grad_norm=0.764, loss_final=0.926, loss_mean=0.879, loss_mean_cls=0.[[34m2025-10-04 12:11:11[39m] Step: 2613, Training Logs: loss_final: 0.910931, loss_mean: 0.863720, loss_mean_cls: 0.047211, grad_norm: 0.509021
Steps:   0%| | 2620/1000000 [12:56<83:09:02,  3.33it/s, grad_norm=0.583, loss_final=0.923, loss_mean=0.877, loss_mean_cls=0.[[34m2025-10-04 12:11:13[39m] Step: 2620, Training Logs: loss_final: 0.899238, loss_mean: 0.851715, loss_mean_cls: 0.047523, grad_norm: 0.401691
Steps:   0%| | 2627/1000000 [12:58<83:10:00,  3.33it/s, grad_norm=0.514, loss_final=0.897, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:11:15[39m] Step: 2627, Training Logs: loss_final: 0.917507, loss_mean: 0.870578, loss_mean_cls: 0.046928, grad_norm: 0.482353
Steps:   0%| | 2634/1000000 [13:00<82:26:07,  3.36it/s, grad_norm=0.59, loss_final=0.925, loss_mean=0.877, loss_mean_cls=0.0[[34m2025-10-04 12:11:17[39m] Step: 2634, Training Logs: loss_final: 0.912505, loss_mean: 0.865430, loss_mean_cls: 0.047075, grad_norm: 0.493639
Steps:   0%| | 2641/1000000 [13:02<80:43:31,  3.43it/s, grad_norm=0.581, loss_final=0.917, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:11:19[39m] Step: 2641, Training Logs: loss_final: 0.913852, loss_mean: 0.867961, loss_mean_cls: 0.045892, grad_norm: 0.429663
Steps:   0%| | 2648/1000000 [13:04<80:37:35,  3.44it/s, grad_norm=0.495, loss_final=0.928, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:11:21[39m] Step: 2648, Training Logs: loss_final: 0.909517, loss_mean: 0.862807, loss_mean_cls: 0.046709, grad_norm: 0.429220
Steps:   0%| | 2655/1000000 [13:06<81:13:57,  3.41it/s, grad_norm=0.429, loss_final=0.928, loss_mean=0.882, loss_mean_cls=0.[[34m2025-10-04 12:11:23[39m] Step: 2655, Training Logs: loss_final: 0.902130, loss_mean: 0.855045, loss_mean_cls: 0.047084, grad_norm: 0.561329
Steps:   0%| | 2662/1000000 [13:08<81:56:23,  3.38it/s, grad_norm=0.513, loss_final=0.888, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:11:25[39m] Step: 2662, Training Logs: loss_final: 0.909162, loss_mean: 0.861903, loss_mean_cls: 0.047259, grad_norm: 0.653426
Steps:   0%| | 2669/1000000 [13:10<80:45:52,  3.43it/s, grad_norm=0.438, loss_final=0.917, loss_mean=0.871, loss_mean_cls=0.[[34m2025-10-04 12:11:27[39m] Step: 2669, Training Logs: loss_final: 0.901661, loss_mean: 0.853412, loss_mean_cls: 0.048249, grad_norm: 0.642681
Steps:   0%| | 2676/1000000 [13:12<81:00:25,  3.42it/s, grad_norm=0.647, loss_final=0.921, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:11:29[39m] Step: 2676, Training Logs: loss_final: 0.924502, loss_mean: 0.877499, loss_mean_cls: 0.047003, grad_norm: 0.616621
Steps:   0%| | 2682/1000000 [13:14<82:48:40,  3.35it/s, grad_norm=0.533, loss_final=0.939, loss_mean=0.894, loss_mean_cls=0.[[34m2025-10-04 12:11:29[39m] Step: 2676, Training Logs: loss_final: 0.924502, loss_mean: 0.877499, loss_mean_cls: 0.047003, grad_norm: 0.616621
Steps:   0%| | 2690/1000000 [13:17<80:28:36,  3.44it/s, grad_norm=0.597, loss_final=0.907, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:11:33[39m] Step: 2690, Training Logs: loss_final: 0.934264, loss_mean: 0.887648, loss_mean_cls: 0.046616, grad_norm: 0.551434
Steps:   0%| | 2697/1000000 [13:19<80:30:33,  3.44it/s, grad_norm=0.539, loss_final=0.942, loss_mean=0.895, loss_mean_cls=0.[[34m2025-10-04 12:11:35[39m] Step: 2697, Training Logs: loss_final: 0.912705, loss_mean: 0.865786, loss_mean_cls: 0.046919, grad_norm: 0.618900
Steps:   0%| | 2703/1000000 [13:20<82:20:34,  3.36it/s, grad_norm=0.365, loss_final=0.915, loss_mean=0.87, loss_mean_cls=0.0[[34m2025-10-04 12:11:35[39m] Step: 2697, Training Logs: loss_final: 0.912705, loss_mean: 0.865786, loss_mean_cls: 0.046919, grad_norm: 0.618900
Steps:   0%| | 2710/1000000 [13:22<81:34:54,  3.40it/s, grad_norm=0.568, loss_final=0.905, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:11:37[39m] Step: 2704, Training Logs: loss_final: 0.903384, loss_mean: 0.856080, loss_mean_cls: 0.047305, grad_norm: 0.362517
Steps:   0%| | 2717/1000000 [13:25<83:04:50,  3.33it/s, grad_norm=0.459, loss_final=0.922, loss_mean=0.876, loss_mean_cls=0.[[34m2025-10-04 12:11:39[39m] Step: 2711, Training Logs: loss_final: 0.891500, loss_mean: 0.844090, loss_mean_cls: 0.047411, grad_norm: 0.355531
Steps:   0%| | 2724/1000000 [13:27<80:16:01,  3.45it/s, grad_norm=0.436, loss_final=0.899, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:11:41[39m] Step: 2718, Training Logs: loss_final: 0.898222, loss_mean: 0.851803, loss_mean_cls: 0.046419, grad_norm: 0.460003
Steps:   0%| | 2731/1000000 [13:29<80:32:51,  3.44it/s, grad_norm=0.404, loss_final=0.916, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:11:43[39m] Step: 2725, Training Logs: loss_final: 0.900383, loss_mean: 0.853460, loss_mean_cls: 0.046923, grad_norm: 0.567097
Steps:   0%| | 2738/1000000 [13:31<83:19:06,  3.32it/s, grad_norm=0.421, loss_final=0.898, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:11:46[39m] Step: 2732, Training Logs: loss_final: 0.912913, loss_mean: 0.865512, loss_mean_cls: 0.047401, grad_norm: 0.521987
Steps:   0%| | 2745/1000000 [13:33<80:08:52,  3.46it/s, grad_norm=0.515, loss_final=0.938, loss_mean=0.891, loss_mean_cls=0.[[34m2025-10-04 12:11:48[39m] Step: 2739, Training Logs: loss_final: 0.888479, loss_mean: 0.841987, loss_mean_cls: 0.046493, grad_norm: 0.579907
Steps:   0%| | 2746/1000000 [13:33<81:15:16,  3.41it/s, grad_norm=0.515, loss_final=0.938, loss_mean=0.891, loss_mean_cls=0.[[34m2025-10-04 12:11:50[39m] Step: 2746, Training Logs: loss_final: 0.918546, loss_mean: 0.872312, loss_mean_cls: 0.046233, grad_norm: 0.455143
Steps:   0%| | 2753/1000000 [13:35<80:43:57,  3.43it/s, grad_norm=0.751, loss_final=0.917, loss_mean=0.871, loss_mean_cls=0.[[34m2025-10-04 12:11:52[39m] Step: 2753, Training Logs: loss_final: 0.894745, loss_mean: 0.848890, loss_mean_cls: 0.045855, grad_norm: 0.676361
Steps:   0%| | 2760/1000000 [13:37<81:06:03,  3.42it/s, grad_norm=0.843, loss_final=0.91, loss_mean=0.863, loss_mean_cls=0.0[[34m2025-10-04 12:11:54[39m] Step: 2760, Training Logs: loss_final: 0.880825, loss_mean: 0.832746, loss_mean_cls: 0.048079, grad_norm: 0.579502
Steps:   0%| | 2767/1000000 [13:39<81:01:44,  3.42it/s, grad_norm=0.66, loss_final=0.918, loss_mean=0.871, loss_mean_cls=0.0[[34m2025-10-04 12:11:56[39m] Step: 2767, Training Logs: loss_final: 0.934006, loss_mean: 0.886377, loss_mean_cls: 0.047629, grad_norm: 0.713709
Steps:   0%| | 2774/1000000 [13:41<82:11:19,  3.37it/s, grad_norm=0.396, loss_final=0.916, loss_mean=0.87, loss_mean_cls=0.0[[34m2025-10-04 12:11:58[39m] Step: 2774, Training Logs: loss_final: 0.908217, loss_mean: 0.860852, loss_mean_cls: 0.047364, grad_norm: 0.567320
Steps:   0%| | 2781/1000000 [13:43<81:37:34,  3.39it/s, grad_norm=0.595, loss_final=0.906, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:12:00[39m] Step: 2781, Training Logs: loss_final: 0.899013, loss_mean: 0.850587, loss_mean_cls: 0.048426, grad_norm: 0.489425
Steps:   0%| | 2788/1000000 [13:45<80:59:39,  3.42it/s, grad_norm=0.473, loss_final=0.907, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:12:02[39m] Step: 2788, Training Logs: loss_final: 0.911809, loss_mean: 0.864130, loss_mean_cls: 0.047679, grad_norm: 0.423216
Steps:   0%| | 2795/1000000 [13:47<82:59:01,  3.34it/s, grad_norm=0.555, loss_final=0.915, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:12:04[39m] Step: 2795, Training Logs: loss_final: 0.906669, loss_mean: 0.858831, loss_mean_cls: 0.047838, grad_norm: 0.545384
Steps:   0%| | 2802/1000000 [13:49<79:52:14,  3.47it/s, grad_norm=0.709, loss_final=0.894, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:12:06[39m] Step: 2802, Training Logs: loss_final: 0.902356, loss_mean: 0.855925, loss_mean_cls: 0.046432, grad_norm: 0.424944
Steps:   0%| | 2809/1000000 [13:52<81:38:56,  3.39it/s, grad_norm=0.538, loss_final=0.894, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:12:08[39m] Step: 2809, Training Logs: loss_final: 0.889500, loss_mean: 0.841744, loss_mean_cls: 0.047756, grad_norm: 0.660969
Steps:   0%| | 2816/1000000 [13:54<81:46:10,  3.39it/s, grad_norm=0.711, loss_final=0.915, loss_mean=0.87, loss_mean_cls=0.0[[34m2025-10-04 12:12:10[39m] Step: 2816, Training Logs: loss_final: 0.932524, loss_mean: 0.886528, loss_mean_cls: 0.045997, grad_norm: 0.392862
Steps:   0%| | 2823/1000000 [13:56<84:00:22,  3.30it/s, grad_norm=0.489, loss_final=0.893, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:12:12[39m] Step: 2823, Training Logs: loss_final: 0.888693, loss_mean: 0.842727, loss_mean_cls: 0.045966, grad_norm: 0.421138
Steps:   0%| | 2830/1000000 [13:58<80:09:30,  3.46it/s, grad_norm=0.374, loss_final=0.891, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:12:14[39m] Step: 2830, Training Logs: loss_final: 0.917132, loss_mean: 0.869016, loss_mean_cls: 0.048116, grad_norm: 0.436405
Steps:   0%| | 2837/1000000 [14:00<85:24:10,  3.24it/s, grad_norm=0.478, loss_final=0.895, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:12:17[39m] Step: 2837, Training Logs: loss_final: 0.898360, loss_mean: 0.851284, loss_mean_cls: 0.047075, grad_norm: 0.482054
Steps:   0%| | 2844/1000000 [14:02<79:47:14,  3.47it/s, grad_norm=0.57, loss_final=0.904, loss_mean=0.856, loss_mean_cls=0.0[[34m2025-10-04 12:12:19[39m] Step: 2844, Training Logs: loss_final: 0.906202, loss_mean: 0.859997, loss_mean_cls: 0.046205, grad_norm: 0.406747
Steps:   0%| | 2851/1000000 [14:04<81:21:45,  3.40it/s, grad_norm=0.539, loss_final=0.892, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:12:21[39m] Step: 2851, Training Logs: loss_final: 0.905046, loss_mean: 0.858137, loss_mean_cls: 0.046908, grad_norm: 0.499682
Steps:   0%| | 2858/1000000 [14:06<82:14:24,  3.37it/s, grad_norm=0.543, loss_final=0.912, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:12:23[39m] Step: 2858, Training Logs: loss_final: 0.887162, loss_mean: 0.838576, loss_mean_cls: 0.048586, grad_norm: 0.453574
Steps:   0%| | 2865/1000000 [14:08<82:08:51,  3.37it/s, grad_norm=0.427, loss_final=0.907, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:12:25[39m] Step: 2865, Training Logs: loss_final: 0.902538, loss_mean: 0.857610, loss_mean_cls: 0.044928, grad_norm: 0.599940
Steps:   0%| | 2872/1000000 [14:10<80:18:34,  3.45it/s, grad_norm=0.589, loss_final=0.909, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:12:27[39m] Step: 2872, Training Logs: loss_final: 0.906584, loss_mean: 0.859878, loss_mean_cls: 0.046706, grad_norm: 0.462073
Steps:   0%| | 2879/1000000 [14:12<80:40:56,  3.43it/s, grad_norm=0.39, loss_final=0.909, loss_mean=0.863, loss_mean_cls=0.0[[34m2025-10-04 12:12:29[39m] Step: 2879, Training Logs: loss_final: 0.912604, loss_mean: 0.866627, loss_mean_cls: 0.045977, grad_norm: 0.560868
Steps:   0%| | 2886/1000000 [14:14<81:13:51,  3.41it/s, grad_norm=0.447, loss_final=0.915, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:12:31[39m] Step: 2886, Training Logs: loss_final: 0.921797, loss_mean: 0.874752, loss_mean_cls: 0.047045, grad_norm: 0.597158
Steps:   0%| | 2893/1000000 [14:16<80:53:14,  3.42it/s, grad_norm=0.555, loss_final=0.913, loss_mean=0.866, loss_mean_cls=0.[[34m2025-10-04 12:12:33[39m] Step: 2893, Training Logs: loss_final: 0.911147, loss_mean: 0.864806, loss_mean_cls: 0.046341, grad_norm: 0.395846
Steps:   0%| | 2900/1000000 [14:18<80:29:36,  3.44it/s, grad_norm=0.518, loss_final=0.894, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:12:35[39m] Step: 2900, Training Logs: loss_final: 0.913126, loss_mean: 0.865970, loss_mean_cls: 0.047156, grad_norm: 0.496921
Steps:   0%| | 2907/1000000 [14:20<82:25:41,  3.36it/s, grad_norm=0.612, loss_final=0.896, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:12:37[39m] Step: 2907, Training Logs: loss_final: 0.914213, loss_mean: 0.867072, loss_mean_cls: 0.047141, grad_norm: 0.523485
Steps:   0%| | 2914/1000000 [14:22<84:11:09,  3.29it/s, grad_norm=0.327, loss_final=0.92, loss_mean=0.874, loss_mean_cls=0.0[[34m2025-10-04 12:12:39[39m] Step: 2914, Training Logs: loss_final: 0.904015, loss_mean: 0.857159, loss_mean_cls: 0.046856, grad_norm: 0.291333
Steps:   0%| | 2921/1000000 [14:25<81:14:09,  3.41it/s, grad_norm=0.4, loss_final=0.914, loss_mean=0.867, loss_mean_cls=0.04[[34m2025-10-04 12:12:41[39m] Step: 2921, Training Logs: loss_final: 0.926196, loss_mean: 0.880607, loss_mean_cls: 0.045589, grad_norm: 0.334679
Steps:   0%| | 2928/1000000 [14:27<81:16:19,  3.41it/s, grad_norm=0.398, loss_final=0.883, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:12:43[39m] Step: 2928, Training Logs: loss_final: 0.902181, loss_mean: 0.856333, loss_mean_cls: 0.045848, grad_norm: 0.332207
Steps:   0%| | 2935/1000000 [14:29<82:30:03,  3.36it/s, grad_norm=0.396, loss_final=0.9, loss_mean=0.854, loss_mean_cls=0.04[[34m2025-10-04 12:12:45[39m] Step: 2935, Training Logs: loss_final: 0.896971, loss_mean: 0.850787, loss_mean_cls: 0.046184, grad_norm: 0.327789
Steps:   0%| | 2942/1000000 [14:31<79:53:29,  3.47it/s, grad_norm=0.576, loss_final=0.93, loss_mean=0.884, loss_mean_cls=0.0[[34m2025-10-04 12:12:47[39m] Step: 2942, Training Logs: loss_final: 0.915341, loss_mean: 0.867729, loss_mean_cls: 0.047612, grad_norm: 0.526959
Steps:   0%| | 2949/1000000 [14:33<80:40:28,  3.43it/s, grad_norm=0.44, loss_final=0.919, loss_mean=0.873, loss_mean_cls=0.0[[34m2025-10-04 12:12:49[39m] Step: 2949, Training Logs: loss_final: 0.915088, loss_mean: 0.868891, loss_mean_cls: 0.046198, grad_norm: 0.626907
Steps:   0%| | 2956/1000000 [14:35<80:03:54,  3.46it/s, grad_norm=0.351, loss_final=0.915, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:12:51[39m] Step: 2956, Training Logs: loss_final: 0.910953, loss_mean: 0.863772, loss_mean_cls: 0.047181, grad_norm: 0.388767
Steps:   0%| | 2963/1000000 [14:37<82:49:46,  3.34it/s, grad_norm=0.553, loss_final=0.897, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:12:54[39m] Step: 2963, Training Logs: loss_final: 0.885185, loss_mean: 0.838213, loss_mean_cls: 0.046972, grad_norm: 0.361995
Steps:   0%| | 2970/1000000 [14:39<81:36:52,  3.39it/s, grad_norm=0.475, loss_final=0.912, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:12:56[39m] Step: 2970, Training Logs: loss_final: 0.896331, loss_mean: 0.848845, loss_mean_cls: 0.047486, grad_norm: 0.482087
Steps:   0%| | 2977/1000000 [14:41<79:53:25,  3.47it/s, grad_norm=0.543, loss_final=0.892, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:12:58[39m] Step: 2977, Training Logs: loss_final: 0.902827, loss_mean: 0.856412, loss_mean_cls: 0.046415, grad_norm: 0.413066
Steps:   0%| | 2984/1000000 [14:43<81:37:57,  3.39it/s, grad_norm=0.647, loss_final=0.909, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:12:58[39m] Step: 2977, Training Logs: loss_final: 0.902827, loss_mean: 0.856412, loss_mean_cls: 0.046415, grad_norm: 0.413066
Steps:   0%| | 2990/1000000 [14:45<80:51:34,  3.43it/s, grad_norm=0.472, loss_final=0.931, loss_mean=0.886, loss_mean_cls=0.[[34m2025-10-04 12:13:00[39m] Step: 2984, Training Logs: loss_final: 0.922021, loss_mean: 0.874084, loss_mean_cls: 0.047937, grad_norm: 0.491530
Steps:   0%| | 2997/1000000 [14:47<80:37:40,  3.43it/s, grad_norm=0.358, loss_final=0.894, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:13:02[39m] Step: 2991, Training Logs: loss_final: 0.888697, loss_mean: 0.840301, loss_mean_cls: 0.048396, grad_norm: 0.435099
Steps:   0%| | 3004/1000000 [14:49<81:33:11,  3.40it/s, grad_norm=0.362, loss_final=0.921, loss_mean=0.875, loss_mean_cls=0.[[34m2025-10-04 12:13:04[39m] Step: 2998, Training Logs: loss_final: 0.914213, loss_mean: 0.867636, loss_mean_cls: 0.046577, grad_norm: 0.392212
Steps:   0%| | 3011/1000000 [14:51<82:26:31,  3.36it/s, grad_norm=0.339, loss_final=0.905, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:13:06[39m] Step: 3005, Training Logs: loss_final: 0.893741, loss_mean: 0.848181, loss_mean_cls: 0.045559, grad_norm: 0.679879
Steps:   0%| | 3018/1000000 [14:53<80:41:53,  3.43it/s, grad_norm=0.439, loss_final=0.923, loss_mean=0.877, loss_mean_cls=0.[[34m2025-10-04 12:13:08[39m] Step: 3012, Training Logs: loss_final: 0.892997, loss_mean: 0.846273, loss_mean_cls: 0.046724, grad_norm: 0.416047
Steps:   0%| | 3025/1000000 [14:55<80:31:02,  3.44it/s, grad_norm=0.404, loss_final=0.901, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:13:10[39m] Step: 3019, Training Logs: loss_final: 0.902984, loss_mean: 0.856382, loss_mean_cls: 0.046601, grad_norm: 0.402661
Steps:   0%| | 3032/1000000 [14:57<81:06:12,  3.41it/s, grad_norm=0.385, loss_final=0.918, loss_mean=0.872, loss_mean_cls=0.[[34m2025-10-04 12:13:12[39m] Step: 3026, Training Logs: loss_final: 0.896903, loss_mean: 0.850248, loss_mean_cls: 0.046656, grad_norm: 0.386057
Steps:   0%| | 3033/1000000 [14:57<80:21:29,  3.45it/s, grad_norm=0.385, loss_final=0.918, loss_mean=0.872, loss_mean_cls=0.[[34m2025-10-04 12:13:14[39m] Step: 3033, Training Logs: loss_final: 0.863862, loss_mean: 0.816842, loss_mean_cls: 0.047020, grad_norm: 0.335520
Steps:   0%| | 3040/1000000 [14:59<82:27:04,  3.36it/s, grad_norm=0.364, loss_final=0.903, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:13:16[39m] Step: 3040, Training Logs: loss_final: 0.906262, loss_mean: 0.859748, loss_mean_cls: 0.046514, grad_norm: 0.375200
Steps:   0%| | 3047/1000000 [15:01<79:35:18,  3.48it/s, grad_norm=0.453, loss_final=0.923, loss_mean=0.878, loss_mean_cls=0.[[34m2025-10-04 12:13:18[39m] Step: 3047, Training Logs: loss_final: 0.902929, loss_mean: 0.855664, loss_mean_cls: 0.047264, grad_norm: 0.528111
Steps:   0%| | 3054/1000000 [15:04<80:56:34,  3.42it/s, grad_norm=0.451, loss_final=0.92, loss_mean=0.874, loss_mean_cls=0.0[[34m2025-10-04 12:13:20[39m] Step: 3054, Training Logs: loss_final: 0.900209, loss_mean: 0.853986, loss_mean_cls: 0.046223, grad_norm: 0.567645
Steps:   0%| | 3061/1000000 [15:06<80:19:36,  3.45it/s, grad_norm=0.321, loss_final=0.902, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:13:22[39m] Step: 3061, Training Logs: loss_final: 0.928105, loss_mean: 0.881497, loss_mean_cls: 0.046608, grad_norm: 0.531610
Steps:   0%| | 3068/1000000 [15:08<80:09:06,  3.46it/s, grad_norm=0.358, loss_final=0.906, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:13:24[39m] Step: 3068, Training Logs: loss_final: 0.903866, loss_mean: 0.857745, loss_mean_cls: 0.046121, grad_norm: 0.325822
Steps:   0%| | 3075/1000000 [15:10<80:46:11,  3.43it/s, grad_norm=0.518, loss_final=0.911, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:13:26[39m] Step: 3075, Training Logs: loss_final: 0.895410, loss_mean: 0.848918, loss_mean_cls: 0.046492, grad_norm: 0.674858
Steps:   0%| | 3082/1000000 [15:12<82:56:55,  3.34it/s, grad_norm=0.427, loss_final=0.902, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:13:28[39m] Step: 3082, Training Logs: loss_final: 0.895419, loss_mean: 0.849839, loss_mean_cls: 0.045581, grad_norm: 0.424525
Steps:   0%| | 3089/1000000 [15:14<84:57:41,  3.26it/s, grad_norm=0.496, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.0[[34m2025-10-04 12:13:31[39m] Step: 3089, Training Logs: loss_final: 0.904240, loss_mean: 0.856506, loss_mean_cls: 0.047734, grad_norm: 0.428634
Steps:   0%| | 3096/1000000 [15:16<82:36:07,  3.35it/s, grad_norm=0.516, loss_final=0.902, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:13:33[39m] Step: 3096, Training Logs: loss_final: 0.909996, loss_mean: 0.863752, loss_mean_cls: 0.046245, grad_norm: 0.558660
Steps:   0%| | 3103/1000000 [15:18<84:34:18,  3.27it/s, grad_norm=0.527, loss_final=0.903, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:13:35[39m] Step: 3103, Training Logs: loss_final: 0.914452, loss_mean: 0.868751, loss_mean_cls: 0.045701, grad_norm: 0.301903
Steps:   0%| | 3110/1000000 [15:20<81:47:27,  3.39it/s, grad_norm=0.483, loss_final=0.899, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:13:37[39m] Step: 3110, Training Logs: loss_final: 0.910923, loss_mean: 0.864011, loss_mean_cls: 0.046912, grad_norm: 0.356899
Steps:   0%| | 3117/1000000 [15:22<81:11:03,  3.41it/s, grad_norm=0.522, loss_final=0.902, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:13:39[39m] Step: 3117, Training Logs: loss_final: 0.891117, loss_mean: 0.844935, loss_mean_cls: 0.046183, grad_norm: 0.490410
Steps:   0%| | 3124/1000000 [15:24<81:34:44,  3.39it/s, grad_norm=0.626, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.0[[34m2025-10-04 12:13:41[39m] Step: 3124, Training Logs: loss_final: 0.889401, loss_mean: 0.842351, loss_mean_cls: 0.047050, grad_norm: 0.312896
Steps:   0%| | 3131/1000000 [15:26<83:24:24,  3.32it/s, grad_norm=0.37, loss_final=0.882, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:13:43[39m] Step: 3131, Training Logs: loss_final: 0.909367, loss_mean: 0.862771, loss_mean_cls: 0.046596, grad_norm: 0.410957
Steps:   0%| | 3138/1000000 [15:28<83:26:06,  3.32it/s, grad_norm=0.379, loss_final=0.888, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:13:45[39m] Step: 3138, Training Logs: loss_final: 0.902073, loss_mean: 0.854871, loss_mean_cls: 0.047202, grad_norm: 0.469952
Steps:   0%| | 3145/1000000 [15:30<80:49:19,  3.43it/s, grad_norm=0.542, loss_final=0.897, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:13:47[39m] Step: 3145, Training Logs: loss_final: 0.922371, loss_mean: 0.875581, loss_mean_cls: 0.046790, grad_norm: 0.591005
Steps:   0%| | 3152/1000000 [15:33<82:10:09,  3.37it/s, grad_norm=0.388, loss_final=0.903, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:13:49[39m] Step: 3152, Training Logs: loss_final: 0.910382, loss_mean: 0.862999, loss_mean_cls: 0.047383, grad_norm: 0.467512
Steps:   0%| | 3159/1000000 [15:35<81:41:31,  3.39it/s, grad_norm=0.504, loss_final=0.914, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:13:51[39m] Step: 3159, Training Logs: loss_final: 0.903717, loss_mean: 0.857806, loss_mean_cls: 0.045911, grad_norm: 0.662377
Steps:   0%| | 3166/1000000 [15:37<79:12:58,  3.50it/s, grad_norm=0.693, loss_final=0.901, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:13:53[39m] Step: 3166, Training Logs: loss_final: 0.895082, loss_mean: 0.847892, loss_mean_cls: 0.047190, grad_norm: 0.521687
Steps:   0%| | 3173/1000000 [15:39<79:08:22,  3.50it/s, grad_norm=0.742, loss_final=0.891, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:13:55[39m] Step: 3173, Training Logs: loss_final: 0.914993, loss_mean: 0.868804, loss_mean_cls: 0.046189, grad_norm: 0.449200
Steps:   0%| | 3180/1000000 [15:41<81:11:25,  3.41it/s, grad_norm=0.507, loss_final=0.888, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:13:57[39m] Step: 3180, Training Logs: loss_final: 0.917888, loss_mean: 0.871888, loss_mean_cls: 0.046000, grad_norm: 0.475717
Steps:   0%| | 3187/1000000 [15:43<82:26:48,  3.36it/s, grad_norm=0.425, loss_final=0.885, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:13:59[39m] Step: 3187, Training Logs: loss_final: 0.899613, loss_mean: 0.852929, loss_mean_cls: 0.046684, grad_norm: 0.390994
Steps:   0%| | 3194/1000000 [15:45<80:57:05,  3.42it/s, grad_norm=0.389, loss_final=0.895, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:14:01[39m] Step: 3194, Training Logs: loss_final: 0.889954, loss_mean: 0.844284, loss_mean_cls: 0.045670, grad_norm: 0.338378
Steps:   0%| | 3201/1000000 [15:47<82:54:41,  3.34it/s, grad_norm=0.294, loss_final=0.888, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:14:03[39m] Step: 3201, Training Logs: loss_final: 0.926575, loss_mean: 0.881166, loss_mean_cls: 0.045410, grad_norm: 0.387672
Steps:   0%| | 3208/1000000 [15:49<83:51:27,  3.30it/s, grad_norm=0.504, loss_final=0.906, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:14:06[39m] Step: 3208, Training Logs: loss_final: 0.882683, loss_mean: 0.836340, loss_mean_cls: 0.046343, grad_norm: 0.590679
Steps:   0%| | 3215/1000000 [15:51<82:39:45,  3.35it/s, grad_norm=0.643, loss_final=0.911, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:14:08[39m] Step: 3215, Training Logs: loss_final: 0.909235, loss_mean: 0.862650, loss_mean_cls: 0.046585, grad_norm: 0.429492
Steps:   0%| | 3222/1000000 [15:53<80:50:37,  3.42it/s, grad_norm=0.641, loss_final=0.92, loss_mean=0.873, loss_mean_cls=0.0[[34m2025-10-04 12:14:10[39m] Step: 3222, Training Logs: loss_final: 0.895725, loss_mean: 0.848549, loss_mean_cls: 0.047176, grad_norm: 0.542196
Steps:   0%| | 3229/1000000 [15:55<79:33:56,  3.48it/s, grad_norm=0.374, loss_final=0.9, loss_mean=0.854, loss_mean_cls=0.04[[34m2025-10-04 12:14:12[39m] Step: 3229, Training Logs: loss_final: 0.914354, loss_mean: 0.868163, loss_mean_cls: 0.046192, grad_norm: 0.450879
Steps:   0%| | 3236/1000000 [15:57<82:49:35,  3.34it/s, grad_norm=0.53, loss_final=0.892, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:14:14[39m] Step: 3236, Training Logs: loss_final: 0.906353, loss_mean: 0.860154, loss_mean_cls: 0.046199, grad_norm: 0.496873
Steps:   0%| | 3242/1000000 [15:59<84:46:00,  3.27it/s, grad_norm=0.406, loss_final=0.891, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:14:14[39m] Step: 3236, Training Logs: loss_final: 0.906353, loss_mean: 0.860154, loss_mean_cls: 0.046199, grad_norm: 0.496873
Steps:   0%| | 3249/1000000 [16:01<81:44:22,  3.39it/s, grad_norm=0.36, loss_final=0.876, loss_mean=0.829, loss_mean_cls=0.0[[34m2025-10-04 12:14:16[39m] Step: 3243, Training Logs: loss_final: 0.886819, loss_mean: 0.840049, loss_mean_cls: 0.046770, grad_norm: 0.312024
Steps:   0%| | 3256/1000000 [16:03<82:52:54,  3.34it/s, grad_norm=0.438, loss_final=0.908, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:14:18[39m] Step: 3250, Training Logs: loss_final: 0.905062, loss_mean: 0.859058, loss_mean_cls: 0.046004, grad_norm: 0.655991
Steps:   0%| | 3263/1000000 [16:05<82:28:55,  3.36it/s, grad_norm=0.452, loss_final=0.877, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:14:20[39m] Step: 3257, Training Logs: loss_final: 0.874696, loss_mean: 0.826948, loss_mean_cls: 0.047748, grad_norm: 0.490543
Steps:   0%| | 3270/1000000 [16:07<81:30:44,  3.40it/s, grad_norm=0.666, loss_final=0.906, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:14:22[39m] Step: 3264, Training Logs: loss_final: 0.905485, loss_mean: 0.859876, loss_mean_cls: 0.045609, grad_norm: 0.308022
Steps:   0%| | 3277/1000000 [16:09<80:05:13,  3.46it/s, grad_norm=0.382, loss_final=0.895, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:14:24[39m] Step: 3271, Training Logs: loss_final: 0.907089, loss_mean: 0.859874, loss_mean_cls: 0.047215, grad_norm: 0.435274
Steps:   0%| | 3278/1000000 [16:10<80:51:06,  3.42it/s, grad_norm=0.382, loss_final=0.895, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:14:26[39m] Step: 3278, Training Logs: loss_final: 0.922460, loss_mean: 0.875984, loss_mean_cls: 0.046476, grad_norm: 0.321709
Steps:   0%| | 3285/1000000 [16:12<79:51:33,  3.47it/s, grad_norm=0.352, loss_final=0.905, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:14:28[39m] Step: 3285, Training Logs: loss_final: 0.912624, loss_mean: 0.868022, loss_mean_cls: 0.044601, grad_norm: 0.458554
Steps:   0%| | 3292/1000000 [16:14<80:21:05,  3.45it/s, grad_norm=0.356, loss_final=0.906, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:14:30[39m] Step: 3292, Training Logs: loss_final: 0.884306, loss_mean: 0.837262, loss_mean_cls: 0.047043, grad_norm: 0.366697
Steps:   0%| | 3299/1000000 [16:16<81:23:02,  3.40it/s, grad_norm=0.479, loss_final=0.891, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:14:32[39m] Step: 3299, Training Logs: loss_final: 0.902542, loss_mean: 0.855069, loss_mean_cls: 0.047473, grad_norm: 0.480549
Steps:   0%| | 3306/1000000 [16:18<80:57:05,  3.42it/s, grad_norm=0.395, loss_final=0.896, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:14:34[39m] Step: 3306, Training Logs: loss_final: 0.902755, loss_mean: 0.855650, loss_mean_cls: 0.047106, grad_norm: 0.367933
Steps:   0%| | 3313/1000000 [16:20<80:32:26,  3.44it/s, grad_norm=0.463, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.0[[34m2025-10-04 12:14:36[39m] Step: 3313, Training Logs: loss_final: 0.891775, loss_mean: 0.845686, loss_mean_cls: 0.046089, grad_norm: 0.507715
Steps:   0%| | 3320/1000000 [16:22<79:37:55,  3.48it/s, grad_norm=0.509, loss_final=0.908, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:14:39[39m] Step: 3320, Training Logs: loss_final: 0.878334, loss_mean: 0.831543, loss_mean_cls: 0.046791, grad_norm: 0.437957
Steps:   0%| | 3327/1000000 [16:24<82:32:02,  3.35it/s, grad_norm=0.567, loss_final=0.912, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:14:41[39m] Step: 3327, Training Logs: loss_final: 0.892347, loss_mean: 0.845359, loss_mean_cls: 0.046987, grad_norm: 0.445272
Steps:   0%| | 3334/1000000 [16:26<82:05:15,  3.37it/s, grad_norm=0.508, loss_final=0.899, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:14:43[39m] Step: 3334, Training Logs: loss_final: 0.931199, loss_mean: 0.886684, loss_mean_cls: 0.044515, grad_norm: 0.441701
Steps:   0%| | 3341/1000000 [16:28<80:21:11,  3.45it/s, grad_norm=0.498, loss_final=0.897, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:14:45[39m] Step: 3341, Training Logs: loss_final: 0.860963, loss_mean: 0.814306, loss_mean_cls: 0.046657, grad_norm: 0.393206
Steps:   0%| | 3348/1000000 [16:30<80:14:23,  3.45it/s, grad_norm=0.641, loss_final=0.903, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:14:47[39m] Step: 3348, Training Logs: loss_final: 0.900309, loss_mean: 0.853768, loss_mean_cls: 0.046541, grad_norm: 0.402154
Steps:   0%| | 3355/1000000 [16:32<83:40:44,  3.31it/s, grad_norm=0.582, loss_final=0.908, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:14:49[39m] Step: 3355, Training Logs: loss_final: 0.897501, loss_mean: 0.851048, loss_mean_cls: 0.046452, grad_norm: 0.694689
Steps:   0%| | 3362/1000000 [16:34<83:10:48,  3.33it/s, grad_norm=0.435, loss_final=0.886, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:14:51[39m] Step: 3362, Training Logs: loss_final: 0.904388, loss_mean: 0.857371, loss_mean_cls: 0.047017, grad_norm: 0.540081
Steps:   0%| | 3369/1000000 [16:36<82:28:35,  3.36it/s, grad_norm=0.348, loss_final=0.896, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:14:53[39m] Step: 3369, Training Logs: loss_final: 0.909824, loss_mean: 0.864405, loss_mean_cls: 0.045418, grad_norm: 0.477549
Steps:   0%| | 3376/1000000 [16:38<83:08:04,  3.33it/s, grad_norm=0.452, loss_final=0.901, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:14:55[39m] Step: 3376, Training Logs: loss_final: 0.890905, loss_mean: 0.844652, loss_mean_cls: 0.046253, grad_norm: 0.284429
Steps:   0%| | 3383/1000000 [16:40<81:11:35,  3.41it/s, grad_norm=0.376, loss_final=0.907, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:14:57[39m] Step: 3383, Training Logs: loss_final: 0.902475, loss_mean: 0.856617, loss_mean_cls: 0.045858, grad_norm: 0.536920
Steps:   0%| | 3390/1000000 [16:42<81:49:30,  3.38it/s, grad_norm=0.582, loss_final=0.925, loss_mean=0.88, loss_mean_cls=0.0[[34m2025-10-04 12:14:59[39m] Step: 3390, Training Logs: loss_final: 0.885674, loss_mean: 0.839035, loss_mean_cls: 0.046639, grad_norm: 0.398158
Steps:   0%| | 3397/1000000 [16:44<79:34:43,  3.48it/s, grad_norm=0.533, loss_final=0.892, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:15:01[39m] Step: 3397, Training Logs: loss_final: 0.900686, loss_mean: 0.853597, loss_mean_cls: 0.047089, grad_norm: 0.407133
Steps:   0%| | 3404/1000000 [16:47<81:58:24,  3.38it/s, grad_norm=0.407, loss_final=0.927, loss_mean=0.882, loss_mean_cls=0.[[34m2025-10-04 12:15:03[39m] Step: 3404, Training Logs: loss_final: 0.897331, loss_mean: 0.852843, loss_mean_cls: 0.044489, grad_norm: 0.524586
Steps:   0%| | 3411/1000000 [16:49<82:56:15,  3.34it/s, grad_norm=0.504, loss_final=0.899, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:15:05[39m] Step: 3411, Training Logs: loss_final: 0.883900, loss_mean: 0.837522, loss_mean_cls: 0.046378, grad_norm: 0.552834
Steps:   0%| | 3418/1000000 [16:51<80:32:30,  3.44it/s, grad_norm=0.584, loss_final=0.89, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:15:07[39m] Step: 3418, Training Logs: loss_final: 0.912285, loss_mean: 0.866706, loss_mean_cls: 0.045579, grad_norm: 0.502831
Steps:   0%| | 3425/1000000 [16:53<82:33:56,  3.35it/s, grad_norm=0.489, loss_final=0.907, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:15:09[39m] Step: 3425, Training Logs: loss_final: 0.913617, loss_mean: 0.867256, loss_mean_cls: 0.046361, grad_norm: 0.272148
Steps:   0%| | 3432/1000000 [16:55<80:59:45,  3.42it/s, grad_norm=0.508, loss_final=0.906, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:15:11[39m] Step: 3432, Training Logs: loss_final: 0.904463, loss_mean: 0.858428, loss_mean_cls: 0.046035, grad_norm: 0.313725
Steps:   0%| | 3439/1000000 [16:57<81:48:49,  3.38it/s, grad_norm=0.439, loss_final=0.908, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:15:14[39m] Step: 3439, Training Logs: loss_final: 0.902884, loss_mean: 0.856909, loss_mean_cls: 0.045975, grad_norm: 0.461310
Steps:   0%| | 3446/1000000 [16:59<79:55:15,  3.46it/s, grad_norm=0.443, loss_final=0.893, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:15:16[39m] Step: 3446, Training Logs: loss_final: 0.882852, loss_mean: 0.836008, loss_mean_cls: 0.046845, grad_norm: 0.490239
Steps:   0%| | 3453/1000000 [17:01<81:02:11,  3.42it/s, grad_norm=0.444, loss_final=0.917, loss_mean=0.871, loss_mean_cls=0.[[34m2025-10-04 12:15:18[39m] Step: 3453, Training Logs: loss_final: 0.926495, loss_mean: 0.881029, loss_mean_cls: 0.045465, grad_norm: 0.438974
Steps:   0%| | 3460/1000000 [17:03<81:35:08,  3.39it/s, grad_norm=0.402, loss_final=0.89, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:15:20[39m] Step: 3460, Training Logs: loss_final: 0.902823, loss_mean: 0.855952, loss_mean_cls: 0.046871, grad_norm: 0.356736
Steps:   0%| | 3467/1000000 [17:05<81:18:10,  3.40it/s, grad_norm=0.396, loss_final=0.9, loss_mean=0.853, loss_mean_cls=0.04[[34m2025-10-04 12:15:22[39m] Step: 3467, Training Logs: loss_final: 0.905108, loss_mean: 0.860467, loss_mean_cls: 0.044641, grad_norm: 0.318386
Steps:   0%| | 3474/1000000 [17:07<83:01:16,  3.33it/s, grad_norm=0.402, loss_final=0.903, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:15:24[39m] Step: 3474, Training Logs: loss_final: 0.902942, loss_mean: 0.857629, loss_mean_cls: 0.045313, grad_norm: 0.380809
Steps:   0%| | 3481/1000000 [17:09<80:43:16,  3.43it/s, grad_norm=0.511, loss_final=0.916, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:15:26[39m] Step: 3481, Training Logs: loss_final: 0.878749, loss_mean: 0.832611, loss_mean_cls: 0.046138, grad_norm: 0.396024
Steps:   0%| | 3488/1000000 [17:11<81:40:14,  3.39it/s, grad_norm=0.271, loss_final=0.912, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:15:28[39m] Step: 3488, Training Logs: loss_final: 0.921205, loss_mean: 0.875338, loss_mean_cls: 0.045866, grad_norm: 0.420848
Steps:   0%| | 3495/1000000 [17:13<81:20:24,  3.40it/s, grad_norm=0.383, loss_final=0.909, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:15:30[39m] Step: 3495, Training Logs: loss_final: 0.898099, loss_mean: 0.851563, loss_mean_cls: 0.046535, grad_norm: 0.344134
Steps:   0%| | 3502/1000000 [17:15<82:43:58,  3.35it/s, grad_norm=0.453, loss_final=0.901, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:15:32[39m] Step: 3502, Training Logs: loss_final: 0.891336, loss_mean: 0.845108, loss_mean_cls: 0.046228, grad_norm: 0.429769
Steps:   0%| | 3509/1000000 [17:17<80:47:13,  3.43it/s, grad_norm=0.387, loss_final=0.888, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:15:34[39m] Step: 3509, Training Logs: loss_final: 0.896695, loss_mean: 0.849332, loss_mean_cls: 0.047362, grad_norm: 0.392720
Steps:   0%| | 3515/1000000 [17:19<80:48:17,  3.43it/s, grad_norm=0.309, loss_final=0.875, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:15:34[39m] Step: 3509, Training Logs: loss_final: 0.896695, loss_mean: 0.849332, loss_mean_cls: 0.047362, grad_norm: 0.392720
Steps:   0%| | 3522/1000000 [17:21<84:17:55,  3.28it/s, grad_norm=0.565, loss_final=0.903, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:15:36[39m] Step: 3516, Training Logs: loss_final: 0.910499, loss_mean: 0.864270, loss_mean_cls: 0.046228, grad_norm: 0.400001
Steps:   0%| | 3529/1000000 [17:24<83:03:37,  3.33it/s, grad_norm=0.545, loss_final=0.904, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:15:38[39m] Step: 3523, Training Logs: loss_final: 0.924202, loss_mean: 0.877471, loss_mean_cls: 0.046731, grad_norm: 0.445081
Steps:   0%| | 3530/1000000 [17:24<83:37:12,  3.31it/s, grad_norm=0.545, loss_final=0.904, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:15:40[39m] Step: 3530, Training Logs: loss_final: 0.900224, loss_mean: 0.853643, loss_mean_cls: 0.046582, grad_norm: 0.406256
Steps:   0%| | 3537/1000000 [17:26<83:24:03,  3.32it/s, grad_norm=0.407, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.0[[34m2025-10-04 12:15:43[39m] Step: 3537, Training Logs: loss_final: 0.908236, loss_mean: 0.862946, loss_mean_cls: 0.045290, grad_norm: 0.372643
Steps:   0%| | 3544/1000000 [17:28<81:44:26,  3.39it/s, grad_norm=0.357, loss_final=0.898, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:15:45[39m] Step: 3544, Training Logs: loss_final: 0.926194, loss_mean: 0.880043, loss_mean_cls: 0.046151, grad_norm: 0.527083
Steps:   0%| | 3551/1000000 [17:30<82:19:06,  3.36it/s, grad_norm=0.484, loss_final=0.903, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:15:47[39m] Step: 3551, Training Logs: loss_final: 0.893862, loss_mean: 0.847536, loss_mean_cls: 0.046326, grad_norm: 0.369233
Steps:   0%| | 3558/1000000 [17:32<82:02:10,  3.37it/s, grad_norm=0.457, loss_final=0.896, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:15:49[39m] Step: 3558, Training Logs: loss_final: 0.891433, loss_mean: 0.843892, loss_mean_cls: 0.047541, grad_norm: 0.653510
Steps:   0%| | 3565/1000000 [17:34<82:15:21,  3.36it/s, grad_norm=0.425, loss_final=0.901, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:15:51[39m] Step: 3565, Training Logs: loss_final: 0.882740, loss_mean: 0.837405, loss_mean_cls: 0.045334, grad_norm: 0.437075
Steps:   0%| | 3572/1000000 [17:36<81:36:36,  3.39it/s, grad_norm=0.6, loss_final=0.895, loss_mean=0.848, loss_mean_cls=0.04[[34m2025-10-04 12:15:53[39m] Step: 3572, Training Logs: loss_final: 0.875894, loss_mean: 0.828362, loss_mean_cls: 0.047532, grad_norm: 0.428402
Steps:   0%| | 3579/1000000 [17:38<83:36:29,  3.31it/s, grad_norm=0.533, loss_final=0.891, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:15:55[39m] Step: 3579, Training Logs: loss_final: 0.887251, loss_mean: 0.840941, loss_mean_cls: 0.046310, grad_norm: 0.525784
Steps:   0%| | 3586/1000000 [17:41<82:50:45,  3.34it/s, grad_norm=0.527, loss_final=0.916, loss_mean=0.871, loss_mean_cls=0.[[34m2025-10-04 12:15:57[39m] Step: 3586, Training Logs: loss_final: 0.899889, loss_mean: 0.854383, loss_mean_cls: 0.045507, grad_norm: 0.360510
Steps:   0%| | 3593/1000000 [17:43<80:56:10,  3.42it/s, grad_norm=0.423, loss_final=0.901, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:15:59[39m] Step: 3593, Training Logs: loss_final: 0.889008, loss_mean: 0.843920, loss_mean_cls: 0.045089, grad_norm: 0.417611
Steps:   0%| | 3600/1000000 [17:45<80:07:57,  3.45it/s, grad_norm=0.398, loss_final=0.893, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:16:01[39m] Step: 3600, Training Logs: loss_final: 0.913420, loss_mean: 0.866512, loss_mean_cls: 0.046908, grad_norm: 0.420829
Steps:   0%| | 3607/1000000 [17:47<80:34:11,  3.44it/s, grad_norm=0.393, loss_final=0.891, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:16:03[39m] Step: 3607, Training Logs: loss_final: 0.902122, loss_mean: 0.856292, loss_mean_cls: 0.045830, grad_norm: 0.438373
Steps:   0%| | 3614/1000000 [17:49<81:23:52,  3.40it/s, grad_norm=0.629, loss_final=0.906, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:16:05[39m] Step: 3614, Training Logs: loss_final: 0.920271, loss_mean: 0.874262, loss_mean_cls: 0.046009, grad_norm: 0.715074
Steps:   0%| | 3621/1000000 [17:51<79:58:37,  3.46it/s, grad_norm=0.662, loss_final=0.897, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:16:07[39m] Step: 3621, Training Logs: loss_final: 0.887856, loss_mean: 0.842078, loss_mean_cls: 0.045779, grad_norm: 0.526729
Steps:   0%| | 3628/1000000 [17:53<80:50:37,  3.42it/s, grad_norm=0.416, loss_final=0.891, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:16:09[39m] Step: 3628, Training Logs: loss_final: 0.914906, loss_mean: 0.868655, loss_mean_cls: 0.046251, grad_norm: 0.877111
Steps:   0%| | 3635/1000000 [17:55<82:21:41,  3.36it/s, grad_norm=0.815, loss_final=0.937, loss_mean=0.893, loss_mean_cls=0.[[34m2025-10-04 12:16:12[39m] Step: 3635, Training Logs: loss_final: 0.873716, loss_mean: 0.826409, loss_mean_cls: 0.047306, grad_norm: 0.499873
Steps:   0%| | 3642/1000000 [17:57<82:57:45,  3.34it/s, grad_norm=0.82, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.04[[34m2025-10-04 12:16:14[39m] Step: 3642, Training Logs: loss_final: 0.909380, loss_mean: 0.863244, loss_mean_cls: 0.046136, grad_norm: 0.676749
Steps:   0%| | 3649/1000000 [17:59<80:47:17,  3.43it/s, grad_norm=0.524, loss_final=0.885, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:16:16[39m] Step: 3649, Training Logs: loss_final: 0.898900, loss_mean: 0.852573, loss_mean_cls: 0.046327, grad_norm: 0.800585
Steps:   0%| | 3656/1000000 [18:01<79:05:41,  3.50it/s, grad_norm=0.532, loss_final=0.898, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:16:18[39m] Step: 3656, Training Logs: loss_final: 0.876772, loss_mean: 0.831062, loss_mean_cls: 0.045710, grad_norm: 0.432825
Steps:   0%| | 3663/1000000 [18:03<81:14:52,  3.41it/s, grad_norm=0.341, loss_final=0.91, loss_mean=0.865, loss_mean_cls=0.0[[34m2025-10-04 12:16:20[39m] Step: 3663, Training Logs: loss_final: 0.922254, loss_mean: 0.875670, loss_mean_cls: 0.046585, grad_norm: 0.338075
Steps:   0%| | 3670/1000000 [18:05<82:01:17,  3.37it/s, grad_norm=0.518, loss_final=0.885, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:16:22[39m] Step: 3670, Training Logs: loss_final: 0.897252, loss_mean: 0.851447, loss_mean_cls: 0.045805, grad_norm: 0.444036
Steps:   0%| | 3677/1000000 [18:07<83:31:22,  3.31it/s, grad_norm=0.385, loss_final=0.924, loss_mean=0.879, loss_mean_cls=0.[[34m2025-10-04 12:16:24[39m] Step: 3677, Training Logs: loss_final: 0.910430, loss_mean: 0.863913, loss_mean_cls: 0.046518, grad_norm: 0.493549
Steps:   0%| | 3684/1000000 [18:09<80:23:04,  3.44it/s, grad_norm=0.456, loss_final=0.877, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:16:26[39m] Step: 3684, Training Logs: loss_final: 0.903493, loss_mean: 0.857074, loss_mean_cls: 0.046419, grad_norm: 0.299973
Steps:   0%| | 3691/1000000 [18:11<82:32:49,  3.35it/s, grad_norm=0.463, loss_final=0.907, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:16:28[39m] Step: 3691, Training Logs: loss_final: 0.901609, loss_mean: 0.856016, loss_mean_cls: 0.045593, grad_norm: 0.412504
Steps:   0%| | 3698/1000000 [18:13<80:38:33,  3.43it/s, grad_norm=0.459, loss_final=0.881, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:16:30[39m] Step: 3698, Training Logs: loss_final: 0.898972, loss_mean: 0.853259, loss_mean_cls: 0.045714, grad_norm: 0.439549
Steps:   0%| | 3705/1000000 [18:15<80:23:48,  3.44it/s, grad_norm=0.55, loss_final=0.915, loss_mean=0.87, loss_mean_cls=0.04[[34m2025-10-04 12:16:32[39m] Step: 3705, Training Logs: loss_final: 0.887472, loss_mean: 0.841186, loss_mean_cls: 0.046286, grad_norm: 0.339475
Steps:   0%| | 3711/1000000 [18:17<83:39:28,  3.31it/s, grad_norm=0.4, loss_final=0.886, loss_mean=0.838, loss_mean_cls=0.04[[34m2025-10-04 12:16:32[39m] Step: 3705, Training Logs: loss_final: 0.887472, loss_mean: 0.841186, loss_mean_cls: 0.046286, grad_norm: 0.339475
Steps:   0%| | 3718/1000000 [18:19<81:55:34,  3.38it/s, grad_norm=0.487, loss_final=0.902, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:16:34[39m] Step: 3712, Training Logs: loss_final: 0.919439, loss_mean: 0.873934, loss_mean_cls: 0.045504, grad_norm: 0.497734
Steps:   0%| | 3725/1000000 [18:21<81:15:28,  3.41it/s, grad_norm=0.387, loss_final=0.885, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:16:36[39m] Step: 3719, Training Logs: loss_final: 0.917172, loss_mean: 0.871251, loss_mean_cls: 0.045921, grad_norm: 0.506901
Steps:   0%| | 3732/1000000 [18:24<83:27:16,  3.32it/s, grad_norm=0.654, loss_final=0.918, loss_mean=0.873, loss_mean_cls=0.[[34m2025-10-04 12:16:38[39m] Step: 3726, Training Logs: loss_final: 0.894056, loss_mean: 0.847180, loss_mean_cls: 0.046876, grad_norm: 0.424457
Steps:   0%| | 3739/1000000 [18:26<81:05:22,  3.41it/s, grad_norm=0.394, loss_final=0.899, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:16:41[39m] Step: 3733, Training Logs: loss_final: 0.911968, loss_mean: 0.865474, loss_mean_cls: 0.046494, grad_norm: 0.391112
Steps:   0%| | 3746/1000000 [18:28<81:18:25,  3.40it/s, grad_norm=0.393, loss_final=0.871, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:16:43[39m] Step: 3740, Training Logs: loss_final: 0.880169, loss_mean: 0.833589, loss_mean_cls: 0.046581, grad_norm: 0.277812
Steps:   0%| | 3747/1000000 [18:28<83:16:49,  3.32it/s, grad_norm=0.393, loss_final=0.871, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:16:45[39m] Step: 3747, Training Logs: loss_final: 0.909013, loss_mean: 0.862701, loss_mean_cls: 0.046312, grad_norm: 0.390937
Steps:   0%| | 3754/1000000 [18:30<81:27:26,  3.40it/s, grad_norm=0.357, loss_final=0.903, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:16:47[39m] Step: 3754, Training Logs: loss_final: 0.894551, loss_mean: 0.848572, loss_mean_cls: 0.045979, grad_norm: 0.334236
Steps:   0%| | 3761/1000000 [18:32<80:32:41,  3.44it/s, grad_norm=0.398, loss_final=0.906, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:16:49[39m] Step: 3761, Training Logs: loss_final: 0.879183, loss_mean: 0.832920, loss_mean_cls: 0.046263, grad_norm: 0.522094
Steps:   0%| | 3768/1000000 [18:34<80:24:03,  3.44it/s, grad_norm=0.581, loss_final=0.901, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:16:51[39m] Step: 3768, Training Logs: loss_final: 0.910952, loss_mean: 0.863814, loss_mean_cls: 0.047138, grad_norm: 0.874067
Steps:   0%| | 3775/1000000 [18:36<82:01:14,  3.37it/s, grad_norm=0.544, loss_final=0.898, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:16:53[39m] Step: 3775, Training Logs: loss_final: 0.888446, loss_mean: 0.840984, loss_mean_cls: 0.047463, grad_norm: 0.347501
Steps:   0%| | 3782/1000000 [18:38<80:14:00,  3.45it/s, grad_norm=0.383, loss_final=0.907, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:16:55[39m] Step: 3782, Training Logs: loss_final: 0.909941, loss_mean: 0.863653, loss_mean_cls: 0.046288, grad_norm: 0.463657
Steps:   0%| | 3789/1000000 [18:40<83:22:42,  3.32it/s, grad_norm=0.406, loss_final=0.901, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:16:57[39m] Step: 3789, Training Logs: loss_final: 0.893895, loss_mean: 0.847508, loss_mean_cls: 0.046387, grad_norm: 0.627689
Steps:   0%| | 3796/1000000 [18:42<81:43:05,  3.39it/s, grad_norm=0.434, loss_final=0.913, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:16:59[39m] Step: 3796, Training Logs: loss_final: 0.887318, loss_mean: 0.842272, loss_mean_cls: 0.045046, grad_norm: 0.453848
Steps:   0%| | 3802/1000000 [18:44<80:49:57,  3.42it/s, grad_norm=0.264, loss_final=0.896, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:17:01[39m] Step: 3802, Training Logs: loss_final: 0.876493, loss_mean: 0.829616, loss_mean_cls: 0.046877, grad_norm: 0.357988
Steps:   0%| | 3809/1000000 [18:46<81:38:52,  3.39it/s, grad_norm=0.418, loss_final=0.888, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:17:03[39m] Step: 3809, Training Logs: loss_final: 0.897558, loss_mean: 0.852616, loss_mean_cls: 0.044942, grad_norm: 0.479989
Steps:   0%| | 3816/1000000 [18:48<81:52:32,  3.38it/s, grad_norm=0.439, loss_final=0.871, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:17:05[39m] Step: 3816, Training Logs: loss_final: 0.907521, loss_mean: 0.862099, loss_mean_cls: 0.045423, grad_norm: 0.495214
Steps:   0%| | 3823/1000000 [18:50<82:42:37,  3.35it/s, grad_norm=0.417, loss_final=0.908, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:17:07[39m] Step: 3823, Training Logs: loss_final: 0.899291, loss_mean: 0.853495, loss_mean_cls: 0.045796, grad_norm: 0.434065
Steps:   0%| | 3830/1000000 [18:52<83:02:18,  3.33it/s, grad_norm=0.375, loss_final=0.898, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:17:09[39m] Step: 3830, Training Logs: loss_final: 0.895729, loss_mean: 0.850055, loss_mean_cls: 0.045675, grad_norm: 0.411768
Steps:   0%| | 3836/1000000 [18:54<81:13:17,  3.41it/s, grad_norm=0.427, loss_final=0.902, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:17:11[39m] Step: 3836, Training Logs: loss_final: 0.899631, loss_mean: 0.854004, loss_mean_cls: 0.045628, grad_norm: 0.429982
Steps:   0%| | 3843/1000000 [18:56<80:24:37,  3.44it/s, grad_norm=0.35, loss_final=0.896, loss_mean=0.849, loss_mean_cls=0.0[[34m2025-10-04 12:17:13[39m] Step: 3843, Training Logs: loss_final: 0.903648, loss_mean: 0.857740, loss_mean_cls: 0.045908, grad_norm: 0.361877
Steps:   0%| | 3850/1000000 [18:58<82:32:24,  3.35it/s, grad_norm=0.41, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.04[[34m2025-10-04 12:17:15[39m] Step: 3850, Training Logs: loss_final: 0.923524, loss_mean: 0.878679, loss_mean_cls: 0.044845, grad_norm: 0.497427
Steps:   0%| | 3857/1000000 [19:00<83:26:53,  3.32it/s, grad_norm=0.517, loss_final=0.897, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:17:17[39m] Step: 3857, Training Logs: loss_final: 0.889230, loss_mean: 0.843801, loss_mean_cls: 0.045429, grad_norm: 0.473267
Steps:   0%| | 3864/1000000 [19:03<81:45:06,  3.38it/s, grad_norm=0.389, loss_final=0.896, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:17:19[39m] Step: 3864, Training Logs: loss_final: 0.891578, loss_mean: 0.845575, loss_mean_cls: 0.046003, grad_norm: 0.313417
Steps:   0%| | 3870/1000000 [19:04<83:06:04,  3.33it/s, grad_norm=0.466, loss_final=0.904, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:17:21[39m] Step: 3870, Training Logs: loss_final: 0.877259, loss_mean: 0.830143, loss_mean_cls: 0.047116, grad_norm: 0.320289
Steps:   0%| | 3877/1000000 [19:06<84:46:22,  3.26it/s, grad_norm=0.396, loss_final=0.908, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:17:23[39m] Step: 3877, Training Logs: loss_final: 0.875695, loss_mean: 0.829629, loss_mean_cls: 0.046065, grad_norm: 0.268006
Steps:   0%| | 3884/1000000 [19:08<80:17:36,  3.45it/s, grad_norm=0.491, loss_final=0.901, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:17:25[39m] Step: 3884, Training Logs: loss_final: 0.886871, loss_mean: 0.841448, loss_mean_cls: 0.045423, grad_norm: 0.318460
Steps:   0%| | 3889/1000000 [19:10<82:23:22,  3.36it/s, grad_norm=0.534, loss_final=0.896, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:17:27[39m] Step: 3889, Training Logs: loss_final: 0.877905, loss_mean: 0.831550, loss_mean_cls: 0.046355, grad_norm: 0.423424
Steps:   0%| | 3896/1000000 [19:12<81:33:24,  3.39it/s, grad_norm=0.722, loss_final=0.911, loss_mean=0.865, loss_mean_cls=0.[[34m2025-10-04 12:17:29[39m] Step: 3896, Training Logs: loss_final: 0.905790, loss_mean: 0.860633, loss_mean_cls: 0.045157, grad_norm: 0.593280
Steps:   0%| | 3903/1000000 [19:14<82:59:10,  3.33it/s, grad_norm=0.441, loss_final=0.906, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:17:31[39m] Step: 3903, Training Logs: loss_final: 0.921920, loss_mean: 0.877029, loss_mean_cls: 0.044891, grad_norm: 0.338828
Steps:   0%| | 3910/1000000 [19:16<81:58:17,  3.38it/s, grad_norm=0.52, loss_final=0.891, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:17:33[39m] Step: 3910, Training Logs: loss_final: 0.892809, loss_mean: 0.846480, loss_mean_cls: 0.046330, grad_norm: 0.491778
Steps:   0%| | 3917/1000000 [19:18<81:15:49,  3.40it/s, grad_norm=0.402, loss_final=0.889, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:17:35[39m] Step: 3917, Training Logs: loss_final: 0.899857, loss_mean: 0.853420, loss_mean_cls: 0.046436, grad_norm: 0.527606
Steps:   0%| | 3923/1000000 [19:20<82:35:33,  3.35it/s, grad_norm=0.552, loss_final=0.89, loss_mean=0.843, loss_mean_cls=0.0[[34m2025-10-04 12:17:37[39m] Step: 3923, Training Logs: loss_final: 0.896886, loss_mean: 0.852422, loss_mean_cls: 0.044463, grad_norm: 0.389598
Steps:   0%| | 3930/1000000 [19:22<81:46:27,  3.38it/s, grad_norm=0.41, loss_final=0.898, loss_mean=0.854, loss_mean_cls=0.0[[34m2025-10-04 12:17:39[39m] Step: 3930, Training Logs: loss_final: 0.878566, loss_mean: 0.832149, loss_mean_cls: 0.046417, grad_norm: 0.456392
Steps:   0%| | 3937/1000000 [19:24<80:51:19,  3.42it/s, grad_norm=0.413, loss_final=0.899, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:17:41[39m] Step: 3937, Training Logs: loss_final: 0.889676, loss_mean: 0.843074, loss_mean_cls: 0.046602, grad_norm: 0.441062
Steps:   0%| | 3944/1000000 [19:26<81:13:24,  3.41it/s, grad_norm=0.42, loss_final=0.89, loss_mean=0.843, loss_mean_cls=0.04[[34m2025-10-04 12:17:43[39m] Step: 3944, Training Logs: loss_final: 0.894266, loss_mean: 0.848078, loss_mean_cls: 0.046188, grad_norm: 0.309984
Steps:   0%| | 3951/1000000 [19:28<83:05:16,  3.33it/s, grad_norm=0.38, loss_final=0.901, loss_mean=0.855, loss_mean_cls=0.0[[34m2025-10-04 12:17:45[39m] Step: 3951, Training Logs: loss_final: 0.911338, loss_mean: 0.866421, loss_mean_cls: 0.044917, grad_norm: 0.450754
Steps:   0%| | 3963/1000000 [19:32<82:20:10,  3.36it/s, grad_norm=0.323, loss_final=0.894, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:17:47[39m] Step: 3957, Training Logs: loss_final: 0.888158, loss_mean: 0.842495, loss_mean_cls: 0.045663, grad_norm: 0.365154
Steps:   0%| | 3964/1000000 [19:32<82:56:25,  3.34it/s, grad_norm=0.323, loss_final=0.894, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:17:49[39m] Step: 3964, Training Logs: loss_final: 0.905164, loss_mean: 0.857426, loss_mean_cls: 0.047739, grad_norm: 0.489174
Steps:   0%| | 3971/1000000 [19:34<81:34:59,  3.39it/s, grad_norm=0.325, loss_final=0.909, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:17:51[39m] Step: 3971, Training Logs: loss_final: 0.912199, loss_mean: 0.866111, loss_mean_cls: 0.046088, grad_norm: 0.548967
Steps:   0%| | 3978/1000000 [19:36<83:43:37,  3.30it/s, grad_norm=0.512, loss_final=0.894, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:17:53[39m] Step: 3978, Training Logs: loss_final: 0.899771, loss_mean: 0.854165, loss_mean_cls: 0.045606, grad_norm: 0.407439
Steps:   0%| | 3985/1000000 [19:38<81:43:11,  3.39it/s, grad_norm=0.295, loss_final=0.911, loss_mean=0.866, loss_mean_cls=0.[[34m2025-10-04 12:17:55[39m] Step: 3985, Training Logs: loss_final: 0.896310, loss_mean: 0.850531, loss_mean_cls: 0.045779, grad_norm: 0.286086
Steps:   0%| | 3992/1000000 [19:40<80:51:42,  3.42it/s, grad_norm=0.347, loss_final=0.883, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:17:57[39m] Step: 3992, Training Logs: loss_final: 0.896604, loss_mean: 0.850434, loss_mean_cls: 0.046170, grad_norm: 0.385562
Steps:   0%| | 3999/1000000 [19:42<82:46:25,  3.34it/s, grad_norm=0.453, loss_final=0.901, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:17:59[39m] Step: 3999, Training Logs: loss_final: 0.872758, loss_mean: 0.826687, loss_mean_cls: 0.046070, grad_norm: 0.332869
Steps:   0%| | 4005/1000000 [19:44<81:19:19,  3.40it/s, grad_norm=0.283, loss_final=0.889, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:18:01[39m] Step: 4005, Training Logs: loss_final: 0.881453, loss_mean: 0.834962, loss_mean_cls: 0.046490, grad_norm: 0.360099
Steps:   0%| | 4012/1000000 [19:46<82:49:23,  3.34it/s, grad_norm=0.472, loss_final=0.907, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:18:03[39m] Step: 4012, Training Logs: loss_final: 0.917054, loss_mean: 0.872685, loss_mean_cls: 0.044369, grad_norm: 0.394447
Steps:   0%| | 4019/1000000 [19:48<81:30:47,  3.39it/s, grad_norm=0.511, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:18:05[39m] Step: 4019, Training Logs: loss_final: 0.907910, loss_mean: 0.861656, loss_mean_cls: 0.046254, grad_norm: 0.319626
Steps:   0%| | 4026/1000000 [19:50<82:38:22,  3.35it/s, grad_norm=0.375, loss_final=0.889, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:18:07[39m] Step: 4026, Training Logs: loss_final: 0.887311, loss_mean: 0.842822, loss_mean_cls: 0.044488, grad_norm: 0.342026
Steps:   0%| | 4033/1000000 [19:53<83:28:48,  3.31it/s, grad_norm=0.447, loss_final=0.893, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:18:09[39m] Step: 4033, Training Logs: loss_final: 0.917311, loss_mean: 0.872327, loss_mean_cls: 0.044984, grad_norm: 0.586637
Steps:   0%| | 4039/1000000 [19:54<81:18:29,  3.40it/s, grad_norm=0.421, loss_final=0.879, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:18:11[39m] Step: 4039, Training Logs: loss_final: 0.896671, loss_mean: 0.851426, loss_mean_cls: 0.045246, grad_norm: 0.380907
Steps:   0%| | 4046/1000000 [19:56<80:19:25,  3.44it/s, grad_norm=0.286, loss_final=0.893, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:18:13[39m] Step: 4046, Training Logs: loss_final: 0.911825, loss_mean: 0.865103, loss_mean_cls: 0.046721, grad_norm: 0.444443
Steps:   0%| | 4053/1000000 [19:58<81:27:40,  3.40it/s, grad_norm=0.294, loss_final=0.886, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:18:15[39m] Step: 4053, Training Logs: loss_final: 0.891020, loss_mean: 0.845271, loss_mean_cls: 0.045749, grad_norm: 0.514715
Steps:   0%| | 4060/1000000 [20:00<80:37:45,  3.43it/s, grad_norm=0.39, loss_final=0.902, loss_mean=0.856, loss_mean_cls=0.0[[34m2025-10-04 12:18:17[39m] Step: 4060, Training Logs: loss_final: 0.889488, loss_mean: 0.843311, loss_mean_cls: 0.046177, grad_norm: 0.691420
Steps:   0%| | 4067/1000000 [20:03<84:12:51,  3.29it/s, grad_norm=0.476, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:18:19[39m] Step: 4067, Training Logs: loss_final: 0.884427, loss_mean: 0.838739, loss_mean_cls: 0.045688, grad_norm: 0.621025
Steps:   0%| | 4073/1000000 [20:04<81:52:10,  3.38it/s, grad_norm=0.469, loss_final=0.921, loss_mean=0.875, loss_mean_cls=0.[[34m2025-10-04 12:18:21[39m] Step: 4073, Training Logs: loss_final: 0.892861, loss_mean: 0.847064, loss_mean_cls: 0.045797, grad_norm: 0.397413
Steps:   0%| | 4080/1000000 [20:06<82:07:02,  3.37it/s, grad_norm=0.446, loss_final=0.907, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:18:23[39m] Step: 4080, Training Logs: loss_final: 0.876038, loss_mean: 0.829332, loss_mean_cls: 0.046705, grad_norm: 0.341988
Steps:   0%| | 4087/1000000 [20:09<82:45:34,  3.34it/s, grad_norm=0.456, loss_final=0.909, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:18:25[39m] Step: 4087, Training Logs: loss_final: 0.886889, loss_mean: 0.840894, loss_mean_cls: 0.045995, grad_norm: 0.289514
Steps:   0%| | 4094/1000000 [20:11<81:28:14,  3.40it/s, grad_norm=0.408, loss_final=0.889, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:18:27[39m] Step: 4094, Training Logs: loss_final: 0.898982, loss_mean: 0.852993, loss_mean_cls: 0.045989, grad_norm: 0.275549
Steps:   0%| | 4101/1000000 [20:13<80:23:29,  3.44it/s, grad_norm=0.302, loss_final=0.89, loss_mean=0.843, loss_mean_cls=0.0[[34m2025-10-04 12:18:29[39m] Step: 4101, Training Logs: loss_final: 0.877002, loss_mean: 0.832699, loss_mean_cls: 0.044303, grad_norm: 0.322977
Steps:   0%| | 4107/1000000 [20:14<82:48:02,  3.34it/s, grad_norm=0.363, loss_final=0.889, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:18:31[39m] Step: 4107, Training Logs: loss_final: 0.892824, loss_mean: 0.847165, loss_mean_cls: 0.045659, grad_norm: 0.339130
Steps:   0%| | 4114/1000000 [20:17<82:43:03,  3.34it/s, grad_norm=0.573, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.0[[34m2025-10-04 12:18:33[39m] Step: 4114, Training Logs: loss_final: 0.911363, loss_mean: 0.866711, loss_mean_cls: 0.044651, grad_norm: 0.459875
Steps:   0%| | 4121/1000000 [20:19<81:26:04,  3.40it/s, grad_norm=0.41, loss_final=0.865, loss_mean=0.818, loss_mean_cls=0.0[[34m2025-10-04 12:18:35[39m] Step: 4121, Training Logs: loss_final: 0.891795, loss_mean: 0.844836, loss_mean_cls: 0.046959, grad_norm: 0.332177
Steps:   0%| | 4128/1000000 [20:21<79:44:04,  3.47it/s, grad_norm=0.507, loss_final=0.9, loss_mean=0.854, loss_mean_cls=0.04[[34m2025-10-04 12:18:37[39m] Step: 4128, Training Logs: loss_final: 0.913907, loss_mean: 0.869553, loss_mean_cls: 0.044354, grad_norm: 0.494671
Steps:   0%| | 4135/1000000 [20:23<80:34:51,  3.43it/s, grad_norm=0.512, loss_final=0.903, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:18:39[39m] Step: 4135, Training Logs: loss_final: 0.900189, loss_mean: 0.854618, loss_mean_cls: 0.045571, grad_norm: 0.391349
Steps:   0%| | 4142/1000000 [20:25<82:19:26,  3.36it/s, grad_norm=0.518, loss_final=0.899, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:18:41[39m] Step: 4142, Training Logs: loss_final: 0.873304, loss_mean: 0.828285, loss_mean_cls: 0.045019, grad_norm: 0.504419
Steps:   0%| | 4149/1000000 [20:27<80:03:08,  3.46it/s, grad_norm=0.419, loss_final=0.913, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:18:43[39m] Step: 4149, Training Logs: loss_final: 0.912873, loss_mean: 0.867710, loss_mean_cls: 0.045162, grad_norm: 0.326187
Steps:   0%| | 4156/1000000 [20:29<84:58:40,  3.26it/s, grad_norm=0.372, loss_final=0.883, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:18:46[39m] Step: 4156, Training Logs: loss_final: 0.898269, loss_mean: 0.853022, loss_mean_cls: 0.045246, grad_norm: 0.300404
Steps:   0%| | 4162/1000000 [20:31<81:24:26,  3.40it/s, grad_norm=0.298, loss_final=0.899, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:18:47[39m] Step: 4162, Training Logs: loss_final: 0.895213, loss_mean: 0.849406, loss_mean_cls: 0.045806, grad_norm: 0.435432
Steps:   0%| | 4167/1000000 [20:32<81:42:15,  3.39it/s, grad_norm=0.287, loss_final=0.901, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:18:49[39m] Step: 4167, Training Logs: loss_final: 0.874373, loss_mean: 0.828656, loss_mean_cls: 0.045716, grad_norm: 0.433960
Steps:   0%| | 4174/1000000 [20:34<83:18:21,  3.32it/s, grad_norm=0.463, loss_final=0.889, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:18:51[39m] Step: 4174, Training Logs: loss_final: 0.880004, loss_mean: 0.835310, loss_mean_cls: 0.044694, grad_norm: 0.379348
Steps:   0%| | 4181/1000000 [20:36<80:28:20,  3.44it/s, grad_norm=0.452, loss_final=0.899, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:18:53[39m] Step: 4181, Training Logs: loss_final: 0.912717, loss_mean: 0.868051, loss_mean_cls: 0.044666, grad_norm: 0.356694
Steps:   0%| | 4188/1000000 [20:38<79:52:30,  3.46it/s, grad_norm=0.336, loss_final=0.869, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:18:55[39m] Step: 4188, Training Logs: loss_final: 0.911389, loss_mean: 0.865549, loss_mean_cls: 0.045840, grad_norm: 0.385954
Steps:   0%| | 4195/1000000 [20:40<81:01:10,  3.41it/s, grad_norm=0.435, loss_final=0.899, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:18:57[39m] Step: 4195, Training Logs: loss_final: 0.900605, loss_mean: 0.856070, loss_mean_cls: 0.044535, grad_norm: 0.428090
Steps:   0%| | 4202/1000000 [20:42<82:24:56,  3.36it/s, grad_norm=0.326, loss_final=0.914, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:18:59[39m] Step: 4202, Training Logs: loss_final: 0.892806, loss_mean: 0.847319, loss_mean_cls: 0.045487, grad_norm: 0.336817
Steps:   0%| | 4208/1000000 [20:44<82:42:29,  3.34it/s, grad_norm=0.365, loss_final=0.889, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:19:01[39m] Step: 4208, Training Logs: loss_final: 0.905884, loss_mean: 0.860753, loss_mean_cls: 0.045131, grad_norm: 0.481583
Steps:   0%| | 4215/1000000 [20:46<80:56:07,  3.42it/s, grad_norm=0.472, loss_final=0.893, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:19:03[39m] Step: 4215, Training Logs: loss_final: 0.876809, loss_mean: 0.830677, loss_mean_cls: 0.046132, grad_norm: 0.635979
Steps:   0%| | 4222/1000000 [20:48<81:52:05,  3.38it/s, grad_norm=0.442, loss_final=0.904, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:19:05[39m] Step: 4222, Training Logs: loss_final: 0.871098, loss_mean: 0.825461, loss_mean_cls: 0.045636, grad_norm: 0.449940
Steps:   0%| | 4229/1000000 [20:50<81:44:46,  3.38it/s, grad_norm=0.361, loss_final=0.91, loss_mean=0.865, loss_mean_cls=0.0[[34m2025-10-04 12:19:07[39m] Step: 4229, Training Logs: loss_final: 0.894106, loss_mean: 0.849169, loss_mean_cls: 0.044937, grad_norm: 0.303186
Steps:   0%| | 4236/1000000 [20:53<82:21:22,  3.36it/s, grad_norm=0.474, loss_final=0.881, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:19:09[39m] Step: 4236, Training Logs: loss_final: 0.881540, loss_mean: 0.835040, loss_mean_cls: 0.046500, grad_norm: 0.421925
Steps:   0%| | 4242/1000000 [20:54<81:40:45,  3.39it/s, grad_norm=0.516, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:19:11[39m] Step: 4242, Training Logs: loss_final: 0.897208, loss_mean: 0.853251, loss_mean_cls: 0.043957, grad_norm: 0.337171
Steps:   0%| | 4249/1000000 [20:56<82:27:34,  3.35it/s, grad_norm=0.49, loss_final=0.883, loss_mean=0.837, loss_mean_cls=0.0[[34m2025-10-04 12:19:13[39m] Step: 4249, Training Logs: loss_final: 0.895614, loss_mean: 0.850885, loss_mean_cls: 0.044728, grad_norm: 0.305145
Steps:   0%| | 4256/1000000 [20:59<80:38:17,  3.43it/s, grad_norm=0.329, loss_final=0.912, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:19:15[39m] Step: 4256, Training Logs: loss_final: 0.897497, loss_mean: 0.851414, loss_mean_cls: 0.046084, grad_norm: 0.492099
Steps:   0%| | 4263/1000000 [21:01<79:41:52,  3.47it/s, grad_norm=0.34, loss_final=0.892, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:19:17[39m] Step: 4263, Training Logs: loss_final: 0.889717, loss_mean: 0.845899, loss_mean_cls: 0.043818, grad_norm: 0.401055
Steps:   0%| | 4270/1000000 [21:03<80:14:05,  3.45it/s, grad_norm=0.549, loss_final=0.893, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:19:19[39m] Step: 4270, Training Logs: loss_final: 0.879678, loss_mean: 0.833811, loss_mean_cls: 0.045867, grad_norm: 0.299666
Steps:   0%| | 4277/1000000 [21:05<80:55:53,  3.42it/s, grad_norm=0.28, loss_final=0.892, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:19:21[39m] Step: 4277, Training Logs: loss_final: 0.889065, loss_mean: 0.843655, loss_mean_cls: 0.045410, grad_norm: 0.506652
Steps:   0%| | 4284/1000000 [21:07<81:10:41,  3.41it/s, grad_norm=0.375, loss_final=0.918, loss_mean=0.872, loss_mean_cls=0.[[34m2025-10-04 12:19:23[39m] Step: 4284, Training Logs: loss_final: 0.903818, loss_mean: 0.858176, loss_mean_cls: 0.045642, grad_norm: 0.475651
Steps:   0%| | 4291/1000000 [21:09<79:06:46,  3.50it/s, grad_norm=0.356, loss_final=0.89, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:19:25[39m] Step: 4291, Training Logs: loss_final: 0.901074, loss_mean: 0.856302, loss_mean_cls: 0.044771, grad_norm: 0.413907
Steps:   0%| | 4298/1000000 [21:11<81:32:52,  3.39it/s, grad_norm=0.297, loss_final=0.899, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:19:27[39m] Step: 4298, Training Logs: loss_final: 0.890295, loss_mean: 0.845088, loss_mean_cls: 0.045206, grad_norm: 0.505385
Steps:   0%| | 4304/1000000 [21:13<80:54:04,  3.42it/s, grad_norm=0.461, loss_final=0.886, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:19:29[39m] Step: 4304, Training Logs: loss_final: 0.895308, loss_mean: 0.848980, loss_mean_cls: 0.046328, grad_norm: 0.466248
Steps:   0%| | 4311/1000000 [21:15<79:44:17,  3.47it/s, grad_norm=0.433, loss_final=0.91, loss_mean=0.864, loss_mean_cls=0.0[[34m2025-10-04 12:19:31[39m] Step: 4311, Training Logs: loss_final: 0.897823, loss_mean: 0.852806, loss_mean_cls: 0.045017, grad_norm: 0.361414
Steps:   0%| | 4318/1000000 [21:17<79:39:27,  3.47it/s, grad_norm=0.386, loss_final=0.902, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:19:33[39m] Step: 4318, Training Logs: loss_final: 0.897047, loss_mean: 0.851604, loss_mean_cls: 0.045444, grad_norm: 0.346839
Steps:   0%| | 4325/1000000 [21:19<80:20:20,  3.44it/s, grad_norm=0.412, loss_final=0.894, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:19:35[39m] Step: 4325, Training Logs: loss_final: 0.917978, loss_mean: 0.872336, loss_mean_cls: 0.045641, grad_norm: 0.438577
Steps:   0%| | 4332/1000000 [21:21<80:20:51,  3.44it/s, grad_norm=0.457, loss_final=0.896, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:19:37[39m] Step: 4332, Training Logs: loss_final: 0.884047, loss_mean: 0.838019, loss_mean_cls: 0.046029, grad_norm: 0.639296
Steps:   0%| | 4339/1000000 [21:23<81:33:32,  3.39it/s, grad_norm=0.419, loss_final=0.888, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:19:39[39m] Step: 4339, Training Logs: loss_final: 0.891884, loss_mean: 0.845826, loss_mean_cls: 0.046058, grad_norm: 0.350382
Steps:   0%| | 4346/1000000 [21:25<82:30:28,  3.35it/s, grad_norm=0.424, loss_final=0.894, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:19:42[39m] Step: 4346, Training Logs: loss_final: 0.887338, loss_mean: 0.841702, loss_mean_cls: 0.045635, grad_norm: 0.646382
Steps:   0%| | 4352/1000000 [21:27<83:48:08,  3.30it/s, grad_norm=0.273, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:19:43[39m] Step: 4352, Training Logs: loss_final: 0.901167, loss_mean: 0.854224, loss_mean_cls: 0.046943, grad_norm: 0.495566
Steps:   0%| | 4359/1000000 [21:29<81:23:52,  3.40it/s, grad_norm=0.295, loss_final=0.903, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:19:45[39m] Step: 4359, Training Logs: loss_final: 0.896264, loss_mean: 0.850932, loss_mean_cls: 0.045333, grad_norm: 0.341125
Steps:   0%| | 4366/1000000 [21:31<81:39:08,  3.39it/s, grad_norm=0.336, loss_final=0.892, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:19:47[39m] Step: 4366, Training Logs: loss_final: 0.915213, loss_mean: 0.870766, loss_mean_cls: 0.044447, grad_norm: 0.294412
Steps:   0%| | 4373/1000000 [21:33<83:14:17,  3.32it/s, grad_norm=0.278, loss_final=0.905, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:19:50[39m] Step: 4373, Training Logs: loss_final: 0.892072, loss_mean: 0.846809, loss_mean_cls: 0.045263, grad_norm: 0.590264
Steps:   0%| | 4380/1000000 [21:35<80:51:18,  3.42it/s, grad_norm=0.389, loss_final=0.886, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:19:52[39m] Step: 4380, Training Logs: loss_final: 0.893730, loss_mean: 0.848245, loss_mean_cls: 0.045485, grad_norm: 0.457718
Steps:   0%| | 4387/1000000 [21:37<83:11:54,  3.32it/s, grad_norm=0.322, loss_final=0.895, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:19:54[39m] Step: 4387, Training Logs: loss_final: 0.897389, loss_mean: 0.853116, loss_mean_cls: 0.044274, grad_norm: 0.438819
Steps:   0%| | 4394/1000000 [21:39<82:52:33,  3.34it/s, grad_norm=0.277, loss_final=0.895, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:19:56[39m] Step: 4394, Training Logs: loss_final: 0.899665, loss_mean: 0.856466, loss_mean_cls: 0.043199, grad_norm: 0.391834
Steps:   0%| | 4400/1000000 [21:41<82:13:53,  3.36it/s, grad_norm=0.275, loss_final=0.902, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:19:58[39m] Step: 4400, Training Logs: loss_final: 0.879591, loss_mean: 0.833748, loss_mean_cls: 0.045843, grad_norm: 0.371046
Steps:   0%| | 4407/1000000 [21:43<83:13:56,  3.32it/s, grad_norm=0.319, loss_final=0.881, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:20:00[39m] Step: 4407, Training Logs: loss_final: 0.884087, loss_mean: 0.838554, loss_mean_cls: 0.045534, grad_norm: 0.381778
Steps:   0%| | 4412/1000000 [21:44<80:31:51,  3.43it/s, grad_norm=0.425, loss_final=0.901, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:20:01[39m] Step: 4412, Training Logs: loss_final: 0.905183, loss_mean: 0.860358, loss_mean_cls: 0.044825, grad_norm: 0.318014
Steps:   0%| | 4419/1000000 [21:46<81:12:56,  3.41it/s, grad_norm=0.386, loss_final=0.897, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:20:03[39m] Step: 4419, Training Logs: loss_final: 0.883486, loss_mean: 0.837451, loss_mean_cls: 0.046035, grad_norm: 0.349491
Steps:   0%| | 4426/1000000 [21:49<82:16:29,  3.36it/s, grad_norm=0.303, loss_final=0.882, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:20:05[39m] Step: 4426, Training Logs: loss_final: 0.902840, loss_mean: 0.858173, loss_mean_cls: 0.044666, grad_norm: 0.266379
Steps:   0%| | 4433/1000000 [21:51<80:30:47,  3.43it/s, grad_norm=0.345, loss_final=0.894, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:20:07[39m] Step: 4433, Training Logs: loss_final: 0.904720, loss_mean: 0.859671, loss_mean_cls: 0.045049, grad_norm: 0.258458
Steps:   0%| | 4439/1000000 [21:52<82:16:52,  3.36it/s, grad_norm=0.377, loss_final=0.914, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:20:09[39m] Step: 4439, Training Logs: loss_final: 0.890826, loss_mean: 0.845185, loss_mean_cls: 0.045641, grad_norm: 0.512367
Steps:   0%| | 4446/1000000 [21:55<82:26:48,  3.35it/s, grad_norm=0.425, loss_final=0.898, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:20:11[39m] Step: 4446, Training Logs: loss_final: 0.899325, loss_mean: 0.855184, loss_mean_cls: 0.044142, grad_norm: 0.447729
Steps:   0%| | 4453/1000000 [21:57<81:14:53,  3.40it/s, grad_norm=0.42, loss_final=0.905, loss_mean=0.86, loss_mean_cls=0.04[[34m2025-10-04 12:20:13[39m] Step: 4453, Training Logs: loss_final: 0.892304, loss_mean: 0.847909, loss_mean_cls: 0.044394, grad_norm: 0.419067
Steps:   0%| | 4460/1000000 [21:59<82:14:11,  3.36it/s, grad_norm=0.419, loss_final=0.915, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:20:15[39m] Step: 4460, Training Logs: loss_final: 0.905945, loss_mean: 0.860441, loss_mean_cls: 0.045503, grad_norm: 0.535445
Steps:   0%| | 4467/1000000 [22:01<80:28:44,  3.44it/s, grad_norm=0.491, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:20:17[39m] Step: 4467, Training Logs: loss_final: 0.905675, loss_mean: 0.859658, loss_mean_cls: 0.046016, grad_norm: 0.345249
Steps:   0%| | 4474/1000000 [22:03<80:16:10,  3.45it/s, grad_norm=0.317, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:20:19[39m] Step: 4474, Training Logs: loss_final: 0.895520, loss_mean: 0.850317, loss_mean_cls: 0.045204, grad_norm: 0.308729
Steps:   0%| | 4481/1000000 [22:05<80:39:05,  3.43it/s, grad_norm=0.407, loss_final=0.884, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:20:22[39m] Step: 4481, Training Logs: loss_final: 0.884289, loss_mean: 0.838984, loss_mean_cls: 0.045305, grad_norm: 0.277008
Steps:   0%| | 4487/1000000 [22:07<80:32:41,  3.43it/s, grad_norm=0.32, loss_final=0.902, loss_mean=0.856, loss_mean_cls=0.0[[34m2025-10-04 12:20:23[39m] Step: 4487, Training Logs: loss_final: 0.889016, loss_mean: 0.843063, loss_mean_cls: 0.045953, grad_norm: 0.289901
Steps:   0%| | 4494/1000000 [22:09<79:05:44,  3.50it/s, grad_norm=0.402, loss_final=0.898, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:20:25[39m] Step: 4494, Training Logs: loss_final: 0.876144, loss_mean: 0.831105, loss_mean_cls: 0.045039, grad_norm: 0.489556
Steps:   0%| | 4501/1000000 [22:11<81:00:27,  3.41it/s, grad_norm=0.374, loss_final=0.901, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:20:27[39m] Step: 4501, Training Logs: loss_final: 0.882967, loss_mean: 0.837462, loss_mean_cls: 0.045506, grad_norm: 0.473271
Steps:   0%| | 4508/1000000 [22:13<80:52:41,  3.42it/s, grad_norm=0.285, loss_final=0.897, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:20:29[39m] Step: 4508, Training Logs: loss_final: 0.900455, loss_mean: 0.854137, loss_mean_cls: 0.046318, grad_norm: 0.446553
Steps:   0%| | 4515/1000000 [22:15<81:30:55,  3.39it/s, grad_norm=0.326, loss_final=0.904, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:20:31[39m] Step: 4515, Training Logs: loss_final: 0.886160, loss_mean: 0.841211, loss_mean_cls: 0.044949, grad_norm: 0.449679
Steps:   0%| | 4522/1000000 [22:17<79:59:39,  3.46it/s, grad_norm=0.365, loss_final=0.885, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:20:33[39m] Step: 4522, Training Logs: loss_final: 0.897682, loss_mean: 0.853274, loss_mean_cls: 0.044409, grad_norm: 0.304619
Steps:   0%| | 4529/1000000 [22:19<80:23:50,  3.44it/s, grad_norm=0.295, loss_final=0.898, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:20:36[39m] Step: 4529, Training Logs: loss_final: 0.884240, loss_mean: 0.839680, loss_mean_cls: 0.044560, grad_norm: 0.299362
Steps:   0%| | 4536/1000000 [22:21<80:41:35,  3.43it/s, grad_norm=0.397, loss_final=0.866, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:20:38[39m] Step: 4536, Training Logs: loss_final: 0.891961, loss_mean: 0.846061, loss_mean_cls: 0.045900, grad_norm: 0.347526
Steps:   0%| | 4543/1000000 [22:23<81:18:54,  3.40it/s, grad_norm=0.369, loss_final=0.908, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:20:40[39m] Step: 4543, Training Logs: loss_final: 0.906285, loss_mean: 0.861705, loss_mean_cls: 0.044580, grad_norm: 0.373323
Steps:   0%| | 4550/1000000 [22:25<81:14:05,  3.40it/s, grad_norm=0.309, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:20:42[39m] Step: 4550, Training Logs: loss_final: 0.878581, loss_mean: 0.834487, loss_mean_cls: 0.044094, grad_norm: 0.346373
Steps:   0%| | 4556/1000000 [22:27<82:51:48,  3.34it/s, grad_norm=0.454, loss_final=0.886, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:20:43[39m] Step: 4556, Training Logs: loss_final: 0.887612, loss_mean: 0.842378, loss_mean_cls: 0.045234, grad_norm: 0.298624
Steps:   0%| | 4563/1000000 [22:29<80:16:03,  3.44it/s, grad_norm=0.456, loss_final=0.899, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:20:46[39m] Step: 4563, Training Logs: loss_final: 0.901483, loss_mean: 0.856717, loss_mean_cls: 0.044766, grad_norm: 0.313070
Steps:   0%| | 4570/1000000 [22:31<81:10:27,  3.41it/s, grad_norm=0.433, loss_final=0.877, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:20:48[39m] Step: 4570, Training Logs: loss_final: 0.884114, loss_mean: 0.838277, loss_mean_cls: 0.045837, grad_norm: 0.409174
Steps:   0%| | 4577/1000000 [22:33<80:39:17,  3.43it/s, grad_norm=0.435, loss_final=0.869, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:20:50[39m] Step: 4577, Training Logs: loss_final: 0.895630, loss_mean: 0.851571, loss_mean_cls: 0.044059, grad_norm: 0.356938
Steps:   0%| | 4584/1000000 [22:35<79:45:32,  3.47it/s, grad_norm=0.377, loss_final=0.886, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:20:52[39m] Step: 4584, Training Logs: loss_final: 0.902015, loss_mean: 0.856874, loss_mean_cls: 0.045141, grad_norm: 0.275197
Steps:   0%| | 4591/1000000 [22:37<79:05:46,  3.50it/s, grad_norm=0.255, loss_final=0.901, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:20:54[39m] Step: 4591, Training Logs: loss_final: 0.866861, loss_mean: 0.821183, loss_mean_cls: 0.045679, grad_norm: 0.314681
Steps:   0%| | 4598/1000000 [22:39<80:34:42,  3.43it/s, grad_norm=0.396, loss_final=0.89, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:20:56[39m] Step: 4598, Training Logs: loss_final: 0.879598, loss_mean: 0.835314, loss_mean_cls: 0.044285, grad_norm: 0.380390
Steps:   0%| | 4605/1000000 [22:41<79:47:28,  3.47it/s, grad_norm=0.432, loss_final=0.878, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:20:58[39m] Step: 4605, Training Logs: loss_final: 0.881944, loss_mean: 0.836617, loss_mean_cls: 0.045327, grad_norm: 0.385289
Steps:   0%| | 4612/1000000 [22:43<80:32:33,  3.43it/s, grad_norm=0.335, loss_final=0.909, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:21:00[39m] Step: 4612, Training Logs: loss_final: 0.879141, loss_mean: 0.833887, loss_mean_cls: 0.045254, grad_norm: 0.359236
Steps:   0%| | 4624/1000000 [22:47<80:08:21,  3.45it/s, grad_norm=0.624, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:21:02[39m] Step: 4618, Training Logs: loss_final: 0.894547, loss_mean: 0.849847, loss_mean_cls: 0.044700, grad_norm: 0.362726
Steps:   0%| | 4631/1000000 [22:49<81:52:28,  3.38it/s, grad_norm=0.34, loss_final=0.863, loss_mean=0.817, loss_mean_cls=0.0[[34m2025-10-04 12:21:04[39m] Step: 4625, Training Logs: loss_final: 0.885859, loss_mean: 0.839919, loss_mean_cls: 0.045939, grad_norm: 0.696898
Steps:   0%| | 4638/1000000 [22:51<80:46:39,  3.42it/s, grad_norm=0.342, loss_final=0.9, loss_mean=0.856, loss_mean_cls=0.04[[34m2025-10-04 12:21:06[39m] Step: 4632, Training Logs: loss_final: 0.877015, loss_mean: 0.830434, loss_mean_cls: 0.046581, grad_norm: 0.452659
Steps:   0%| | 4645/1000000 [22:53<80:10:59,  3.45it/s, grad_norm=0.315, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:21:08[39m] Step: 4639, Training Logs: loss_final: 0.918867, loss_mean: 0.874104, loss_mean_cls: 0.044763, grad_norm: 0.463935
Steps:   0%| | 4646/1000000 [22:53<79:40:44,  3.47it/s, grad_norm=0.315, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:21:10[39m] Step: 4646, Training Logs: loss_final: 0.889212, loss_mean: 0.844569, loss_mean_cls: 0.044643, grad_norm: 0.359369
Steps:   0%| | 4653/1000000 [22:55<81:22:12,  3.40it/s, grad_norm=0.317, loss_final=0.904, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:21:12[39m] Step: 4653, Training Logs: loss_final: 0.895035, loss_mean: 0.850598, loss_mean_cls: 0.044437, grad_norm: 0.291739
Steps:   0%| | 4660/1000000 [22:57<80:52:53,  3.42it/s, grad_norm=0.473, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:21:14[39m] Step: 4660, Training Logs: loss_final: 0.868187, loss_mean: 0.822539, loss_mean_cls: 0.045648, grad_norm: 0.483130
Steps:   0%| | 4667/1000000 [22:59<80:19:41,  3.44it/s, grad_norm=0.332, loss_final=0.89, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:21:16[39m] Step: 4667, Training Logs: loss_final: 0.904183, loss_mean: 0.859332, loss_mean_cls: 0.044851, grad_norm: 0.318749
Steps:   0%| | 4673/1000000 [23:01<79:56:36,  3.46it/s, grad_norm=0.396, loss_final=0.878, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:21:18[39m] Step: 4673, Training Logs: loss_final: 0.899443, loss_mean: 0.853720, loss_mean_cls: 0.045722, grad_norm: 0.398775
Steps:   0%| | 4680/1000000 [23:03<79:37:11,  3.47it/s, grad_norm=0.482, loss_final=0.895, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:21:20[39m] Step: 4680, Training Logs: loss_final: 0.913707, loss_mean: 0.868924, loss_mean_cls: 0.044783, grad_norm: 0.318460
Steps:   0%| | 4687/1000000 [23:05<79:32:37,  3.48it/s, grad_norm=0.319, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:21:22[39m] Step: 4687, Training Logs: loss_final: 0.883373, loss_mean: 0.837403, loss_mean_cls: 0.045970, grad_norm: 0.486213
Steps:   0%| | 4693/1000000 [23:07<82:38:33,  3.35it/s, grad_norm=0.619, loss_final=0.892, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:21:23[39m] Step: 4693, Training Logs: loss_final: 0.883759, loss_mean: 0.837931, loss_mean_cls: 0.045828, grad_norm: 0.417790
Steps:   0%| | 4700/1000000 [23:09<82:08:04,  3.37it/s, grad_norm=0.462, loss_final=0.871, loss_mean=0.827, loss_mean_cls=0.[[34m2025-10-04 12:21:25[39m] Step: 4700, Training Logs: loss_final: 0.881180, loss_mean: 0.836630, loss_mean_cls: 0.044550, grad_norm: 0.309833
Steps:   0%| | 4707/1000000 [23:11<82:16:06,  3.36it/s, grad_norm=0.427, loss_final=0.871, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:21:28[39m] Step: 4707, Training Logs: loss_final: 0.877784, loss_mean: 0.833585, loss_mean_cls: 0.044199, grad_norm: 0.453052
Steps:   0%| | 4714/1000000 [23:13<80:31:17,  3.43it/s, grad_norm=0.331, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:21:30[39m] Step: 4714, Training Logs: loss_final: 0.896731, loss_mean: 0.850949, loss_mean_cls: 0.045782, grad_norm: 0.483356
Steps:   0%| | 4721/1000000 [23:15<79:49:23,  3.46it/s, grad_norm=0.353, loss_final=0.886, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:21:32[39m] Step: 4721, Training Logs: loss_final: 0.885964, loss_mean: 0.840566, loss_mean_cls: 0.045398, grad_norm: 0.334250
Steps:   0%| | 4728/1000000 [23:17<79:32:24,  3.48it/s, grad_norm=0.495, loss_final=0.889, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:21:34[39m] Step: 4728, Training Logs: loss_final: 0.877552, loss_mean: 0.833089, loss_mean_cls: 0.044463, grad_norm: 0.498096
Steps:   0%| | 4735/1000000 [23:19<79:29:14,  3.48it/s, grad_norm=0.302, loss_final=0.891, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:21:36[39m] Step: 4735, Training Logs: loss_final: 0.863266, loss_mean: 0.816673, loss_mean_cls: 0.046593, grad_norm: 0.361442
Steps:   0%| | 4742/1000000 [23:21<79:13:17,  3.49it/s, grad_norm=0.362, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:21:38[39m] Step: 4742, Training Logs: loss_final: 0.886876, loss_mean: 0.841648, loss_mean_cls: 0.045228, grad_norm: 0.351619
Steps:   0%| | 4749/1000000 [23:23<80:04:03,  3.45it/s, grad_norm=0.416, loss_final=0.879, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:21:40[39m] Step: 4749, Training Logs: loss_final: 0.903261, loss_mean: 0.858046, loss_mean_cls: 0.045214, grad_norm: 0.381776
Steps:   0%| | 4756/1000000 [23:25<79:13:33,  3.49it/s, grad_norm=0.386, loss_final=0.921, loss_mean=0.877, loss_mean_cls=0.[[34m2025-10-04 12:21:42[39m] Step: 4756, Training Logs: loss_final: 0.871184, loss_mean: 0.825699, loss_mean_cls: 0.045484, grad_norm: 0.271268
Steps:   0%| | 4763/1000000 [23:27<79:07:35,  3.49it/s, grad_norm=0.35, loss_final=0.881, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:21:44[39m] Step: 4763, Training Logs: loss_final: 0.890119, loss_mean: 0.844709, loss_mean_cls: 0.045410, grad_norm: 0.405664
Steps:   0%| | 4770/1000000 [23:29<80:09:26,  3.45it/s, grad_norm=0.351, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:21:46[39m] Step: 4770, Training Logs: loss_final: 0.879666, loss_mean: 0.834440, loss_mean_cls: 0.045225, grad_norm: 0.299133
Steps:   0%| | 4776/1000000 [23:31<84:51:19,  3.26it/s, grad_norm=0.412, loss_final=0.898, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:21:48[39m] Step: 4776, Training Logs: loss_final: 0.886496, loss_mean: 0.841501, loss_mean_cls: 0.044996, grad_norm: 0.380586
Steps:   0%| | 4783/1000000 [23:33<81:58:44,  3.37it/s, grad_norm=0.466, loss_final=0.874, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:21:50[39m] Step: 4783, Training Logs: loss_final: 0.897157, loss_mean: 0.851586, loss_mean_cls: 0.045571, grad_norm: 0.307129
Steps:   0%| | 4790/1000000 [23:35<81:49:55,  3.38it/s, grad_norm=0.318, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:21:52[39m] Step: 4790, Training Logs: loss_final: 0.863729, loss_mean: 0.818603, loss_mean_cls: 0.045126, grad_norm: 0.226501
Steps:   0%| | 4797/1000000 [23:37<80:50:14,  3.42it/s, grad_norm=0.272, loss_final=0.873, loss_mean=0.827, loss_mean_cls=0.[[34m2025-10-04 12:21:54[39m] Step: 4797, Training Logs: loss_final: 0.905561, loss_mean: 0.860786, loss_mean_cls: 0.044775, grad_norm: 0.261679
Steps:   0%| | 4804/1000000 [23:39<83:25:00,  3.31it/s, grad_norm=0.29, loss_final=0.908, loss_mean=0.863, loss_mean_cls=0.0[[34m2025-10-04 12:21:56[39m] Step: 4804, Training Logs: loss_final: 0.894235, loss_mean: 0.849901, loss_mean_cls: 0.044334, grad_norm: 0.381275
Steps:   0%| | 4811/1000000 [23:41<79:53:24,  3.46it/s, grad_norm=0.332, loss_final=0.875, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:21:58[39m] Step: 4811, Training Logs: loss_final: 0.884465, loss_mean: 0.839427, loss_mean_cls: 0.045038, grad_norm: 0.386899
Steps:   0%| | 4817/1000000 [23:43<81:01:27,  3.41it/s, grad_norm=0.464, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:22:00[39m] Step: 4817, Training Logs: loss_final: 0.903015, loss_mean: 0.858908, loss_mean_cls: 0.044107, grad_norm: 0.339899
Steps:   0%| | 4824/1000000 [23:45<81:05:49,  3.41it/s, grad_norm=0.302, loss_final=0.901, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:22:02[39m] Step: 4824, Training Logs: loss_final: 0.864594, loss_mean: 0.819111, loss_mean_cls: 0.045483, grad_norm: 0.317987
Steps:   0%| | 4831/1000000 [23:47<80:18:54,  3.44it/s, grad_norm=0.284, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:22:04[39m] Step: 4831, Training Logs: loss_final: 0.871056, loss_mean: 0.824833, loss_mean_cls: 0.046224, grad_norm: 0.296297
Steps:   0%| | 4838/1000000 [23:49<80:27:06,  3.44it/s, grad_norm=0.296, loss_final=0.902, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:22:06[39m] Step: 4838, Training Logs: loss_final: 0.886555, loss_mean: 0.842617, loss_mean_cls: 0.043938, grad_norm: 0.332500
Steps:   0%| | 4845/1000000 [23:51<79:34:31,  3.47it/s, grad_norm=0.284, loss_final=0.896, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:22:08[39m] Step: 4845, Training Logs: loss_final: 0.891264, loss_mean: 0.846231, loss_mean_cls: 0.045032, grad_norm: 0.368848
Steps:   0%| | 4852/1000000 [23:53<80:26:14,  3.44it/s, grad_norm=0.369, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:22:10[39m] Step: 4852, Training Logs: loss_final: 0.884341, loss_mean: 0.838950, loss_mean_cls: 0.045391, grad_norm: 0.410977
Steps:   0%| | 4859/1000000 [23:55<79:57:28,  3.46it/s, grad_norm=0.33, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.0[[34m2025-10-04 12:22:12[39m] Step: 4859, Training Logs: loss_final: 0.892686, loss_mean: 0.846757, loss_mean_cls: 0.045929, grad_norm: 0.411478
Steps:   0%| | 4866/1000000 [23:57<80:00:10,  3.46it/s, grad_norm=0.361, loss_final=0.885, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:22:14[39m] Step: 4866, Training Logs: loss_final: 0.883065, loss_mean: 0.837522, loss_mean_cls: 0.045543, grad_norm: 0.285689
Steps:   0%| | 4873/1000000 [23:59<80:36:39,  3.43it/s, grad_norm=0.316, loss_final=0.912, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:22:16[39m] Step: 4873, Training Logs: loss_final: 0.882717, loss_mean: 0.837478, loss_mean_cls: 0.045239, grad_norm: 0.331957
Steps:   0%| | 4880/1000000 [24:01<80:41:23,  3.43it/s, grad_norm=0.502, loss_final=0.915, loss_mean=0.87, loss_mean_cls=0.0[[34m2025-10-04 12:22:18[39m] Step: 4880, Training Logs: loss_final: 0.912446, loss_mean: 0.867581, loss_mean_cls: 0.044865, grad_norm: 0.284609
Steps:   0%| | 4886/1000000 [24:03<81:17:52,  3.40it/s, grad_norm=0.352, loss_final=0.909, loss_mean=0.866, loss_mean_cls=0.[[34m2025-10-04 12:22:20[39m] Step: 4886, Training Logs: loss_final: 0.895236, loss_mean: 0.850883, loss_mean_cls: 0.044354, grad_norm: 0.452051
Steps:   0%| | 4893/1000000 [24:05<80:31:52,  3.43it/s, grad_norm=0.385, loss_final=0.875, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:22:22[39m] Step: 4893, Training Logs: loss_final: 0.881541, loss_mean: 0.836032, loss_mean_cls: 0.045509, grad_norm: 0.407220
Steps:   0%| | 4900/1000000 [24:07<81:32:33,  3.39it/s, grad_norm=0.277, loss_final=0.888, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:22:24[39m] Step: 4900, Training Logs: loss_final: 0.883894, loss_mean: 0.837973, loss_mean_cls: 0.045921, grad_norm: 0.451743
Steps:   0%| | 4907/1000000 [24:09<81:39:08,  3.39it/s, grad_norm=0.491, loss_final=0.895, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:22:26[39m] Step: 4907, Training Logs: loss_final: 0.907424, loss_mean: 0.862234, loss_mean_cls: 0.045190, grad_norm: 0.480402
Steps:   0%| | 4914/1000000 [24:11<80:08:18,  3.45it/s, grad_norm=0.572, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:22:28[39m] Step: 4914, Training Logs: loss_final: 0.898885, loss_mean: 0.854298, loss_mean_cls: 0.044587, grad_norm: 0.311254
Steps:   0%| | 4919/1000000 [24:13<79:18:59,  3.48it/s, grad_norm=0.393, loss_final=0.891, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:22:30[39m] Step: 4919, Training Logs: loss_final: 0.889209, loss_mean: 0.844681, loss_mean_cls: 0.044528, grad_norm: 0.566323
Steps:   0%| | 4926/1000000 [24:15<81:04:17,  3.41it/s, grad_norm=0.32, loss_final=0.869, loss_mean=0.825, loss_mean_cls=0.0[[34m2025-10-04 12:22:32[39m] Step: 4926, Training Logs: loss_final: 0.897125, loss_mean: 0.853493, loss_mean_cls: 0.043633, grad_norm: 0.442396
Steps:   0%| | 4933/1000000 [24:17<79:03:54,  3.50it/s, grad_norm=0.313, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:22:34[39m] Step: 4933, Training Logs: loss_final: 0.883047, loss_mean: 0.838839, loss_mean_cls: 0.044207, grad_norm: 0.419738
Steps:   0%| | 4940/1000000 [24:19<82:12:12,  3.36it/s, grad_norm=0.288, loss_final=0.901, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:22:36[39m] Step: 4940, Training Logs: loss_final: 0.896497, loss_mean: 0.851597, loss_mean_cls: 0.044900, grad_norm: 0.331092
Steps:   0%| | 4947/1000000 [24:21<82:59:15,  3.33it/s, grad_norm=0.296, loss_final=0.881, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:22:38[39m] Step: 4947, Training Logs: loss_final: 0.889929, loss_mean: 0.844610, loss_mean_cls: 0.045319, grad_norm: 0.430127
Steps:   0%| | 4954/1000000 [24:23<81:51:37,  3.38it/s, grad_norm=0.418, loss_final=0.891, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:22:40[39m] Step: 4954, Training Logs: loss_final: 0.880160, loss_mean: 0.835943, loss_mean_cls: 0.044217, grad_norm: 0.327166
Steps:   0%| | 4961/1000000 [24:25<81:33:38,  3.39it/s, grad_norm=0.49, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.04[[34m2025-10-04 12:22:42[39m] Step: 4961, Training Logs: loss_final: 0.868557, loss_mean: 0.822952, loss_mean_cls: 0.045605, grad_norm: 0.329552
Steps:   0%| | 4968/1000000 [24:27<82:14:11,  3.36it/s, grad_norm=0.337, loss_final=0.889, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:22:44[39m] Step: 4968, Training Logs: loss_final: 0.888483, loss_mean: 0.845002, loss_mean_cls: 0.043481, grad_norm: 0.352148
Steps:   0%| | 4974/1000000 [24:29<81:15:11,  3.40it/s, grad_norm=0.47, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.0[[34m2025-10-04 12:22:46[39m] Step: 4974, Training Logs: loss_final: 0.914238, loss_mean: 0.869922, loss_mean_cls: 0.044316, grad_norm: 0.387091
Steps:   0%| | 4981/1000000 [24:31<80:37:43,  3.43it/s, grad_norm=0.51, loss_final=0.905, loss_mean=0.859, loss_mean_cls=0.0[[34m2025-10-04 12:22:48[39m] Step: 4981, Training Logs: loss_final: 0.861299, loss_mean: 0.817090, loss_mean_cls: 0.044209, grad_norm: 0.410456
Steps:   0%| | 4988/1000000 [24:33<80:05:42,  3.45it/s, grad_norm=0.363, loss_final=0.892, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:22:50[39m] Step: 4988, Training Logs: loss_final: 0.884683, loss_mean: 0.839862, loss_mean_cls: 0.044821, grad_norm: 0.399690
Steps:   0%| | 4995/1000000 [24:35<83:41:17,  3.30it/s, grad_norm=0.318, loss_final=0.909, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:22:52[39m] Step: 4995, Training Logs: loss_final: 0.884932, loss_mean: 0.839491, loss_mean_cls: 0.045440, grad_norm: 0.429789
Steps:   0%| | 5000/1000000 [24:37<96:01:51,  2.88it/s, grad_norm=0.283, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:22:52[39m] Step: 4995, Training Logs: loss_final: 0.884932, loss_mean: 0.839491, loss_mean_cls: 0.045440, grad_norm: 0.429789
Steps:   1%| | 5002/1000000 [24:40<228:13:36,  1.21it/s, grad_norm=0.382, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0[[34m2025-10-04 12:22:57[39m] Step: 5002, Training Logs: loss_final: 0.876842, loss_mean: 0.832223, loss_mean_cls: 0.044620, grad_norm: 0.349873
Steps:   1%| | 5007/1000000 [24:41<104:29:54,  2.64it/s, grad_norm=0.342, loss_final=0.849, loss_mean=0.804, loss_mean_cls=0[[34m2025-10-04 12:22:58[39m] Step: 5007, Training Logs: loss_final: 0.889755, loss_mean: 0.844776, loss_mean_cls: 0.044979, grad_norm: 0.275567
Steps:   1%| | 5014/1000000 [24:43<83:22:42,  3.31it/s, grad_norm=0.327, loss_final=0.894, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:23:00[39m] Step: 5014, Training Logs: loss_final: 0.895232, loss_mean: 0.850998, loss_mean_cls: 0.044233, grad_norm: 0.261176
Steps:   1%| | 5020/1000000 [24:45<80:50:21,  3.42it/s, grad_norm=0.243, loss_final=0.905, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:23:02[39m] Step: 5020, Training Logs: loss_final: 0.873199, loss_mean: 0.827318, loss_mean_cls: 0.045881, grad_norm: 0.358193
Steps:   1%| | 5027/1000000 [24:47<80:33:47,  3.43it/s, grad_norm=0.328, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:23:04[39m] Step: 5027, Training Logs: loss_final: 0.878820, loss_mean: 0.833393, loss_mean_cls: 0.045427, grad_norm: 0.318744
Steps:   1%| | 5034/1000000 [24:49<79:53:36,  3.46it/s, grad_norm=0.45, loss_final=0.914, loss_mean=0.871, loss_mean_cls=0.0[[34m2025-10-04 12:23:06[39m] Step: 5034, Training Logs: loss_final: 0.898273, loss_mean: 0.852429, loss_mean_cls: 0.045844, grad_norm: 0.459183
Steps:   1%| | 5041/1000000 [24:51<80:50:25,  3.42it/s, grad_norm=0.295, loss_final=0.905, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:23:08[39m] Step: 5041, Training Logs: loss_final: 0.884011, loss_mean: 0.837661, loss_mean_cls: 0.046350, grad_norm: 0.292924
Steps:   1%| | 5048/1000000 [24:53<79:50:52,  3.46it/s, grad_norm=0.376, loss_final=0.873, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:23:10[39m] Step: 5048, Training Logs: loss_final: 0.887395, loss_mean: 0.841843, loss_mean_cls: 0.045553, grad_norm: 0.296563
Steps:   1%| | 5055/1000000 [24:55<79:56:45,  3.46it/s, grad_norm=0.36, loss_final=0.897, loss_mean=0.852, loss_mean_cls=0.0[[34m2025-10-04 12:23:12[39m] Step: 5055, Training Logs: loss_final: 0.873751, loss_mean: 0.830227, loss_mean_cls: 0.043524, grad_norm: 0.248312
Steps:   1%| | 5062/1000000 [24:57<82:36:29,  3.35it/s, grad_norm=0.287, loss_final=0.908, loss_mean=0.864, loss_mean_cls=0.[[34m2025-10-04 12:23:14[39m] Step: 5062, Training Logs: loss_final: 0.882456, loss_mean: 0.836191, loss_mean_cls: 0.046265, grad_norm: 0.331452
Steps:   1%| | 5066/1000000 [25:00<132:41:11,  2.08it/s, grad_norm=0.335, loss_final=0.906, loss_mean=0.862, loss_mean_cls=0[[34m2025-10-04 12:23:16[39m] Step: 5066, Training Logs: loss_final: 0.883876, loss_mean: 0.839501, loss_mean_cls: 0.044376, grad_norm: 0.330351
Steps:   1%| | 5073/1000000 [25:02<83:34:52,  3.31it/s, grad_norm=0.377, loss_final=0.89, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:23:18[39m] Step: 5073, Training Logs: loss_final: 0.900331, loss_mean: 0.855464, loss_mean_cls: 0.044867, grad_norm: 0.409831
Steps:   1%| | 5080/1000000 [25:04<78:48:28,  3.51it/s, grad_norm=0.395, loss_final=0.903, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:23:20[39m] Step: 5080, Training Logs: loss_final: 0.896330, loss_mean: 0.851405, loss_mean_cls: 0.044925, grad_norm: 0.275483
Steps:   1%| | 5087/1000000 [25:06<79:36:08,  3.47it/s, grad_norm=0.536, loss_final=0.896, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:23:22[39m] Step: 5087, Training Logs: loss_final: 0.898349, loss_mean: 0.853954, loss_mean_cls: 0.044395, grad_norm: 0.311616
Steps:   1%| | 5094/1000000 [25:08<79:36:14,  3.47it/s, grad_norm=0.366, loss_final=0.917, loss_mean=0.873, loss_mean_cls=0.[[34m2025-10-04 12:23:24[39m] Step: 5094, Training Logs: loss_final: 0.904919, loss_mean: 0.860963, loss_mean_cls: 0.043956, grad_norm: 0.446169
Steps:   1%| | 5101/1000000 [25:10<82:05:42,  3.37it/s, grad_norm=0.334, loss_final=0.884, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:23:26[39m] Step: 5101, Training Logs: loss_final: 0.889244, loss_mean: 0.844312, loss_mean_cls: 0.044932, grad_norm: 0.336056
Steps:   1%| | 5108/1000000 [25:12<80:13:07,  3.45it/s, grad_norm=0.443, loss_final=0.894, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:23:28[39m] Step: 5108, Training Logs: loss_final: 0.873730, loss_mean: 0.829786, loss_mean_cls: 0.043945, grad_norm: 0.369183
Steps:   1%| | 5115/1000000 [25:14<80:14:17,  3.44it/s, grad_norm=0.373, loss_final=0.894, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:23:30[39m] Step: 5115, Training Logs: loss_final: 0.871169, loss_mean: 0.826285, loss_mean_cls: 0.044884, grad_norm: 0.302667
Steps:   1%| | 5127/1000000 [25:17<79:00:23,  3.50it/s, grad_norm=0.33, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.0[[34m2025-10-04 12:23:32[39m] Step: 5121, Training Logs: loss_final: 0.889098, loss_mean: 0.842164, loss_mean_cls: 0.046934, grad_norm: 0.472579
Steps:   1%| | 5128/1000000 [25:17<78:54:07,  3.50it/s, grad_norm=0.33, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.0[[34m2025-10-04 12:23:34[39m] Step: 5128, Training Logs: loss_final: 0.885679, loss_mean: 0.841060, loss_mean_cls: 0.044620, grad_norm: 0.562335
Steps:   1%| | 5135/1000000 [25:19<81:15:45,  3.40it/s, grad_norm=0.428, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:23:36[39m] Step: 5135, Training Logs: loss_final: 0.890672, loss_mean: 0.845808, loss_mean_cls: 0.044864, grad_norm: 0.462498
Steps:   1%| | 5142/1000000 [25:21<80:04:13,  3.45it/s, grad_norm=0.356, loss_final=0.895, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:23:38[39m] Step: 5142, Training Logs: loss_final: 0.879807, loss_mean: 0.834942, loss_mean_cls: 0.044865, grad_norm: 0.440530
Steps:   1%| | 5149/1000000 [25:23<79:35:29,  3.47it/s, grad_norm=0.555, loss_final=0.887, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:23:40[39m] Step: 5149, Training Logs: loss_final: 0.880090, loss_mean: 0.834463, loss_mean_cls: 0.045627, grad_norm: 0.516643
Steps:   1%| | 5156/1000000 [25:26<80:44:33,  3.42it/s, grad_norm=0.332, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:23:42[39m] Step: 5156, Training Logs: loss_final: 0.892395, loss_mean: 0.849172, loss_mean_cls: 0.043224, grad_norm: 0.436918
Steps:   1%| | 5163/1000000 [25:28<81:24:02,  3.39it/s, grad_norm=0.43, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.0[[34m2025-10-04 12:23:44[39m] Step: 5163, Training Logs: loss_final: 0.884539, loss_mean: 0.840225, loss_mean_cls: 0.044314, grad_norm: 0.317502
Steps:   1%| | 5170/1000000 [25:30<79:11:21,  3.49it/s, grad_norm=0.299, loss_final=0.889, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:23:46[39m] Step: 5170, Training Logs: loss_final: 0.896499, loss_mean: 0.850861, loss_mean_cls: 0.045638, grad_norm: 0.345706
Steps:   1%| | 5176/1000000 [25:31<78:22:13,  3.53it/s, grad_norm=0.322, loss_final=0.893, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:23:48[39m] Step: 5176, Training Logs: loss_final: 0.876180, loss_mean: 0.832029, loss_mean_cls: 0.044151, grad_norm: 0.237786
Steps:   1%| | 5183/1000000 [25:33<81:46:11,  3.38it/s, grad_norm=0.353, loss_final=0.883, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:23:50[39m] Step: 5183, Training Logs: loss_final: 0.890073, loss_mean: 0.846259, loss_mean_cls: 0.043815, grad_norm: 0.345954
Steps:   1%| | 5189/1000000 [25:35<84:28:04,  3.27it/s, grad_norm=0.409, loss_final=0.867, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:23:52[39m] Step: 5189, Training Logs: loss_final: 0.874215, loss_mean: 0.828330, loss_mean_cls: 0.045885, grad_norm: 0.404708
Steps:   1%| | 5196/1000000 [25:37<81:07:24,  3.41it/s, grad_norm=0.465, loss_final=0.897, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:23:54[39m] Step: 5196, Training Logs: loss_final: 0.893832, loss_mean: 0.849551, loss_mean_cls: 0.044281, grad_norm: 0.240864
Steps:   1%| | 5203/1000000 [25:39<82:37:52,  3.34it/s, grad_norm=0.379, loss_final=0.899, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:23:56[39m] Step: 5203, Training Logs: loss_final: 0.902641, loss_mean: 0.858607, loss_mean_cls: 0.044034, grad_norm: 0.321377
Steps:   1%| | 5210/1000000 [25:41<83:10:44,  3.32it/s, grad_norm=0.287, loss_final=0.894, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:23:58[39m] Step: 5210, Training Logs: loss_final: 0.882601, loss_mean: 0.837494, loss_mean_cls: 0.045106, grad_norm: 0.299866
Steps:   1%| | 5216/1000000 [25:43<81:35:50,  3.39it/s, grad_norm=0.366, loss_final=0.897, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:24:00[39m] Step: 5216, Training Logs: loss_final: 0.899398, loss_mean: 0.853770, loss_mean_cls: 0.045628, grad_norm: 0.392546
Steps:   1%| | 5223/1000000 [25:45<81:44:30,  3.38it/s, grad_norm=0.453, loss_final=0.879, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:24:02[39m] Step: 5223, Training Logs: loss_final: 0.893436, loss_mean: 0.848482, loss_mean_cls: 0.044954, grad_norm: 0.415774
Steps:   1%| | 5230/1000000 [25:47<81:46:07,  3.38it/s, grad_norm=0.351, loss_final=0.893, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:24:04[39m] Step: 5230, Training Logs: loss_final: 0.880668, loss_mean: 0.835752, loss_mean_cls: 0.044916, grad_norm: 0.377551
Steps:   1%| | 5237/1000000 [25:49<81:39:31,  3.38it/s, grad_norm=0.317, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:24:06[39m] Step: 5237, Training Logs: loss_final: 0.885075, loss_mean: 0.840528, loss_mean_cls: 0.044547, grad_norm: 0.348390
Steps:   1%| | 5244/1000000 [25:51<80:34:14,  3.43it/s, grad_norm=0.334, loss_final=0.889, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:24:08[39m] Step: 5244, Training Logs: loss_final: 0.869693, loss_mean: 0.824157, loss_mean_cls: 0.045535, grad_norm: 0.489422
Steps:   1%| | 5251/1000000 [25:54<84:07:04,  3.28it/s, grad_norm=0.501, loss_final=0.882, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:24:10[39m] Step: 5251, Training Logs: loss_final: 0.909717, loss_mean: 0.865483, loss_mean_cls: 0.044234, grad_norm: 0.315994
Steps:   1%| | 5257/1000000 [25:55<83:20:36,  3.32it/s, grad_norm=0.315, loss_final=0.905, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:24:12[39m] Step: 5257, Training Logs: loss_final: 0.886950, loss_mean: 0.842704, loss_mean_cls: 0.044246, grad_norm: 0.371280
Steps:   1%| | 5264/1000000 [25:57<80:22:23,  3.44it/s, grad_norm=0.322, loss_final=0.881, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:24:14[39m] Step: 5264, Training Logs: loss_final: 0.880990, loss_mean: 0.836089, loss_mean_cls: 0.044901, grad_norm: 0.334093
Steps:   1%| | 5271/1000000 [26:00<82:28:44,  3.35it/s, grad_norm=0.276, loss_final=0.879, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:24:16[39m] Step: 5271, Training Logs: loss_final: 0.882864, loss_mean: 0.837872, loss_mean_cls: 0.044992, grad_norm: 0.289209
Steps:   1%| | 5278/1000000 [26:02<82:13:31,  3.36it/s, grad_norm=0.378, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:24:18[39m] Step: 5278, Training Logs: loss_final: 0.887319, loss_mean: 0.842532, loss_mean_cls: 0.044788, grad_norm: 0.428962
Steps:   1%| | 5285/1000000 [26:04<79:41:50,  3.47it/s, grad_norm=0.385, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:24:20[39m] Step: 5285, Training Logs: loss_final: 0.876840, loss_mean: 0.832540, loss_mean_cls: 0.044300, grad_norm: 0.373004
Steps:   1%| | 5292/1000000 [26:06<81:50:36,  3.38it/s, grad_norm=0.493, loss_final=0.869, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:24:22[39m] Step: 5292, Training Logs: loss_final: 0.889367, loss_mean: 0.843769, loss_mean_cls: 0.045598, grad_norm: 0.276191
Steps:   1%| | 5298/1000000 [26:07<80:47:43,  3.42it/s, grad_norm=0.376, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:24:24[39m] Step: 5298, Training Logs: loss_final: 0.870173, loss_mean: 0.825164, loss_mean_cls: 0.045009, grad_norm: 0.403847
Steps:   1%| | 5305/1000000 [26:10<81:46:10,  3.38it/s, grad_norm=0.387, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:24:26[39m] Step: 5305, Training Logs: loss_final: 0.909679, loss_mean: 0.866179, loss_mean_cls: 0.043500, grad_norm: 0.388040
Steps:   1%| | 5312/1000000 [26:12<80:54:45,  3.41it/s, grad_norm=0.332, loss_final=0.87, loss_mean=0.824, loss_mean_cls=0.0[[34m2025-10-04 12:24:28[39m] Step: 5312, Training Logs: loss_final: 0.886715, loss_mean: 0.842335, loss_mean_cls: 0.044380, grad_norm: 0.400402
Steps:   1%| | 5319/1000000 [26:14<79:19:41,  3.48it/s, grad_norm=0.413, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:24:30[39m] Step: 5319, Training Logs: loss_final: 0.904732, loss_mean: 0.861019, loss_mean_cls: 0.043713, grad_norm: 0.474958
Steps:   1%| | 5326/1000000 [26:16<82:23:19,  3.35it/s, grad_norm=0.269, loss_final=0.911, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:24:32[39m] Step: 5326, Training Logs: loss_final: 0.895975, loss_mean: 0.852649, loss_mean_cls: 0.043325, grad_norm: 0.331198
Steps:   1%| | 5333/1000000 [26:18<79:21:08,  3.48it/s, grad_norm=0.331, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:24:34[39m] Step: 5333, Training Logs: loss_final: 0.882068, loss_mean: 0.837517, loss_mean_cls: 0.044550, grad_norm: 0.338066
Steps:   1%| | 5340/1000000 [26:20<78:46:18,  3.51it/s, grad_norm=0.393, loss_final=0.876, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:24:36[39m] Step: 5340, Training Logs: loss_final: 0.888295, loss_mean: 0.843773, loss_mean_cls: 0.044522, grad_norm: 0.356687
Steps:   1%| | 5347/1000000 [26:22<80:35:05,  3.43it/s, grad_norm=0.39, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.0[[34m2025-10-04 12:24:38[39m] Step: 5347, Training Logs: loss_final: 0.884305, loss_mean: 0.840693, loss_mean_cls: 0.043612, grad_norm: 0.343876
Steps:   1%| | 5354/1000000 [26:24<80:40:48,  3.42it/s, grad_norm=0.393, loss_final=0.891, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:24:40[39m] Step: 5354, Training Logs: loss_final: 0.893489, loss_mean: 0.849290, loss_mean_cls: 0.044198, grad_norm: 0.279541
Steps:   1%| | 5361/1000000 [26:26<83:37:22,  3.30it/s, grad_norm=0.45, loss_final=0.867, loss_mean=0.822, loss_mean_cls=0.0[[34m2025-10-04 12:24:43[39m] Step: 5361, Training Logs: loss_final: 0.894669, loss_mean: 0.849818, loss_mean_cls: 0.044851, grad_norm: 0.381953
Steps:   1%| | 5368/1000000 [26:28<80:03:16,  3.45it/s, grad_norm=0.665, loss_final=0.884, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:24:45[39m] Step: 5368, Training Logs: loss_final: 0.901013, loss_mean: 0.856802, loss_mean_cls: 0.044212, grad_norm: 0.345581
Steps:   1%| | 5375/1000000 [26:30<80:22:03,  3.44it/s, grad_norm=0.424, loss_final=0.891, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:24:47[39m] Step: 5375, Training Logs: loss_final: 0.898630, loss_mean: 0.854503, loss_mean_cls: 0.044128, grad_norm: 0.306631
Steps:   1%| | 5381/1000000 [26:32<81:51:47,  3.37it/s, grad_norm=0.425, loss_final=0.897, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:24:48[39m] Step: 5381, Training Logs: loss_final: 0.881999, loss_mean: 0.837397, loss_mean_cls: 0.044601, grad_norm: 0.338009
Steps:   1%| | 5388/1000000 [26:34<79:30:02,  3.48it/s, grad_norm=0.395, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:24:50[39m] Step: 5388, Training Logs: loss_final: 0.881389, loss_mean: 0.835187, loss_mean_cls: 0.046202, grad_norm: 0.253357
Steps:   1%| | 5395/1000000 [26:36<80:10:04,  3.45it/s, grad_norm=0.441, loss_final=0.894, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:24:52[39m] Step: 5395, Training Logs: loss_final: 0.882687, loss_mean: 0.838351, loss_mean_cls: 0.044336, grad_norm: 0.361753
Steps:   1%| | 5402/1000000 [26:38<79:27:27,  3.48it/s, grad_norm=0.411, loss_final=0.881, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:24:54[39m] Step: 5402, Training Logs: loss_final: 0.899848, loss_mean: 0.855236, loss_mean_cls: 0.044612, grad_norm: 0.409782
Steps:   1%| | 5409/1000000 [26:40<79:55:36,  3.46it/s, grad_norm=0.372, loss_final=0.891, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:24:56[39m] Step: 5409, Training Logs: loss_final: 0.887200, loss_mean: 0.842639, loss_mean_cls: 0.044561, grad_norm: 0.464399
Steps:   1%| | 5416/1000000 [26:42<80:03:48,  3.45it/s, grad_norm=0.333, loss_final=0.895, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:24:58[39m] Step: 5416, Training Logs: loss_final: 0.908218, loss_mean: 0.863627, loss_mean_cls: 0.044591, grad_norm: 0.421813
Steps:   1%| | 5423/1000000 [26:44<79:36:35,  3.47it/s, grad_norm=0.323, loss_final=0.881, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:25:01[39m] Step: 5423, Training Logs: loss_final: 0.895302, loss_mean: 0.850181, loss_mean_cls: 0.045121, grad_norm: 0.290401
Steps:   1%| | 5430/1000000 [26:46<80:14:31,  3.44it/s, grad_norm=0.405, loss_final=0.901, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:25:03[39m] Step: 5430, Training Logs: loss_final: 0.891079, loss_mean: 0.846748, loss_mean_cls: 0.044331, grad_norm: 0.283545
Steps:   1%| | 5436/1000000 [26:48<79:56:57,  3.46it/s, grad_norm=0.405, loss_final=0.897, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:25:04[39m] Step: 5436, Training Logs: loss_final: 0.872647, loss_mean: 0.829451, loss_mean_cls: 0.043196, grad_norm: 0.336839
Steps:   1%| | 5442/1000000 [26:49<81:58:03,  3.37it/s, grad_norm=0.326, loss_final=0.895, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:25:06[39m] Step: 5442, Training Logs: loss_final: 0.885616, loss_mean: 0.840762, loss_mean_cls: 0.044853, grad_norm: 0.304247
Steps:   1%| | 5449/1000000 [26:51<79:49:06,  3.46it/s, grad_norm=0.282, loss_final=0.879, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:25:08[39m] Step: 5449, Training Logs: loss_final: 0.885140, loss_mean: 0.839831, loss_mean_cls: 0.045309, grad_norm: 0.349379
Steps:   1%| | 5456/1000000 [26:53<79:16:36,  3.48it/s, grad_norm=0.321, loss_final=0.897, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:25:10[39m] Step: 5456, Training Logs: loss_final: 0.909184, loss_mean: 0.865067, loss_mean_cls: 0.044117, grad_norm: 0.239458
Steps:   1%| | 5463/1000000 [26:55<80:43:07,  3.42it/s, grad_norm=0.361, loss_final=0.889, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:25:12[39m] Step: 5463, Training Logs: loss_final: 0.872115, loss_mean: 0.827768, loss_mean_cls: 0.044346, grad_norm: 0.345853
Steps:   1%| | 5470/1000000 [26:58<80:15:31,  3.44it/s, grad_norm=0.412, loss_final=0.896, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:25:14[39m] Step: 5470, Training Logs: loss_final: 0.894009, loss_mean: 0.849877, loss_mean_cls: 0.044132, grad_norm: 0.403134
Steps:   1%| | 5477/1000000 [27:00<79:24:40,  3.48it/s, grad_norm=0.627, loss_final=0.895, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:25:16[39m] Step: 5477, Training Logs: loss_final: 0.884818, loss_mean: 0.840100, loss_mean_cls: 0.044718, grad_norm: 0.400236
Steps:   1%| | 5484/1000000 [27:02<82:40:00,  3.34it/s, grad_norm=0.317, loss_final=0.894, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:25:18[39m] Step: 5484, Training Logs: loss_final: 0.868196, loss_mean: 0.823104, loss_mean_cls: 0.045092, grad_norm: 0.440789
Steps:   1%| | 5491/1000000 [27:04<81:25:57,  3.39it/s, grad_norm=0.296, loss_final=0.877, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:25:20[39m] Step: 5491, Training Logs: loss_final: 0.880571, loss_mean: 0.837073, loss_mean_cls: 0.043498, grad_norm: 0.299079
Steps:   1%| | 5498/1000000 [27:06<81:04:00,  3.41it/s, grad_norm=0.414, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:25:22[39m] Step: 5498, Training Logs: loss_final: 0.893004, loss_mean: 0.848943, loss_mean_cls: 0.044060, grad_norm: 0.473285
Steps:   1%| | 5504/1000000 [27:08<81:43:49,  3.38it/s, grad_norm=0.376, loss_final=0.9, loss_mean=0.855, loss_mean_cls=0.04[[34m2025-10-04 12:25:24[39m] Step: 5504, Training Logs: loss_final: 0.914045, loss_mean: 0.871138, loss_mean_cls: 0.042907, grad_norm: 0.581495
Steps:   1%| | 5511/1000000 [27:10<82:43:01,  3.34it/s, grad_norm=0.351, loss_final=0.86, loss_mean=0.815, loss_mean_cls=0.0[[34m2025-10-04 12:25:26[39m] Step: 5511, Training Logs: loss_final: 0.902535, loss_mean: 0.858597, loss_mean_cls: 0.043938, grad_norm: 0.349059
Steps:   1%| | 5518/1000000 [27:12<85:55:13,  3.22it/s, grad_norm=0.455, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:25:28[39m] Step: 5518, Training Logs: loss_final: 0.899011, loss_mean: 0.854339, loss_mean_cls: 0.044672, grad_norm: 0.264198
Steps:   1%| | 5524/1000000 [27:14<86:39:42,  3.19it/s, grad_norm=0.465, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:25:30[39m] Step: 5524, Training Logs: loss_final: 0.909970, loss_mean: 0.866251, loss_mean_cls: 0.043719, grad_norm: 0.708151
Steps:   1%| | 5531/1000000 [27:16<83:47:54,  3.30it/s, grad_norm=0.571, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:25:32[39m] Step: 5531, Training Logs: loss_final: 0.906152, loss_mean: 0.862830, loss_mean_cls: 0.043322, grad_norm: 0.529878
Steps:   1%| | 5538/1000000 [27:18<81:42:49,  3.38it/s, grad_norm=0.486, loss_final=0.904, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:25:35[39m] Step: 5538, Training Logs: loss_final: 0.889405, loss_mean: 0.845174, loss_mean_cls: 0.044231, grad_norm: 0.356560
Steps:   1%| | 5545/1000000 [27:20<81:51:15,  3.37it/s, grad_norm=0.366, loss_final=0.905, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:25:37[39m] Step: 5545, Training Logs: loss_final: 0.893203, loss_mean: 0.848483, loss_mean_cls: 0.044720, grad_norm: 0.268645
Steps:   1%| | 5551/1000000 [27:22<81:34:35,  3.39it/s, grad_norm=0.38, loss_final=0.86, loss_mean=0.815, loss_mean_cls=0.04[[34m2025-10-04 12:25:38[39m] Step: 5551, Training Logs: loss_final: 0.889704, loss_mean: 0.844997, loss_mean_cls: 0.044707, grad_norm: 0.430984
Steps:   1%| | 5558/1000000 [27:24<81:42:41,  3.38it/s, grad_norm=0.412, loss_final=0.89, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:25:40[39m] Step: 5558, Training Logs: loss_final: 0.889925, loss_mean: 0.845807, loss_mean_cls: 0.044118, grad_norm: 0.504266
Steps:   1%| | 5565/1000000 [27:26<79:45:47,  3.46it/s, grad_norm=0.374, loss_final=0.899, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:25:42[39m] Step: 5565, Training Logs: loss_final: 0.873038, loss_mean: 0.826573, loss_mean_cls: 0.046464, grad_norm: 0.287862
Steps:   1%| | 5572/1000000 [27:28<80:44:57,  3.42it/s, grad_norm=0.524, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:25:44[39m] Step: 5572, Training Logs: loss_final: 0.900137, loss_mean: 0.855555, loss_mean_cls: 0.044582, grad_norm: 0.440208
Steps:   1%| | 5579/1000000 [27:30<79:29:39,  3.47it/s, grad_norm=0.328, loss_final=0.92, loss_mean=0.876, loss_mean_cls=0.0[[34m2025-10-04 12:25:47[39m] Step: 5579, Training Logs: loss_final: 0.881050, loss_mean: 0.836735, loss_mean_cls: 0.044315, grad_norm: 0.429953
Steps:   1%| | 5586/1000000 [27:32<81:37:01,  3.38it/s, grad_norm=0.449, loss_final=0.897, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:25:49[39m] Step: 5586, Training Logs: loss_final: 0.890479, loss_mean: 0.846064, loss_mean_cls: 0.044415, grad_norm: 0.360772
Steps:   1%| | 5593/1000000 [27:34<80:56:44,  3.41it/s, grad_norm=0.29, loss_final=0.886, loss_mean=0.841, loss_mean_cls=0.0[[34m2025-10-04 12:25:51[39m] Step: 5593, Training Logs: loss_final: 0.898656, loss_mean: 0.853427, loss_mean_cls: 0.045229, grad_norm: 0.412235
Steps:   1%| | 5600/1000000 [27:36<79:31:03,  3.47it/s, grad_norm=0.297, loss_final=0.89, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:25:53[39m] Step: 5600, Training Logs: loss_final: 0.886565, loss_mean: 0.841881, loss_mean_cls: 0.044684, grad_norm: 0.498798
Steps:   1%| | 5607/1000000 [27:38<80:04:51,  3.45it/s, grad_norm=0.308, loss_final=0.867, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:25:55[39m] Step: 5607, Training Logs: loss_final: 0.865933, loss_mean: 0.820373, loss_mean_cls: 0.045560, grad_norm: 0.313711
Steps:   1%| | 5614/1000000 [27:40<79:18:26,  3.48it/s, grad_norm=0.417, loss_final=0.9, loss_mean=0.856, loss_mean_cls=0.04[[34m2025-10-04 12:25:57[39m] Step: 5614, Training Logs: loss_final: 0.896146, loss_mean: 0.852524, loss_mean_cls: 0.043621, grad_norm: 0.337283
Steps:   1%| | 5621/1000000 [27:42<80:45:58,  3.42it/s, grad_norm=0.303, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:25:59[39m] Step: 5621, Training Logs: loss_final: 0.898759, loss_mean: 0.853607, loss_mean_cls: 0.045152, grad_norm: 0.546212
Steps:   1%| | 5628/1000000 [27:44<81:54:51,  3.37it/s, grad_norm=0.353, loss_final=0.884, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:26:01[39m] Step: 5628, Training Logs: loss_final: 0.897346, loss_mean: 0.852485, loss_mean_cls: 0.044861, grad_norm: 0.317751
Steps:   1%| | 5635/1000000 [27:46<80:05:59,  3.45it/s, grad_norm=0.251, loss_final=0.878, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:26:03[39m] Step: 5635, Training Logs: loss_final: 0.906611, loss_mean: 0.863318, loss_mean_cls: 0.043293, grad_norm: 0.378293
Steps:   1%| | 5641/1000000 [27:48<81:26:09,  3.39it/s, grad_norm=0.412, loss_final=0.904, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:26:05[39m] Step: 5641, Training Logs: loss_final: 0.883031, loss_mean: 0.839073, loss_mean_cls: 0.043958, grad_norm: 0.273832
Steps:   1%| | 5648/1000000 [27:50<80:25:24,  3.43it/s, grad_norm=0.351, loss_final=0.891, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:26:07[39m] Step: 5648, Training Logs: loss_final: 0.877221, loss_mean: 0.831520, loss_mean_cls: 0.045701, grad_norm: 0.373669
Steps:   1%| | 5655/1000000 [27:52<81:02:51,  3.41it/s, grad_norm=0.434, loss_final=0.9, loss_mean=0.856, loss_mean_cls=0.04[[34m2025-10-04 12:26:09[39m] Step: 5655, Training Logs: loss_final: 0.896967, loss_mean: 0.853001, loss_mean_cls: 0.043965, grad_norm: 0.258270
Steps:   1%| | 5662/1000000 [27:54<80:53:53,  3.41it/s, grad_norm=0.374, loss_final=0.91, loss_mean=0.866, loss_mean_cls=0.0[[34m2025-10-04 12:26:11[39m] Step: 5662, Training Logs: loss_final: 0.895773, loss_mean: 0.851852, loss_mean_cls: 0.043921, grad_norm: 0.261593
Steps:   1%| | 5669/1000000 [27:56<81:00:25,  3.41it/s, grad_norm=0.418, loss_final=0.889, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:26:13[39m] Step: 5669, Training Logs: loss_final: 0.895144, loss_mean: 0.851028, loss_mean_cls: 0.044116, grad_norm: 0.459939
Steps:   1%| | 5674/1000000 [27:58<80:44:42,  3.42it/s, grad_norm=0.331, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:26:14[39m] Step: 5674, Training Logs: loss_final: 0.885690, loss_mean: 0.842620, loss_mean_cls: 0.043069, grad_norm: 0.452125
Steps:   1%| | 5681/1000000 [28:00<79:42:12,  3.47it/s, grad_norm=0.265, loss_final=0.884, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:26:16[39m] Step: 5681, Training Logs: loss_final: 0.867885, loss_mean: 0.822104, loss_mean_cls: 0.045781, grad_norm: 0.282257
Steps:   1%| | 5688/1000000 [28:02<79:42:27,  3.47it/s, grad_norm=0.286, loss_final=0.889, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:26:18[39m] Step: 5688, Training Logs: loss_final: 0.880200, loss_mean: 0.835135, loss_mean_cls: 0.045065, grad_norm: 0.268169
Steps:   1%| | 5695/1000000 [28:04<80:20:37,  3.44it/s, grad_norm=0.417, loss_final=0.894, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:26:20[39m] Step: 5695, Training Logs: loss_final: 0.860311, loss_mean: 0.814630, loss_mean_cls: 0.045680, grad_norm: 0.599581
Steps:   1%| | 5702/1000000 [28:06<79:11:50,  3.49it/s, grad_norm=0.392, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:26:22[39m] Step: 5702, Training Logs: loss_final: 0.866941, loss_mean: 0.822555, loss_mean_cls: 0.044386, grad_norm: 0.462356
Steps:   1%| | 5709/1000000 [28:08<81:11:53,  3.40it/s, grad_norm=0.329, loss_final=0.907, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:26:24[39m] Step: 5709, Training Logs: loss_final: 0.889059, loss_mean: 0.843604, loss_mean_cls: 0.045455, grad_norm: 0.279931
Steps:   1%| | 5716/1000000 [28:10<79:40:44,  3.47it/s, grad_norm=0.397, loss_final=0.88, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:26:26[39m] Step: 5716, Training Logs: loss_final: 0.877444, loss_mean: 0.831819, loss_mean_cls: 0.045625, grad_norm: 0.469713
Steps:   1%| | 5723/1000000 [28:12<80:33:08,  3.43it/s, grad_norm=0.307, loss_final=0.876, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:26:29[39m] Step: 5723, Training Logs: loss_final: 0.885049, loss_mean: 0.840680, loss_mean_cls: 0.044369, grad_norm: 0.472093
Steps:   1%| | 5730/1000000 [28:14<80:25:25,  3.43it/s, grad_norm=0.294, loss_final=0.892, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:26:31[39m] Step: 5730, Training Logs: loss_final: 0.896006, loss_mean: 0.851798, loss_mean_cls: 0.044209, grad_norm: 0.331366
Steps:   1%| | 5736/1000000 [28:16<80:22:49,  3.44it/s, grad_norm=0.282, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:26:32[39m] Step: 5736, Training Logs: loss_final: 0.878708, loss_mean: 0.834805, loss_mean_cls: 0.043903, grad_norm: 0.356356
Steps:   1%| | 5743/1000000 [28:18<80:37:56,  3.43it/s, grad_norm=0.332, loss_final=0.897, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:26:34[39m] Step: 5743, Training Logs: loss_final: 0.906323, loss_mean: 0.861717, loss_mean_cls: 0.044606, grad_norm: 0.375311
Steps:   1%| | 5750/1000000 [28:20<82:38:22,  3.34it/s, grad_norm=0.399, loss_final=0.874, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:26:37[39m] Step: 5750, Training Logs: loss_final: 0.866702, loss_mean: 0.822187, loss_mean_cls: 0.044515, grad_norm: 0.476476
Steps:   1%| | 5757/1000000 [28:22<81:58:13,  3.37it/s, grad_norm=0.374, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:26:39[39m] Step: 5757, Training Logs: loss_final: 0.869755, loss_mean: 0.824652, loss_mean_cls: 0.045104, grad_norm: 0.451334
Steps:   1%| | 5764/1000000 [28:24<80:20:05,  3.44it/s, grad_norm=0.44, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.04[[34m2025-10-04 12:26:41[39m] Step: 5764, Training Logs: loss_final: 0.873536, loss_mean: 0.828754, loss_mean_cls: 0.044782, grad_norm: 0.434448
Steps:   1%| | 5771/1000000 [28:26<80:58:16,  3.41it/s, grad_norm=0.331, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:26:43[39m] Step: 5771, Training Logs: loss_final: 0.908488, loss_mean: 0.865260, loss_mean_cls: 0.043228, grad_norm: 0.285739
Steps:   1%| | 5778/1000000 [28:28<81:55:40,  3.37it/s, grad_norm=0.251, loss_final=0.864, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:26:45[39m] Step: 5778, Training Logs: loss_final: 0.897043, loss_mean: 0.852376, loss_mean_cls: 0.044666, grad_norm: 0.458654
Steps:   1%| | 5784/1000000 [28:30<79:40:55,  3.47it/s, grad_norm=0.312, loss_final=0.885, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:26:47[39m] Step: 5784, Training Logs: loss_final: 0.902676, loss_mean: 0.858894, loss_mean_cls: 0.043782, grad_norm: 0.279079
Steps:   1%| | 5791/1000000 [28:32<81:14:57,  3.40it/s, grad_norm=0.299, loss_final=0.868, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:26:49[39m] Step: 5791, Training Logs: loss_final: 0.877100, loss_mean: 0.833569, loss_mean_cls: 0.043530, grad_norm: 0.167684
Steps:   1%| | 5798/1000000 [28:34<82:51:54,  3.33it/s, grad_norm=0.322, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:26:51[39m] Step: 5798, Training Logs: loss_final: 0.912948, loss_mean: 0.870385, loss_mean_cls: 0.042563, grad_norm: 0.332523
Steps:   1%| | 5805/1000000 [28:36<82:14:32,  3.36it/s, grad_norm=0.38, loss_final=0.888, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:26:53[39m] Step: 5805, Training Logs: loss_final: 0.894362, loss_mean: 0.851441, loss_mean_cls: 0.042922, grad_norm: 0.369792
Steps:   1%| | 5812/1000000 [28:38<80:22:46,  3.44it/s, grad_norm=0.358, loss_final=0.882, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:26:55[39m] Step: 5812, Training Logs: loss_final: 0.878577, loss_mean: 0.833920, loss_mean_cls: 0.044656, grad_norm: 0.341329
Steps:   1%| | 5819/1000000 [28:40<81:14:19,  3.40it/s, grad_norm=0.452, loss_final=0.875, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:26:57[39m] Step: 5819, Training Logs: loss_final: 0.894769, loss_mean: 0.851458, loss_mean_cls: 0.043311, grad_norm: 0.405139
Steps:   1%| | 5826/1000000 [28:42<81:41:59,  3.38it/s, grad_norm=0.436, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:26:59[39m] Step: 5826, Training Logs: loss_final: 0.878507, loss_mean: 0.833802, loss_mean_cls: 0.044705, grad_norm: 0.428583
Steps:   1%| | 5833/1000000 [28:44<82:06:26,  3.36it/s, grad_norm=0.322, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:27:01[39m] Step: 5833, Training Logs: loss_final: 0.888531, loss_mean: 0.843784, loss_mean_cls: 0.044747, grad_norm: 0.539867
Steps:   1%| | 5839/1000000 [28:46<78:52:21,  3.50it/s, grad_norm=0.285, loss_final=0.877, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:27:03[39m] Step: 5839, Training Logs: loss_final: 0.890802, loss_mean: 0.846225, loss_mean_cls: 0.044577, grad_norm: 0.347635
Steps:   1%| | 5846/1000000 [28:48<81:45:53,  3.38it/s, grad_norm=0.315, loss_final=0.864, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:27:05[39m] Step: 5846, Training Logs: loss_final: 0.871682, loss_mean: 0.826075, loss_mean_cls: 0.045607, grad_norm: 0.280469
Steps:   1%| | 5853/1000000 [28:50<79:53:34,  3.46it/s, grad_norm=0.262, loss_final=0.854, loss_mean=0.809, loss_mean_cls=0.[[34m2025-10-04 12:27:07[39m] Step: 5853, Training Logs: loss_final: 0.893591, loss_mean: 0.850393, loss_mean_cls: 0.043198, grad_norm: 0.297394
Steps:   1%| | 5860/1000000 [28:52<82:01:26,  3.37it/s, grad_norm=0.35, loss_final=0.884, loss_mean=0.837, loss_mean_cls=0.0[[34m2025-10-04 12:27:09[39m] Step: 5860, Training Logs: loss_final: 0.902963, loss_mean: 0.858745, loss_mean_cls: 0.044218, grad_norm: 0.239833
Steps:   1%| | 5867/1000000 [28:54<80:22:59,  3.44it/s, grad_norm=0.37, loss_final=0.881, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:27:11[39m] Step: 5867, Training Logs: loss_final: 0.877482, loss_mean: 0.832780, loss_mean_cls: 0.044702, grad_norm: 0.338355
Steps:   1%| | 5874/1000000 [28:56<82:07:11,  3.36it/s, grad_norm=0.219, loss_final=0.881, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:27:13[39m] Step: 5874, Training Logs: loss_final: 0.884176, loss_mean: 0.839378, loss_mean_cls: 0.044798, grad_norm: 0.337291
Steps:   1%| | 5881/1000000 [28:58<84:36:39,  3.26it/s, grad_norm=0.418, loss_final=0.862, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:27:15[39m] Step: 5881, Training Logs: loss_final: 0.877809, loss_mean: 0.832379, loss_mean_cls: 0.045430, grad_norm: 0.328291
Steps:   1%| | 5887/1000000 [29:00<80:42:29,  3.42it/s, grad_norm=0.348, loss_final=0.883, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:27:17[39m] Step: 5887, Training Logs: loss_final: 0.911554, loss_mean: 0.867931, loss_mean_cls: 0.043624, grad_norm: 0.324170
Steps:   1%| | 5894/1000000 [29:02<79:58:53,  3.45it/s, grad_norm=0.33, loss_final=0.881, loss_mean=0.837, loss_mean_cls=0.0[[34m2025-10-04 12:27:19[39m] Step: 5894, Training Logs: loss_final: 0.877045, loss_mean: 0.832268, loss_mean_cls: 0.044777, grad_norm: 0.228009
Steps:   1%| | 5901/1000000 [29:04<80:22:00,  3.44it/s, grad_norm=0.284, loss_final=0.907, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:27:21[39m] Step: 5901, Training Logs: loss_final: 0.884052, loss_mean: 0.839927, loss_mean_cls: 0.044125, grad_norm: 0.301485
Steps:   1%| | 5908/1000000 [29:06<82:24:14,  3.35it/s, grad_norm=0.292, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:27:23[39m] Step: 5908, Training Logs: loss_final: 0.894377, loss_mean: 0.849818, loss_mean_cls: 0.044559, grad_norm: 0.236711
Steps:   1%| | 5915/1000000 [29:08<80:08:48,  3.45it/s, grad_norm=0.209, loss_final=0.916, loss_mean=0.872, loss_mean_cls=0.[[34m2025-10-04 12:27:25[39m] Step: 5915, Training Logs: loss_final: 0.909700, loss_mean: 0.865388, loss_mean_cls: 0.044312, grad_norm: 0.262439
Steps:   1%| | 5922/1000000 [29:10<80:58:15,  3.41it/s, grad_norm=0.246, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:27:27[39m] Step: 5922, Training Logs: loss_final: 0.881282, loss_mean: 0.836866, loss_mean_cls: 0.044416, grad_norm: 0.266674
Steps:   1%| | 5929/1000000 [29:12<82:53:18,  3.33it/s, grad_norm=0.285, loss_final=0.881, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:27:29[39m] Step: 5929, Training Logs: loss_final: 0.885157, loss_mean: 0.841269, loss_mean_cls: 0.043888, grad_norm: 0.484504
Steps:   1%| | 5935/1000000 [29:14<81:47:01,  3.38it/s, grad_norm=0.246, loss_final=0.897, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:27:31[39m] Step: 5935, Training Logs: loss_final: 0.879090, loss_mean: 0.836078, loss_mean_cls: 0.043011, grad_norm: 0.292288
Steps:   1%| | 5942/1000000 [29:16<79:59:08,  3.45it/s, grad_norm=0.335, loss_final=0.861, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:27:33[39m] Step: 5942, Training Logs: loss_final: 0.883458, loss_mean: 0.839051, loss_mean_cls: 0.044407, grad_norm: 0.361928
Steps:   1%| | 5949/1000000 [29:18<82:42:08,  3.34it/s, grad_norm=0.317, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:27:35[39m] Step: 5949, Training Logs: loss_final: 0.905041, loss_mean: 0.861914, loss_mean_cls: 0.043127, grad_norm: 0.336336
Steps:   1%| | 5956/1000000 [29:20<83:52:33,  3.29it/s, grad_norm=0.292, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:27:37[39m] Step: 5956, Training Logs: loss_final: 0.884900, loss_mean: 0.841897, loss_mean_cls: 0.043003, grad_norm: 0.289357
Steps:   1%| | 5963/1000000 [29:23<81:28:12,  3.39it/s, grad_norm=0.39, loss_final=0.88, loss_mean=0.834, loss_mean_cls=0.04[[34m2025-10-04 12:27:39[39m] Step: 5963, Training Logs: loss_final: 0.899198, loss_mean: 0.853907, loss_mean_cls: 0.045291, grad_norm: 0.489327
Steps:   1%| | 5968/1000000 [29:24<79:49:46,  3.46it/s, grad_norm=0.4, loss_final=0.893, loss_mean=0.85, loss_mean_cls=0.043[[34m2025-10-04 12:27:41[39m] Step: 5968, Training Logs: loss_final: 0.891153, loss_mean: 0.845761, loss_mean_cls: 0.045392, grad_norm: 0.347061
Steps:   1%| | 5975/1000000 [29:26<79:46:45,  3.46it/s, grad_norm=0.47, loss_final=0.862, loss_mean=0.817, loss_mean_cls=0.0[[34m2025-10-04 12:27:43[39m] Step: 5975, Training Logs: loss_final: 0.899258, loss_mean: 0.855604, loss_mean_cls: 0.043654, grad_norm: 0.511406
Steps:   1%| | 5982/1000000 [29:28<79:51:05,  3.46it/s, grad_norm=0.404, loss_final=0.888, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:27:45[39m] Step: 5982, Training Logs: loss_final: 0.877640, loss_mean: 0.832670, loss_mean_cls: 0.044970, grad_norm: 0.319317
Steps:   1%| | 5994/1000000 [29:32<80:32:38,  3.43it/s, grad_norm=0.276, loss_final=0.9, loss_mean=0.856, loss_mean_cls=0.04[[34m2025-10-04 12:27:47[39m] Step: 5988, Training Logs: loss_final: 0.884879, loss_mean: 0.840664, loss_mean_cls: 0.044215, grad_norm: 0.567514
Steps:   1%| | 6001/1000000 [29:34<81:01:28,  3.41it/s, grad_norm=0.213, loss_final=0.895, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:27:49[39m] Step: 5995, Training Logs: loss_final: 0.891734, loss_mean: 0.846743, loss_mean_cls: 0.044991, grad_norm: 0.362990
Steps:   1%| | 6008/1000000 [29:36<79:19:40,  3.48it/s, grad_norm=0.299, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:27:51[39m] Step: 6002, Training Logs: loss_final: 0.906023, loss_mean: 0.861935, loss_mean_cls: 0.044088, grad_norm: 0.287943
Steps:   1%| | 6015/1000000 [29:38<79:20:07,  3.48it/s, grad_norm=0.333, loss_final=0.897, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:27:53[39m] Step: 6009, Training Logs: loss_final: 0.883121, loss_mean: 0.837772, loss_mean_cls: 0.045349, grad_norm: 0.354140
Steps:   1%| | 6016/1000000 [29:38<79:29:20,  3.47it/s, grad_norm=0.333, loss_final=0.897, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:27:55[39m] Step: 6016, Training Logs: loss_final: 0.882934, loss_mean: 0.838144, loss_mean_cls: 0.044789, grad_norm: 0.413249
Steps:   1%| | 6023/1000000 [29:40<80:13:53,  3.44it/s, grad_norm=0.442, loss_final=0.897, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:27:57[39m] Step: 6023, Training Logs: loss_final: 0.870091, loss_mean: 0.826879, loss_mean_cls: 0.043212, grad_norm: 0.481160
Steps:   1%| | 6030/1000000 [29:42<81:03:18,  3.41it/s, grad_norm=0.372, loss_final=0.892, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:27:59[39m] Step: 6030, Training Logs: loss_final: 0.870325, loss_mean: 0.824982, loss_mean_cls: 0.045343, grad_norm: 0.350397
Steps:   1%| | 6037/1000000 [29:44<78:51:55,  3.50it/s, grad_norm=0.317, loss_final=0.89, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:28:01[39m] Step: 6037, Training Logs: loss_final: 0.897653, loss_mean: 0.853705, loss_mean_cls: 0.043948, grad_norm: 0.375466
Steps:   1%| | 6044/1000000 [29:46<80:03:07,  3.45it/s, grad_norm=0.314, loss_final=0.877, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:28:03[39m] Step: 6044, Training Logs: loss_final: 0.883876, loss_mean: 0.838611, loss_mean_cls: 0.045265, grad_norm: 0.281767
Steps:   1%| | 6051/1000000 [29:48<81:16:04,  3.40it/s, grad_norm=0.42, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.0[[34m2025-10-04 12:28:05[39m] Step: 6051, Training Logs: loss_final: 0.899724, loss_mean: 0.855984, loss_mean_cls: 0.043741, grad_norm: 0.323069
Steps:   1%| | 6058/1000000 [29:50<84:24:34,  3.27it/s, grad_norm=0.231, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:28:07[39m] Step: 6058, Training Logs: loss_final: 0.874615, loss_mean: 0.829641, loss_mean_cls: 0.044974, grad_norm: 0.318416
Steps:   1%| | 6065/1000000 [29:52<84:11:39,  3.28it/s, grad_norm=0.392, loss_final=0.896, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:28:09[39m] Step: 6065, Training Logs: loss_final: 0.880422, loss_mean: 0.835102, loss_mean_cls: 0.045320, grad_norm: 0.357408
Steps:   1%| | 6071/1000000 [29:54<84:58:57,  3.25it/s, grad_norm=0.409, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:28:11[39m] Step: 6071, Training Logs: loss_final: 0.891485, loss_mean: 0.846950, loss_mean_cls: 0.044535, grad_norm: 0.284286
Steps:   1%| | 6078/1000000 [29:56<81:26:31,  3.39it/s, grad_norm=0.278, loss_final=0.889, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:28:13[39m] Step: 6078, Training Logs: loss_final: 0.891850, loss_mean: 0.847110, loss_mean_cls: 0.044740, grad_norm: 0.273926
Steps:   1%| | 6085/1000000 [29:58<80:21:16,  3.44it/s, grad_norm=0.328, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:28:15[39m] Step: 6085, Training Logs: loss_final: 0.899306, loss_mean: 0.856073, loss_mean_cls: 0.043232, grad_norm: 0.358808
Steps:   1%| | 6092/1000000 [30:00<82:27:08,  3.35it/s, grad_norm=0.267, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:28:17[39m] Step: 6092, Training Logs: loss_final: 0.865641, loss_mean: 0.821050, loss_mean_cls: 0.044592, grad_norm: 0.405539
Steps:   1%| | 6098/1000000 [30:02<82:36:02,  3.34it/s, grad_norm=0.279, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:28:19[39m] Step: 6098, Training Logs: loss_final: 0.873450, loss_mean: 0.829608, loss_mean_cls: 0.043842, grad_norm: 0.243565
Steps:   1%| | 6105/1000000 [30:04<81:34:00,  3.38it/s, grad_norm=0.363, loss_final=0.884, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:28:21[39m] Step: 6105, Training Logs: loss_final: 0.899948, loss_mean: 0.855402, loss_mean_cls: 0.044546, grad_norm: 0.363778
Steps:   1%| | 6112/1000000 [30:06<83:42:17,  3.30it/s, grad_norm=0.298, loss_final=0.868, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:28:23[39m] Step: 6112, Training Logs: loss_final: 0.885615, loss_mean: 0.840078, loss_mean_cls: 0.045536, grad_norm: 0.283123
Steps:   1%| | 6119/1000000 [30:08<79:35:51,  3.47it/s, grad_norm=0.27, loss_final=0.878, loss_mean=0.833, loss_mean_cls=0.0[[34m2025-10-04 12:28:25[39m] Step: 6119, Training Logs: loss_final: 0.875999, loss_mean: 0.830974, loss_mean_cls: 0.045025, grad_norm: 0.278281
Steps:   1%| | 6126/1000000 [30:10<79:30:50,  3.47it/s, grad_norm=0.222, loss_final=0.898, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:28:27[39m] Step: 6126, Training Logs: loss_final: 0.877022, loss_mean: 0.832747, loss_mean_cls: 0.044276, grad_norm: 0.235225
Steps:   1%| | 6133/1000000 [30:12<79:19:21,  3.48it/s, grad_norm=0.325, loss_final=0.88, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:28:29[39m] Step: 6133, Training Logs: loss_final: 0.905884, loss_mean: 0.861862, loss_mean_cls: 0.044022, grad_norm: 0.368524
Steps:   1%| | 6140/1000000 [30:14<80:30:49,  3.43it/s, grad_norm=0.281, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:28:31[39m] Step: 6140, Training Logs: loss_final: 0.887282, loss_mean: 0.844226, loss_mean_cls: 0.043057, grad_norm: 0.364371
Steps:   1%| | 6147/1000000 [30:17<80:48:05,  3.42it/s, grad_norm=0.442, loss_final=0.878, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:28:33[39m] Step: 6147, Training Logs: loss_final: 0.876927, loss_mean: 0.830989, loss_mean_cls: 0.045938, grad_norm: 0.253630
Steps:   1%| | 6154/1000000 [30:19<81:03:51,  3.41it/s, grad_norm=0.242, loss_final=0.865, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:28:35[39m] Step: 6154, Training Logs: loss_final: 0.893251, loss_mean: 0.849450, loss_mean_cls: 0.043801, grad_norm: 0.316044
Steps:   1%| | 6160/1000000 [30:20<81:29:24,  3.39it/s, grad_norm=0.395, loss_final=0.896, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:28:37[39m] Step: 6160, Training Logs: loss_final: 0.890489, loss_mean: 0.845380, loss_mean_cls: 0.045109, grad_norm: 0.319647
Steps:   1%| | 6167/1000000 [30:22<80:33:48,  3.43it/s, grad_norm=0.396, loss_final=0.911, loss_mean=0.867, loss_mean_cls=0.[[34m2025-10-04 12:28:39[39m] Step: 6167, Training Logs: loss_final: 0.892573, loss_mean: 0.848345, loss_mean_cls: 0.044229, grad_norm: 0.267723
Steps:   1%| | 6174/1000000 [30:24<80:14:37,  3.44it/s, grad_norm=0.441, loss_final=0.899, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:28:41[39m] Step: 6174, Training Logs: loss_final: 0.891278, loss_mean: 0.847152, loss_mean_cls: 0.044126, grad_norm: 0.352950
Steps:   1%| | 6181/1000000 [30:26<80:49:35,  3.42it/s, grad_norm=0.802, loss_final=0.88, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:28:43[39m] Step: 6181, Training Logs: loss_final: 0.875972, loss_mean: 0.830838, loss_mean_cls: 0.045135, grad_norm: 0.498092
Steps:   1%| | 6188/1000000 [30:29<81:39:31,  3.38it/s, grad_norm=0.377, loss_final=0.868, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:28:45[39m] Step: 6188, Training Logs: loss_final: 0.888669, loss_mean: 0.844904, loss_mean_cls: 0.043765, grad_norm: 0.281628
Steps:   1%| | 6195/1000000 [30:31<79:20:17,  3.48it/s, grad_norm=0.333, loss_final=0.896, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:28:47[39m] Step: 6195, Training Logs: loss_final: 0.914256, loss_mean: 0.870828, loss_mean_cls: 0.043428, grad_norm: 0.480242
Steps:   1%| | 6202/1000000 [30:33<79:15:05,  3.48it/s, grad_norm=0.232, loss_final=0.862, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:28:49[39m] Step: 6202, Training Logs: loss_final: 0.869727, loss_mean: 0.824838, loss_mean_cls: 0.044888, grad_norm: 0.401245
Steps:   1%| | 6209/1000000 [30:35<80:22:44,  3.43it/s, grad_norm=0.353, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:28:51[39m] Step: 6209, Training Logs: loss_final: 0.885259, loss_mean: 0.841098, loss_mean_cls: 0.044161, grad_norm: 0.318724
Steps:   1%| | 6216/1000000 [30:37<81:03:04,  3.41it/s, grad_norm=0.251, loss_final=0.885, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:28:53[39m] Step: 6216, Training Logs: loss_final: 0.886520, loss_mean: 0.843410, loss_mean_cls: 0.043111, grad_norm: 0.239477
Steps:   1%| | 6223/1000000 [30:39<81:23:03,  3.39it/s, grad_norm=0.301, loss_final=0.89, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:28:55[39m] Step: 6223, Training Logs: loss_final: 0.896208, loss_mean: 0.852480, loss_mean_cls: 0.043728, grad_norm: 0.288989
Steps:   1%| | 6230/1000000 [30:41<83:53:53,  3.29it/s, grad_norm=0.28, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.04[[34m2025-10-04 12:28:57[39m] Step: 6230, Training Logs: loss_final: 0.892578, loss_mean: 0.848727, loss_mean_cls: 0.043851, grad_norm: 0.320898
Steps:   1%| | 6236/1000000 [30:43<81:57:44,  3.37it/s, grad_norm=0.305, loss_final=0.891, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:28:59[39m] Step: 6236, Training Logs: loss_final: 0.888992, loss_mean: 0.845108, loss_mean_cls: 0.043884, grad_norm: 0.379993
Steps:   1%| | 6243/1000000 [30:45<81:42:14,  3.38it/s, grad_norm=0.292, loss_final=0.903, loss_mean=0.859, loss_mean_cls=0.[[34m2025-10-04 12:29:01[39m] Step: 6243, Training Logs: loss_final: 0.894086, loss_mean: 0.851331, loss_mean_cls: 0.042755, grad_norm: 0.372786
Steps:   1%| | 6250/1000000 [30:47<80:10:46,  3.44it/s, grad_norm=0.301, loss_final=0.881, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:29:03[39m] Step: 6250, Training Logs: loss_final: 0.892589, loss_mean: 0.849318, loss_mean_cls: 0.043271, grad_norm: 0.295375
Steps:   1%| | 6255/1000000 [30:48<80:08:17,  3.44it/s, grad_norm=0.305, loss_final=0.898, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:29:05[39m] Step: 6255, Training Logs: loss_final: 0.866694, loss_mean: 0.821884, loss_mean_cls: 0.044810, grad_norm: 0.398888
Steps:   1%| | 6262/1000000 [30:50<80:14:43,  3.44it/s, grad_norm=0.436, loss_final=0.87, loss_mean=0.826, loss_mean_cls=0.0[[34m2025-10-04 12:29:07[39m] Step: 6262, Training Logs: loss_final: 0.885935, loss_mean: 0.841829, loss_mean_cls: 0.044106, grad_norm: 0.424366
Steps:   1%| | 6269/1000000 [30:52<81:09:21,  3.40it/s, grad_norm=0.254, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:29:09[39m] Step: 6269, Training Logs: loss_final: 0.895725, loss_mean: 0.851248, loss_mean_cls: 0.044477, grad_norm: 0.399366
Steps:   1%| | 6276/1000000 [30:54<81:37:00,  3.38it/s, grad_norm=0.36, loss_final=0.87, loss_mean=0.826, loss_mean_cls=0.04[[34m2025-10-04 12:29:11[39m] Step: 6276, Training Logs: loss_final: 0.893187, loss_mean: 0.849415, loss_mean_cls: 0.043772, grad_norm: 0.319209
Steps:   1%| | 6283/1000000 [30:56<80:19:20,  3.44it/s, grad_norm=0.361, loss_final=0.907, loss_mean=0.863, loss_mean_cls=0.[[34m2025-10-04 12:29:13[39m] Step: 6283, Training Logs: loss_final: 0.882015, loss_mean: 0.837869, loss_mean_cls: 0.044146, grad_norm: 0.432728
Steps:   1%| | 6290/1000000 [30:58<78:57:03,  3.50it/s, grad_norm=0.345, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:29:15[39m] Step: 6290, Training Logs: loss_final: 0.891243, loss_mean: 0.847260, loss_mean_cls: 0.043982, grad_norm: 0.276098
Steps:   1%| | 6297/1000000 [31:00<79:29:53,  3.47it/s, grad_norm=0.281, loss_final=0.88, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:29:17[39m] Step: 6297, Training Logs: loss_final: 0.860431, loss_mean: 0.816140, loss_mean_cls: 0.044291, grad_norm: 0.424231
Steps:   1%| | 6304/1000000 [31:02<80:30:09,  3.43it/s, grad_norm=0.256, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:29:19[39m] Step: 6304, Training Logs: loss_final: 0.893905, loss_mean: 0.850281, loss_mean_cls: 0.043624, grad_norm: 0.250771
Steps:   1%| | 6311/1000000 [31:04<81:09:40,  3.40it/s, grad_norm=0.323, loss_final=0.89, loss_mean=0.845, loss_mean_cls=0.0[[34m2025-10-04 12:29:21[39m] Step: 6311, Training Logs: loss_final: 0.872316, loss_mean: 0.827034, loss_mean_cls: 0.045282, grad_norm: 0.257455
Steps:   1%| | 6318/1000000 [31:07<84:37:30,  3.26it/s, grad_norm=0.261, loss_final=0.9, loss_mean=0.855, loss_mean_cls=0.04[[34m2025-10-04 12:29:23[39m] Step: 6318, Training Logs: loss_final: 0.909302, loss_mean: 0.865935, loss_mean_cls: 0.043367, grad_norm: 0.315204
Steps:   1%| | 6324/1000000 [31:08<81:18:44,  3.39it/s, grad_norm=0.412, loss_final=0.87, loss_mean=0.825, loss_mean_cls=0.0[[34m2025-10-04 12:29:25[39m] Step: 6324, Training Logs: loss_final: 0.870348, loss_mean: 0.826950, loss_mean_cls: 0.043398, grad_norm: 0.263342
Steps:   1%| | 6331/1000000 [31:10<82:22:20,  3.35it/s, grad_norm=0.444, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:29:27[39m] Step: 6331, Training Logs: loss_final: 0.883095, loss_mean: 0.838643, loss_mean_cls: 0.044453, grad_norm: 0.285930
Steps:   1%| | 6338/1000000 [31:12<80:13:18,  3.44it/s, grad_norm=0.417, loss_final=0.902, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:29:29[39m] Step: 6338, Training Logs: loss_final: 0.882844, loss_mean: 0.838663, loss_mean_cls: 0.044181, grad_norm: 0.348520
Steps:   1%| | 6345/1000000 [31:14<81:09:25,  3.40it/s, grad_norm=0.254, loss_final=0.893, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:29:31[39m] Step: 6345, Training Logs: loss_final: 0.885104, loss_mean: 0.841671, loss_mean_cls: 0.043433, grad_norm: 0.267409
Steps:   1%| | 6352/1000000 [31:16<81:50:30,  3.37it/s, grad_norm=0.201, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:29:33[39m] Step: 6352, Training Logs: loss_final: 0.874186, loss_mean: 0.830654, loss_mean_cls: 0.043531, grad_norm: 0.274284
Steps:   1%| | 6359/1000000 [31:19<80:17:10,  3.44it/s, grad_norm=0.529, loss_final=0.882, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:29:35[39m] Step: 6359, Training Logs: loss_final: 0.887045, loss_mean: 0.842972, loss_mean_cls: 0.044073, grad_norm: 0.232421
Steps:   1%| | 6366/1000000 [31:21<81:42:02,  3.38it/s, grad_norm=0.576, loss_final=0.881, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:29:37[39m] Step: 6366, Training Logs: loss_final: 0.870990, loss_mean: 0.825360, loss_mean_cls: 0.045630, grad_norm: 0.283622
Steps:   1%| | 6373/1000000 [31:23<80:00:29,  3.45it/s, grad_norm=0.362, loss_final=0.877, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:29:39[39m] Step: 6373, Training Logs: loss_final: 0.863359, loss_mean: 0.818896, loss_mean_cls: 0.044463, grad_norm: 0.309934
Steps:   1%| | 6380/1000000 [31:25<81:09:01,  3.40it/s, grad_norm=0.285, loss_final=0.878, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:29:41[39m] Step: 6380, Training Logs: loss_final: 0.894996, loss_mean: 0.851247, loss_mean_cls: 0.043749, grad_norm: 0.261832
Steps:   1%| | 6387/1000000 [31:27<81:25:19,  3.39it/s, grad_norm=0.401, loss_final=0.87, loss_mean=0.824, loss_mean_cls=0.0[[34m2025-10-04 12:29:43[39m] Step: 6387, Training Logs: loss_final: 0.898059, loss_mean: 0.854285, loss_mean_cls: 0.043774, grad_norm: 0.653601
Steps:   1%| | 6393/1000000 [31:28<81:29:55,  3.39it/s, grad_norm=0.314, loss_final=0.88, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:29:45[39m] Step: 6393, Training Logs: loss_final: 0.890702, loss_mean: 0.847477, loss_mean_cls: 0.043225, grad_norm: 0.475637
Steps:   1%| | 6400/1000000 [31:31<81:11:22,  3.40it/s, grad_norm=0.257, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:29:47[39m] Step: 6400, Training Logs: loss_final: 0.872898, loss_mean: 0.827599, loss_mean_cls: 0.045299, grad_norm: 0.519117
Steps:   1%| | 6407/1000000 [31:33<81:16:37,  3.40it/s, grad_norm=0.277, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:29:49[39m] Step: 6407, Training Logs: loss_final: 0.875569, loss_mean: 0.830995, loss_mean_cls: 0.044575, grad_norm: 0.401239
Steps:   1%| | 6414/1000000 [31:35<79:06:43,  3.49it/s, grad_norm=0.265, loss_final=0.875, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:29:51[39m] Step: 6414, Training Logs: loss_final: 0.882053, loss_mean: 0.838796, loss_mean_cls: 0.043256, grad_norm: 0.381747
Steps:   1%| | 6421/1000000 [31:37<82:49:30,  3.33it/s, grad_norm=0.37, loss_final=0.892, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:29:53[39m] Step: 6421, Training Logs: loss_final: 0.872441, loss_mean: 0.827198, loss_mean_cls: 0.045243, grad_norm: 0.309719
Steps:   1%| | 6428/1000000 [31:39<80:55:07,  3.41it/s, grad_norm=0.463, loss_final=0.882, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:29:55[39m] Step: 6428, Training Logs: loss_final: 0.903776, loss_mean: 0.859946, loss_mean_cls: 0.043831, grad_norm: 0.309757
Steps:   1%| | 6435/1000000 [31:41<81:48:11,  3.37it/s, grad_norm=0.376, loss_final=0.873, loss_mean=0.827, loss_mean_cls=0.[[34m2025-10-04 12:29:57[39m] Step: 6435, Training Logs: loss_final: 0.883294, loss_mean: 0.837262, loss_mean_cls: 0.046032, grad_norm: 0.548889
Steps:   1%| | 6442/1000000 [31:43<79:40:31,  3.46it/s, grad_norm=0.321, loss_final=0.918, loss_mean=0.874, loss_mean_cls=0.[[34m2025-10-04 12:29:59[39m] Step: 6442, Training Logs: loss_final: 0.898360, loss_mean: 0.854923, loss_mean_cls: 0.043436, grad_norm: 0.459982
Steps:   1%| | 6449/1000000 [31:45<81:50:57,  3.37it/s, grad_norm=0.324, loss_final=0.904, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:30:02[39m] Step: 6449, Training Logs: loss_final: 0.874205, loss_mean: 0.829155, loss_mean_cls: 0.045050, grad_norm: 0.454682
Steps:   1%| | 6456/1000000 [31:47<81:07:19,  3.40it/s, grad_norm=0.306, loss_final=0.889, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:30:04[39m] Step: 6456, Training Logs: loss_final: 0.908029, loss_mean: 0.863230, loss_mean_cls: 0.044799, grad_norm: 0.339859
Steps:   1%| | 6462/1000000 [31:49<81:02:33,  3.41it/s, grad_norm=0.391, loss_final=0.862, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:30:05[39m] Step: 6462, Training Logs: loss_final: 0.893430, loss_mean: 0.849373, loss_mean_cls: 0.044057, grad_norm: 0.312354
Steps:   1%| | 6469/1000000 [31:51<80:42:40,  3.42it/s, grad_norm=0.221, loss_final=0.899, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:30:07[39m] Step: 6469, Training Logs: loss_final: 0.921766, loss_mean: 0.878166, loss_mean_cls: 0.043600, grad_norm: 0.323191
Steps:   1%| | 6476/1000000 [31:53<81:10:22,  3.40it/s, grad_norm=0.302, loss_final=0.877, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:30:09[39m] Step: 6476, Training Logs: loss_final: 0.900385, loss_mean: 0.856402, loss_mean_cls: 0.043983, grad_norm: 0.301263
Steps:   1%| | 6483/1000000 [31:55<80:24:45,  3.43it/s, grad_norm=0.31, loss_final=0.893, loss_mean=0.849, loss_mean_cls=0.0[[34m2025-10-04 12:30:11[39m] Step: 6483, Training Logs: loss_final: 0.873757, loss_mean: 0.829151, loss_mean_cls: 0.044606, grad_norm: 0.335834
Steps:   1%| | 6490/1000000 [31:57<81:26:28,  3.39it/s, grad_norm=0.241, loss_final=0.896, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:30:14[39m] Step: 6490, Training Logs: loss_final: 0.888244, loss_mean: 0.844805, loss_mean_cls: 0.043439, grad_norm: 0.426763
Steps:   1%| | 6497/1000000 [31:59<79:38:19,  3.47it/s, grad_norm=0.332, loss_final=0.885, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:30:16[39m] Step: 6497, Training Logs: loss_final: 0.887477, loss_mean: 0.842955, loss_mean_cls: 0.044523, grad_norm: 0.534970
Steps:   1%| | 6504/1000000 [32:01<85:12:02,  3.24it/s, grad_norm=0.296, loss_final=0.859, loss_mean=0.814, loss_mean_cls=0.[[34m2025-10-04 12:30:18[39m] Step: 6504, Training Logs: loss_final: 0.891358, loss_mean: 0.847744, loss_mean_cls: 0.043613, grad_norm: 0.587771
Steps:   1%| | 6510/1000000 [32:03<81:41:38,  3.38it/s, grad_norm=0.385, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:30:19[39m] Step: 6510, Training Logs: loss_final: 0.891665, loss_mean: 0.848684, loss_mean_cls: 0.042981, grad_norm: 0.290976
Steps:   1%| | 6517/1000000 [32:05<80:35:40,  3.42it/s, grad_norm=0.285, loss_final=0.862, loss_mean=0.818, loss_mean_cls=0.[[34m2025-10-04 12:30:22[39m] Step: 6517, Training Logs: loss_final: 0.886407, loss_mean: 0.842053, loss_mean_cls: 0.044354, grad_norm: 0.383793
Steps:   1%| | 6524/1000000 [32:07<80:24:55,  3.43it/s, grad_norm=0.256, loss_final=0.887, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:30:24[39m] Step: 6524, Training Logs: loss_final: 0.871402, loss_mean: 0.827960, loss_mean_cls: 0.043442, grad_norm: 0.289497
Steps:   1%| | 6531/1000000 [32:09<80:28:07,  3.43it/s, grad_norm=0.475, loss_final=0.899, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:30:26[39m] Step: 6531, Training Logs: loss_final: 0.886143, loss_mean: 0.841482, loss_mean_cls: 0.044662, grad_norm: 0.325344
Steps:   1%| | 6538/1000000 [32:11<81:29:27,  3.39it/s, grad_norm=0.286, loss_final=0.878, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:30:28[39m] Step: 6538, Training Logs: loss_final: 0.867114, loss_mean: 0.823297, loss_mean_cls: 0.043818, grad_norm: 0.288194
Steps:   1%| | 6545/1000000 [32:13<80:32:45,  3.43it/s, grad_norm=0.223, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:30:30[39m] Step: 6545, Training Logs: loss_final: 0.883476, loss_mean: 0.839376, loss_mean_cls: 0.044100, grad_norm: 0.393282
Steps:   1%| | 6552/1000000 [32:15<79:34:37,  3.47it/s, grad_norm=0.339, loss_final=0.866, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:30:32[39m] Step: 6552, Training Logs: loss_final: 0.882522, loss_mean: 0.839122, loss_mean_cls: 0.043400, grad_norm: 0.400958
Steps:   1%| | 6557/1000000 [32:16<79:18:48,  3.48it/s, grad_norm=0.421, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:30:33[39m] Step: 6557, Training Logs: loss_final: 0.885984, loss_mean: 0.843330, loss_mean_cls: 0.042654, grad_norm: 0.377866
Steps:   1%| | 6564/1000000 [32:19<81:57:19,  3.37it/s, grad_norm=0.492, loss_final=0.883, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:30:35[39m] Step: 6564, Training Logs: loss_final: 0.872409, loss_mean: 0.828886, loss_mean_cls: 0.043523, grad_norm: 0.416970
Steps:   1%| | 6571/1000000 [32:21<79:49:30,  3.46it/s, grad_norm=0.61, loss_final=0.875, loss_mean=0.829, loss_mean_cls=0.0[[34m2025-10-04 12:30:37[39m] Step: 6571, Training Logs: loss_final: 0.878646, loss_mean: 0.835336, loss_mean_cls: 0.043311, grad_norm: 0.552749
Steps:   1%| | 6578/1000000 [32:23<81:34:09,  3.38it/s, grad_norm=0.409, loss_final=0.903, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:30:39[39m] Step: 6578, Training Logs: loss_final: 0.887302, loss_mean: 0.842154, loss_mean_cls: 0.045148, grad_norm: 0.466431
Steps:   1%| | 6585/1000000 [32:25<84:07:41,  3.28it/s, grad_norm=0.329, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:30:41[39m] Step: 6585, Training Logs: loss_final: 0.886684, loss_mean: 0.842571, loss_mean_cls: 0.044114, grad_norm: 0.330523
Steps:   1%| | 6592/1000000 [32:27<80:53:51,  3.41it/s, grad_norm=0.687, loss_final=0.862, loss_mean=0.818, loss_mean_cls=0.[[34m2025-10-04 12:30:43[39m] Step: 6592, Training Logs: loss_final: 0.879019, loss_mean: 0.835374, loss_mean_cls: 0.043645, grad_norm: 0.291535
Steps:   1%| | 6598/1000000 [32:29<82:39:48,  3.34it/s, grad_norm=0.555, loss_final=0.881, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:30:45[39m] Step: 6598, Training Logs: loss_final: 0.885680, loss_mean: 0.841193, loss_mean_cls: 0.044487, grad_norm: 0.286321
Steps:   1%| | 6605/1000000 [32:31<81:57:22,  3.37it/s, grad_norm=0.334, loss_final=0.884, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:30:47[39m] Step: 6605, Training Logs: loss_final: 0.874587, loss_mean: 0.831746, loss_mean_cls: 0.042841, grad_norm: 0.386927
Steps:   1%| | 6612/1000000 [32:33<79:34:17,  3.47it/s, grad_norm=0.322, loss_final=0.886, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:30:49[39m] Step: 6612, Training Logs: loss_final: 0.910551, loss_mean: 0.867903, loss_mean_cls: 0.042647, grad_norm: 0.420054
Steps:   1%| | 6619/1000000 [32:35<80:28:52,  3.43it/s, grad_norm=0.328, loss_final=0.876, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:30:51[39m] Step: 6619, Training Logs: loss_final: 0.858060, loss_mean: 0.814398, loss_mean_cls: 0.043662, grad_norm: 0.291698
Steps:   1%| | 6626/1000000 [32:37<81:31:21,  3.38it/s, grad_norm=0.271, loss_final=0.879, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:30:53[39m] Step: 6626, Training Logs: loss_final: 0.869046, loss_mean: 0.824030, loss_mean_cls: 0.045017, grad_norm: 0.396830
Steps:   1%| | 6633/1000000 [32:39<81:28:54,  3.39it/s, grad_norm=0.398, loss_final=0.893, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:30:56[39m] Step: 6633, Training Logs: loss_final: 0.885453, loss_mean: 0.842105, loss_mean_cls: 0.043348, grad_norm: 0.252448
Steps:   1%| | 6640/1000000 [32:41<80:28:58,  3.43it/s, grad_norm=0.306, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:30:58[39m] Step: 6640, Training Logs: loss_final: 0.883323, loss_mean: 0.839355, loss_mean_cls: 0.043968, grad_norm: 0.208057
Steps:   1%| | 6647/1000000 [32:43<79:34:27,  3.47it/s, grad_norm=0.334, loss_final=0.891, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:31:00[39m] Step: 6647, Training Logs: loss_final: 0.876077, loss_mean: 0.832334, loss_mean_cls: 0.043743, grad_norm: 0.282785
Steps:   1%| | 6653/1000000 [32:45<80:15:50,  3.44it/s, grad_norm=0.3, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.04[[34m2025-10-04 12:31:01[39m] Step: 6653, Training Logs: loss_final: 0.879443, loss_mean: 0.835155, loss_mean_cls: 0.044288, grad_norm: 0.281259
Steps:   1%| | 6660/1000000 [32:47<79:57:37,  3.45it/s, grad_norm=0.231, loss_final=0.879, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:31:03[39m] Step: 6660, Training Logs: loss_final: 0.881560, loss_mean: 0.837548, loss_mean_cls: 0.044013, grad_norm: 0.284217
Steps:   1%| | 6667/1000000 [32:49<79:18:26,  3.48it/s, grad_norm=0.234, loss_final=0.882, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:31:05[39m] Step: 6667, Training Logs: loss_final: 0.880216, loss_mean: 0.835724, loss_mean_cls: 0.044492, grad_norm: 0.316117
Steps:   1%| | 6674/1000000 [32:51<81:46:21,  3.37it/s, grad_norm=0.304, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:31:08[39m] Step: 6674, Training Logs: loss_final: 0.879599, loss_mean: 0.835917, loss_mean_cls: 0.043682, grad_norm: 0.287818
Steps:   1%| | 6681/1000000 [32:53<80:38:16,  3.42it/s, grad_norm=0.441, loss_final=0.891, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:31:10[39m] Step: 6681, Training Logs: loss_final: 0.875587, loss_mean: 0.832188, loss_mean_cls: 0.043399, grad_norm: 0.325511
Steps:   1%| | 6688/1000000 [32:55<80:00:04,  3.45it/s, grad_norm=0.258, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:31:12[39m] Step: 6688, Training Logs: loss_final: 0.906574, loss_mean: 0.862290, loss_mean_cls: 0.044284, grad_norm: 0.359423
Steps:   1%| | 6695/1000000 [32:57<80:15:03,  3.44it/s, grad_norm=0.29, loss_final=0.862, loss_mean=0.818, loss_mean_cls=0.0[[34m2025-10-04 12:31:14[39m] Step: 6695, Training Logs: loss_final: 0.878392, loss_mean: 0.834748, loss_mean_cls: 0.043645, grad_norm: 0.417268
Steps:   1%| | 6702/1000000 [32:59<80:27:56,  3.43it/s, grad_norm=0.303, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:31:16[39m] Step: 6702, Training Logs: loss_final: 0.877447, loss_mean: 0.832729, loss_mean_cls: 0.044718, grad_norm: 0.331720
Steps:   1%| | 6709/1000000 [33:01<83:30:40,  3.30it/s, grad_norm=0.358, loss_final=0.889, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:31:18[39m] Step: 6709, Training Logs: loss_final: 0.881818, loss_mean: 0.837695, loss_mean_cls: 0.044123, grad_norm: 0.342720
Steps:   1%| | 6715/1000000 [33:03<83:26:07,  3.31it/s, grad_norm=0.266, loss_final=0.891, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:31:20[39m] Step: 6715, Training Logs: loss_final: 0.875726, loss_mean: 0.831376, loss_mean_cls: 0.044349, grad_norm: 0.331942
Steps:   1%| | 6722/1000000 [33:05<81:40:34,  3.38it/s, grad_norm=0.243, loss_final=0.9, loss_mean=0.856, loss_mean_cls=0.04[[34m2025-10-04 12:31:22[39m] Step: 6722, Training Logs: loss_final: 0.886346, loss_mean: 0.842431, loss_mean_cls: 0.043915, grad_norm: 0.258270
Steps:   1%| | 6729/1000000 [33:07<82:43:06,  3.34it/s, grad_norm=0.331, loss_final=0.878, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:31:24[39m] Step: 6729, Training Logs: loss_final: 0.869855, loss_mean: 0.824983, loss_mean_cls: 0.044872, grad_norm: 0.369717
Steps:   1%| | 6736/1000000 [33:09<84:04:25,  3.28it/s, grad_norm=0.247, loss_final=0.87, loss_mean=0.826, loss_mean_cls=0.0[[34m2025-10-04 12:31:26[39m] Step: 6736, Training Logs: loss_final: 0.870481, loss_mean: 0.825179, loss_mean_cls: 0.045302, grad_norm: 0.327900
Steps:   1%| | 6743/1000000 [33:11<80:53:29,  3.41it/s, grad_norm=0.256, loss_final=0.884, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:31:28[39m] Step: 6743, Training Logs: loss_final: 0.869680, loss_mean: 0.825005, loss_mean_cls: 0.044675, grad_norm: 0.386371
Steps:   1%| | 6749/1000000 [33:13<83:54:01,  3.29it/s, grad_norm=0.29, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.04[[34m2025-10-04 12:31:30[39m] Step: 6749, Training Logs: loss_final: 0.852046, loss_mean: 0.807715, loss_mean_cls: 0.044330, grad_norm: 0.288990
Steps:   1%| | 6761/1000000 [33:17<82:24:31,  3.35it/s, grad_norm=0.336, loss_final=0.889, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:31:32[39m] Step: 6755, Training Logs: loss_final: 0.901988, loss_mean: 0.858036, loss_mean_cls: 0.043953, grad_norm: 0.298744
Steps:   1%| | 6768/1000000 [33:19<80:55:17,  3.41it/s, grad_norm=0.258, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:31:34[39m] Step: 6762, Training Logs: loss_final: 0.876564, loss_mean: 0.834175, loss_mean_cls: 0.042389, grad_norm: 0.291505
Steps:   1%| | 6775/1000000 [33:21<79:32:17,  3.47it/s, grad_norm=0.37, loss_final=0.878, loss_mean=0.834, loss_mean_cls=0.0[[34m2025-10-04 12:31:36[39m] Step: 6769, Training Logs: loss_final: 0.874061, loss_mean: 0.828390, loss_mean_cls: 0.045671, grad_norm: 0.354757
Steps:   1%| | 6782/1000000 [33:23<80:15:55,  3.44it/s, grad_norm=0.349, loss_final=0.877, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:31:38[39m] Step: 6776, Training Logs: loss_final: 0.868610, loss_mean: 0.824804, loss_mean_cls: 0.043806, grad_norm: 0.391767
Steps:   1%| | 6789/1000000 [33:25<80:18:38,  3.44it/s, grad_norm=0.291, loss_final=0.879, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:31:40[39m] Step: 6783, Training Logs: loss_final: 0.872443, loss_mean: 0.828462, loss_mean_cls: 0.043981, grad_norm: 0.450742
Steps:   1%| | 6790/1000000 [33:25<79:58:13,  3.45it/s, grad_norm=0.291, loss_final=0.879, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:31:42[39m] Step: 6790, Training Logs: loss_final: 0.868900, loss_mean: 0.824359, loss_mean_cls: 0.044540, grad_norm: 0.199856
Steps:   1%| | 6797/1000000 [33:27<82:52:30,  3.33it/s, grad_norm=0.229, loss_final=0.894, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:31:44[39m] Step: 6797, Training Logs: loss_final: 0.871603, loss_mean: 0.827237, loss_mean_cls: 0.044366, grad_norm: 0.345019
Steps:   1%| | 6804/1000000 [33:29<82:16:11,  3.35it/s, grad_norm=0.449, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:31:46[39m] Step: 6804, Training Logs: loss_final: 0.891414, loss_mean: 0.846944, loss_mean_cls: 0.044471, grad_norm: 0.319171
Steps:   1%| | 6811/1000000 [33:31<79:42:36,  3.46it/s, grad_norm=0.493, loss_final=0.875, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:31:48[39m] Step: 6811, Training Logs: loss_final: 0.901117, loss_mean: 0.858561, loss_mean_cls: 0.042556, grad_norm: 0.506829
Steps:   1%| | 6818/1000000 [33:33<79:34:13,  3.47it/s, grad_norm=0.495, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:31:50[39m] Step: 6818, Training Logs: loss_final: 0.869027, loss_mean: 0.825369, loss_mean_cls: 0.043657, grad_norm: 0.271005
Steps:   1%| | 6823/1000000 [33:35<79:51:12,  3.45it/s, grad_norm=0.367, loss_final=0.883, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:31:51[39m] Step: 6823, Training Logs: loss_final: 0.872760, loss_mean: 0.827693, loss_mean_cls: 0.045067, grad_norm: 0.310052
Steps:   1%| | 6830/1000000 [33:37<79:14:03,  3.48it/s, grad_norm=0.287, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:31:53[39m] Step: 6830, Training Logs: loss_final: 0.886217, loss_mean: 0.842185, loss_mean_cls: 0.044032, grad_norm: 0.353496
Steps:   1%| | 6837/1000000 [33:39<79:12:15,  3.48it/s, grad_norm=0.29, loss_final=0.898, loss_mean=0.855, loss_mean_cls=0.0[[34m2025-10-04 12:31:55[39m] Step: 6837, Training Logs: loss_final: 0.890718, loss_mean: 0.845855, loss_mean_cls: 0.044863, grad_norm: 0.198826
Steps:   1%| | 6844/1000000 [33:41<81:27:20,  3.39it/s, grad_norm=0.25, loss_final=0.881, loss_mean=0.837, loss_mean_cls=0.0[[34m2025-10-04 12:31:57[39m] Step: 6844, Training Logs: loss_final: 0.883777, loss_mean: 0.839974, loss_mean_cls: 0.043803, grad_norm: 0.214675
Steps:   1%| | 6851/1000000 [33:43<80:14:30,  3.44it/s, grad_norm=0.258, loss_final=0.875, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:32:00[39m] Step: 6851, Training Logs: loss_final: 0.891200, loss_mean: 0.846928, loss_mean_cls: 0.044272, grad_norm: 0.230666
Steps:   1%| | 6858/1000000 [33:45<79:06:32,  3.49it/s, grad_norm=0.376, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:32:02[39m] Step: 6858, Training Logs: loss_final: 0.876457, loss_mean: 0.832819, loss_mean_cls: 0.043638, grad_norm: 0.328231
Steps:   1%| | 6865/1000000 [33:47<81:30:02,  3.38it/s, grad_norm=0.292, loss_final=0.861, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:32:04[39m] Step: 6865, Training Logs: loss_final: 0.862495, loss_mean: 0.818252, loss_mean_cls: 0.044242, grad_norm: 0.347372
Steps:   1%| | 6872/1000000 [33:49<80:33:28,  3.42it/s, grad_norm=0.253, loss_final=0.875, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:32:06[39m] Step: 6872, Training Logs: loss_final: 0.888922, loss_mean: 0.844768, loss_mean_cls: 0.044154, grad_norm: 0.400472
Steps:   1%| | 6879/1000000 [33:51<81:13:07,  3.40it/s, grad_norm=0.266, loss_final=0.894, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:32:08[39m] Step: 6879, Training Logs: loss_final: 0.882607, loss_mean: 0.838757, loss_mean_cls: 0.043850, grad_norm: 0.255143
Steps:   1%| | 6886/1000000 [33:53<80:12:44,  3.44it/s, grad_norm=0.232, loss_final=0.881, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:32:10[39m] Step: 6886, Training Logs: loss_final: 0.872946, loss_mean: 0.828288, loss_mean_cls: 0.044658, grad_norm: 0.406976
Steps:   1%| | 6893/1000000 [33:55<80:13:26,  3.44it/s, grad_norm=0.257, loss_final=0.87, loss_mean=0.827, loss_mean_cls=0.0[[34m2025-10-04 12:32:12[39m] Step: 6893, Training Logs: loss_final: 0.873808, loss_mean: 0.829061, loss_mean_cls: 0.044747, grad_norm: 0.276575
Steps:   1%| | 6900/1000000 [33:57<81:22:05,  3.39it/s, grad_norm=0.288, loss_final=0.875, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:32:14[39m] Step: 6900, Training Logs: loss_final: 0.880088, loss_mean: 0.836266, loss_mean_cls: 0.043822, grad_norm: 0.274734
Steps:   1%| | 6907/1000000 [33:59<79:18:41,  3.48it/s, grad_norm=0.346, loss_final=0.904, loss_mean=0.861, loss_mean_cls=0.[[34m2025-10-04 12:32:16[39m] Step: 6907, Training Logs: loss_final: 0.872943, loss_mean: 0.828263, loss_mean_cls: 0.044680, grad_norm: 0.395144
Steps:   1%| | 6914/1000000 [34:01<78:52:18,  3.50it/s, grad_norm=0.408, loss_final=0.882, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:32:18[39m] Step: 6914, Training Logs: loss_final: 0.885458, loss_mean: 0.841031, loss_mean_cls: 0.044427, grad_norm: 0.293808
Steps:   1%| | 6921/1000000 [34:03<81:31:49,  3.38it/s, grad_norm=0.396, loss_final=0.888, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:32:20[39m] Step: 6921, Training Logs: loss_final: 0.857978, loss_mean: 0.812857, loss_mean_cls: 0.045121, grad_norm: 0.232343
Steps:   1%| | 6933/1000000 [34:07<80:04:38,  3.44it/s, grad_norm=0.382, loss_final=0.89, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:32:22[39m] Step: 6927, Training Logs: loss_final: 0.867771, loss_mean: 0.822233, loss_mean_cls: 0.045539, grad_norm: 0.378222
Steps:   1%| | 6940/1000000 [34:09<79:11:56,  3.48it/s, grad_norm=0.337, loss_final=0.883, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:32:24[39m] Step: 6934, Training Logs: loss_final: 0.875370, loss_mean: 0.831353, loss_mean_cls: 0.044016, grad_norm: 0.410547
Steps:   1%| | 6947/1000000 [34:11<79:22:27,  3.48it/s, grad_norm=0.344, loss_final=0.87, loss_mean=0.826, loss_mean_cls=0.0[[34m2025-10-04 12:32:26[39m] Step: 6941, Training Logs: loss_final: 0.860335, loss_mean: 0.816494, loss_mean_cls: 0.043841, grad_norm: 0.402199
Steps:   1%| | 6954/1000000 [34:13<79:00:24,  3.49it/s, grad_norm=0.286, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:32:28[39m] Step: 6948, Training Logs: loss_final: 0.876664, loss_mean: 0.833767, loss_mean_cls: 0.042896, grad_norm: 0.350323
Steps:   1%| | 6955/1000000 [34:13<79:11:00,  3.48it/s, grad_norm=0.286, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:32:30[39m] Step: 6955, Training Logs: loss_final: 0.871776, loss_mean: 0.826548, loss_mean_cls: 0.045228, grad_norm: 0.385982
Steps:   1%| | 6962/1000000 [34:15<82:58:07,  3.32it/s, grad_norm=0.354, loss_final=0.896, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:32:32[39m] Step: 6962, Training Logs: loss_final: 0.893243, loss_mean: 0.849642, loss_mean_cls: 0.043601, grad_norm: 0.213530
Steps:   1%| | 6969/1000000 [34:17<79:34:42,  3.47it/s, grad_norm=0.303, loss_final=0.868, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:32:34[39m] Step: 6969, Training Logs: loss_final: 0.861661, loss_mean: 0.816672, loss_mean_cls: 0.044989, grad_norm: 0.227287
Steps:   1%| | 6976/1000000 [34:19<79:00:54,  3.49it/s, grad_norm=0.306, loss_final=0.883, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:32:36[39m] Step: 6976, Training Logs: loss_final: 0.862595, loss_mean: 0.817541, loss_mean_cls: 0.045054, grad_norm: 0.324380
Steps:   1%| | 6983/1000000 [34:21<80:35:35,  3.42it/s, grad_norm=0.323, loss_final=0.895, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:32:38[39m] Step: 6983, Training Logs: loss_final: 0.872890, loss_mean: 0.828468, loss_mean_cls: 0.044422, grad_norm: 0.301146
Steps:   1%| | 6990/1000000 [34:23<80:24:15,  3.43it/s, grad_norm=0.223, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:32:40[39m] Step: 6990, Training Logs: loss_final: 0.874373, loss_mean: 0.831370, loss_mean_cls: 0.043003, grad_norm: 0.401929
Steps:   1%| | 6997/1000000 [34:25<81:21:29,  3.39it/s, grad_norm=0.3, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.04[[34m2025-10-04 12:32:42[39m] Step: 6997, Training Logs: loss_final: 0.875244, loss_mean: 0.830536, loss_mean_cls: 0.044707, grad_norm: 0.410183
Steps:   1%| | 7004/1000000 [34:27<79:18:22,  3.48it/s, grad_norm=0.304, loss_final=0.877, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:32:44[39m] Step: 7004, Training Logs: loss_final: 0.863402, loss_mean: 0.819085, loss_mean_cls: 0.044317, grad_norm: 0.219960
Steps:   1%| | 7011/1000000 [34:29<79:50:49,  3.45it/s, grad_norm=0.356, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:32:46[39m] Step: 7011, Training Logs: loss_final: 0.901236, loss_mean: 0.857808, loss_mean_cls: 0.043428, grad_norm: 0.373182
Steps:   1%| | 7018/1000000 [34:31<79:42:45,  3.46it/s, grad_norm=0.252, loss_final=0.88, loss_mean=0.837, loss_mean_cls=0.0[[34m2025-10-04 12:32:48[39m] Step: 7018, Training Logs: loss_final: 0.884966, loss_mean: 0.842647, loss_mean_cls: 0.042319, grad_norm: 0.422376
Steps:   1%| | 7025/1000000 [34:33<80:44:48,  3.42it/s, grad_norm=0.245, loss_final=0.867, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:32:50[39m] Step: 7025, Training Logs: loss_final: 0.891336, loss_mean: 0.847094, loss_mean_cls: 0.044242, grad_norm: 0.292742
Steps:   1%| | 7032/1000000 [34:36<81:50:34,  3.37it/s, grad_norm=0.26, loss_final=0.879, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:32:52[39m] Step: 7032, Training Logs: loss_final: 0.887986, loss_mean: 0.845028, loss_mean_cls: 0.042958, grad_norm: 0.237585
Steps:   1%| | 7039/1000000 [34:38<79:26:34,  3.47it/s, grad_norm=0.337, loss_final=0.862, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:32:54[39m] Step: 7039, Training Logs: loss_final: 0.883898, loss_mean: 0.839229, loss_mean_cls: 0.044669, grad_norm: 0.244644
Steps:   1%| | 7046/1000000 [34:40<78:57:18,  3.49it/s, grad_norm=0.328, loss_final=0.896, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:32:56[39m] Step: 7046, Training Logs: loss_final: 0.870063, loss_mean: 0.825633, loss_mean_cls: 0.044429, grad_norm: 0.362267
Steps:   1%| | 7053/1000000 [34:42<81:20:08,  3.39it/s, grad_norm=0.354, loss_final=0.884, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:32:58[39m] Step: 7053, Training Logs: loss_final: 0.862628, loss_mean: 0.817769, loss_mean_cls: 0.044859, grad_norm: 0.459107
Steps:   1%| | 7060/1000000 [34:44<82:27:07,  3.35it/s, grad_norm=0.34, loss_final=0.878, loss_mean=0.834, loss_mean_cls=0.0[[34m2025-10-04 12:33:00[39m] Step: 7060, Training Logs: loss_final: 0.879294, loss_mean: 0.836346, loss_mean_cls: 0.042948, grad_norm: 0.440349
Steps:   1%| | 7066/1000000 [34:45<79:48:17,  3.46it/s, grad_norm=0.283, loss_final=0.864, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:33:02[39m] Step: 7066, Training Logs: loss_final: 0.885079, loss_mean: 0.840878, loss_mean_cls: 0.044201, grad_norm: 0.281072
Steps:   1%| | 7072/1000000 [34:47<80:45:38,  3.42it/s, grad_norm=0.323, loss_final=0.896, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:33:04[39m] Step: 7072, Training Logs: loss_final: 0.863058, loss_mean: 0.817492, loss_mean_cls: 0.045565, grad_norm: 0.286694
Steps:   1%| | 7079/1000000 [34:49<81:05:42,  3.40it/s, grad_norm=0.275, loss_final=0.875, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:33:06[39m] Step: 7079, Training Logs: loss_final: 0.876865, loss_mean: 0.834006, loss_mean_cls: 0.042859, grad_norm: 0.252168
Steps:   1%| | 7086/1000000 [34:51<81:08:44,  3.40it/s, grad_norm=0.232, loss_final=0.902, loss_mean=0.858, loss_mean_cls=0.[[34m2025-10-04 12:33:08[39m] Step: 7086, Training Logs: loss_final: 0.890086, loss_mean: 0.846491, loss_mean_cls: 0.043595, grad_norm: 0.294118
Steps:   1%| | 7093/1000000 [34:53<79:44:36,  3.46it/s, grad_norm=0.347, loss_final=0.895, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:33:10[39m] Step: 7093, Training Logs: loss_final: 0.873344, loss_mean: 0.829985, loss_mean_cls: 0.043359, grad_norm: 0.230924
Steps:   1%| | 7100/1000000 [34:55<80:34:38,  3.42it/s, grad_norm=0.286, loss_final=0.868, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:33:12[39m] Step: 7100, Training Logs: loss_final: 0.904477, loss_mean: 0.861465, loss_mean_cls: 0.043012, grad_norm: 0.450005
Steps:   1%| | 7107/1000000 [34:57<79:22:20,  3.47it/s, grad_norm=0.323, loss_final=0.904, loss_mean=0.862, loss_mean_cls=0.[[34m2025-10-04 12:33:14[39m] Step: 7107, Training Logs: loss_final: 0.862217, loss_mean: 0.818484, loss_mean_cls: 0.043733, grad_norm: 0.302942
Steps:   1%| | 7114/1000000 [34:59<79:27:41,  3.47it/s, grad_norm=0.297, loss_final=0.877, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:33:16[39m] Step: 7114, Training Logs: loss_final: 0.901918, loss_mean: 0.859124, loss_mean_cls: 0.042794, grad_norm: 0.341532
Steps:   1%| | 7120/1000000 [35:01<82:30:47,  3.34it/s, grad_norm=0.317, loss_final=0.859, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:33:18[39m] Step: 7120, Training Logs: loss_final: 0.877700, loss_mean: 0.834482, loss_mean_cls: 0.043217, grad_norm: 0.386683
Steps:   1%| | 7127/1000000 [35:03<81:53:45,  3.37it/s, grad_norm=0.276, loss_final=0.9, loss_mean=0.857, loss_mean_cls=0.04[[34m2025-10-04 12:33:20[39m] Step: 7127, Training Logs: loss_final: 0.881100, loss_mean: 0.836856, loss_mean_cls: 0.044244, grad_norm: 0.455914
Steps:   1%| | 7134/1000000 [35:05<80:50:05,  3.41it/s, grad_norm=0.396, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:33:22[39m] Step: 7134, Training Logs: loss_final: 0.883026, loss_mean: 0.840081, loss_mean_cls: 0.042945, grad_norm: 0.269913
Steps:   1%| | 7141/1000000 [35:07<80:02:34,  3.45it/s, grad_norm=0.225, loss_final=0.887, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:33:24[39m] Step: 7141, Training Logs: loss_final: 0.880289, loss_mean: 0.837121, loss_mean_cls: 0.043168, grad_norm: 0.208944
Steps:   1%| | 7148/1000000 [35:09<80:29:34,  3.43it/s, grad_norm=0.276, loss_final=0.889, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:33:26[39m] Step: 7148, Training Logs: loss_final: 0.871184, loss_mean: 0.825914, loss_mean_cls: 0.045270, grad_norm: 0.212916
Steps:   1%| | 7155/1000000 [35:11<79:50:49,  3.45it/s, grad_norm=0.364, loss_final=0.877, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:33:28[39m] Step: 7155, Training Logs: loss_final: 0.879271, loss_mean: 0.835312, loss_mean_cls: 0.043959, grad_norm: 0.306994
Steps:   1%| | 7162/1000000 [35:13<81:38:40,  3.38it/s, grad_norm=0.36, loss_final=0.886, loss_mean=0.844, loss_mean_cls=0.0[[34m2025-10-04 12:33:30[39m] Step: 7162, Training Logs: loss_final: 0.852825, loss_mean: 0.807942, loss_mean_cls: 0.044883, grad_norm: 0.297866
Steps:   1%| | 7169/1000000 [35:15<80:44:33,  3.42it/s, grad_norm=0.277, loss_final=0.873, loss_mean=0.827, loss_mean_cls=0.[[34m2025-10-04 12:33:32[39m] Step: 7169, Training Logs: loss_final: 0.883968, loss_mean: 0.839471, loss_mean_cls: 0.044497, grad_norm: 0.313490
Steps:   1%| | 7176/1000000 [35:17<79:09:59,  3.48it/s, grad_norm=0.304, loss_final=0.855, loss_mean=0.81, loss_mean_cls=0.0[[34m2025-10-04 12:33:34[39m] Step: 7176, Training Logs: loss_final: 0.872863, loss_mean: 0.829945, loss_mean_cls: 0.042917, grad_norm: 0.264869
Steps:   1%| | 7183/1000000 [35:20<81:04:19,  3.40it/s, grad_norm=0.215, loss_final=0.869, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:33:36[39m] Step: 7183, Training Logs: loss_final: 0.882701, loss_mean: 0.839660, loss_mean_cls: 0.043041, grad_norm: 0.264058
Steps:   1%| | 7189/1000000 [35:21<83:26:50,  3.30it/s, grad_norm=0.601, loss_final=0.869, loss_mean=0.826, loss_mean_cls=0.[[34m2025-10-04 12:33:38[39m] Step: 7189, Training Logs: loss_final: 0.872788, loss_mean: 0.829694, loss_mean_cls: 0.043095, grad_norm: 0.348616
Steps:   1%| | 7196/1000000 [35:23<79:40:45,  3.46it/s, grad_norm=0.46, loss_final=0.868, loss_mean=0.823, loss_mean_cls=0.0[[34m2025-10-04 12:33:40[39m] Step: 7196, Training Logs: loss_final: 0.904200, loss_mean: 0.862170, loss_mean_cls: 0.042030, grad_norm: 0.254071
Steps:   1%| | 7203/1000000 [35:25<80:01:47,  3.45it/s, grad_norm=0.246, loss_final=0.861, loss_mean=0.816, loss_mean_cls=0.[[34m2025-10-04 12:33:42[39m] Step: 7203, Training Logs: loss_final: 0.867724, loss_mean: 0.824563, loss_mean_cls: 0.043161, grad_norm: 0.360547
Steps:   1%| | 7210/1000000 [35:27<80:01:59,  3.45it/s, grad_norm=0.27, loss_final=0.906, loss_mean=0.862, loss_mean_cls=0.0[[34m2025-10-04 12:33:44[39m] Step: 7210, Training Logs: loss_final: 0.896316, loss_mean: 0.852036, loss_mean_cls: 0.044280, grad_norm: 0.230403
Steps:   1%| | 7217/1000000 [35:29<80:36:42,  3.42it/s, grad_norm=0.287, loss_final=0.873, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:33:46[39m] Step: 7217, Training Logs: loss_final: 0.889908, loss_mean: 0.846795, loss_mean_cls: 0.043113, grad_norm: 0.313154
Steps:   1%| | 7224/1000000 [35:31<79:23:24,  3.47it/s, grad_norm=0.259, loss_final=0.88, loss_mean=0.838, loss_mean_cls=0.0[[34m2025-10-04 12:33:48[39m] Step: 7224, Training Logs: loss_final: 0.880403, loss_mean: 0.837750, loss_mean_cls: 0.042654, grad_norm: 0.284505
Steps:   1%| | 7231/1000000 [35:33<78:53:38,  3.50it/s, grad_norm=0.278, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:33:50[39m] Step: 7231, Training Logs: loss_final: 0.884875, loss_mean: 0.841421, loss_mean_cls: 0.043455, grad_norm: 0.372834
Steps:   1%| | 7238/1000000 [35:36<81:49:50,  3.37it/s, grad_norm=0.277, loss_final=0.878, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:33:52[39m] Step: 7238, Training Logs: loss_final: 0.880889, loss_mean: 0.837471, loss_mean_cls: 0.043418, grad_norm: 0.316396
Steps:   1%| | 7245/1000000 [35:38<82:35:38,  3.34it/s, grad_norm=0.237, loss_final=0.891, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:33:54[39m] Step: 7245, Training Logs: loss_final: 0.853621, loss_mean: 0.808694, loss_mean_cls: 0.044927, grad_norm: 0.369239
Steps:   1%| | 7252/1000000 [35:40<84:06:36,  3.28it/s, grad_norm=0.242, loss_final=0.861, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:33:56[39m] Step: 7252, Training Logs: loss_final: 0.875019, loss_mean: 0.830574, loss_mean_cls: 0.044445, grad_norm: 0.259045
Steps:   1%| | 7258/1000000 [35:41<83:20:02,  3.31it/s, grad_norm=0.158, loss_final=0.899, loss_mean=0.857, loss_mean_cls=0.[[34m2025-10-04 12:33:58[39m] Step: 7258, Training Logs: loss_final: 0.872882, loss_mean: 0.829654, loss_mean_cls: 0.043228, grad_norm: 0.342588
Steps:   1%| | 7265/1000000 [35:44<81:03:01,  3.40it/s, grad_norm=0.249, loss_final=0.875, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:34:00[39m] Step: 7265, Training Logs: loss_final: 0.856198, loss_mean: 0.812227, loss_mean_cls: 0.043971, grad_norm: 0.201958
Steps:   1%| | 7272/1000000 [35:46<81:30:35,  3.38it/s, grad_norm=0.294, loss_final=0.873, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:34:02[39m] Step: 7272, Training Logs: loss_final: 0.882986, loss_mean: 0.838903, loss_mean_cls: 0.044082, grad_norm: 0.242872
Steps:   1%| | 7279/1000000 [35:48<80:05:40,  3.44it/s, grad_norm=0.405, loss_final=0.882, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:34:04[39m] Step: 7279, Training Logs: loss_final: 0.879769, loss_mean: 0.835485, loss_mean_cls: 0.044284, grad_norm: 0.305929
Steps:   1%| | 7286/1000000 [35:50<84:19:19,  3.27it/s, grad_norm=0.296, loss_final=0.891, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:34:06[39m] Step: 7286, Training Logs: loss_final: 0.877073, loss_mean: 0.832889, loss_mean_cls: 0.044184, grad_norm: 0.190475
Steps:   1%| | 7292/1000000 [35:52<82:16:54,  3.35it/s, grad_norm=0.23, loss_final=0.879, loss_mean=0.837, loss_mean_cls=0.0[[34m2025-10-04 12:34:08[39m] Step: 7292, Training Logs: loss_final: 0.871656, loss_mean: 0.827771, loss_mean_cls: 0.043885, grad_norm: 0.295950
Steps:   1%| | 7299/1000000 [35:54<79:39:50,  3.46it/s, grad_norm=0.348, loss_final=0.869, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:34:10[39m] Step: 7299, Training Logs: loss_final: 0.872442, loss_mean: 0.827844, loss_mean_cls: 0.044598, grad_norm: 0.266601
Steps:   1%| | 7306/1000000 [35:56<80:55:07,  3.41it/s, grad_norm=0.286, loss_final=0.883, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:34:12[39m] Step: 7306, Training Logs: loss_final: 0.871027, loss_mean: 0.828853, loss_mean_cls: 0.042174, grad_norm: 0.364233
Steps:   1%| | 7313/1000000 [35:58<79:39:48,  3.46it/s, grad_norm=0.23, loss_final=0.895, loss_mean=0.851, loss_mean_cls=0.0[[34m2025-10-04 12:34:14[39m] Step: 7313, Training Logs: loss_final: 0.879263, loss_mean: 0.835522, loss_mean_cls: 0.043740, grad_norm: 0.402592
Steps:   1%| | 7320/1000000 [36:00<79:01:02,  3.49it/s, grad_norm=0.333, loss_final=0.882, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:34:16[39m] Step: 7320, Training Logs: loss_final: 0.876265, loss_mean: 0.832087, loss_mean_cls: 0.044178, grad_norm: 0.325426
Steps:   1%| | 7327/1000000 [36:02<79:05:37,  3.49it/s, grad_norm=0.214, loss_final=0.853, loss_mean=0.809, loss_mean_cls=0.[[34m2025-10-04 12:34:18[39m] Step: 7327, Training Logs: loss_final: 0.883550, loss_mean: 0.838593, loss_mean_cls: 0.044958, grad_norm: 0.376786
Steps:   1%| | 7333/1000000 [36:03<79:42:57,  3.46it/s, grad_norm=0.461, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:34:20[39m] Step: 7333, Training Logs: loss_final: 0.855977, loss_mean: 0.811730, loss_mean_cls: 0.044247, grad_norm: 0.279635
Steps:   1%| | 7339/1000000 [36:06<137:24:17,  2.01it/s, grad_norm=0.314, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.[[34m2025-10-04 12:34:23[39m] Step: 7339, Training Logs: loss_final: 0.885998, loss_mean: 0.841441, loss_mean_cls: 0.044556, grad_norm: 0.233408
Steps:   1%| | 7344/1000000 [36:07<90:03:20,  3.06it/s, grad_norm=0.386, loss_final=0.866, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:34:24[39m] Step: 7344, Training Logs: loss_final: 0.877338, loss_mean: 0.833390, loss_mean_cls: 0.043947, grad_norm: 0.241979
Steps:   1%| | 7351/1000000 [36:09<81:54:32,  3.37it/s, grad_norm=0.302, loss_final=0.855, loss_mean=0.809, loss_mean_cls=0.[[34m2025-10-04 12:34:26[39m] Step: 7351, Training Logs: loss_final: 0.872331, loss_mean: 0.828179, loss_mean_cls: 0.044152, grad_norm: 0.487196
Steps:   1%| | 7358/1000000 [36:11<79:26:47,  3.47it/s, grad_norm=0.452, loss_final=0.866, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:34:28[39m] Step: 7358, Training Logs: loss_final: 0.885533, loss_mean: 0.841734, loss_mean_cls: 0.043799, grad_norm: 0.231369
Steps:   1%| | 7365/1000000 [36:13<78:44:28,  3.50it/s, grad_norm=0.322, loss_final=0.892, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:34:30[39m] Step: 7365, Training Logs: loss_final: 0.873683, loss_mean: 0.829732, loss_mean_cls: 0.043951, grad_norm: 0.336515
Steps:   1%| | 7372/1000000 [36:15<80:39:01,  3.42it/s, grad_norm=0.389, loss_final=0.895, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:34:32[39m] Step: 7372, Training Logs: loss_final: 0.888553, loss_mean: 0.845906, loss_mean_cls: 0.042646, grad_norm: 0.305930
Steps:   1%| | 7379/1000000 [36:18<79:55:39,  3.45it/s, grad_norm=0.426, loss_final=0.863, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:34:34[39m] Step: 7379, Training Logs: loss_final: 0.860687, loss_mean: 0.815405, loss_mean_cls: 0.045282, grad_norm: 0.354464
Steps:   1%| | 7386/1000000 [36:20<80:11:56,  3.44it/s, grad_norm=0.423, loss_final=0.87, loss_mean=0.826, loss_mean_cls=0.0[[34m2025-10-04 12:34:36[39m] Step: 7386, Training Logs: loss_final: 0.882455, loss_mean: 0.839102, loss_mean_cls: 0.043353, grad_norm: 0.209654
Steps:   1%| | 7393/1000000 [36:22<79:06:51,  3.49it/s, grad_norm=0.4, loss_final=0.872, loss_mean=0.827, loss_mean_cls=0.04[[34m2025-10-04 12:34:38[39m] Step: 7393, Training Logs: loss_final: 0.887673, loss_mean: 0.843401, loss_mean_cls: 0.044272, grad_norm: 0.313128
Steps:   1%| | 7400/1000000 [36:24<79:12:42,  3.48it/s, grad_norm=0.317, loss_final=0.89, loss_mean=0.846, loss_mean_cls=0.0[[34m2025-10-04 12:34:40[39m] Step: 7400, Training Logs: loss_final: 0.884206, loss_mean: 0.840575, loss_mean_cls: 0.043631, grad_norm: 0.391344
Steps:   1%| | 7407/1000000 [36:26<80:22:53,  3.43it/s, grad_norm=0.307, loss_final=0.893, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:34:42[39m] Step: 7407, Training Logs: loss_final: 0.884644, loss_mean: 0.842130, loss_mean_cls: 0.042514, grad_norm: 0.344760
Steps:   1%| | 7414/1000000 [36:28<80:08:01,  3.44it/s, grad_norm=0.324, loss_final=0.876, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:34:44[39m] Step: 7414, Training Logs: loss_final: 0.877711, loss_mean: 0.833875, loss_mean_cls: 0.043835, grad_norm: 0.226196
Steps:   1%| | 7421/1000000 [36:30<80:54:11,  3.41it/s, grad_norm=0.276, loss_final=0.862, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:34:46[39m] Step: 7421, Training Logs: loss_final: 0.860408, loss_mean: 0.815787, loss_mean_cls: 0.044620, grad_norm: 0.253027
Steps:   1%| | 7427/1000000 [36:32<84:26:33,  3.27it/s, grad_norm=0.257, loss_final=0.876, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:34:48[39m] Step: 7427, Training Logs: loss_final: 0.886701, loss_mean: 0.842736, loss_mean_cls: 0.043965, grad_norm: 0.287348
Steps:   1%| | 7434/1000000 [36:34<82:18:51,  3.35it/s, grad_norm=0.352, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:34:50[39m] Step: 7434, Training Logs: loss_final: 0.896430, loss_mean: 0.854093, loss_mean_cls: 0.042337, grad_norm: 0.296708
Steps:   1%| | 7441/1000000 [36:36<79:41:13,  3.46it/s, grad_norm=0.206, loss_final=0.87, loss_mean=0.825, loss_mean_cls=0.0[[34m2025-10-04 12:34:52[39m] Step: 7441, Training Logs: loss_final: 0.888379, loss_mean: 0.844286, loss_mean_cls: 0.044093, grad_norm: 0.345007
Steps:   1%| | 7448/1000000 [36:38<79:46:44,  3.46it/s, grad_norm=0.197, loss_final=0.873, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:34:54[39m] Step: 7448, Training Logs: loss_final: 0.880433, loss_mean: 0.836980, loss_mean_cls: 0.043452, grad_norm: 0.295348
Steps:   1%| | 7455/1000000 [36:40<79:05:30,  3.49it/s, grad_norm=0.215, loss_final=0.911, loss_mean=0.868, loss_mean_cls=0.[[34m2025-10-04 12:34:56[39m] Step: 7455, Training Logs: loss_final: 0.875872, loss_mean: 0.832130, loss_mean_cls: 0.043742, grad_norm: 0.231075
Steps:   1%| | 7462/1000000 [36:42<80:21:37,  3.43it/s, grad_norm=0.236, loss_final=0.884, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:34:58[39m] Step: 7462, Training Logs: loss_final: 0.873590, loss_mean: 0.830278, loss_mean_cls: 0.043312, grad_norm: 0.351151
Steps:   1%| | 7469/1000000 [36:44<80:03:36,  3.44it/s, grad_norm=0.348, loss_final=0.89, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:35:00[39m] Step: 7469, Training Logs: loss_final: 0.870245, loss_mean: 0.827569, loss_mean_cls: 0.042676, grad_norm: 0.385007
Steps:   1%| | 7476/1000000 [36:46<79:03:38,  3.49it/s, grad_norm=0.481, loss_final=0.877, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:35:02[39m] Step: 7476, Training Logs: loss_final: 0.861093, loss_mean: 0.816315, loss_mean_cls: 0.044779, grad_norm: 0.306654
Steps:   1%| | 7483/1000000 [36:48<81:32:17,  3.38it/s, grad_norm=0.359, loss_final=0.865, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:35:05[39m] Step: 7483, Training Logs: loss_final: 0.868759, loss_mean: 0.824895, loss_mean_cls: 0.043864, grad_norm: 0.292977
Steps:   1%| | 7490/1000000 [36:50<79:55:29,  3.45it/s, grad_norm=0.243, loss_final=0.853, loss_mean=0.81, loss_mean_cls=0.0[[34m2025-10-04 12:35:07[39m] Step: 7490, Training Logs: loss_final: 0.879245, loss_mean: 0.834892, loss_mean_cls: 0.044354, grad_norm: 0.259554
Steps:   1%| | 7496/1000000 [36:52<80:25:06,  3.43it/s, grad_norm=0.277, loss_final=0.874, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:35:08[39m] Step: 7496, Training Logs: loss_final: 0.895134, loss_mean: 0.851622, loss_mean_cls: 0.043512, grad_norm: 0.340824
Steps:   1%| | 7503/1000000 [36:54<79:50:49,  3.45it/s, grad_norm=0.281, loss_final=0.868, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:35:10[39m] Step: 7503, Training Logs: loss_final: 0.875226, loss_mean: 0.831242, loss_mean_cls: 0.043984, grad_norm: 0.305772
Steps:   1%| | 7510/1000000 [36:56<81:48:37,  3.37it/s, grad_norm=0.352, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:35:12[39m] Step: 7510, Training Logs: loss_final: 0.870559, loss_mean: 0.827031, loss_mean_cls: 0.043527, grad_norm: 0.236914
Steps:   1%| | 7517/1000000 [36:58<82:21:25,  3.35it/s, grad_norm=0.248, loss_final=0.861, loss_mean=0.818, loss_mean_cls=0.[[34m2025-10-04 12:35:15[39m] Step: 7517, Training Logs: loss_final: 0.890985, loss_mean: 0.848671, loss_mean_cls: 0.042313, grad_norm: 0.265650
Steps:   1%| | 7524/1000000 [37:00<81:10:55,  3.40it/s, grad_norm=0.304, loss_final=0.887, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:35:17[39m] Step: 7524, Training Logs: loss_final: 0.879305, loss_mean: 0.835578, loss_mean_cls: 0.043728, grad_norm: 0.364879
Steps:   1%| | 7530/1000000 [37:02<80:26:15,  3.43it/s, grad_norm=0.322, loss_final=0.849, loss_mean=0.806, loss_mean_cls=0.[[34m2025-10-04 12:35:18[39m] Step: 7530, Training Logs: loss_final: 0.890162, loss_mean: 0.846327, loss_mean_cls: 0.043836, grad_norm: 0.352634
Steps:   1%| | 7537/1000000 [37:04<80:32:48,  3.42it/s, grad_norm=0.325, loss_final=0.869, loss_mean=0.826, loss_mean_cls=0.[[34m2025-10-04 12:35:20[39m] Step: 7537, Training Logs: loss_final: 0.882834, loss_mean: 0.839641, loss_mean_cls: 0.043194, grad_norm: 0.327788
Steps:   1%| | 7544/1000000 [37:06<84:31:39,  3.26it/s, grad_norm=0.303, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:35:23[39m] Step: 7544, Training Logs: loss_final: 0.893303, loss_mean: 0.849149, loss_mean_cls: 0.044154, grad_norm: 0.310521
Steps:   1%| | 7551/1000000 [37:08<83:49:49,  3.29it/s, grad_norm=0.306, loss_final=0.888, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:35:25[39m] Step: 7551, Training Logs: loss_final: 0.855289, loss_mean: 0.811375, loss_mean_cls: 0.043914, grad_norm: 0.281874
Steps:   1%| | 7557/1000000 [37:10<83:12:10,  3.31it/s, grad_norm=0.441, loss_final=0.874, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:35:27[39m] Step: 7557, Training Logs: loss_final: 0.905230, loss_mean: 0.861604, loss_mean_cls: 0.043626, grad_norm: 0.328969
Steps:   1%| | 7564/1000000 [37:12<82:36:05,  3.34it/s, grad_norm=0.336, loss_final=0.88, loss_mean=0.837, loss_mean_cls=0.0[[34m2025-10-04 12:35:29[39m] Step: 7564, Training Logs: loss_final: 0.867082, loss_mean: 0.822163, loss_mean_cls: 0.044919, grad_norm: 0.493278
Steps:   1%| | 7571/1000000 [37:14<81:14:32,  3.39it/s, grad_norm=0.348, loss_final=0.869, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:35:31[39m] Step: 7571, Training Logs: loss_final: 0.881200, loss_mean: 0.837572, loss_mean_cls: 0.043629, grad_norm: 0.482359
Steps:   1%| | 7578/1000000 [37:16<81:20:14,  3.39it/s, grad_norm=0.224, loss_final=0.873, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:35:33[39m] Step: 7578, Training Logs: loss_final: 0.873443, loss_mean: 0.831121, loss_mean_cls: 0.042322, grad_norm: 0.430217
Steps:   1%| | 7584/1000000 [37:18<82:26:10,  3.34it/s, grad_norm=0.273, loss_final=0.867, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:35:35[39m] Step: 7584, Training Logs: loss_final: 0.879686, loss_mean: 0.836330, loss_mean_cls: 0.043356, grad_norm: 0.333603
Steps:   1%| | 7591/1000000 [37:20<84:19:51,  3.27it/s, grad_norm=0.33, loss_final=0.884, loss_mean=0.841, loss_mean_cls=0.0[[34m2025-10-04 12:35:37[39m] Step: 7591, Training Logs: loss_final: 0.880537, loss_mean: 0.836769, loss_mean_cls: 0.043768, grad_norm: 0.257330
Steps:   1%| | 7598/1000000 [37:22<81:17:51,  3.39it/s, grad_norm=0.317, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:35:39[39m] Step: 7598, Training Logs: loss_final: 0.870261, loss_mean: 0.826891, loss_mean_cls: 0.043371, grad_norm: 0.262907
Steps:   1%| | 7605/1000000 [37:24<80:36:26,  3.42it/s, grad_norm=0.223, loss_final=0.875, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:35:41[39m] Step: 7605, Training Logs: loss_final: 0.898376, loss_mean: 0.855124, loss_mean_cls: 0.043252, grad_norm: 0.333299
Steps:   1%| | 7611/1000000 [37:26<79:37:37,  3.46it/s, grad_norm=0.385, loss_final=0.873, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:35:43[39m] Step: 7611, Training Logs: loss_final: 0.875057, loss_mean: 0.832280, loss_mean_cls: 0.042778, grad_norm: 0.234679
Steps:   1%| | 7618/1000000 [37:28<80:31:50,  3.42it/s, grad_norm=0.251, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:35:45[39m] Step: 7618, Training Logs: loss_final: 0.859201, loss_mean: 0.814601, loss_mean_cls: 0.044600, grad_norm: 0.227535
Steps:   1%| | 7625/1000000 [37:30<79:41:18,  3.46it/s, grad_norm=0.353, loss_final=0.859, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:35:47[39m] Step: 7625, Training Logs: loss_final: 0.861421, loss_mean: 0.816559, loss_mean_cls: 0.044862, grad_norm: 0.440142
Steps:   1%| | 7632/1000000 [37:32<79:40:13,  3.46it/s, grad_norm=0.225, loss_final=0.866, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:35:49[39m] Step: 7632, Training Logs: loss_final: 0.891724, loss_mean: 0.848600, loss_mean_cls: 0.043124, grad_norm: 0.309717
Steps:   1%| | 7639/1000000 [37:34<80:46:09,  3.41it/s, grad_norm=0.351, loss_final=0.874, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:35:51[39m] Step: 7639, Training Logs: loss_final: 0.874689, loss_mean: 0.830178, loss_mean_cls: 0.044511, grad_norm: 0.250491
Steps:   1%| | 7644/1000000 [37:36<82:39:30,  3.33it/s, grad_norm=0.261, loss_final=0.894, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:35:52[39m] Step: 7644, Training Logs: loss_final: 0.877651, loss_mean: 0.834226, loss_mean_cls: 0.043425, grad_norm: 0.251756
Steps:   1%| | 7651/1000000 [37:38<81:14:35,  3.39it/s, grad_norm=0.285, loss_final=0.878, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:35:54[39m] Step: 7651, Training Logs: loss_final: 0.874136, loss_mean: 0.830806, loss_mean_cls: 0.043330, grad_norm: 0.203807
Steps:   1%| | 7658/1000000 [37:40<79:39:06,  3.46it/s, grad_norm=0.243, loss_final=0.864, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:35:56[39m] Step: 7658, Training Logs: loss_final: 0.897965, loss_mean: 0.856539, loss_mean_cls: 0.041426, grad_norm: 0.319580
Steps:   1%| | 7665/1000000 [37:42<81:52:37,  3.37it/s, grad_norm=0.342, loss_final=0.871, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:35:58[39m] Step: 7665, Training Logs: loss_final: 0.868318, loss_mean: 0.825272, loss_mean_cls: 0.043045, grad_norm: 0.198420
Steps:   1%| | 7672/1000000 [37:44<79:19:34,  3.47it/s, grad_norm=0.211, loss_final=0.885, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:36:00[39m] Step: 7672, Training Logs: loss_final: 0.855640, loss_mean: 0.811195, loss_mean_cls: 0.044445, grad_norm: 0.284044
Steps:   1%| | 7679/1000000 [37:46<78:52:51,  3.49it/s, grad_norm=0.257, loss_final=0.878, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:36:02[39m] Step: 7679, Training Logs: loss_final: 0.864857, loss_mean: 0.821661, loss_mean_cls: 0.043196, grad_norm: 0.317833
Steps:   1%| | 7686/1000000 [37:48<78:37:15,  3.51it/s, grad_norm=0.412, loss_final=0.87, loss_mean=0.825, loss_mean_cls=0.0[[34m2025-10-04 12:36:04[39m] Step: 7686, Training Logs: loss_final: 0.881765, loss_mean: 0.837578, loss_mean_cls: 0.044187, grad_norm: 0.284505
Steps:   1%| | 7693/1000000 [37:50<79:22:29,  3.47it/s, grad_norm=0.353, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:36:07[39m] Step: 7693, Training Logs: loss_final: 0.880511, loss_mean: 0.836813, loss_mean_cls: 0.043698, grad_norm: 0.564014
Steps:   1%| | 7700/1000000 [37:52<83:05:09,  3.32it/s, grad_norm=0.604, loss_final=0.872, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:36:09[39m] Step: 7700, Training Logs: loss_final: 0.885594, loss_mean: 0.843042, loss_mean_cls: 0.042552, grad_norm: 0.386846
Steps:   1%| | 7706/1000000 [37:54<82:42:51,  3.33it/s, grad_norm=0.311, loss_final=0.879, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:36:10[39m] Step: 7706, Training Logs: loss_final: 0.873773, loss_mean: 0.830307, loss_mean_cls: 0.043465, grad_norm: 0.415930
Steps:   1%| | 7713/1000000 [37:56<81:59:04,  3.36it/s, grad_norm=0.52, loss_final=0.887, loss_mean=0.843, loss_mean_cls=0.0[[34m2025-10-04 12:36:12[39m] Step: 7713, Training Logs: loss_final: 0.886205, loss_mean: 0.843261, loss_mean_cls: 0.042944, grad_norm: 0.301779
Steps:   1%| | 7720/1000000 [37:58<81:08:42,  3.40it/s, grad_norm=0.238, loss_final=0.865, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:36:15[39m] Step: 7720, Training Logs: loss_final: 0.888747, loss_mean: 0.845723, loss_mean_cls: 0.043024, grad_norm: 0.620516
Steps:   1%| | 7727/1000000 [38:00<84:14:58,  3.27it/s, grad_norm=0.268, loss_final=0.863, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:36:17[39m] Step: 7727, Training Logs: loss_final: 0.888039, loss_mean: 0.843670, loss_mean_cls: 0.044369, grad_norm: 0.394946
Steps:   1%| | 7733/1000000 [38:02<80:44:22,  3.41it/s, grad_norm=0.289, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:36:18[39m] Step: 7733, Training Logs: loss_final: 0.862862, loss_mean: 0.817352, loss_mean_cls: 0.045510, grad_norm: 0.253588
Steps:   1%| | 7740/1000000 [38:04<79:16:45,  3.48it/s, grad_norm=0.382, loss_final=0.897, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:36:20[39m] Step: 7740, Training Logs: loss_final: 0.883493, loss_mean: 0.839450, loss_mean_cls: 0.044044, grad_norm: 0.235364
Steps:   1%| | 7747/1000000 [38:06<79:06:11,  3.48it/s, grad_norm=0.245, loss_final=0.864, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:36:22[39m] Step: 7747, Training Logs: loss_final: 0.875833, loss_mean: 0.832651, loss_mean_cls: 0.043182, grad_norm: 0.236872
Steps:   1%| | 7754/1000000 [38:08<81:01:24,  3.40it/s, grad_norm=0.255, loss_final=0.881, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:36:25[39m] Step: 7754, Training Logs: loss_final: 0.882617, loss_mean: 0.839330, loss_mean_cls: 0.043287, grad_norm: 0.272852
Steps:   1%| | 7761/1000000 [38:10<80:36:56,  3.42it/s, grad_norm=0.276, loss_final=0.873, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:36:27[39m] Step: 7761, Training Logs: loss_final: 0.873712, loss_mean: 0.831061, loss_mean_cls: 0.042652, grad_norm: 0.273411
Steps:   1%| | 7768/1000000 [38:12<81:01:03,  3.40it/s, grad_norm=0.266, loss_final=0.887, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:36:29[39m] Step: 7768, Training Logs: loss_final: 0.893847, loss_mean: 0.850579, loss_mean_cls: 0.043268, grad_norm: 0.345418
Steps:   1%| | 7775/1000000 [38:14<80:06:44,  3.44it/s, grad_norm=0.388, loss_final=0.873, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:36:31[39m] Step: 7775, Training Logs: loss_final: 0.858622, loss_mean: 0.813903, loss_mean_cls: 0.044719, grad_norm: 0.422563
Steps:   1%| | 7782/1000000 [38:16<79:10:12,  3.48it/s, grad_norm=0.36, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.0[[34m2025-10-04 12:36:33[39m] Step: 7782, Training Logs: loss_final: 0.872829, loss_mean: 0.829274, loss_mean_cls: 0.043555, grad_norm: 0.271244
Steps:   1%| | 7789/1000000 [38:18<80:43:13,  3.41it/s, grad_norm=0.27, loss_final=0.87, loss_mean=0.826, loss_mean_cls=0.04[[34m2025-10-04 12:36:35[39m] Step: 7789, Training Logs: loss_final: 0.863447, loss_mean: 0.820093, loss_mean_cls: 0.043355, grad_norm: 0.253998
Steps:   1%| | 7796/1000000 [38:20<79:14:26,  3.48it/s, grad_norm=0.306, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:36:37[39m] Step: 7796, Training Logs: loss_final: 0.896661, loss_mean: 0.854558, loss_mean_cls: 0.042103, grad_norm: 0.246736
Steps:   1%| | 7803/1000000 [38:22<79:00:16,  3.49it/s, grad_norm=0.19, loss_final=0.863, loss_mean=0.817, loss_mean_cls=0.0[[34m2025-10-04 12:36:39[39m] Step: 7803, Training Logs: loss_final: 0.868148, loss_mean: 0.826928, loss_mean_cls: 0.041220, grad_norm: 0.248513
Steps:   1%| | 7810/1000000 [38:24<79:10:10,  3.48it/s, grad_norm=0.194, loss_final=0.863, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:36:41[39m] Step: 7810, Training Logs: loss_final: 0.853179, loss_mean: 0.809810, loss_mean_cls: 0.043370, grad_norm: 0.321664
Steps:   1%| | 7817/1000000 [38:26<79:04:22,  3.49it/s, grad_norm=0.284, loss_final=0.87, loss_mean=0.827, loss_mean_cls=0.0[[34m2025-10-04 12:36:43[39m] Step: 7817, Training Logs: loss_final: 0.844272, loss_mean: 0.799512, loss_mean_cls: 0.044760, grad_norm: 0.204090
Steps:   1%| | 7824/1000000 [38:28<79:00:57,  3.49it/s, grad_norm=0.334, loss_final=0.874, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:36:45[39m] Step: 7824, Training Logs: loss_final: 0.887727, loss_mean: 0.845218, loss_mean_cls: 0.042509, grad_norm: 0.201988
Steps:   1%| | 7836/1000000 [38:32<83:18:48,  3.31it/s, grad_norm=0.294, loss_final=0.867, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:36:47[39m] Step: 7830, Training Logs: loss_final: 0.880743, loss_mean: 0.837285, loss_mean_cls: 0.043458, grad_norm: 0.273065
Steps:   1%| | 7843/1000000 [38:34<83:00:06,  3.32it/s, grad_norm=0.228, loss_final=0.892, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:36:49[39m] Step: 7837, Training Logs: loss_final: 0.877703, loss_mean: 0.834405, loss_mean_cls: 0.043298, grad_norm: 0.181743
Steps:   1%| | 7850/1000000 [38:36<79:52:19,  3.45it/s, grad_norm=0.253, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:36:51[39m] Step: 7844, Training Logs: loss_final: 0.871247, loss_mean: 0.826982, loss_mean_cls: 0.044265, grad_norm: 0.261979
Steps:   1%| | 7857/1000000 [38:38<79:48:05,  3.45it/s, grad_norm=0.268, loss_final=0.864, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:36:53[39m] Step: 7851, Training Logs: loss_final: 0.884848, loss_mean: 0.840769, loss_mean_cls: 0.044080, grad_norm: 0.306484
Steps:   1%| | 7864/1000000 [38:40<80:00:36,  3.44it/s, grad_norm=0.193, loss_final=0.867, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:36:55[39m] Step: 7858, Training Logs: loss_final: 0.869767, loss_mean: 0.826301, loss_mean_cls: 0.043466, grad_norm: 0.202161
Steps:   1%| | 7871/1000000 [38:42<80:38:51,  3.42it/s, grad_norm=0.199, loss_final=0.856, loss_mean=0.813, loss_mean_cls=0.[[34m2025-10-04 12:36:57[39m] Step: 7865, Training Logs: loss_final: 0.868663, loss_mean: 0.823866, loss_mean_cls: 0.044797, grad_norm: 0.259092
Steps:   1%| | 7872/1000000 [38:42<82:20:48,  3.35it/s, grad_norm=0.199, loss_final=0.856, loss_mean=0.813, loss_mean_cls=0.[[34m2025-10-04 12:36:59[39m] Step: 7872, Training Logs: loss_final: 0.883056, loss_mean: 0.839144, loss_mean_cls: 0.043912, grad_norm: 0.263697
Steps:   1%| | 7879/1000000 [38:44<80:06:16,  3.44it/s, grad_norm=0.238, loss_final=0.858, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:37:01[39m] Step: 7879, Training Logs: loss_final: 0.865897, loss_mean: 0.822728, loss_mean_cls: 0.043169, grad_norm: 0.408928
Steps:   1%| | 7886/1000000 [38:46<79:17:01,  3.48it/s, grad_norm=0.339, loss_final=0.859, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:37:03[39m] Step: 7886, Training Logs: loss_final: 0.882656, loss_mean: 0.838802, loss_mean_cls: 0.043854, grad_norm: 0.393229
Steps:   1%| | 7893/1000000 [38:48<80:08:49,  3.44it/s, grad_norm=0.436, loss_final=0.866, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:37:05[39m] Step: 7893, Training Logs: loss_final: 0.896202, loss_mean: 0.853925, loss_mean_cls: 0.042277, grad_norm: 0.225083
Steps:   1%| | 7900/1000000 [38:50<81:17:49,  3.39it/s, grad_norm=0.314, loss_final=0.867, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:37:07[39m] Step: 7900, Training Logs: loss_final: 0.864188, loss_mean: 0.819677, loss_mean_cls: 0.044510, grad_norm: 0.344107
Steps:   1%| | 7907/1000000 [38:52<81:23:04,  3.39it/s, grad_norm=0.229, loss_final=0.859, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:37:09[39m] Step: 7907, Training Logs: loss_final: 0.888344, loss_mean: 0.845231, loss_mean_cls: 0.043113, grad_norm: 0.232015
Steps:   1%| | 7912/1000000 [38:54<83:11:52,  3.31it/s, grad_norm=0.241, loss_final=0.89, loss_mean=0.848, loss_mean_cls=0.0[[34m2025-10-04 12:37:11[39m] Step: 7912, Training Logs: loss_final: 0.881466, loss_mean: 0.838029, loss_mean_cls: 0.043437, grad_norm: 0.287452
Steps:   1%| | 7919/1000000 [38:56<81:10:26,  3.39it/s, grad_norm=0.474, loss_final=0.881, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:37:13[39m] Step: 7919, Training Logs: loss_final: 0.881675, loss_mean: 0.838010, loss_mean_cls: 0.043665, grad_norm: 0.237897
Steps:   1%| | 7926/1000000 [38:58<81:35:02,  3.38it/s, grad_norm=0.244, loss_final=0.89, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:37:15[39m] Step: 7926, Training Logs: loss_final: 0.877519, loss_mean: 0.834677, loss_mean_cls: 0.042842, grad_norm: 0.273974
Steps:   1%| | 7933/1000000 [39:00<79:31:39,  3.47it/s, grad_norm=0.237, loss_final=0.865, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:37:17[39m] Step: 7933, Training Logs: loss_final: 0.890178, loss_mean: 0.847019, loss_mean_cls: 0.043158, grad_norm: 0.291857
Steps:   1%| | 7945/1000000 [39:04<78:50:13,  3.50it/s, grad_norm=0.207, loss_final=0.873, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:37:18[39m] Step: 7939, Training Logs: loss_final: 0.862196, loss_mean: 0.817402, loss_mean_cls: 0.044794, grad_norm: 0.254308
Steps:   1%| | 7952/1000000 [39:06<78:34:20,  3.51it/s, grad_norm=0.225, loss_final=0.872, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:37:20[39m] Step: 7946, Training Logs: loss_final: 0.883117, loss_mean: 0.840818, loss_mean_cls: 0.042299, grad_norm: 0.194305
Steps:   1%| | 7959/1000000 [39:08<81:00:51,  3.40it/s, grad_norm=0.263, loss_final=0.889, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:37:22[39m] Step: 7953, Training Logs: loss_final: 0.880642, loss_mean: 0.838710, loss_mean_cls: 0.041932, grad_norm: 0.195821
Steps:   1%| | 7960/1000000 [39:08<85:24:18,  3.23it/s, grad_norm=0.263, loss_final=0.889, loss_mean=0.846, loss_mean_cls=0.[[34m2025-10-04 12:37:25[39m] Step: 7960, Training Logs: loss_final: 0.890337, loss_mean: 0.845842, loss_mean_cls: 0.044496, grad_norm: 0.257565
Steps:   1%| | 7967/1000000 [39:10<80:20:59,  3.43it/s, grad_norm=0.257, loss_final=0.885, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:37:27[39m] Step: 7967, Training Logs: loss_final: 0.876452, loss_mean: 0.833088, loss_mean_cls: 0.043364, grad_norm: 0.225647
Steps:   1%| | 7974/1000000 [39:12<79:16:30,  3.48it/s, grad_norm=0.393, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:37:29[39m] Step: 7974, Training Logs: loss_final: 0.883939, loss_mean: 0.840938, loss_mean_cls: 0.043000, grad_norm: 0.192311
Steps:   1%| | 7981/1000000 [39:14<79:16:12,  3.48it/s, grad_norm=0.24, loss_final=0.889, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:37:31[39m] Step: 7981, Training Logs: loss_final: 0.889236, loss_mean: 0.845968, loss_mean_cls: 0.043267, grad_norm: 0.313363
Steps:   1%| | 7988/1000000 [39:16<79:50:52,  3.45it/s, grad_norm=0.268, loss_final=0.883, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:37:33[39m] Step: 7988, Training Logs: loss_final: 0.873970, loss_mean: 0.830485, loss_mean_cls: 0.043485, grad_norm: 0.184063
Steps:   1%| | 7995/1000000 [39:18<79:37:53,  3.46it/s, grad_norm=0.336, loss_final=0.843, loss_mean=0.798, loss_mean_cls=0.[[34m2025-10-04 12:37:35[39m] Step: 7995, Training Logs: loss_final: 0.894951, loss_mean: 0.851049, loss_mean_cls: 0.043902, grad_norm: 0.387184
Steps:   1%| | 8008/1000000 [39:22<78:54:29,  3.49it/s, grad_norm=0.253, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:37:37[39m] Step: 8002, Training Logs: loss_final: 0.860560, loss_mean: 0.816688, loss_mean_cls: 0.043873, grad_norm: 0.311632
Steps:   1%| | 8009/1000000 [39:22<79:21:23,  3.47it/s, grad_norm=0.253, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:37:39[39m] Step: 8009, Training Logs: loss_final: 0.884608, loss_mean: 0.841055, loss_mean_cls: 0.043553, grad_norm: 0.320230
Steps:   1%| | 8022/1000000 [39:26<78:51:25,  3.49it/s, grad_norm=0.229, loss_final=0.86, loss_mean=0.816, loss_mean_cls=0.0[[34m2025-10-04 12:37:41[39m] Step: 8016, Training Logs: loss_final: 0.885170, loss_mean: 0.841178, loss_mean_cls: 0.043992, grad_norm: 0.313614
Steps:   1%| | 8023/1000000 [39:26<79:08:28,  3.48it/s, grad_norm=0.229, loss_final=0.86, loss_mean=0.816, loss_mean_cls=0.0[[34m2025-10-04 12:37:43[39m] Step: 8023, Training Logs: loss_final: 0.885206, loss_mean: 0.842295, loss_mean_cls: 0.042911, grad_norm: 0.287642
Steps:   1%| | 8030/1000000 [39:28<82:29:37,  3.34it/s, grad_norm=0.203, loss_final=0.877, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:37:45[39m] Step: 8030, Training Logs: loss_final: 0.872157, loss_mean: 0.830324, loss_mean_cls: 0.041833, grad_norm: 0.189279
Steps:   1%| | 8037/1000000 [39:30<81:25:22,  3.38it/s, grad_norm=0.281, loss_final=0.842, loss_mean=0.799, loss_mean_cls=0.[[34m2025-10-04 12:37:47[39m] Step: 8037, Training Logs: loss_final: 0.875359, loss_mean: 0.831164, loss_mean_cls: 0.044195, grad_norm: 0.264444
Steps:   1%| | 8044/1000000 [39:32<82:57:47,  3.32it/s, grad_norm=0.238, loss_final=0.895, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:37:49[39m] Step: 8044, Training Logs: loss_final: 0.889967, loss_mean: 0.846903, loss_mean_cls: 0.043063, grad_norm: 0.267992
Steps:   1%| | 8051/1000000 [39:34<80:49:28,  3.41it/s, grad_norm=0.245, loss_final=0.87, loss_mean=0.825, loss_mean_cls=0.0[[34m2025-10-04 12:37:51[39m] Step: 8051, Training Logs: loss_final: 0.872107, loss_mean: 0.828408, loss_mean_cls: 0.043698, grad_norm: 0.398849
Steps:   1%| | 8058/1000000 [39:37<82:19:24,  3.35it/s, grad_norm=0.296, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:37:53[39m] Step: 8058, Training Logs: loss_final: 0.887000, loss_mean: 0.842838, loss_mean_cls: 0.044162, grad_norm: 0.245617
Steps:   1%| | 8070/1000000 [39:40<79:14:31,  3.48it/s, grad_norm=0.275, loss_final=0.875, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:37:55[39m] Step: 8064, Training Logs: loss_final: 0.890169, loss_mean: 0.846906, loss_mean_cls: 0.043262, grad_norm: 0.292661
Steps:   1%| | 8071/1000000 [39:40<80:59:13,  3.40it/s, grad_norm=0.275, loss_final=0.875, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:37:57[39m] Step: 8071, Training Logs: loss_final: 0.869829, loss_mean: 0.826877, loss_mean_cls: 0.042952, grad_norm: 0.172121
Steps:   1%| | 8078/1000000 [39:42<79:45:38,  3.45it/s, grad_norm=0.256, loss_final=0.871, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:37:59[39m] Step: 8078, Training Logs: loss_final: 0.870894, loss_mean: 0.826903, loss_mean_cls: 0.043991, grad_norm: 0.251676
Steps:   1%| | 8085/1000000 [39:44<82:49:02,  3.33it/s, grad_norm=0.218, loss_final=0.87, loss_mean=0.826, loss_mean_cls=0.0[[34m2025-10-04 12:38:01[39m] Step: 8085, Training Logs: loss_final: 0.872007, loss_mean: 0.828712, loss_mean_cls: 0.043295, grad_norm: 0.489402
Steps:   1%| | 8092/1000000 [39:46<81:11:04,  3.39it/s, grad_norm=0.244, loss_final=0.884, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:38:03[39m] Step: 8092, Training Logs: loss_final: 0.864211, loss_mean: 0.819660, loss_mean_cls: 0.044551, grad_norm: 0.262139
Steps:   1%| | 8099/1000000 [39:49<81:28:32,  3.38it/s, grad_norm=0.323, loss_final=0.896, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:38:05[39m] Step: 8099, Training Logs: loss_final: 0.874889, loss_mean: 0.832354, loss_mean_cls: 0.042534, grad_norm: 0.259279
Steps:   1%| | 8106/1000000 [39:51<81:28:17,  3.38it/s, grad_norm=0.232, loss_final=0.858, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:38:07[39m] Step: 8106, Training Logs: loss_final: 0.890303, loss_mean: 0.847264, loss_mean_cls: 0.043039, grad_norm: 0.306252
Steps:   1%| | 8113/1000000 [39:53<82:53:04,  3.32it/s, grad_norm=0.295, loss_final=0.891, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:38:09[39m] Step: 8113, Training Logs: loss_final: 0.880384, loss_mean: 0.837553, loss_mean_cls: 0.042831, grad_norm: 0.267233
Steps:   1%| | 8119/1000000 [39:54<79:32:28,  3.46it/s, grad_norm=0.309, loss_final=0.865, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:38:11[39m] Step: 8119, Training Logs: loss_final: 0.851822, loss_mean: 0.808593, loss_mean_cls: 0.043229, grad_norm: 0.248615
Steps:   1%| | 8126/1000000 [39:56<79:56:56,  3.45it/s, grad_norm=0.184, loss_final=0.896, loss_mean=0.852, loss_mean_cls=0.[[34m2025-10-04 12:38:13[39m] Step: 8126, Training Logs: loss_final: 0.872821, loss_mean: 0.829306, loss_mean_cls: 0.043515, grad_norm: 0.438542
Steps:   1%| | 8133/1000000 [39:58<79:00:13,  3.49it/s, grad_norm=0.258, loss_final=0.867, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:38:15[39m] Step: 8133, Training Logs: loss_final: 0.878211, loss_mean: 0.833717, loss_mean_cls: 0.044493, grad_norm: 0.236518
Steps:   1%| | 8140/1000000 [40:00<80:27:39,  3.42it/s, grad_norm=0.304, loss_final=0.864, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:38:17[39m] Step: 8140, Training Logs: loss_final: 0.867906, loss_mean: 0.823316, loss_mean_cls: 0.044590, grad_norm: 0.232890
Steps:   1%| | 8147/1000000 [40:03<81:10:01,  3.39it/s, grad_norm=0.31, loss_final=0.873, loss_mean=0.829, loss_mean_cls=0.0[[34m2025-10-04 12:38:19[39m] Step: 8147, Training Logs: loss_final: 0.864053, loss_mean: 0.820457, loss_mean_cls: 0.043597, grad_norm: 0.329763
Steps:   1%| | 8154/1000000 [40:05<79:12:39,  3.48it/s, grad_norm=0.37, loss_final=0.866, loss_mean=0.824, loss_mean_cls=0.0[[34m2025-10-04 12:38:21[39m] Step: 8154, Training Logs: loss_final: 0.866007, loss_mean: 0.822396, loss_mean_cls: 0.043611, grad_norm: 0.349778
Steps:   1%| | 8161/1000000 [40:07<79:39:42,  3.46it/s, grad_norm=0.207, loss_final=0.871, loss_mean=0.827, loss_mean_cls=0.[[34m2025-10-04 12:38:23[39m] Step: 8161, Training Logs: loss_final: 0.883299, loss_mean: 0.840497, loss_mean_cls: 0.042803, grad_norm: 0.350122
Steps:   1%| | 8168/1000000 [40:09<78:48:32,  3.50it/s, grad_norm=0.447, loss_final=0.875, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:38:25[39m] Step: 8168, Training Logs: loss_final: 0.873551, loss_mean: 0.830192, loss_mean_cls: 0.043359, grad_norm: 0.245219
Steps:   1%| | 8175/1000000 [40:11<80:00:04,  3.44it/s, grad_norm=0.273, loss_final=0.885, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:38:27[39m] Step: 8175, Training Logs: loss_final: 0.867070, loss_mean: 0.822660, loss_mean_cls: 0.044409, grad_norm: 0.362284
Steps:   1%| | 8180/1000000 [40:12<81:40:55,  3.37it/s, grad_norm=0.3, loss_final=0.87, loss_mean=0.827, loss_mean_cls=0.043[[34m2025-10-04 12:38:29[39m] Step: 8180, Training Logs: loss_final: 0.853856, loss_mean: 0.809093, loss_mean_cls: 0.044763, grad_norm: 0.351148
Steps:   1%| | 8187/1000000 [40:14<82:36:30,  3.34it/s, grad_norm=0.274, loss_final=0.865, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:38:31[39m] Step: 8187, Training Logs: loss_final: 0.873743, loss_mean: 0.829911, loss_mean_cls: 0.043832, grad_norm: 0.244324
Steps:   1%| | 8194/1000000 [40:16<79:23:16,  3.47it/s, grad_norm=0.211, loss_final=0.886, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:38:33[39m] Step: 8194, Training Logs: loss_final: 0.859930, loss_mean: 0.816423, loss_mean_cls: 0.043507, grad_norm: 0.290721
Steps:   1%| | 8201/1000000 [40:18<81:52:21,  3.36it/s, grad_norm=0.243, loss_final=0.87, loss_mean=0.827, loss_mean_cls=0.0[[34m2025-10-04 12:38:35[39m] Step: 8201, Training Logs: loss_final: 0.890507, loss_mean: 0.848363, loss_mean_cls: 0.042144, grad_norm: 0.216464
Steps:   1%| | 8208/1000000 [40:20<79:32:10,  3.46it/s, grad_norm=0.3, loss_final=0.877, loss_mean=0.833, loss_mean_cls=0.04[[34m2025-10-04 12:38:37[39m] Step: 8208, Training Logs: loss_final: 0.876810, loss_mean: 0.833466, loss_mean_cls: 0.043343, grad_norm: 0.209962
Steps:   1%| | 8215/1000000 [40:22<78:46:48,  3.50it/s, grad_norm=0.292, loss_final=0.869, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:38:39[39m] Step: 8215, Training Logs: loss_final: 0.863271, loss_mean: 0.819146, loss_mean_cls: 0.044125, grad_norm: 0.265396
Steps:   1%| | 8222/1000000 [40:24<81:25:42,  3.38it/s, grad_norm=0.399, loss_final=0.868, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:38:41[39m] Step: 8222, Training Logs: loss_final: 0.870245, loss_mean: 0.827055, loss_mean_cls: 0.043190, grad_norm: 0.274349
Steps:   1%| | 8229/1000000 [40:26<80:00:13,  3.44it/s, grad_norm=0.258, loss_final=0.864, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:38:43[39m] Step: 8229, Training Logs: loss_final: 0.894565, loss_mean: 0.851553, loss_mean_cls: 0.043011, grad_norm: 0.273337
Steps:   1%| | 8236/1000000 [40:28<79:50:29,  3.45it/s, grad_norm=0.297, loss_final=0.88, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:38:45[39m] Step: 8236, Training Logs: loss_final: 0.865003, loss_mean: 0.820737, loss_mean_cls: 0.044266, grad_norm: 0.279012
Steps:   1%| | 8243/1000000 [40:30<81:17:54,  3.39it/s, grad_norm=0.307, loss_final=0.897, loss_mean=0.855, loss_mean_cls=0.[[34m2025-10-04 12:38:47[39m] Step: 8243, Training Logs: loss_final: 0.881422, loss_mean: 0.837673, loss_mean_cls: 0.043749, grad_norm: 0.341547
Steps:   1%| | 8250/1000000 [40:33<81:31:29,  3.38it/s, grad_norm=0.354, loss_final=0.899, loss_mean=0.856, loss_mean_cls=0.[[34m2025-10-04 12:38:49[39m] Step: 8250, Training Logs: loss_final: 0.867727, loss_mean: 0.823243, loss_mean_cls: 0.044485, grad_norm: 0.307682
Steps:   1%| | 8257/1000000 [40:35<80:05:38,  3.44it/s, grad_norm=0.356, loss_final=0.874, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:38:51[39m] Step: 8257, Training Logs: loss_final: 0.880060, loss_mean: 0.837885, loss_mean_cls: 0.042175, grad_norm: 0.228653
Steps:   1%| | 8263/1000000 [40:36<81:34:27,  3.38it/s, grad_norm=0.242, loss_final=0.868, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:38:53[39m] Step: 8263, Training Logs: loss_final: 0.873587, loss_mean: 0.829135, loss_mean_cls: 0.044452, grad_norm: 0.247907
Steps:   1%| | 8270/1000000 [40:38<79:53:03,  3.45it/s, grad_norm=0.182, loss_final=0.898, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:38:55[39m] Step: 8270, Training Logs: loss_final: 0.888107, loss_mean: 0.843697, loss_mean_cls: 0.044410, grad_norm: 0.218633
Steps:   1%| | 8277/1000000 [40:40<80:36:46,  3.42it/s, grad_norm=0.351, loss_final=0.891, loss_mean=0.848, loss_mean_cls=0.[[34m2025-10-04 12:38:57[39m] Step: 8277, Training Logs: loss_final: 0.888287, loss_mean: 0.845203, loss_mean_cls: 0.043085, grad_norm: 0.262193
Steps:   1%| | 8284/1000000 [40:42<79:06:47,  3.48it/s, grad_norm=0.339, loss_final=0.877, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:38:59[39m] Step: 8284, Training Logs: loss_final: 0.873492, loss_mean: 0.830727, loss_mean_cls: 0.042765, grad_norm: 0.307436
Steps:   1%| | 8291/1000000 [40:44<79:43:11,  3.46it/s, grad_norm=0.217, loss_final=0.842, loss_mean=0.799, loss_mean_cls=0.[[34m2025-10-04 12:39:01[39m] Step: 8291, Training Logs: loss_final: 0.876213, loss_mean: 0.832555, loss_mean_cls: 0.043658, grad_norm: 0.257222
Steps:   1%| | 8298/1000000 [40:47<80:54:26,  3.40it/s, grad_norm=0.276, loss_final=0.872, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:39:03[39m] Step: 8298, Training Logs: loss_final: 0.889533, loss_mean: 0.845893, loss_mean_cls: 0.043640, grad_norm: 0.245018
Steps:   1%| | 8305/1000000 [40:49<80:36:18,  3.42it/s, grad_norm=0.196, loss_final=0.881, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:39:05[39m] Step: 8305, Training Logs: loss_final: 0.862160, loss_mean: 0.819405, loss_mean_cls: 0.042754, grad_norm: 0.185793
Steps:   1%| | 8312/1000000 [40:51<79:17:00,  3.47it/s, grad_norm=0.242, loss_final=0.885, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:39:07[39m] Step: 8312, Training Logs: loss_final: 0.871533, loss_mean: 0.828335, loss_mean_cls: 0.043199, grad_norm: 0.394144
Steps:   1%| | 8319/1000000 [40:53<78:53:12,  3.49it/s, grad_norm=0.391, loss_final=0.882, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:39:09[39m] Step: 8319, Training Logs: loss_final: 0.865299, loss_mean: 0.820881, loss_mean_cls: 0.044418, grad_norm: 0.248689
Steps:   1%| | 8326/1000000 [40:55<81:45:11,  3.37it/s, grad_norm=0.437, loss_final=0.878, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:39:11[39m] Step: 8326, Training Logs: loss_final: 0.865451, loss_mean: 0.822202, loss_mean_cls: 0.043249, grad_norm: 0.227055
Steps:   1%| | 8333/1000000 [40:57<83:58:19,  3.28it/s, grad_norm=0.19, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.0[[34m2025-10-04 12:39:13[39m] Step: 8333, Training Logs: loss_final: 0.862034, loss_mean: 0.818163, loss_mean_cls: 0.043871, grad_norm: 0.247974
Steps:   1%| | 8339/1000000 [40:59<82:30:10,  3.34it/s, grad_norm=0.208, loss_final=0.876, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:39:15[39m] Step: 8339, Training Logs: loss_final: 0.879794, loss_mean: 0.835955, loss_mean_cls: 0.043839, grad_norm: 0.257113
Steps:   1%| | 8346/1000000 [41:01<82:02:28,  3.36it/s, grad_norm=0.385, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:39:17[39m] Step: 8346, Training Logs: loss_final: 0.867690, loss_mean: 0.824777, loss_mean_cls: 0.042913, grad_norm: 0.170147
Steps:   1%| | 8353/1000000 [41:03<82:29:20,  3.34it/s, grad_norm=0.294, loss_final=0.859, loss_mean=0.816, loss_mean_cls=0.[[34m2025-10-04 12:39:19[39m] Step: 8353, Training Logs: loss_final: 0.862808, loss_mean: 0.819859, loss_mean_cls: 0.042948, grad_norm: 0.229988
Steps:   1%| | 8360/1000000 [41:05<82:56:15,  3.32it/s, grad_norm=0.203, loss_final=0.866, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:39:22[39m] Step: 8360, Training Logs: loss_final: 0.894798, loss_mean: 0.851207, loss_mean_cls: 0.043591, grad_norm: 0.263023
Steps:   1%| | 8366/1000000 [41:07<81:18:19,  3.39it/s, grad_norm=0.258, loss_final=0.891, loss_mean=0.847, loss_mean_cls=0.[[34m2025-10-04 12:39:23[39m] Step: 8366, Training Logs: loss_final: 0.865604, loss_mean: 0.822155, loss_mean_cls: 0.043449, grad_norm: 0.204704
Steps:   1%| | 8373/1000000 [41:09<83:31:29,  3.30it/s, grad_norm=0.296, loss_final=0.856, loss_mean=0.812, loss_mean_cls=0.[[34m2025-10-04 12:39:25[39m] Step: 8373, Training Logs: loss_final: 0.856806, loss_mean: 0.813772, loss_mean_cls: 0.043034, grad_norm: 0.223958
Steps:   1%| | 8380/1000000 [41:11<81:09:57,  3.39it/s, grad_norm=0.246, loss_final=0.859, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:39:28[39m] Step: 8380, Training Logs: loss_final: 0.915131, loss_mean: 0.872272, loss_mean_cls: 0.042859, grad_norm: 0.235945
Steps:   1%| | 8387/1000000 [41:13<81:41:42,  3.37it/s, grad_norm=0.31, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.04[[34m2025-10-04 12:39:30[39m] Step: 8387, Training Logs: loss_final: 0.876678, loss_mean: 0.832740, loss_mean_cls: 0.043938, grad_norm: 0.236646
Steps:   1%| | 8394/1000000 [41:15<80:48:50,  3.41it/s, grad_norm=0.269, loss_final=0.871, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:39:32[39m] Step: 8394, Training Logs: loss_final: 0.871046, loss_mean: 0.827814, loss_mean_cls: 0.043232, grad_norm: 0.325241
Steps:   1%| | 8401/1000000 [41:17<79:50:52,  3.45it/s, grad_norm=0.25, loss_final=0.863, loss_mean=0.819, loss_mean_cls=0.0[[34m2025-10-04 12:39:34[39m] Step: 8401, Training Logs: loss_final: 0.895675, loss_mean: 0.852263, loss_mean_cls: 0.043413, grad_norm: 0.265189
Steps:   1%| | 8407/1000000 [41:19<80:33:50,  3.42it/s, grad_norm=0.177, loss_final=0.875, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:39:35[39m] Step: 8407, Training Logs: loss_final: 0.878722, loss_mean: 0.835524, loss_mean_cls: 0.043198, grad_norm: 0.309989
Steps:   1%| | 8414/1000000 [41:21<81:03:26,  3.40it/s, grad_norm=0.27, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.04[[34m2025-10-04 12:39:37[39m] Step: 8414, Training Logs: loss_final: 0.881711, loss_mean: 0.839430, loss_mean_cls: 0.042282, grad_norm: 0.229977
Steps:   1%| | 8421/1000000 [41:23<80:36:20,  3.42it/s, grad_norm=0.276, loss_final=0.893, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:39:40[39m] Step: 8421, Training Logs: loss_final: 0.865603, loss_mean: 0.822973, loss_mean_cls: 0.042630, grad_norm: 0.277095
Steps:   1%| | 8428/1000000 [41:25<81:44:49,  3.37it/s, grad_norm=0.222, loss_final=0.885, loss_mean=0.843, loss_mean_cls=0.[[34m2025-10-04 12:39:42[39m] Step: 8428, Training Logs: loss_final: 0.879617, loss_mean: 0.837610, loss_mean_cls: 0.042007, grad_norm: 0.204824
Steps:   1%| | 8433/1000000 [41:26<82:39:47,  3.33it/s, grad_norm=0.316, loss_final=0.884, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:39:43[39m] Step: 8433, Training Logs: loss_final: 0.874716, loss_mean: 0.831339, loss_mean_cls: 0.043376, grad_norm: 0.181625
Steps:   1%| | 8440/1000000 [41:28<79:43:28,  3.45it/s, grad_norm=0.317, loss_final=0.882, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:39:45[39m] Step: 8440, Training Logs: loss_final: 0.876102, loss_mean: 0.834386, loss_mean_cls: 0.041716, grad_norm: 0.196392
Steps:   1%| | 8447/1000000 [41:31<82:20:26,  3.35it/s, grad_norm=0.28, loss_final=0.852, loss_mean=0.81, loss_mean_cls=0.04[[34m2025-10-04 12:39:47[39m] Step: 8447, Training Logs: loss_final: 0.880693, loss_mean: 0.836695, loss_mean_cls: 0.043998, grad_norm: 0.400896
Steps:   1%| | 8454/1000000 [41:33<80:42:57,  3.41it/s, grad_norm=0.238, loss_final=0.888, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:39:49[39m] Step: 8454, Training Logs: loss_final: 0.895768, loss_mean: 0.852183, loss_mean_cls: 0.043585, grad_norm: 0.271779
Steps:   1%| | 8460/1000000 [41:34<84:37:03,  3.25it/s, grad_norm=0.211, loss_final=0.887, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:39:51[39m] Step: 8460, Training Logs: loss_final: 0.883685, loss_mean: 0.841352, loss_mean_cls: 0.042333, grad_norm: 0.221725
Steps:   1%| | 8467/1000000 [41:37<80:16:48,  3.43it/s, grad_norm=0.209, loss_final=0.886, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:39:53[39m] Step: 8467, Training Logs: loss_final: 0.888721, loss_mean: 0.845353, loss_mean_cls: 0.043368, grad_norm: 0.212558
Steps:   1%| | 8474/1000000 [41:39<78:55:21,  3.49it/s, grad_norm=0.355, loss_final=0.866, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:39:55[39m] Step: 8474, Training Logs: loss_final: 0.885237, loss_mean: 0.842103, loss_mean_cls: 0.043134, grad_norm: 0.293823
Steps:   1%| | 8481/1000000 [41:41<83:05:14,  3.31it/s, grad_norm=0.211, loss_final=0.876, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:39:57[39m] Step: 8481, Training Logs: loss_final: 0.893249, loss_mean: 0.849710, loss_mean_cls: 0.043540, grad_norm: 0.296045
Steps:   1%| | 8488/1000000 [41:43<82:47:59,  3.33it/s, grad_norm=0.369, loss_final=0.87, loss_mean=0.827, loss_mean_cls=0.0[[34m2025-10-04 12:39:59[39m] Step: 8488, Training Logs: loss_final: 0.895769, loss_mean: 0.853612, loss_mean_cls: 0.042157, grad_norm: 0.258422
Steps:   1%| | 8495/1000000 [41:45<82:45:43,  3.33it/s, grad_norm=0.319, loss_final=0.865, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:40:01[39m] Step: 8495, Training Logs: loss_final: 0.862715, loss_mean: 0.819307, loss_mean_cls: 0.043409, grad_norm: 0.397671
Steps:   1%| | 8501/1000000 [41:47<83:44:30,  3.29it/s, grad_norm=0.615, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:40:03[39m] Step: 8501, Training Logs: loss_final: 0.878366, loss_mean: 0.835695, loss_mean_cls: 0.042671, grad_norm: 0.171536
Steps:   1%| | 8508/1000000 [41:49<84:21:51,  3.26it/s, grad_norm=0.305, loss_final=0.896, loss_mean=0.853, loss_mean_cls=0.[[34m2025-10-04 12:40:05[39m] Step: 8508, Training Logs: loss_final: 0.888262, loss_mean: 0.844728, loss_mean_cls: 0.043534, grad_norm: 0.514900
Steps:   1%| | 8515/1000000 [41:51<84:37:57,  3.25it/s, grad_norm=0.321, loss_final=0.867, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:40:08[39m] Step: 8515, Training Logs: loss_final: 0.871493, loss_mean: 0.828019, loss_mean_cls: 0.043474, grad_norm: 0.265213
Steps:   1%| | 8527/1000000 [41:54<79:14:49,  3.48it/s, grad_norm=0.199, loss_final=0.857, loss_mean=0.813, loss_mean_cls=0.[[34m2025-10-04 12:40:09[39m] Step: 8521, Training Logs: loss_final: 0.884771, loss_mean: 0.841579, loss_mean_cls: 0.043192, grad_norm: 0.378486
Steps:   1%| | 8528/1000000 [41:55<79:01:43,  3.48it/s, grad_norm=0.199, loss_final=0.857, loss_mean=0.813, loss_mean_cls=0.[[34m2025-10-04 12:40:11[39m] Step: 8528, Training Logs: loss_final: 0.881080, loss_mean: 0.838275, loss_mean_cls: 0.042805, grad_norm: 0.350963
Steps:   1%| | 8535/1000000 [41:57<81:24:42,  3.38it/s, grad_norm=0.333, loss_final=0.874, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:40:13[39m] Step: 8535, Training Logs: loss_final: 0.891086, loss_mean: 0.848429, loss_mean_cls: 0.042657, grad_norm: 0.227978
Steps:   1%| | 8542/1000000 [41:59<83:23:13,  3.30it/s, grad_norm=0.224, loss_final=0.889, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:40:15[39m] Step: 8542, Training Logs: loss_final: 0.853165, loss_mean: 0.808726, loss_mean_cls: 0.044439, grad_norm: 0.300142
Steps:   1%| | 8548/1000000 [42:01<84:25:08,  3.26it/s, grad_norm=0.235, loss_final=0.876, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:40:17[39m] Step: 8548, Training Logs: loss_final: 0.866801, loss_mean: 0.824521, loss_mean_cls: 0.042280, grad_norm: 0.183307
Steps:   1%| | 8555/1000000 [42:03<80:00:09,  3.44it/s, grad_norm=0.242, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:40:19[39m] Step: 8555, Training Logs: loss_final: 0.884012, loss_mean: 0.841766, loss_mean_cls: 0.042245, grad_norm: 0.313728
Steps:   1%| | 8562/1000000 [42:05<81:22:49,  3.38it/s, grad_norm=0.34, loss_final=0.858, loss_mean=0.815, loss_mean_cls=0.0[[34m2025-10-04 12:40:21[39m] Step: 8562, Training Logs: loss_final: 0.874987, loss_mean: 0.831370, loss_mean_cls: 0.043618, grad_norm: 0.372482
Steps:   1%| | 8569/1000000 [42:07<82:35:18,  3.33it/s, grad_norm=0.392, loss_final=0.878, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:40:24[39m] Step: 8569, Training Logs: loss_final: 0.868941, loss_mean: 0.825796, loss_mean_cls: 0.043146, grad_norm: 0.215153
Steps:   1%| | 8576/1000000 [42:09<80:40:35,  3.41it/s, grad_norm=0.204, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:40:26[39m] Step: 8576, Training Logs: loss_final: 0.865301, loss_mean: 0.820789, loss_mean_cls: 0.044512, grad_norm: 0.331872
Steps:   1%| | 8583/1000000 [42:11<80:18:04,  3.43it/s, grad_norm=0.371, loss_final=0.876, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:40:28[39m] Step: 8583, Training Logs: loss_final: 0.872474, loss_mean: 0.829972, loss_mean_cls: 0.042503, grad_norm: 0.284109
Steps:   1%| | 8590/1000000 [42:13<80:50:09,  3.41it/s, grad_norm=0.418, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:40:30[39m] Step: 8590, Training Logs: loss_final: 0.863000, loss_mean: 0.820325, loss_mean_cls: 0.042675, grad_norm: 0.282971
Steps:   1%| | 8597/1000000 [42:15<82:22:57,  3.34it/s, grad_norm=0.248, loss_final=0.884, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:40:32[39m] Step: 8597, Training Logs: loss_final: 0.868404, loss_mean: 0.824766, loss_mean_cls: 0.043638, grad_norm: 0.296379
Steps:   1%| | 8603/1000000 [42:17<79:26:50,  3.47it/s, grad_norm=0.297, loss_final=0.894, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:40:33[39m] Step: 8603, Training Logs: loss_final: 0.883520, loss_mean: 0.840201, loss_mean_cls: 0.043318, grad_norm: 0.256816
Steps:   1%| | 8610/1000000 [42:19<79:18:48,  3.47it/s, grad_norm=0.227, loss_final=0.869, loss_mean=0.826, loss_mean_cls=0.[[34m2025-10-04 12:40:36[39m] Step: 8610, Training Logs: loss_final: 0.877980, loss_mean: 0.834890, loss_mean_cls: 0.043090, grad_norm: 0.275911
Steps:   1%| | 8617/1000000 [42:21<79:45:18,  3.45it/s, grad_norm=0.287, loss_final=0.876, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:40:38[39m] Step: 8617, Training Logs: loss_final: 0.879870, loss_mean: 0.837028, loss_mean_cls: 0.042842, grad_norm: 0.280096
Steps:   1%| | 8624/1000000 [42:23<79:18:43,  3.47it/s, grad_norm=0.351, loss_final=0.85, loss_mean=0.807, loss_mean_cls=0.0[[34m2025-10-04 12:40:40[39m] Step: 8624, Training Logs: loss_final: 0.877132, loss_mean: 0.833948, loss_mean_cls: 0.043184, grad_norm: 0.256661
Steps:   1%| | 8631/1000000 [42:25<82:30:24,  3.34it/s, grad_norm=0.188, loss_final=0.876, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:40:42[39m] Step: 8631, Training Logs: loss_final: 0.877026, loss_mean: 0.834867, loss_mean_cls: 0.042159, grad_norm: 0.259203
Steps:   1%| | 8638/1000000 [42:27<79:54:52,  3.45it/s, grad_norm=0.225, loss_final=0.866, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:40:44[39m] Step: 8638, Training Logs: loss_final: 0.855888, loss_mean: 0.813058, loss_mean_cls: 0.042829, grad_norm: 0.213189
Steps:   1%| | 8645/1000000 [42:29<79:02:49,  3.48it/s, grad_norm=0.292, loss_final=0.866, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:40:46[39m] Step: 8645, Training Logs: loss_final: 0.869721, loss_mean: 0.825994, loss_mean_cls: 0.043726, grad_norm: 0.196381
Steps:   1%| | 8652/1000000 [42:31<79:00:55,  3.49it/s, grad_norm=0.226, loss_final=0.866, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:40:48[39m] Step: 8652, Training Logs: loss_final: 0.863863, loss_mean: 0.820322, loss_mean_cls: 0.043541, grad_norm: 0.321563
Steps:   1%| | 8659/1000000 [42:33<80:11:24,  3.43it/s, grad_norm=0.237, loss_final=0.87, loss_mean=0.829, loss_mean_cls=0.0[[34m2025-10-04 12:40:50[39m] Step: 8659, Training Logs: loss_final: 0.877519, loss_mean: 0.834613, loss_mean_cls: 0.042906, grad_norm: 0.593091
Steps:   1%| | 8666/1000000 [42:35<79:21:28,  3.47it/s, grad_norm=0.533, loss_final=0.912, loss_mean=0.869, loss_mean_cls=0.[[34m2025-10-04 12:40:52[39m] Step: 8666, Training Logs: loss_final: 0.873064, loss_mean: 0.828893, loss_mean_cls: 0.044170, grad_norm: 0.237953
Steps:   1%| | 8673/1000000 [42:37<79:33:43,  3.46it/s, grad_norm=0.207, loss_final=0.858, loss_mean=0.815, loss_mean_cls=0.[[34m2025-10-04 12:40:54[39m] Step: 8673, Training Logs: loss_final: 0.879797, loss_mean: 0.837312, loss_mean_cls: 0.042485, grad_norm: 0.426341
Steps:   1%| | 8680/1000000 [42:39<78:50:28,  3.49it/s, grad_norm=0.312, loss_final=0.871, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:40:56[39m] Step: 8680, Training Logs: loss_final: 0.861394, loss_mean: 0.817969, loss_mean_cls: 0.043425, grad_norm: 0.193921
Steps:   1%| | 8687/1000000 [42:41<79:21:24,  3.47it/s, grad_norm=0.336, loss_final=0.872, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:40:58[39m] Step: 8687, Training Logs: loss_final: 0.890693, loss_mean: 0.847585, loss_mean_cls: 0.043107, grad_norm: 0.220338
Steps:   1%| | 8694/1000000 [42:43<81:10:53,  3.39it/s, grad_norm=0.21, loss_final=0.861, loss_mean=0.819, loss_mean_cls=0.0[[34m2025-10-04 12:41:00[39m] Step: 8694, Training Logs: loss_final: 0.889247, loss_mean: 0.846405, loss_mean_cls: 0.042842, grad_norm: 0.262596
Steps:   1%| | 8699/1000000 [42:45<80:41:20,  3.41it/s, grad_norm=0.219, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:41:01[39m] Step: 8699, Training Logs: loss_final: 0.879528, loss_mean: 0.836598, loss_mean_cls: 0.042930, grad_norm: 0.424328
Steps:   1%| | 8706/1000000 [42:47<81:20:40,  3.39it/s, grad_norm=0.213, loss_final=0.851, loss_mean=0.807, loss_mean_cls=0.[[34m2025-10-04 12:41:03[39m] Step: 8706, Training Logs: loss_final: 0.882130, loss_mean: 0.838922, loss_mean_cls: 0.043208, grad_norm: 0.233163
Steps:   1%| | 8713/1000000 [42:49<80:41:05,  3.41it/s, grad_norm=0.314, loss_final=0.869, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:41:05[39m] Step: 8713, Training Logs: loss_final: 0.879305, loss_mean: 0.836535, loss_mean_cls: 0.042770, grad_norm: 0.201620
Steps:   1%| | 8720/1000000 [42:51<79:20:38,  3.47it/s, grad_norm=0.26, loss_final=0.878, loss_mean=0.835, loss_mean_cls=0.0[[34m2025-10-04 12:41:07[39m] Step: 8720, Training Logs: loss_final: 0.872357, loss_mean: 0.828597, loss_mean_cls: 0.043760, grad_norm: 0.301771
Steps:   1%| | 8727/1000000 [42:53<79:45:09,  3.45it/s, grad_norm=0.329, loss_final=0.871, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:41:10[39m] Step: 8727, Training Logs: loss_final: 0.856392, loss_mean: 0.813601, loss_mean_cls: 0.042791, grad_norm: 0.227096
Steps:   1%| | 8734/1000000 [42:55<80:48:43,  3.41it/s, grad_norm=0.189, loss_final=0.863, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:41:12[39m] Step: 8734, Training Logs: loss_final: 0.859871, loss_mean: 0.816300, loss_mean_cls: 0.043571, grad_norm: 0.216203
Steps:   1%| | 8741/1000000 [42:57<83:07:37,  3.31it/s, grad_norm=0.309, loss_final=0.871, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:41:14[39m] Step: 8741, Training Logs: loss_final: 0.873738, loss_mean: 0.830995, loss_mean_cls: 0.042744, grad_norm: 0.220826
Steps:   1%| | 8747/1000000 [42:59<81:06:01,  3.40it/s, grad_norm=0.388, loss_final=0.892, loss_mean=0.849, loss_mean_cls=0.[[34m2025-10-04 12:41:15[39m] Step: 8747, Training Logs: loss_final: 0.871997, loss_mean: 0.828577, loss_mean_cls: 0.043421, grad_norm: 0.222196
Steps:   1%| | 8754/1000000 [43:01<81:22:37,  3.38it/s, grad_norm=0.335, loss_final=0.867, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:41:17[39m] Step: 8754, Training Logs: loss_final: 0.884956, loss_mean: 0.842683, loss_mean_cls: 0.042273, grad_norm: 0.251330
Steps:   1%| | 8761/1000000 [43:03<79:36:57,  3.46it/s, grad_norm=0.287, loss_final=0.871, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:41:20[39m] Step: 8761, Training Logs: loss_final: 0.854541, loss_mean: 0.809977, loss_mean_cls: 0.044563, grad_norm: 0.320333
Steps:   1%| | 8768/1000000 [43:05<78:34:27,  3.50it/s, grad_norm=0.331, loss_final=0.863, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:41:22[39m] Step: 8768, Training Logs: loss_final: 0.872295, loss_mean: 0.829771, loss_mean_cls: 0.042525, grad_norm: 0.366733
Steps:   1%| | 8775/1000000 [43:07<79:39:39,  3.46it/s, grad_norm=0.2, loss_final=0.869, loss_mean=0.826, loss_mean_cls=0.04[[34m2025-10-04 12:41:24[39m] Step: 8775, Training Logs: loss_final: 0.888815, loss_mean: 0.845613, loss_mean_cls: 0.043202, grad_norm: 0.275476
Steps:   1%| | 8782/1000000 [43:09<79:14:47,  3.47it/s, grad_norm=0.263, loss_final=0.877, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:41:26[39m] Step: 8782, Training Logs: loss_final: 0.885963, loss_mean: 0.842881, loss_mean_cls: 0.043082, grad_norm: 0.184805
Steps:   1%| | 8789/1000000 [43:11<80:28:44,  3.42it/s, grad_norm=0.284, loss_final=0.876, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:41:28[39m] Step: 8789, Training Logs: loss_final: 0.857471, loss_mean: 0.813477, loss_mean_cls: 0.043993, grad_norm: 0.200409
Steps:   1%| | 8796/1000000 [43:13<80:13:23,  3.43it/s, grad_norm=0.231, loss_final=0.873, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:41:30[39m] Step: 8796, Training Logs: loss_final: 0.881096, loss_mean: 0.838669, loss_mean_cls: 0.042427, grad_norm: 0.214189
Steps:   1%| | 8803/1000000 [43:15<81:06:26,  3.39it/s, grad_norm=0.236, loss_final=0.875, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:41:32[39m] Step: 8803, Training Logs: loss_final: 0.853858, loss_mean: 0.809984, loss_mean_cls: 0.043874, grad_norm: 0.188436
Steps:   1%| | 8810/1000000 [43:17<80:10:49,  3.43it/s, grad_norm=0.196, loss_final=0.882, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:41:34[39m] Step: 8810, Training Logs: loss_final: 0.863685, loss_mean: 0.820603, loss_mean_cls: 0.043082, grad_norm: 0.241217
Steps:   1%| | 8817/1000000 [43:19<79:03:26,  3.48it/s, grad_norm=0.194, loss_final=0.867, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:41:36[39m] Step: 8817, Training Logs: loss_final: 0.885473, loss_mean: 0.842369, loss_mean_cls: 0.043104, grad_norm: 0.286157
Steps:   1%| | 8824/1000000 [43:21<79:14:34,  3.47it/s, grad_norm=0.176, loss_final=0.884, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:41:38[39m] Step: 8824, Training Logs: loss_final: 0.881367, loss_mean: 0.838451, loss_mean_cls: 0.042916, grad_norm: 0.239759
Steps:   1%| | 8831/1000000 [43:23<78:56:50,  3.49it/s, grad_norm=0.22, loss_final=0.868, loss_mean=0.825, loss_mean_cls=0.0[[34m2025-10-04 12:41:40[39m] Step: 8831, Training Logs: loss_final: 0.876521, loss_mean: 0.832977, loss_mean_cls: 0.043545, grad_norm: 0.248338
Steps:   1%| | 8838/1000000 [43:25<79:32:15,  3.46it/s, grad_norm=0.167, loss_final=0.86, loss_mean=0.817, loss_mean_cls=0.0[[34m2025-10-04 12:41:42[39m] Step: 8838, Training Logs: loss_final: 0.877832, loss_mean: 0.834985, loss_mean_cls: 0.042847, grad_norm: 0.336954
Steps:   1%| | 8844/1000000 [43:27<82:08:59,  3.35it/s, grad_norm=0.245, loss_final=0.868, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:41:44[39m] Step: 8844, Training Logs: loss_final: 0.880189, loss_mean: 0.837351, loss_mean_cls: 0.042838, grad_norm: 0.241885
Steps:   1%| | 8851/1000000 [43:29<81:37:04,  3.37it/s, grad_norm=0.245, loss_final=0.871, loss_mean=0.827, loss_mean_cls=0.[[34m2025-10-04 12:41:46[39m] Step: 8851, Training Logs: loss_final: 0.879125, loss_mean: 0.835906, loss_mean_cls: 0.043218, grad_norm: 0.253349
Steps:   1%| | 8858/1000000 [43:31<82:02:35,  3.36it/s, grad_norm=0.282, loss_final=0.855, loss_mean=0.811, loss_mean_cls=0.[[34m2025-10-04 12:41:48[39m] Step: 8858, Training Logs: loss_final: 0.870900, loss_mean: 0.827207, loss_mean_cls: 0.043693, grad_norm: 0.339308
Steps:   1%| | 8865/1000000 [43:33<81:26:24,  3.38it/s, grad_norm=0.215, loss_final=0.889, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:41:50[39m] Step: 8865, Training Logs: loss_final: 0.867019, loss_mean: 0.824077, loss_mean_cls: 0.042942, grad_norm: 0.512706
Steps:   1%| | 8872/1000000 [43:35<80:31:55,  3.42it/s, grad_norm=0.283, loss_final=0.878, loss_mean=0.834, loss_mean_cls=0.[[34m2025-10-04 12:41:52[39m] Step: 8872, Training Logs: loss_final: 0.875122, loss_mean: 0.832168, loss_mean_cls: 0.042954, grad_norm: 0.373046
Steps:   1%| | 8879/1000000 [43:37<80:31:25,  3.42it/s, grad_norm=0.37, loss_final=0.89, loss_mean=0.847, loss_mean_cls=0.04[[34m2025-10-04 12:41:54[39m] Step: 8879, Training Logs: loss_final: 0.869085, loss_mean: 0.827102, loss_mean_cls: 0.041983, grad_norm: 0.288350
Steps:   1%| | 8885/1000000 [43:39<81:26:17,  3.38it/s, grad_norm=0.29, loss_final=0.871, loss_mean=0.827, loss_mean_cls=0.0[[34m2025-10-04 12:41:56[39m] Step: 8885, Training Logs: loss_final: 0.887937, loss_mean: 0.845280, loss_mean_cls: 0.042657, grad_norm: 0.174284
Steps:   1%| | 8892/1000000 [43:41<81:07:44,  3.39it/s, grad_norm=0.175, loss_final=0.855, loss_mean=0.811, loss_mean_cls=0.[[34m2025-10-04 12:41:58[39m] Step: 8892, Training Logs: loss_final: 0.873325, loss_mean: 0.830147, loss_mean_cls: 0.043178, grad_norm: 0.233387
Steps:   1%| | 8899/1000000 [43:43<85:33:39,  3.22it/s, grad_norm=0.245, loss_final=0.87, loss_mean=0.827, loss_mean_cls=0.0[[34m2025-10-04 12:42:00[39m] Step: 8899, Training Logs: loss_final: 0.877442, loss_mean: 0.834236, loss_mean_cls: 0.043206, grad_norm: 0.327046
Steps:   1%| | 8911/1000000 [43:47<79:32:48,  3.46it/s, grad_norm=0.204, loss_final=0.87, loss_mean=0.828, loss_mean_cls=0.0[[34m2025-10-04 12:42:02[39m] Step: 8905, Training Logs: loss_final: 0.867594, loss_mean: 0.824784, loss_mean_cls: 0.042809, grad_norm: 0.284157
Steps:   1%| | 8918/1000000 [43:49<79:45:48,  3.45it/s, grad_norm=0.496, loss_final=0.878, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:42:04[39m] Step: 8912, Training Logs: loss_final: 0.862890, loss_mean: 0.818335, loss_mean_cls: 0.044555, grad_norm: 0.235531
Steps:   1%| | 8925/1000000 [43:51<81:47:18,  3.37it/s, grad_norm=0.304, loss_final=0.888, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:42:06[39m] Step: 8919, Training Logs: loss_final: 0.869275, loss_mean: 0.825438, loss_mean_cls: 0.043838, grad_norm: 0.278177
Steps:   1%| | 8926/1000000 [43:51<81:11:25,  3.39it/s, grad_norm=0.304, loss_final=0.888, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:42:08[39m] Step: 8926, Training Logs: loss_final: 0.879949, loss_mean: 0.837925, loss_mean_cls: 0.042024, grad_norm: 0.357943
Steps:   1%| | 8933/1000000 [43:53<84:09:23,  3.27it/s, grad_norm=0.225, loss_final=0.861, loss_mean=0.816, loss_mean_cls=0.[[34m2025-10-04 12:42:10[39m] Step: 8933, Training Logs: loss_final: 0.852011, loss_mean: 0.807623, loss_mean_cls: 0.044388, grad_norm: 0.332327
Steps:   1%| | 8940/1000000 [43:55<82:31:40,  3.34it/s, grad_norm=0.312, loss_final=0.886, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:42:12[39m] Step: 8940, Training Logs: loss_final: 0.872735, loss_mean: 0.829209, loss_mean_cls: 0.043526, grad_norm: 0.300317
Steps:   1%| | 8945/1000000 [43:57<81:51:54,  3.36it/s, grad_norm=0.3, loss_final=0.875, loss_mean=0.831, loss_mean_cls=0.04[[34m2025-10-04 12:42:14[39m] Step: 8945, Training Logs: loss_final: 0.888620, loss_mean: 0.845337, loss_mean_cls: 0.043284, grad_norm: 0.336618
Steps:   1%| | 8952/1000000 [43:59<81:32:38,  3.38it/s, grad_norm=0.221, loss_final=0.865, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:42:16[39m] Step: 8952, Training Logs: loss_final: 0.884113, loss_mean: 0.841282, loss_mean_cls: 0.042831, grad_norm: 0.219970
Steps:   1%| | 8959/1000000 [44:01<80:31:19,  3.42it/s, grad_norm=0.22, loss_final=0.859, loss_mean=0.815, loss_mean_cls=0.0[[34m2025-10-04 12:42:18[39m] Step: 8959, Training Logs: loss_final: 0.873019, loss_mean: 0.829045, loss_mean_cls: 0.043974, grad_norm: 0.290788
Steps:   1%| | 8965/1000000 [44:03<81:29:47,  3.38it/s, grad_norm=0.247, loss_final=0.885, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:42:20[39m] Step: 8965, Training Logs: loss_final: 0.872624, loss_mean: 0.829596, loss_mean_cls: 0.043028, grad_norm: 0.381823
Steps:   1%| | 8972/1000000 [44:05<80:45:19,  3.41it/s, grad_norm=0.366, loss_final=0.886, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:42:22[39m] Step: 8972, Training Logs: loss_final: 0.871343, loss_mean: 0.828308, loss_mean_cls: 0.043035, grad_norm: 0.294801
Steps:   1%| | 8979/1000000 [44:07<81:51:30,  3.36it/s, grad_norm=0.446, loss_final=0.872, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:42:24[39m] Step: 8979, Training Logs: loss_final: 0.875989, loss_mean: 0.833417, loss_mean_cls: 0.042572, grad_norm: 0.161798
Steps:   1%| | 8986/1000000 [44:09<81:55:48,  3.36it/s, grad_norm=0.231, loss_final=0.867, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:42:26[39m] Step: 8986, Training Logs: loss_final: 0.871282, loss_mean: 0.827341, loss_mean_cls: 0.043940, grad_norm: 0.187251
Steps:   1%| | 8993/1000000 [44:11<78:48:36,  3.49it/s, grad_norm=0.206, loss_final=0.848, loss_mean=0.805, loss_mean_cls=0.[[34m2025-10-04 12:42:28[39m] Step: 8993, Training Logs: loss_final: 0.869246, loss_mean: 0.826551, loss_mean_cls: 0.042694, grad_norm: 0.387557
Steps:   1%| | 8999/1000000 [44:13<81:33:09,  3.38it/s, grad_norm=0.282, loss_final=0.872, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:42:30[39m] Step: 8999, Training Logs: loss_final: 0.847128, loss_mean: 0.802322, loss_mean_cls: 0.044806, grad_norm: 0.193705
Steps:   1%| | 9006/1000000 [44:15<80:07:47,  3.44it/s, grad_norm=0.279, loss_final=0.845, loss_mean=0.801, loss_mean_cls=0.[[34m2025-10-04 12:42:32[39m] Step: 9006, Training Logs: loss_final: 0.888243, loss_mean: 0.845464, loss_mean_cls: 0.042779, grad_norm: 0.211597
Steps:   1%| | 9013/1000000 [44:17<79:03:41,  3.48it/s, grad_norm=0.305, loss_final=0.887, loss_mean=0.844, loss_mean_cls=0.[[34m2025-10-04 12:42:34[39m] Step: 9013, Training Logs: loss_final: 0.862304, loss_mean: 0.819341, loss_mean_cls: 0.042963, grad_norm: 0.217777
Steps:   1%| | 9020/1000000 [44:19<84:28:34,  3.26it/s, grad_norm=0.343, loss_final=0.876, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:42:36[39m] Step: 9020, Training Logs: loss_final: 0.879554, loss_mean: 0.835828, loss_mean_cls: 0.043726, grad_norm: 0.284024
Steps:   1%| | 9027/1000000 [44:21<80:37:12,  3.41it/s, grad_norm=0.292, loss_final=0.872, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:42:38[39m] Step: 9027, Training Logs: loss_final: 0.875127, loss_mean: 0.832219, loss_mean_cls: 0.042908, grad_norm: 0.196407
Steps:   1%| | 9034/1000000 [44:23<77:56:05,  3.53it/s, grad_norm=0.213, loss_final=0.894, loss_mean=0.85, loss_mean_cls=0.0[[34m2025-10-04 12:42:40[39m] Step: 9034, Training Logs: loss_final: 0.869326, loss_mean: 0.825943, loss_mean_cls: 0.043384, grad_norm: 0.335836
Steps:   1%| | 9041/1000000 [44:25<81:48:57,  3.36it/s, grad_norm=0.193, loss_final=0.881, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:42:42[39m] Step: 9041, Training Logs: loss_final: 0.866932, loss_mean: 0.823557, loss_mean_cls: 0.043375, grad_norm: 0.253828
Steps:   1%| | 9048/1000000 [44:27<84:20:18,  3.26it/s, grad_norm=0.2, loss_final=0.876, loss_mean=0.833, loss_mean_cls=0.04[[34m2025-10-04 12:42:44[39m] Step: 9048, Training Logs: loss_final: 0.899082, loss_mean: 0.855769, loss_mean_cls: 0.043313, grad_norm: 0.333824
Steps:   1%| | 9060/1000000 [44:31<78:39:57,  3.50it/s, grad_norm=0.225, loss_final=0.867, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:42:46[39m] Step: 9054, Training Logs: loss_final: 0.875362, loss_mean: 0.832670, loss_mean_cls: 0.042692, grad_norm: 0.228571
Steps:   1%| | 9061/1000000 [44:31<80:11:51,  3.43it/s, grad_norm=0.225, loss_final=0.867, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:42:48[39m] Step: 9061, Training Logs: loss_final: 0.865594, loss_mean: 0.821902, loss_mean_cls: 0.043692, grad_norm: 0.264465
Steps:   1%| | 9068/1000000 [44:33<86:20:55,  3.19it/s, grad_norm=0.29, loss_final=0.873, loss_mean=0.83, loss_mean_cls=0.04[[34m2025-10-04 12:42:50[39m] Step: 9068, Training Logs: loss_final: 0.868926, loss_mean: 0.824024, loss_mean_cls: 0.044902, grad_norm: 0.228404
Steps:   1%| | 9075/1000000 [44:35<81:17:48,  3.39it/s, grad_norm=0.22, loss_final=0.871, loss_mean=0.829, loss_mean_cls=0.0[[34m2025-10-04 12:42:52[39m] Step: 9075, Training Logs: loss_final: 0.863448, loss_mean: 0.820233, loss_mean_cls: 0.043214, grad_norm: 0.182784
Steps:   1%| | 9082/1000000 [44:37<80:01:51,  3.44it/s, grad_norm=0.261, loss_final=0.869, loss_mean=0.826, loss_mean_cls=0.[[34m2025-10-04 12:42:54[39m] Step: 9082, Training Logs: loss_final: 0.865183, loss_mean: 0.821839, loss_mean_cls: 0.043344, grad_norm: 0.255256
Steps:   1%| | 9089/1000000 [44:39<79:47:22,  3.45it/s, grad_norm=0.188, loss_final=0.882, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:42:56[39m] Step: 9089, Training Logs: loss_final: 0.859227, loss_mean: 0.815223, loss_mean_cls: 0.044005, grad_norm: 0.240070
Steps:   1%| | 9096/1000000 [44:42<80:10:53,  3.43it/s, grad_norm=0.26, loss_final=0.866, loss_mean=0.823, loss_mean_cls=0.0[[34m2025-10-04 12:42:58[39m] Step: 9096, Training Logs: loss_final: 0.867858, loss_mean: 0.824668, loss_mean_cls: 0.043191, grad_norm: 0.184285
Steps:   1%| | 9103/1000000 [44:44<79:46:45,  3.45it/s, grad_norm=0.197, loss_final=0.897, loss_mean=0.854, loss_mean_cls=0.[[34m2025-10-04 12:43:00[39m] Step: 9103, Training Logs: loss_final: 0.861183, loss_mean: 0.818765, loss_mean_cls: 0.042418, grad_norm: 0.369757
Steps:   1%| | 9110/1000000 [44:46<78:52:34,  3.49it/s, grad_norm=0.213, loss_final=0.873, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:43:02[39m] Step: 9110, Training Logs: loss_final: 0.861638, loss_mean: 0.817416, loss_mean_cls: 0.044222, grad_norm: 0.295043
Steps:   1%| | 9117/1000000 [44:48<79:43:26,  3.45it/s, grad_norm=0.346, loss_final=0.882, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:43:04[39m] Step: 9117, Training Logs: loss_final: 0.846251, loss_mean: 0.802249, loss_mean_cls: 0.044002, grad_norm: 0.180538
Steps:   1%| | 9124/1000000 [44:50<78:58:11,  3.49it/s, grad_norm=0.293, loss_final=0.873, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:43:06[39m] Step: 9124, Training Logs: loss_final: 0.873106, loss_mean: 0.830991, loss_mean_cls: 0.042115, grad_norm: 0.189601
Steps:   1%| | 9131/1000000 [44:52<78:53:45,  3.49it/s, grad_norm=0.214, loss_final=0.868, loss_mean=0.825, loss_mean_cls=0.[[34m2025-10-04 12:43:08[39m] Step: 9131, Training Logs: loss_final: 0.866388, loss_mean: 0.823227, loss_mean_cls: 0.043162, grad_norm: 0.260175
Steps:   1%| | 9138/1000000 [44:54<80:06:45,  3.44it/s, grad_norm=0.393, loss_final=0.867, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:43:10[39m] Step: 9138, Training Logs: loss_final: 0.860996, loss_mean: 0.819015, loss_mean_cls: 0.041981, grad_norm: 0.469183
Steps:   1%| | 9145/1000000 [44:56<81:27:29,  3.38it/s, grad_norm=0.224, loss_final=0.876, loss_mean=0.833, loss_mean_cls=0.[[34m2025-10-04 12:43:12[39m] Step: 9145, Training Logs: loss_final: 0.876236, loss_mean: 0.834371, loss_mean_cls: 0.041865, grad_norm: 0.240529
Steps:   1%| | 9152/1000000 [44:58<81:07:06,  3.39it/s, grad_norm=0.209, loss_final=0.868, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:43:14[39m] Step: 9152, Training Logs: loss_final: 0.879901, loss_mean: 0.836756, loss_mean_cls: 0.043145, grad_norm: 0.256077
Steps:   1%| | 9158/1000000 [45:00<81:28:42,  3.38it/s, grad_norm=0.238, loss_final=0.884, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:43:16[39m] Step: 9158, Training Logs: loss_final: 0.876676, loss_mean: 0.832604, loss_mean_cls: 0.044073, grad_norm: 0.271075
Steps:   1%| | 9165/1000000 [45:02<79:37:59,  3.46it/s, grad_norm=0.213, loss_final=0.871, loss_mean=0.827, loss_mean_cls=0.[[34m2025-10-04 12:43:18[39m] Step: 9165, Training Logs: loss_final: 0.875492, loss_mean: 0.831758, loss_mean_cls: 0.043734, grad_norm: 0.222803
Steps:   1%| | 9172/1000000 [45:04<79:14:22,  3.47it/s, grad_norm=0.223, loss_final=0.893, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:43:20[39m] Step: 9172, Training Logs: loss_final: 0.854753, loss_mean: 0.811306, loss_mean_cls: 0.043447, grad_norm: 0.176365
Steps:   1%| | 9179/1000000 [45:06<78:55:59,  3.49it/s, grad_norm=0.223, loss_final=0.881, loss_mean=0.837, loss_mean_cls=0.[[34m2025-10-04 12:43:22[39m] Step: 9179, Training Logs: loss_final: 0.884285, loss_mean: 0.842365, loss_mean_cls: 0.041921, grad_norm: 0.201956
Steps:   1%| | 9186/1000000 [45:08<81:44:54,  3.37it/s, grad_norm=0.251, loss_final=0.86, loss_mean=0.817, loss_mean_cls=0.0[[34m2025-10-04 12:43:24[39m] Step: 9186, Training Logs: loss_final: 0.857277, loss_mean: 0.814670, loss_mean_cls: 0.042607, grad_norm: 0.215841
Steps:   1%| | 9193/1000000 [45:10<79:19:26,  3.47it/s, grad_norm=0.19, loss_final=0.874, loss_mean=0.831, loss_mean_cls=0.0[[34m2025-10-04 12:43:26[39m] Step: 9193, Training Logs: loss_final: 0.871392, loss_mean: 0.828153, loss_mean_cls: 0.043239, grad_norm: 0.175134
Steps:   1%| | 9200/1000000 [45:12<79:50:22,  3.45it/s, grad_norm=0.26, loss_final=0.861, loss_mean=0.819, loss_mean_cls=0.0[[34m2025-10-04 12:43:28[39m] Step: 9200, Training Logs: loss_final: 0.854944, loss_mean: 0.811093, loss_mean_cls: 0.043851, grad_norm: 0.235508
Steps:   1%| | 9205/1000000 [45:13<82:20:24,  3.34it/s, grad_norm=0.195, loss_final=0.854, loss_mean=0.81, loss_mean_cls=0.0[[34m2025-10-04 12:43:30[39m] Step: 9205, Training Logs: loss_final: 0.889698, loss_mean: 0.847800, loss_mean_cls: 0.041898, grad_norm: 0.205817
Steps:   1%| | 9217/1000000 [45:17<80:41:56,  3.41it/s, grad_norm=0.325, loss_final=0.873, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:43:32[39m] Step: 9211, Training Logs: loss_final: 0.882381, loss_mean: 0.839025, loss_mean_cls: 0.043356, grad_norm: 0.223077
Steps:   1%| | 9224/1000000 [45:19<79:21:30,  3.47it/s, grad_norm=0.271, loss_final=0.888, loss_mean=0.845, loss_mean_cls=0.[[34m2025-10-04 12:43:34[39m] Step: 9218, Training Logs: loss_final: 0.873276, loss_mean: 0.829504, loss_mean_cls: 0.043772, grad_norm: 0.360319
Steps:   1%| | 9231/1000000 [45:21<79:40:21,  3.45it/s, grad_norm=0.302, loss_final=0.874, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:43:36[39m] Step: 9225, Training Logs: loss_final: 0.875885, loss_mean: 0.833815, loss_mean_cls: 0.042070, grad_norm: 0.376281
Steps:   1%| | 9238/1000000 [45:23<79:06:41,  3.48it/s, grad_norm=0.281, loss_final=0.893, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:43:38[39m] Step: 9232, Training Logs: loss_final: 0.862829, loss_mean: 0.820153, loss_mean_cls: 0.042675, grad_norm: 0.172279
Steps:   1%| | 9239/1000000 [45:23<79:30:05,  3.46it/s, grad_norm=0.281, loss_final=0.893, loss_mean=0.851, loss_mean_cls=0.[[34m2025-10-04 12:43:40[39m] Step: 9239, Training Logs: loss_final: 0.859026, loss_mean: 0.815956, loss_mean_cls: 0.043070, grad_norm: 0.184922
Steps:   1%| | 9252/1000000 [45:27<78:36:59,  3.50it/s, grad_norm=0.379, loss_final=0.872, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:43:42[39m] Step: 9246, Training Logs: loss_final: 0.895724, loss_mean: 0.853816, loss_mean_cls: 0.041908, grad_norm: 0.190237
Steps:   1%| | 9253/1000000 [45:27<78:48:11,  3.49it/s, grad_norm=0.379, loss_final=0.872, loss_mean=0.829, loss_mean_cls=0.[[34m2025-10-04 12:43:44[39m] Step: 9253, Training Logs: loss_final: 0.877900, loss_mean: 0.834880, loss_mean_cls: 0.043021, grad_norm: 0.269500
Steps:   1%| | 9260/1000000 [45:29<80:51:00,  3.40it/s, grad_norm=0.387, loss_final=0.867, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:43:46[39m] Step: 9260, Training Logs: loss_final: 0.893535, loss_mean: 0.851913, loss_mean_cls: 0.041622, grad_norm: 0.271861
Steps:   1%| | 9267/1000000 [45:31<79:00:12,  3.48it/s, grad_norm=0.254, loss_final=0.855, loss_mean=0.811, loss_mean_cls=0.[[34m2025-10-04 12:43:48[39m] Step: 9267, Training Logs: loss_final: 0.872716, loss_mean: 0.830134, loss_mean_cls: 0.042582, grad_norm: 0.358170
Steps:   1%| | 9280/1000000 [45:35<78:49:12,  3.49it/s, grad_norm=0.262, loss_final=0.85, loss_mean=0.806, loss_mean_cls=0.0[[34m2025-10-04 12:43:50[39m] Step: 9274, Training Logs: loss_final: 0.871262, loss_mean: 0.829017, loss_mean_cls: 0.042245, grad_norm: 0.246289
Steps:   1%| | 9281/1000000 [45:35<78:40:37,  3.50it/s, grad_norm=0.262, loss_final=0.85, loss_mean=0.806, loss_mean_cls=0.0[[34m2025-10-04 12:43:52[39m] Step: 9281, Training Logs: loss_final: 0.878531, loss_mean: 0.835901, loss_mean_cls: 0.042630, grad_norm: 0.284709
Steps:   1%| | 9294/1000000 [45:39<78:51:20,  3.49it/s, grad_norm=0.261, loss_final=0.89, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:43:54[39m] Step: 9288, Training Logs: loss_final: 0.855173, loss_mean: 0.812402, loss_mean_cls: 0.042771, grad_norm: 0.345724
Steps:   1%| | 9295/1000000 [45:39<79:32:39,  3.46it/s, grad_norm=0.261, loss_final=0.89, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:43:56[39m] Step: 9295, Training Logs: loss_final: 0.876173, loss_mean: 0.832903, loss_mean_cls: 0.043270, grad_norm: 0.172914
Steps:   1%| | 9302/1000000 [45:41<82:24:34,  3.34it/s, grad_norm=0.298, loss_final=0.864, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:43:58[39m] Step: 9302, Training Logs: loss_final: 0.860449, loss_mean: 0.815557, loss_mean_cls: 0.044892, grad_norm: 0.386611
Steps:   1%| | 9309/1000000 [45:43<79:00:00,  3.48it/s, grad_norm=0.336, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:44:00[39m] Step: 9309, Training Logs: loss_final: 0.868180, loss_mean: 0.825507, loss_mean_cls: 0.042673, grad_norm: 0.194491
Steps:   1%| | 9316/1000000 [45:45<79:17:31,  3.47it/s, grad_norm=0.236, loss_final=0.869, loss_mean=0.826, loss_mean_cls=0.[[34m2025-10-04 12:44:02[39m] Step: 9316, Training Logs: loss_final: 0.891021, loss_mean: 0.849319, loss_mean_cls: 0.041702, grad_norm: 0.236385
Steps:   1%| | 9323/1000000 [45:47<78:56:15,  3.49it/s, grad_norm=0.175, loss_final=0.858, loss_mean=0.814, loss_mean_cls=0.[[34m2025-10-04 12:44:04[39m] Step: 9323, Training Logs: loss_final: 0.875598, loss_mean: 0.832854, loss_mean_cls: 0.042743, grad_norm: 0.188442
Steps:   1%| | 9330/1000000 [45:50<81:01:29,  3.40it/s, grad_norm=0.28, loss_final=0.874, loss_mean=0.832, loss_mean_cls=0.0[[34m2025-10-04 12:44:06[39m] Step: 9330, Training Logs: loss_final: 0.872037, loss_mean: 0.830548, loss_mean_cls: 0.041489, grad_norm: 0.242405
Steps:   1%| | 9337/1000000 [45:52<79:16:10,  3.47it/s, grad_norm=0.373, loss_final=0.89, loss_mean=0.847, loss_mean_cls=0.0[[34m2025-10-04 12:44:08[39m] Step: 9337, Training Logs: loss_final: 0.866025, loss_mean: 0.822604, loss_mean_cls: 0.043422, grad_norm: 0.204042
Steps:   1%| | 9344/1000000 [45:54<78:57:55,  3.48it/s, grad_norm=0.336, loss_final=0.903, loss_mean=0.86, loss_mean_cls=0.0[[34m2025-10-04 12:44:10[39m] Step: 9344, Training Logs: loss_final: 0.875032, loss_mean: 0.831562, loss_mean_cls: 0.043470, grad_norm: 0.276248
Steps:   1%| | 9351/1000000 [45:56<81:01:16,  3.40it/s, grad_norm=0.435, loss_final=0.87, loss_mean=0.828, loss_mean_cls=0.0[[34m2025-10-04 12:44:12[39m] Step: 9351, Training Logs: loss_final: 0.877850, loss_mean: 0.835182, loss_mean_cls: 0.042668, grad_norm: 0.483611
Steps:   1%| | 9358/1000000 [45:58<79:57:24,  3.44it/s, grad_norm=0.391, loss_final=0.883, loss_mean=0.842, loss_mean_cls=0.[[34m2025-10-04 12:44:14[39m] Step: 9358, Training Logs: loss_final: 0.860811, loss_mean: 0.817920, loss_mean_cls: 0.042891, grad_norm: 0.301719
Steps:   1%| | 9365/1000000 [46:00<80:03:50,  3.44it/s, grad_norm=0.326, loss_final=0.882, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:44:16[39m] Step: 9365, Training Logs: loss_final: 0.867641, loss_mean: 0.824495, loss_mean_cls: 0.043146, grad_norm: 0.382358
Steps:   1%| | 9372/1000000 [46:02<86:09:05,  3.19it/s, grad_norm=0.399, loss_final=0.866, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:44:19[39m] Step: 9372, Training Logs: loss_final: 0.869929, loss_mean: 0.826593, loss_mean_cls: 0.043336, grad_norm: 0.362295
Steps:   1%| | 9379/1000000 [46:04<84:30:58,  3.26it/s, grad_norm=0.303, loss_final=0.864, loss_mean=0.82, loss_mean_cls=0.0[[34m2025-10-04 12:44:21[39m] Step: 9379, Training Logs: loss_final: 0.859943, loss_mean: 0.816192, loss_mean_cls: 0.043750, grad_norm: 0.426024
Steps:   1%| | 9385/1000000 [46:06<85:11:49,  3.23it/s, grad_norm=0.378, loss_final=0.874, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:44:22[39m] Step: 9385, Training Logs: loss_final: 0.878039, loss_mean: 0.833786, loss_mean_cls: 0.044253, grad_norm: 0.388236
Steps:   1%| | 9392/1000000 [46:08<79:13:33,  3.47it/s, grad_norm=0.599, loss_final=0.874, loss_mean=0.832, loss_mean_cls=0.[[34m2025-10-04 12:44:24[39m] Step: 9392, Training Logs: loss_final: 0.875795, loss_mean: 0.833659, loss_mean_cls: 0.042136, grad_norm: 0.222551
Steps:   1%| | 9399/1000000 [46:10<78:35:58,  3.50it/s, grad_norm=0.348, loss_final=0.86, loss_mean=0.815, loss_mean_cls=0.0[[34m2025-10-04 12:44:26[39m] Step: 9399, Training Logs: loss_final: 0.878775, loss_mean: 0.836398, loss_mean_cls: 0.042377, grad_norm: 0.279380
Steps:   1%| | 9406/1000000 [46:12<78:37:12,  3.50it/s, grad_norm=0.266, loss_final=0.857, loss_mean=0.813, loss_mean_cls=0.[[34m2025-10-04 12:44:28[39m] Step: 9406, Training Logs: loss_final: 0.859122, loss_mean: 0.815221, loss_mean_cls: 0.043901, grad_norm: 0.198718
Steps:   1%| | 9413/1000000 [46:14<82:00:04,  3.36it/s, grad_norm=0.208, loss_final=0.89, loss_mean=0.848, loss_mean_cls=0.0[[34m2025-10-04 12:44:31[39m] Step: 9413, Training Logs: loss_final: 0.877152, loss_mean: 0.833561, loss_mean_cls: 0.043591, grad_norm: 0.209840
Steps:   1%| | 9420/1000000 [46:16<79:27:13,  3.46it/s, grad_norm=0.199, loss_final=0.871, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:44:33[39m] Step: 9420, Training Logs: loss_final: 0.875260, loss_mean: 0.831893, loss_mean_cls: 0.043367, grad_norm: 0.360446
Steps:   1%| | 9427/1000000 [46:18<78:45:17,  3.49it/s, grad_norm=0.236, loss_final=0.867, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:44:35[39m] Step: 9427, Training Logs: loss_final: 0.879945, loss_mean: 0.836907, loss_mean_cls: 0.043038, grad_norm: 0.260129
Steps:   1%| | 9434/1000000 [46:20<78:45:08,  3.49it/s, grad_norm=0.21, loss_final=0.861, loss_mean=0.818, loss_mean_cls=0.0[[34m2025-10-04 12:44:37[39m] Step: 9434, Training Logs: loss_final: 0.859598, loss_mean: 0.817706, loss_mean_cls: 0.041891, grad_norm: 0.163328
Steps:   1%| | 9441/1000000 [46:22<80:01:21,  3.44it/s, grad_norm=0.193, loss_final=0.856, loss_mean=0.813, loss_mean_cls=0.[[34m2025-10-04 12:44:39[39m] Step: 9441, Training Logs: loss_final: 0.888462, loss_mean: 0.846158, loss_mean_cls: 0.042304, grad_norm: 0.205419
Steps:   1%| | 9448/1000000 [46:24<80:48:07,  3.41it/s, grad_norm=0.189, loss_final=0.871, loss_mean=0.828, loss_mean_cls=0.[[34m2025-10-04 12:44:41[39m] Step: 9448, Training Logs: loss_final: 0.863277, loss_mean: 0.821085, loss_mean_cls: 0.042192, grad_norm: 0.222070
Steps:   1%| | 9453/1000000 [46:26<79:38:26,  3.45it/s, grad_norm=0.306, loss_final=0.864, loss_mean=0.819, loss_mean_cls=0.[[34m2025-10-04 12:44:42[39m] Step: 9453, Training Logs: loss_final: 0.865588, loss_mean: 0.822222, loss_mean_cls: 0.043366, grad_norm: 0.226025
Steps:   1%| | 9460/1000000 [46:28<83:43:13,  3.29it/s, grad_norm=0.189, loss_final=0.861, loss_mean=0.818, loss_mean_cls=0.[[34m2025-10-04 12:44:44[39m] Step: 9460, Training Logs: loss_final: 0.872915, loss_mean: 0.832076, loss_mean_cls: 0.040839, grad_norm: 0.199132
Steps:   1%| | 9466/1000000 [46:29<81:14:27,  3.39it/s, grad_norm=0.307, loss_final=0.87, loss_mean=0.828, loss_mean_cls=0.0[[34m2025-10-04 12:44:46[39m] Step: 9466, Training Logs: loss_final: 0.872383, loss_mean: 0.828772, loss_mean_cls: 0.043612, grad_norm: 0.183840
Steps:   1%| | 9473/1000000 [46:31<81:06:56,  3.39it/s, grad_norm=0.168, loss_final=0.861, loss_mean=0.817, loss_mean_cls=0.[[34m2025-10-04 12:44:48[39m] Step: 9473, Training Logs: loss_final: 0.880843, loss_mean: 0.838455, loss_mean_cls: 0.042387, grad_norm: 0.413554
Steps:   1%| | 9480/1000000 [46:33<80:17:22,  3.43it/s, grad_norm=0.239, loss_final=0.866, loss_mean=0.823, loss_mean_cls=0.[[34m2025-10-04 12:44:50[39m] Step: 9480, Training Logs: loss_final: 0.832564, loss_mean: 0.787671, loss_mean_cls: 0.044893, grad_norm: 0.238282
Steps:   1%| | 9487/1000000 [46:36<81:24:41,  3.38it/s, grad_norm=0.2, loss_final=0.863, loss_mean=0.82, loss_mean_cls=0.043[[34m2025-10-04 12:44:52[39m] Step: 9487, Training Logs: loss_final: 0.868916, loss_mean: 0.826662, loss_mean_cls: 0.042254, grad_norm: 0.215685
Steps:   1%| | 9494/1000000 [46:38<81:23:03,  3.38it/s, grad_norm=0.299, loss_final=0.854, loss_mean=0.811, loss_mean_cls=0.[[34m2025-10-04 12:44:54[39m] Step: 9494, Training Logs: loss_final: 0.852822, loss_mean: 0.808720, loss_mean_cls: 0.044103, grad_norm: 0.318675
Steps:   1%| | 9501/1000000 [46:40<82:16:04,  3.34it/s, grad_norm=0.185, loss_final=0.864, loss_mean=0.821, loss_mean_cls=0.[[34m2025-10-04 12:44:56[39m] Step: 9501, Training Logs: loss_final: 0.860185, loss_mean: 0.816806, loss_mean_cls: 0.043378, grad_norm: 0.245488
Steps:   1%| | 9508/1000000 [46:42<79:10:16,  3.48it/s, grad_norm=0.277, loss_final=0.882, loss_mean=0.84, loss_mean_cls=0.0[[34m2025-10-04 12:44:58[39m] Step: 9508, Training Logs: loss_final: 0.876778, loss_mean: 0.834254, loss_mean_cls: 0.042524, grad_norm: 0.355096
Steps:   1%| | 9515/1000000 [46:44<81:00:47,  3.40it/s, grad_norm=0.339, loss_final=0.879, loss_mean=0.836, loss_mean_cls=0.[[34m2025-10-04 12:45:00[39m] Step: 9515, Training Logs: loss_final: 0.878467, loss_mean: 0.836577, loss_mean_cls: 0.041890, grad_norm: 0.221923
Steps:   1%| | 9521/1000000 [46:46<85:33:06,  3.22it/s, grad_norm=0.269, loss_final=0.875, loss_mean=0.831, loss_mean_cls=0.[[34m2025-10-04 12:45:02[39m] Step: 9521, Training Logs: loss_final: 0.883704, loss_mean: 0.840888, loss_mean_cls: 0.042816, grad_norm: 0.251511
Steps:   1%| | 9528/1000000 [46:48<79:55:48,  3.44it/s, grad_norm=0.232, loss_final=0.865, loss_mean=0.822, loss_mean_cls=0.[[34m2025-10-04 12:45:04[39m] Step: 9528, Training Logs: loss_final: 0.860783, loss_mean: 0.817732, loss_mean_cls: 0.043050, grad_norm: 0.214573
Steps:   1%| | 9535/1000000 [46:50<82:07:49,  3.35it/s, grad_norm=0.207, loss_final=0.881, loss_mean=0.838, loss_mean_cls=0.[[34m2025-10-04 12:45:06[39m] Step: 9535, Training Logs: loss_final: 0.863606, loss_mean: 0.820390, loss_mean_cls: 0.043215, grad_norm: 0.231994
Steps:   1%| | 9542/1000000 [46:52<79:39:03,  3.45it/s, grad_norm=0.221, loss_final=0.881, loss_mean=0.839, loss_mean_cls=0.[[34m2025-10-04 12:45:08[39m] Step: 9542, Training Logs: loss_final: 0.875835, loss_mean: 0.832572, loss_mean_cls: 0.043263, grad_norm: 0.182045
Steps:   1%| | 9549/1000000 [46:54<78:51:39,  3.49it/s, grad_norm=0.326, loss_final=0.883, loss_mean=0.841, loss_mean_cls=0.[[34m2025-10-04 12:45:10[39m] Step: 9549, Training Logs: loss_final: 0.865752, loss_mean: 0.822589, loss_mean_cls: 0.043164, grad_norm: 0.228231
Steps:   1%| | 9556/1000000 [46:56<78:40:41,  3.50it/s, grad_norm=0.246, loss_final=0.851, loss_mean=0.807, loss_mean_cls=0.[[34m2025-10-04 12:45:12[39m] Step: 9556, Training Logs: loss_final: 0.857492, loss_mean: 0.813413, loss_mean_cls: 0.044079, grad_norm: 0.283442
Steps:   1%| | 9563/1000000 [46:58<80:26:21,  3.42it/s, grad_norm=0.273, loss_final=0.856, loss_mean=0.813, loss_mean_cls=0.[[34m2025-10-04 12:45:14[39m] Step: 9563, Training Logs: loss_final: 0.881012, loss_mean: 0.838550, loss_mean_cls: 0.042462, grad_norm: 0.189603
Steps:   1%| | 9570/1000000 [47:00<82:02:37,  3.35it/s, grad_norm=0.175, loss_final=0.867, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:45:17[39m] Step: 9570, Training Logs: loss_final: 0.884729, loss_mean: 0.840818, loss_mean_cls: 0.043911, grad_norm: 0.489239
Steps:   1%| | 9577/1000000 [47:02<78:46:22,  3.49it/s, grad_norm=0.34, loss_final=0.869, loss_mean=0.826, loss_mean_cls=0.0[[34m2025-10-04 12:45:19[39m] Step: 9577, Training Logs: loss_final: 0.885669, loss_mean: 0.844603, loss_mean_cls: 0.041066, grad_norm: 0.375360
Steps:   1%| | 9584/1000000 [47:04<80:50:56,  3.40it/s, grad_norm=0.257, loss_final=0.868, loss_mean=0.826, loss_mean_cls=0.[[34m2025-10-04 12:45:21[39m] Step: 9584, Training Logs: loss_final: 0.878957, loss_mean: 0.836809, loss_mean_cls: 0.042148, grad_norm: 0.196969
Steps:   1%| | 9591/1000000 [47:06<79:35:52,  3.46it/s, grad_norm=0.197, loss_final=0.873, loss_mean=0.83, loss_mean_cls=0.0[[34m2025-10-04 12:45:23[39m] Step: 9591, Training Logs: loss_final: 0.868405, loss_mean: 0.826330, loss_mean_cls: 0.042075, grad_norm: 0.234400
Steps:   1%| | 9598/1000000 [47:08<78:47:06,  3.49it/s, grad_norm=0.175, loss_final=0.877, loss_mean=0.835, loss_mean_cls=0.[[34m2025-10-04 12:45:25[39m] Step: 9598, Training Logs: loss_final: 0.877266, loss_mean: 0.834582, loss_mean_cls: 0.042684, grad_norm: 0.199815
Steps:   1%| | 9605/1000000 [47:10<79:53:34,  3.44it/s, grad_norm=0.301, loss_final=0.866, loss_mean=0.824, loss_mean_cls=0.[[34m2025-10-04 12:45:27[39m] Step: 9605, Training Logs: loss_final: 0.869029, loss_mean: 0.826043, loss_mean_cls: 0.042985, grad_norm: 0.245305
