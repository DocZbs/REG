
Steps:   0%|                                                                       | 1/1000000 [00:05<1451:27:04,  5.23s/it][[34m2025-10-03 23:19:27[39m] Generating EMA samples done.
[[34m2025-10-03 23:19:27[39m] Step: 1, Training Logs: loss_final: 1.902024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 2/1000000 [00:06<787:34:46,  2.84s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.9, loss_mean=1.69, loss_mea[[34m2025-10-03 23:19:29[39m] Step: 2, Training Logs: loss_final: 1.907075, loss_mean: 1.690185, proj_loss: -0.001133, loss_mean_cls: 0.118023, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 3/1000000 [00:07<574:50:24,  2.07s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.69, loss_me[[34m2025-10-03 23:19:30[39m] Step: 3, Training Logs: loss_final: 1.943526, loss_mean: 1.725911, proj_loss: -0.000692, loss_mean_cls: 0.118307, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 4/1000000 [00:08<475:04:01,  1.71s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.94, loss_mean=1.73, loss_me[[34m2025-10-03 23:19:31[39m] Step: 4, Training Logs: loss_final: 1.913802, loss_mean: 1.696726, proj_loss: -0.000918, loss_mean_cls: 0.117994, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 5/1000000 [00:09<419:59:21,  1.51s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.7, loss_mea[[34m2025-10-03 23:19:32[39m] Step: 5, Training Logs: loss_final: 1.926213, loss_mean: 1.708497, proj_loss: -0.001132, loss_mean_cls: 0.118849, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 6/1000000 [00:11<387:22:27,  1.39s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.93, loss_mean=1.71, loss_me[[34m2025-10-03 23:19:33[39m] Step: 6, Training Logs: loss_final: 1.911109, loss_mean: 1.693091, proj_loss: -0.000262, loss_mean_cls: 0.118281, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 7/1000000 [00:12<366:31:06,  1.32s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.69, loss_me[[34m2025-10-03 23:19:34[39m] Step: 7, Training Logs: loss_final: 1.905225, loss_mean: 1.687699, proj_loss: -0.000456, loss_mean_cls: 0.117981, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 8/1000000 [00:13<352:17:16,  1.27s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.69, loss_me[[34m2025-10-03 23:19:36[39m] Step: 8, Training Logs: loss_final: 1.915341, loss_mean: 1.697358, proj_loss: -0.000295, loss_mean_cls: 0.118278, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 9/1000000 [00:14<342:34:59,  1.23s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.92, loss_mean=1.7, loss_mea[[34m2025-10-03 23:19:37[39m] Step: 9, Training Logs: loss_final: 1.906894, loss_mean: 1.689859, proj_loss: -0.001537, loss_mean_cls: 0.118572, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 10/1000000 [00:15<335:30:24,  1.21s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.69, loss_m[[34m2025-10-03 23:19:38[39m] Step: 10, Training Logs: loss_final: 1.930090, loss_mean: 1.711735, proj_loss: 0.000060, loss_mean_cls: 0.118295, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 11/1000000 [00:16<332:01:15,  1.20s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.93, loss_mean=1.71, loss_m[[34m2025-10-03 23:19:39[39m] Step: 11, Training Logs: loss_final: 1.908421, loss_mean: 1.690543, proj_loss: -0.000472, loss_mean_cls: 0.118350, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 12/1000000 [00:17<329:29:26,  1.19s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.69, loss_m[[34m2025-10-03 23:19:40[39m] Step: 12, Training Logs: loss_final: 1.926679, loss_mean: 1.708093, proj_loss: 0.000305, loss_mean_cls: 0.118281, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 13/1000000 [00:19<326:23:32,  1.18s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.93, loss_mean=1.71, loss_m[[34m2025-10-03 23:19:41[39m] Step: 13, Training Logs: loss_final: 1.894913, loss_mean: 1.677251, proj_loss: -0.000555, loss_mean_cls: 0.118217, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 14/1000000 [00:20<325:20:31,  1.17s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.89, loss_mean=1.68, loss_m[[34m2025-10-03 23:19:42[39m] Step: 14, Training Logs: loss_final: 1.910075, loss_mean: 1.692572, proj_loss: -0.000743, loss_mean_cls: 0.118246, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 15/1000000 [00:21<323:04:48,  1.16s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.69, loss_m[[34m2025-10-03 23:19:44[39m] Step: 15, Training Logs: loss_final: 1.908943, loss_mean: 1.690525, proj_loss: 0.000005, loss_mean_cls: 0.118413, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 16/1000000 [00:22<322:10:27,  1.16s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.91, loss_mean=1.69, loss_m[[34m2025-10-03 23:19:45[39m] Step: 16, Training Logs: loss_final: 1.932615, loss_mean: 1.715268, proj_loss: -0.000550, loss_mean_cls: 0.117897, deep_loss: 0.100000, grad_norm: nan
Steps:   0%| | 17/1000000 [00:23<321:11:16,  1.16s/it, deep_loss=0.1, grad_norm=nan, loss_final=1.93, loss_mean=1.72, loss_m[[34m2025-10-03 23:19:46[39m] Step: 17, Training Logs: loss_final: 1.899142, loss_mean: 1.680075, proj_loss: 0.000387, loss_mean_cls: 0.118681, deep_loss: 0.100000, grad_norm: inf
Steps:   0%| | 18/1000000 [00:24<321:14:38,  1.16s/it, deep_loss=0.1, grad_norm=inf, loss_final=1.9, loss_mean=1.68, loss_me[[34m2025-10-03 23:19:47[39m] Step: 18, Training Logs: loss_final: 1.896044, loss_mean: 1.678042, proj_loss: -0.000140, loss_mean_cls: 0.118143, deep_loss: 0.100000, grad_norm: inf
Steps:   0%| | 19/1000000 [00:26<322:05:53,  1.16s/it, deep_loss=0.1, grad_norm=inf, loss_final=1.9, loss_mean=1.68, loss_me[[34m2025-10-03 23:19:48[39m] Step: 19, Training Logs: loss_final: 1.914490, loss_mean: 1.697539, proj_loss: -0.001746, loss_mean_cls: 0.118697, deep_loss: 0.100000, grad_norm: inf
Steps:   0%| | 20/1000000 [00:27<327:54:00,  1.18s/it, deep_loss=0.1, grad_norm=inf, loss_final=1.91, loss_mean=1.7, loss_me[[34m2025-10-03 23:19:49[39m] Step: 20, Training Logs: loss_final: 1.929560, loss_mean: 1.711682, proj_loss: -0.000745, loss_mean_cls: 0.118622, deep_loss: 0.100000, grad_norm: 9431432.000000
Steps:   0%| | 21/1000000 [00:28<327:29:06,  1.18s/it, deep_loss=0.1, grad_norm=9.43e+6, loss_final=1.93, loss_mean=1.71, lo[[34m2025-10-03 23:19:51[39m] Step: 21, Training Logs: loss_final: 1.899181, loss_mean: 1.690341, proj_loss: -0.000138, loss_mean_cls: 0.118155, deep_loss: 0.090823, grad_norm: 1.330141
Steps:   0%| | 22/1000000 [00:29<328:50:32,  1.18s/it, deep_loss=0.0908, grad_norm=1.33, loss_final=1.9, loss_mean=1.69, los[[34m2025-10-03 23:19:52[39m] Step: 22, Training Logs: loss_final: 1.883099, loss_mean: 1.683681, proj_loss: -0.008920, loss_mean_cls: 0.118272, deep_loss: 0.090067, grad_norm: 1.056343
Steps:   0%| | 23/1000000 [00:30<328:20:46,  1.18s/it, deep_loss=0.0901, grad_norm=1.06, loss_final=1.88, loss_mean=1.68, lo[[34m2025-10-03 23:19:53[39m] Step: 23, Training Logs: loss_final: 1.883696, loss_mean: 1.691116, proj_loss: -0.015808, loss_mean_cls: 0.119007, deep_loss: 0.089382, grad_norm: 0.978164
Steps:   0%| | 24/1000000 [00:32<327:11:38,  1.18s/it, deep_loss=0.0894, grad_norm=0.978, loss_final=1.88, loss_mean=1.69, l[[34m2025-10-03 23:19:54[39m] Step: 24, Training Logs: loss_final: 1.836119, loss_mean: 1.656046, proj_loss: -0.025638, loss_mean_cls: 0.118366, deep_loss: 0.087344, grad_norm: 0.925606
Steps:   0%| | 25/1000000 [00:33<326:54:34,  1.18s/it, deep_loss=0.0873, grad_norm=0.926, loss_final=1.84, loss_mean=1.66, l[[34m2025-10-03 23:19:55[39m] Step: 25, Training Logs: loss_final: 1.849776, loss_mean: 1.682803, proj_loss: -0.032778, loss_mean_cls: 0.117807, deep_loss: 0.081944, grad_norm: 1.199117
Steps:   0%| | 26/1000000 [00:34<315:06:16,  1.13s/it, deep_loss=0.0819, grad_norm=1.2, loss_final=1.85, loss_mean=1.68, los[[34m2025-10-03 23:19:56[39m] Step: 26, Training Logs: loss_final: 1.830599, loss_mean: 1.672309, proj_loss: -0.038303, loss_mean_cls: 0.118371, deep_loss: 0.078222, grad_norm: 1.607979
Steps:   0%| | 27/1000000 [00:35<286:30:27,  1.03s/it, deep_loss=0.0782, grad_norm=1.61, loss_final=1.83, loss_mean=1.67, lo[[34m2025-10-03 23:19:57[39m] Step: 27, Training Logs: loss_final: 1.831065, loss_mean: 1.685124, proj_loss: -0.044413, loss_mean_cls: 0.118006, deep_loss: 0.072348, grad_norm: 1.594335
Steps:   0%| | 28/1000000 [00:35<266:21:28,  1.04it/s, deep_loss=0.0723, grad_norm=1.59, loss_final=1.83, loss_mean=1.69, lo[[34m2025-10-03 23:19:58[39m] Step: 28, Training Logs: loss_final: 1.818523, loss_mean: 1.674301, proj_loss: -0.049771, loss_mean_cls: 0.118265, deep_loss: 0.075727, grad_norm: 3.007090
Steps:   0%| | 29/1000000 [00:36<252:34:32,  1.10it/s, deep_loss=0.0757, grad_norm=3.01, loss_final=1.82, loss_mean=1.67, lo[[34m2025-10-03 23:19:59[39m] Step: 29, Training Logs: loss_final: 1.803824, loss_mean: 1.659579, proj_loss: -0.052656, loss_mean_cls: 0.118891, deep_loss: 0.078010, grad_norm: 2.909926
Steps:   0%| | 30/1000000 [00:37<243:03:17,  1.14it/s, deep_loss=0.078, grad_norm=2.91, loss_final=1.8, loss_mean=1.66, loss[[34m2025-10-03 23:20:00[39m] Step: 30, Training Logs: loss_final: 1.811135, loss_mean: 1.673098, proj_loss: -0.055503, loss_mean_cls: 0.117864, deep_loss: 0.075676, grad_norm: 2.776258
Steps:   0%| | 31/1000000 [00:38<236:08:48,  1.18it/s, deep_loss=0.0757, grad_norm=2.78, loss_final=1.81, loss_mean=1.67, lo[[34m2025-10-03 23:20:00[39m] Step: 31, Training Logs: loss_final: 1.769805, loss_mean: 1.638031, proj_loss: -0.060398, loss_mean_cls: 0.117657, deep_loss: 0.074515, grad_norm: 2.545592
Steps:   0%| | 32/1000000 [00:39<250:13:34,  1.11it/s, deep_loss=0.0745, grad_norm=2.55, loss_final=1.77, loss_mean=1.64, lo[[34m2025-10-03 23:20:01[39m] Step: 32, Training Logs: loss_final: 1.772346, loss_mean: 1.645479, proj_loss: -0.060625, loss_mean_cls: 0.117259, deep_loss: 0.070233, grad_norm: 1.898338
Steps:   0%| | 33/1000000 [00:40<274:29:31,  1.01it/s, deep_loss=0.0702, grad_norm=1.9, loss_final=1.77, loss_mean=1.65, los[[34m2025-10-03 23:20:03[39m] Step: 33, Training Logs: loss_final: 1.782344, loss_mean: 1.659762, proj_loss: -0.063786, loss_mean_cls: 0.117645, deep_loss: 0.068722, grad_norm: 1.621598
Steps:   0%| | 34/1000000 [00:41<291:07:54,  1.05s/it, deep_loss=0.0687, grad_norm=1.62, loss_final=1.78, loss_mean=1.66, lo[[34m2025-10-03 23:20:04[39m] Step: 34, Training Logs: loss_final: 1.751178, loss_mean: 1.627709, proj_loss: -0.063425, loss_mean_cls: 0.117320, deep_loss: 0.069574, grad_norm: 1.577727
Steps:   0%| | 35/1000000 [00:42<302:41:44,  1.09s/it, deep_loss=0.0696, grad_norm=1.58, loss_final=1.75, loss_mean=1.63, lo[[34m2025-10-03 23:20:05[39m] Step: 35, Training Logs: loss_final: 1.779682, loss_mean: 1.659419, proj_loss: -0.065727, loss_mean_cls: 0.117760, deep_loss: 0.068230, grad_norm: 1.620125
Steps:   0%| | 36/1000000 [00:43<309:23:41,  1.11s/it, deep_loss=0.0682, grad_norm=1.62, loss_final=1.78, loss_mean=1.66, lo[[34m2025-10-03 23:20:06[39m] Step: 36, Training Logs: loss_final: 1.737975, loss_mean: 1.616188, proj_loss: -0.066503, loss_mean_cls: 0.117616, deep_loss: 0.070673, grad_norm: 1.697962
Steps:   0%| | 37/1000000 [00:45<315:14:13,  1.13s/it, deep_loss=0.0707, grad_norm=1.7, loss_final=1.74, loss_mean=1.62, los[[34m2025-10-03 23:20:07[39m] Step: 37, Training Logs: loss_final: 1.744311, loss_mean: 1.626692, proj_loss: -0.067176, loss_mean_cls: 0.117192, deep_loss: 0.067603, grad_norm: 1.679321
Steps:   0%| | 38/1000000 [00:46<318:52:22,  1.15s/it, deep_loss=0.0676, grad_norm=1.68, loss_final=1.74, loss_mean=1.63, lo[[34m2025-10-03 23:20:08[39m] Step: 38, Training Logs: loss_final: 1.762091, loss_mean: 1.647054, proj_loss: -0.069631, loss_mean_cls: 0.118116, deep_loss: 0.066552, grad_norm: 1.154469
Steps:   0%| | 39/1000000 [00:47<321:07:36,  1.16s/it, deep_loss=0.0666, grad_norm=1.15, loss_final=1.76, loss_mean=1.65, lo[[34m2025-10-03 23:20:10[39m] Step: 39, Training Logs: loss_final: 1.735730, loss_mean: 1.621320, proj_loss: -0.069256, loss_mean_cls: 0.116886, deep_loss: 0.066780, grad_norm: 1.072945
Steps:   0%| | 40/1000000 [00:48<322:40:58,  1.16s/it, deep_loss=0.0668, grad_norm=1.07, loss_final=1.74, loss_mean=1.62, lo[[34m2025-10-03 23:20:11[39m] Step: 40, Training Logs: loss_final: 1.742079, loss_mean: 1.629857, proj_loss: -0.071581, loss_mean_cls: 0.117992, deep_loss: 0.065812, grad_norm: 1.420451
Steps:   0%| | 41/1000000 [00:49<323:44:49,  1.17s/it, deep_loss=0.0658, grad_norm=1.42, loss_final=1.74, loss_mean=1.63, lo[[34m2025-10-03 23:20:12[39m] Step: 41, Training Logs: loss_final: 1.728780, loss_mean: 1.617483, proj_loss: -0.071347, loss_mean_cls: 0.117561, deep_loss: 0.065083, grad_norm: 1.555789
Steps:   0%| | 42/1000000 [00:51<324:06:26,  1.17s/it, deep_loss=0.0651, grad_norm=1.56, loss_final=1.73, loss_mean=1.62, lo[[34m2025-10-03 23:20:13[39m] Step: 42, Training Logs: loss_final: 1.733653, loss_mean: 1.622769, proj_loss: -0.070599, loss_mean_cls: 0.116903, deep_loss: 0.064580, grad_norm: 1.196345
Steps:   0%| | 43/1000000 [00:52<324:36:31,  1.17s/it, deep_loss=0.0646, grad_norm=1.2, loss_final=1.73, loss_mean=1.62, los[[34m2025-10-03 23:20:14[39m] Step: 43, Training Logs: loss_final: 1.724159, loss_mean: 1.613425, proj_loss: -0.070978, loss_mean_cls: 0.117205, deep_loss: 0.064507, grad_norm: 1.092972
Steps:   0%| | 44/1000000 [00:53<325:03:17,  1.17s/it, deep_loss=0.0645, grad_norm=1.09, loss_final=1.72, loss_mean=1.61, lo[[34m2025-10-03 23:20:16[39m] Step: 44, Training Logs: loss_final: 1.716143, loss_mean: 1.605909, proj_loss: -0.071315, loss_mean_cls: 0.116863, deep_loss: 0.064686, grad_norm: 1.182002
Steps:   0%| | 45/1000000 [00:54<325:27:30,  1.17s/it, deep_loss=0.0647, grad_norm=1.18, loss_final=1.72, loss_mean=1.61, lo[[34m2025-10-03 23:20:17[39m] Step: 45, Training Logs: loss_final: 1.700934, loss_mean: 1.589545, proj_loss: -0.071224, loss_mean_cls: 0.117322, deep_loss: 0.065291, grad_norm: 1.303712
Steps:   0%| | 46/1000000 [00:55<326:09:05,  1.17s/it, deep_loss=0.0653, grad_norm=1.3, loss_final=1.7, loss_mean=1.59, loss[[34m2025-10-03 23:20:18[39m] Step: 46, Training Logs: loss_final: 1.714311, loss_mean: 1.604871, proj_loss: -0.072868, loss_mean_cls: 0.116672, deep_loss: 0.065636, grad_norm: 1.097362
Steps:   0%| | 47/1000000 [00:56<326:31:10,  1.18s/it, deep_loss=0.0656, grad_norm=1.1, loss_final=1.71, loss_mean=1.6, loss[[34m2025-10-03 23:20:19[39m] Step: 47, Training Logs: loss_final: 1.698416, loss_mean: 1.589013, proj_loss: -0.072519, loss_mean_cls: 0.116434, deep_loss: 0.065488, grad_norm: 0.990632
Steps:   0%| | 48/1000000 [00:58<326:48:45,  1.18s/it, deep_loss=0.0655, grad_norm=0.991, loss_final=1.7, loss_mean=1.59, lo[[34m2025-10-03 23:20:20[39m] Step: 48, Training Logs: loss_final: 1.708788, loss_mean: 1.601784, proj_loss: -0.073694, loss_mean_cls: 0.116506, deep_loss: 0.064192, grad_norm: 1.084317
Steps:   0%| | 49/1000000 [00:59<327:33:06,  1.18s/it, deep_loss=0.0642, grad_norm=1.08, loss_final=1.71, loss_mean=1.6, los[[34m2025-10-03 23:20:21[39m] Step: 49, Training Logs: loss_final: 1.702227, loss_mean: 1.595073, proj_loss: -0.073568, loss_mean_cls: 0.117185, deep_loss: 0.063536, grad_norm: 1.146986
Steps:   0%| | 50/1000000 [01:00<327:30:55,  1.18s/it, deep_loss=0.0635, grad_norm=1.15, loss_final=1.7, loss_mean=1.6, loss[[34m2025-10-03 23:20:23[39m] Step: 50, Training Logs: loss_final: 1.690100, loss_mean: 1.582529, proj_loss: -0.074031, loss_mean_cls: 0.116878, deep_loss: 0.064725, grad_norm: 1.182403
Steps:   0%| | 51/1000000 [01:01<328:00:00,  1.18s/it, deep_loss=0.0647, grad_norm=1.18, loss_final=1.69, loss_mean=1.58, lo[[34m2025-10-03 23:20:24[39m] Step: 51, Training Logs: loss_final: 1.706744, loss_mean: 1.601034, proj_loss: -0.073846, loss_mean_cls: 0.116621, deep_loss: 0.062935, grad_norm: 1.259385
Steps:   0%| | 52/1000000 [01:02<327:46:01,  1.18s/it, deep_loss=0.0629, grad_norm=1.26, loss_final=1.71, loss_mean=1.6, los[[34m2025-10-03 23:20:25[39m] Step: 52, Training Logs: loss_final: 1.655615, loss_mean: 1.550145, proj_loss: -0.074509, loss_mean_cls: 0.116246, deep_loss: 0.063734, grad_norm: 0.796511
Steps:   0%| | 53/1000000 [01:03<327:37:27,  1.18s/it, deep_loss=0.0637, grad_norm=0.797, loss_final=1.66, loss_mean=1.55, l[[34m2025-10-03 23:20:26[39m] Step: 53, Training Logs: loss_final: 1.675539, loss_mean: 1.568945, proj_loss: -0.072872, loss_mean_cls: 0.116044, deep_loss: 0.063422, grad_norm: 1.075845
Steps:   0%| | 54/1000000 [01:05<327:11:15,  1.18s/it, deep_loss=0.0634, grad_norm=1.08, loss_final=1.68, loss_mean=1.57, lo[[34m2025-10-03 23:20:27[39m] Step: 54, Training Logs: loss_final: 1.700123, loss_mean: 1.594606, proj_loss: -0.074186, loss_mean_cls: 0.116650, deep_loss: 0.063053, grad_norm: 1.018387
Steps:   0%| | 55/1000000 [01:06<327:47:33,  1.18s/it, deep_loss=0.0631, grad_norm=1.02, loss_final=1.7, loss_mean=1.59, los[[34m2025-10-03 23:20:29[39m] Step: 55, Training Logs: loss_final: 1.658770, loss_mean: 1.553430, proj_loss: -0.074844, loss_mean_cls: 0.116818, deep_loss: 0.063367, grad_norm: 1.109345
Steps:   0%| | 56/1000000 [01:07<326:57:34,  1.18s/it, deep_loss=0.0634, grad_norm=1.11, loss_final=1.66, loss_mean=1.55, lo[[34m2025-10-03 23:20:30[39m] Step: 56, Training Logs: loss_final: 1.668267, loss_mean: 1.563094, proj_loss: -0.075813, loss_mean_cls: 0.116202, deep_loss: 0.064785, grad_norm: 0.863162
Steps:   0%| | 57/1000000 [01:08<326:31:54,  1.18s/it, deep_loss=0.0648, grad_norm=0.863, loss_final=1.67, loss_mean=1.56, l[[34m2025-10-03 23:20:31[39m] Step: 57, Training Logs: loss_final: 1.681503, loss_mean: 1.576798, proj_loss: -0.074698, loss_mean_cls: 0.116606, deep_loss: 0.062797, grad_norm: 1.057700
Steps:   0%| | 58/1000000 [01:09<326:32:54,  1.18s/it, deep_loss=0.0628, grad_norm=1.06, loss_final=1.68, loss_mean=1.58, lo[[34m2025-10-03 23:20:32[39m] Step: 58, Training Logs: loss_final: 1.660730, loss_mean: 1.556136, proj_loss: -0.075035, loss_mean_cls: 0.116354, deep_loss: 0.063276, grad_norm: 1.018427
Steps:   0%| | 59/1000000 [01:10<320:22:23,  1.15s/it, deep_loss=0.0633, grad_norm=1.02, loss_final=1.66, loss_mean=1.56, lo[[34m2025-10-03 23:20:33[39m] Step: 59, Training Logs: loss_final: 1.647845, loss_mean: 1.543258, proj_loss: -0.073689, loss_mean_cls: 0.116426, deep_loss: 0.061851, grad_norm: 0.784045
Steps:   0%| | 60/1000000 [01:11<289:41:28,  1.04s/it, deep_loss=0.0619, grad_norm=0.784, loss_final=1.65, loss_mean=1.54, l[[34m2025-10-03 23:20:34[39m] Step: 60, Training Logs: loss_final: 1.668859, loss_mean: 1.565618, proj_loss: -0.074505, loss_mean_cls: 0.115511, deep_loss: 0.062236, grad_norm: 0.814479
Steps:   0%| | 61/1000000 [01:12<268:25:24,  1.03it/s, deep_loss=0.0622, grad_norm=0.814, loss_final=1.67, loss_mean=1.57, l[[34m2025-10-03 23:20:35[39m] Step: 61, Training Logs: loss_final: 1.654315, loss_mean: 1.553599, proj_loss: -0.075798, loss_mean_cls: 0.116709, deep_loss: 0.059805, grad_norm: 0.853377
Steps:   0%| | 62/1000000 [01:13<253:44:46,  1.09it/s, deep_loss=0.0598, grad_norm=0.853, loss_final=1.65, loss_mean=1.55, l[[34m2025-10-03 23:20:35[39m] Step: 62, Training Logs: loss_final: 1.645990, loss_mean: 1.542621, proj_loss: -0.074875, loss_mean_cls: 0.115626, deep_loss: 0.062618, grad_norm: 0.918328
Steps:   0%| | 63/1000000 [01:14<272:01:26,  1.02it/s, deep_loss=0.0626, grad_norm=0.918, loss_final=1.65, loss_mean=1.54, l[[34m2025-10-03 23:20:37[39m] Step: 63, Training Logs: loss_final: 1.650868, loss_mean: 1.549157, proj_loss: -0.075339, loss_mean_cls: 0.117100, deep_loss: 0.059951, grad_norm: 0.797973
Steps:   0%| | 64/1000000 [01:15<287:54:40,  1.04s/it, deep_loss=0.06, grad_norm=0.798, loss_final=1.65, loss_mean=1.55, los[[34m2025-10-03 23:20:38[39m] Step: 64, Training Logs: loss_final: 1.639481, loss_mean: 1.535939, proj_loss: -0.074793, loss_mean_cls: 0.116186, deep_loss: 0.062149, grad_norm: 0.911285
Steps:   0%| | 65/1000000 [01:16<299:39:57,  1.08s/it, deep_loss=0.0621, grad_norm=0.911, loss_final=1.64, loss_mean=1.54, l[[34m2025-10-03 23:20:39[39m] Step: 65, Training Logs: loss_final: 1.646341, loss_mean: 1.542573, proj_loss: -0.075934, loss_mean_cls: 0.114872, deep_loss: 0.064830, grad_norm: 0.749030
Steps:   0%| | 66/1000000 [01:17<307:58:03,  1.11s/it, deep_loss=0.0648, grad_norm=0.749, loss_final=1.65, loss_mean=1.54, l[[34m2025-10-03 23:20:40[39m] Step: 66, Training Logs: loss_final: 1.640822, loss_mean: 1.537794, proj_loss: -0.074352, loss_mean_cls: 0.115542, deep_loss: 0.061838, grad_norm: 0.870440
Steps:   0%| | 67/1000000 [01:19<313:27:31,  1.13s/it, deep_loss=0.0618, grad_norm=0.87, loss_final=1.64, loss_mean=1.54, lo[[34m2025-10-03 23:20:41[39m] Step: 67, Training Logs: loss_final: 1.623683, loss_mean: 1.521634, proj_loss: -0.075943, loss_mean_cls: 0.115822, deep_loss: 0.062171, grad_norm: 0.802655
Steps:   0%| | 68/1000000 [01:20<317:27:18,  1.14s/it, deep_loss=0.0622, grad_norm=0.803, loss_final=1.62, loss_mean=1.52, l[[34m2025-10-03 23:20:43[39m] Step: 68, Training Logs: loss_final: 1.630090, loss_mean: 1.525504, proj_loss: -0.073896, loss_mean_cls: 0.114768, deep_loss: 0.063713, grad_norm: 0.776246
Steps:   0%| | 69/1000000 [01:21<320:05:33,  1.15s/it, deep_loss=0.0637, grad_norm=0.776, loss_final=1.63, loss_mean=1.53, l[[34m2025-10-03 23:20:44[39m] Step: 69, Training Logs: loss_final: 1.630512, loss_mean: 1.524908, proj_loss: -0.073306, loss_mean_cls: 0.115478, deep_loss: 0.063432, grad_norm: 0.791897
Steps:   0%| | 70/1000000 [01:22<321:51:43,  1.16s/it, deep_loss=0.0634, grad_norm=0.792, loss_final=1.63, loss_mean=1.52, l[[34m2025-10-03 23:20:45[39m] Step: 70, Training Logs: loss_final: 1.616361, loss_mean: 1.516179, proj_loss: -0.075520, loss_mean_cls: 0.114867, deep_loss: 0.060835, grad_norm: 0.750096
Steps:   0%| | 71/1000000 [01:23<323:53:21,  1.17s/it, deep_loss=0.0608, grad_norm=0.75, loss_final=1.62, loss_mean=1.52, lo[[34m2025-10-03 23:20:46[39m] Step: 71, Training Logs: loss_final: 1.661766, loss_mean: 1.557480, proj_loss: -0.074651, loss_mean_cls: 0.114894, deep_loss: 0.064043, grad_norm: 0.918805
Steps:   0%| | 72/1000000 [01:25<325:48:25,  1.17s/it, deep_loss=0.064, grad_norm=0.919, loss_final=1.66, loss_mean=1.56, lo[[34m2025-10-03 23:20:47[39m] Step: 72, Training Logs: loss_final: 1.615294, loss_mean: 1.512155, proj_loss: -0.073732, loss_mean_cls: 0.114615, deep_loss: 0.062256, grad_norm: 1.025161
Steps:   0%| | 73/1000000 [01:26<326:00:43,  1.17s/it, deep_loss=0.0623, grad_norm=1.03, loss_final=1.62, loss_mean=1.51, lo[[34m2025-10-03 23:20:48[39m] Step: 73, Training Logs: loss_final: 1.633118, loss_mean: 1.531428, proj_loss: -0.075082, loss_mean_cls: 0.115178, deep_loss: 0.061594, grad_norm: 1.010879
Steps:   0%| | 74/1000000 [01:27<326:10:24,  1.17s/it, deep_loss=0.0616, grad_norm=1.01, loss_final=1.63, loss_mean=1.53, lo[[34m2025-10-03 23:20:50[39m] Step: 74, Training Logs: loss_final: 1.617035, loss_mean: 1.513903, proj_loss: -0.075094, loss_mean_cls: 0.114857, deep_loss: 0.063370, grad_norm: 0.897455
Steps:   0%| | 75/1000000 [01:28<326:05:32,  1.17s/it, deep_loss=0.0634, grad_norm=0.897, loss_final=1.62, loss_mean=1.51, l[[34m2025-10-03 23:20:51[39m] Step: 75, Training Logs: loss_final: 1.610474, loss_mean: 1.505848, proj_loss: -0.073280, loss_mean_cls: 0.115659, deep_loss: 0.062247, grad_norm: 0.782278
Steps:   0%| | 76/1000000 [01:29<327:56:12,  1.18s/it, deep_loss=0.0622, grad_norm=0.782, loss_final=1.61, loss_mean=1.51, l[[34m2025-10-03 23:20:52[39m] Step: 76, Training Logs: loss_final: 1.625771, loss_mean: 1.524221, proj_loss: -0.073059, loss_mean_cls: 0.115215, deep_loss: 0.059394, grad_norm: 0.740512
Steps:   0%| | 77/1000000 [01:30<327:52:51,  1.18s/it, deep_loss=0.0594, grad_norm=0.741, loss_final=1.63, loss_mean=1.52, l[[34m2025-10-03 23:20:53[39m] Step: 77, Training Logs: loss_final: 1.616585, loss_mean: 1.514830, proj_loss: -0.073631, loss_mean_cls: 0.115078, deep_loss: 0.060307, grad_norm: 0.985916
Steps:   0%| | 78/1000000 [01:32<327:44:38,  1.18s/it, deep_loss=0.0603, grad_norm=0.986, loss_final=1.62, loss_mean=1.51, l[[34m2025-10-03 23:20:54[39m] Step: 78, Training Logs: loss_final: 1.609912, loss_mean: 1.507383, proj_loss: -0.074670, loss_mean_cls: 0.114578, deep_loss: 0.062621, grad_norm: 0.978068
Steps:   0%| | 79/1000000 [01:33<327:45:47,  1.18s/it, deep_loss=0.0626, grad_norm=0.978, loss_final=1.61, loss_mean=1.51, l[[34m2025-10-03 23:20:55[39m] Step: 79, Training Logs: loss_final: 1.605688, loss_mean: 1.500794, proj_loss: -0.073109, loss_mean_cls: 0.114919, deep_loss: 0.063085, grad_norm: 0.998759
Steps:   0%| | 80/1000000 [01:34<326:45:44,  1.18s/it, deep_loss=0.0631, grad_norm=0.999, loss_final=1.61, loss_mean=1.5, lo[[34m2025-10-03 23:20:57[39m] Step: 80, Training Logs: loss_final: 1.608002, loss_mean: 1.505553, proj_loss: -0.074113, loss_mean_cls: 0.114634, deep_loss: 0.061927, grad_norm: 0.920342
Steps:   0%| | 81/1000000 [01:35<326:58:07,  1.18s/it, deep_loss=0.0619, grad_norm=0.92, loss_final=1.61, loss_mean=1.51, lo[[34m2025-10-03 23:20:58[39m] Step: 81, Training Logs: loss_final: 1.601809, loss_mean: 1.498944, proj_loss: -0.074311, loss_mean_cls: 0.114146, deep_loss: 0.063029, grad_norm: 1.228466
Steps:   0%| | 82/1000000 [01:36<327:29:48,  1.18s/it, deep_loss=0.063, grad_norm=1.23, loss_final=1.6, loss_mean=1.5, loss_[[34m2025-10-03 23:20:59[39m] Step: 82, Training Logs: loss_final: 1.594330, loss_mean: 1.488438, proj_loss: -0.072816, loss_mean_cls: 0.114571, deep_loss: 0.064138, grad_norm: 1.117621
Steps:   0%| | 83/1000000 [01:38<328:00:08,  1.18s/it, deep_loss=0.0641, grad_norm=1.12, loss_final=1.59, loss_mean=1.49, lo[[34m2025-10-03 23:21:00[39m] Step: 83, Training Logs: loss_final: 1.601402, loss_mean: 1.498230, proj_loss: -0.075344, loss_mean_cls: 0.114178, deep_loss: 0.064338, grad_norm: 0.973945
Steps:   0%| | 84/1000000 [01:39<327:39:54,  1.18s/it, deep_loss=0.0643, grad_norm=0.974, loss_final=1.6, loss_mean=1.5, los[[34m2025-10-03 23:21:01[39m] Step: 84, Training Logs: loss_final: 1.595539, loss_mean: 1.492390, proj_loss: -0.072204, loss_mean_cls: 0.114216, deep_loss: 0.061137, grad_norm: 1.465758
Steps:   0%| | 85/1000000 [01:40<327:56:12,  1.18s/it, deep_loss=0.0611, grad_norm=1.47, loss_final=1.6, loss_mean=1.49, los[[34m2025-10-03 23:21:03[39m] Step: 85, Training Logs: loss_final: 1.571509, loss_mean: 1.469508, proj_loss: -0.073896, loss_mean_cls: 0.114037, deep_loss: 0.061860, grad_norm: 1.263716
Steps:   0%| | 86/1000000 [01:41<327:22:32,  1.18s/it, deep_loss=0.0619, grad_norm=1.26, loss_final=1.57, loss_mean=1.47, lo[[34m2025-10-03 23:21:04[39m] Step: 86, Training Logs: loss_final: 1.572882, loss_mean: 1.469862, proj_loss: -0.075439, loss_mean_cls: 0.114610, deep_loss: 0.063848, grad_norm: 1.498631
Steps:   0%| | 87/1000000 [01:42<327:26:14,  1.18s/it, deep_loss=0.0638, grad_norm=1.5, loss_final=1.57, loss_mean=1.47, los[[34m2025-10-03 23:21:05[39m] Step: 87, Training Logs: loss_final: 1.602030, loss_mean: 1.498076, proj_loss: -0.073785, loss_mean_cls: 0.114046, deep_loss: 0.063694, grad_norm: 1.974572
Steps:   0%| | 88/1000000 [01:43<327:05:59,  1.18s/it, deep_loss=0.0637, grad_norm=1.97, loss_final=1.6, loss_mean=1.5, loss[[34m2025-10-03 23:21:06[39m] Step: 88, Training Logs: loss_final: 1.593231, loss_mean: 1.490632, proj_loss: -0.073272, loss_mean_cls: 0.113666, deep_loss: 0.062207, grad_norm: 1.510055
Steps:   0%| | 89/1000000 [01:45<327:12:08,  1.18s/it, deep_loss=0.0622, grad_norm=1.51, loss_final=1.59, loss_mean=1.49, lo[[34m2025-10-03 23:21:07[39m] Step: 89, Training Logs: loss_final: 1.560277, loss_mean: 1.459600, proj_loss: -0.074431, loss_mean_cls: 0.114072, deep_loss: 0.061037, grad_norm: 1.236701
Steps:   0%| | 90/1000000 [01:46<310:03:59,  1.12s/it, deep_loss=0.061, grad_norm=1.24, loss_final=1.56, loss_mean=1.46, los[[34m2025-10-03 23:21:08[39m] Step: 90, Training Logs: loss_final: 1.557421, loss_mean: 1.454607, proj_loss: -0.073962, loss_mean_cls: 0.113241, deep_loss: 0.063535, grad_norm: 1.430158
Steps:   0%| | 91/1000000 [01:46<283:04:08,  1.02s/it, deep_loss=0.0635, grad_norm=1.43, loss_final=1.56, loss_mean=1.45, lo[[34m2025-10-03 23:21:09[39m] Step: 91, Training Logs: loss_final: 1.562672, loss_mean: 1.459279, proj_loss: -0.074788, loss_mean_cls: 0.113840, deep_loss: 0.064341, grad_norm: 1.419526
Steps:   0%| | 92/1000000 [01:47<264:04:54,  1.05it/s, deep_loss=0.0643, grad_norm=1.42, loss_final=1.56, loss_mean=1.46, lo[[34m2025-10-03 23:21:10[39m] Step: 92, Training Logs: loss_final: 1.549607, loss_mean: 1.447646, proj_loss: -0.075182, loss_mean_cls: 0.113906, deep_loss: 0.063237, grad_norm: 1.244233
Steps:   0%| | 93/1000000 [01:48<251:10:23,  1.11it/s, deep_loss=0.0632, grad_norm=1.24, loss_final=1.55, loss_mean=1.45, lo[[34m2025-10-03 23:21:11[39m] Step: 93, Training Logs: loss_final: 1.577959, loss_mean: 1.474091, proj_loss: -0.072491, loss_mean_cls: 0.112350, deep_loss: 0.064010, grad_norm: 1.969341
Steps:   0%| | 93/1000000 [01:48<251:10:23,  1.11it/s, deep_loss=0.064, grad_norm=1.97, loss_final=1.58, loss_mean=1.47, losException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7bdfe8b6ad40>
Traceback (most recent call last):
  File "/mnt/nvme-fast/zbs/miniconda3/envs/reg/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1478, in __del__
    self._shutdown_workers()
  File "/mnt/nvme-fast/zbs/miniconda3/envs/reg/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1442, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/mnt/nvme-fast/zbs/miniconda3/envs/reg/lib/python3.11/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/nvme-fast/zbs/miniconda3/envs/reg/lib/python3.11/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/nvme-fast/zbs/miniconda3/envs/reg/lib/python3.11/multiprocessing/connection.py", line 948, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/nvme-fast/zbs/miniconda3/envs/reg/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/nvme-fast/zbs/miniconda3/envs/reg/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 641228) is killed by signal: Trace/breakpoint trap.
Traceback (most recent call last):
  File "/mnt/eb20f54b-f016-4501-a5b2-5dd6afff9d90/zbs/REG/train.py", line 466, in <module>
    main(args)
  File "/mnt/eb20f54b-f016-4501-a5b2-5dd6afff9d90/zbs/REG/train.py", line 319, in main
    loss1, proj_loss1, time_input, noises, loss2, deep_loss = loss_fn(
                                                              ^^^^^^^^
  File "/mnt/eb20f54b-f016-4501-a5b2-5dd6afff9d90/zbs/REG/loss.py", line 144, in __call__
    proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/eb20f54b-f016-4501-a5b2-5dd6afff9d90/zbs/REG/loss.py", line 7, in mean_flat
    def mean_flat(x):
KeyboardInterrupt