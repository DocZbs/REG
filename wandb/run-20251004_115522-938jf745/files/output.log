
Steps:   0%|                                                                       | 1/1000000 [00:07<1975:41:20,  7.11s/it][[34m2025-10-04 11:55:38[39m] Generating EMA samples done.
[[34m2025-10-04 11:55:38[39m] Step: 1, Training Logs: loss_final: 1.795647, loss_mean: 1.689609, loss_mean_cls: 0.106038, grad_norm: 1.174266
Steps:   0%|   | 2/1000000 [00:07<859:10:45,  3.09s/it, grad_norm=1.17, loss_final=1.8, loss_mean=1.69, loss_mean_cls=0.106][[34m2025-10-04 11:55:38[39m] Step: 2, Training Logs: loss_final: 1.785069, loss_mean: 1.678836, loss_mean_cls: 0.106234, grad_norm: 0.901399
Steps:   0%| | 3/1000000 [00:07<502:48:23,  1.81s/it, grad_norm=0.901, loss_final=1.79, loss_mean=1.68, loss_mean_cls=0.106][[34m2025-10-04 11:55:38[39m] Step: 3, Training Logs: loss_final: 1.821233, loss_mean: 1.717687, loss_mean_cls: 0.103546, grad_norm: 0.796063
Steps:   0%| | 4/1000000 [00:07<336:54:19,  1.21s/it, grad_norm=0.796, loss_final=1.82, loss_mean=1.72, loss_mean_cls=0.104][[34m2025-10-04 11:55:39[39m] Step: 4, Training Logs: loss_final: 1.778000, loss_mean: 1.672744, loss_mean_cls: 0.105256, grad_norm: 0.684705
Steps:   0%| | 5/1000000 [00:08<243:47:04,  1.14it/s, grad_norm=0.685, loss_final=1.78, loss_mean=1.67, loss_mean_cls=0.105][[34m2025-10-04 11:55:39[39m] Step: 5, Training Logs: loss_final: 1.786037, loss_mean: 1.684995, loss_mean_cls: 0.101042, grad_norm: 0.812753
Steps:   0%| | 6/1000000 [00:08<187:11:07,  1.48it/s, grad_norm=0.813, loss_final=1.79, loss_mean=1.68, loss_mean_cls=0.101][[34m2025-10-04 11:55:39[39m] Step: 6, Training Logs: loss_final: 1.766339, loss_mean: 1.666939, loss_mean_cls: 0.099399, grad_norm: 0.754432
Steps:   0%| | 7/1000000 [00:08<151:28:44,  1.83it/s, grad_norm=0.754, loss_final=1.77, loss_mean=1.67, loss_mean_cls=0.0994[[34m2025-10-04 11:55:39[39m] Step: 7, Training Logs: loss_final: 1.754195, loss_mean: 1.651879, loss_mean_cls: 0.102316, grad_norm: 0.825753
Steps:   0%| | 8/1000000 [00:09<128:02:50,  2.17it/s, grad_norm=0.826, loss_final=1.75, loss_mean=1.65, loss_mean_cls=0.102][[34m2025-10-04 11:55:40[39m] Step: 8, Training Logs: loss_final: 1.751540, loss_mean: 1.652565, loss_mean_cls: 0.098975, grad_norm: 0.830366
Steps:   0%|  | 9/1000000 [00:09<112:23:04,  2.47it/s, grad_norm=0.83, loss_final=1.75, loss_mean=1.65, loss_mean_cls=0.099][[34m2025-10-04 11:55:40[39m] Step: 9, Training Logs: loss_final: 1.738420, loss_mean: 1.636014, loss_mean_cls: 0.102406, grad_norm: 0.858504
Steps:   0%| | 10/1000000 [00:09<102:17:25,  2.72it/s, grad_norm=0.859, loss_final=1.74, loss_mean=1.64, loss_mean_cls=0.102[[34m2025-10-04 11:55:40[39m] Step: 10, Training Logs: loss_final: 1.736533, loss_mean: 1.639352, loss_mean_cls: 0.097181, grad_norm: 1.043699
Steps:   0%| | 11/1000000 [00:09<97:08:15,  2.86it/s, grad_norm=1.04, loss_final=1.74, loss_mean=1.64, loss_mean_cls=0.0972][[34m2025-10-04 11:55:41[39m] Step: 11, Training Logs: loss_final: 1.709282, loss_mean: 1.608108, loss_mean_cls: 0.101174, grad_norm: 1.076937
Steps:   0%|  | 12/1000000 [00:10<92:45:14,  2.99it/s, grad_norm=1.08, loss_final=1.71, loss_mean=1.61, loss_mean_cls=0.101][[34m2025-10-04 11:55:41[39m] Step: 12, Training Logs: loss_final: 1.707039, loss_mean: 1.611309, loss_mean_cls: 0.095730, grad_norm: 1.105962
Steps:   0%| | 13/1000000 [00:10<90:07:47,  3.08it/s, grad_norm=1.11, loss_final=1.71, loss_mean=1.61, loss_mean_cls=0.0957][[34m2025-10-04 11:55:41[39m] Step: 13, Training Logs: loss_final: 1.671346, loss_mean: 1.574493, loss_mean_cls: 0.096853, grad_norm: 0.938797
Steps:   0%| | 14/1000000 [00:10<86:28:18,  3.21it/s, grad_norm=0.939, loss_final=1.67, loss_mean=1.57, loss_mean_cls=0.0969[[34m2025-10-04 11:55:41[39m] Step: 14, Training Logs: loss_final: 1.659532, loss_mean: 1.562734, loss_mean_cls: 0.096798, grad_norm: 0.746807
Steps:   0%| | 15/1000000 [00:11<84:46:09,  3.28it/s, grad_norm=0.747, loss_final=1.66, loss_mean=1.56, loss_mean_cls=0.0968[[34m2025-10-04 11:55:42[39m] Step: 15, Training Logs: loss_final: 1.653482, loss_mean: 1.557485, loss_mean_cls: 0.095997, grad_norm: 0.753487
Steps:   0%| | 16/1000000 [00:11<82:35:55,  3.36it/s, grad_norm=0.753, loss_final=1.65, loss_mean=1.56, loss_mean_cls=0.096][[34m2025-10-04 11:55:42[39m] Step: 16, Training Logs: loss_final: 1.668430, loss_mean: 1.573485, loss_mean_cls: 0.094945, grad_norm: 1.121276
Steps:   0%| | 17/1000000 [00:11<81:23:39,  3.41it/s, grad_norm=1.12, loss_final=1.67, loss_mean=1.57, loss_mean_cls=0.0949][[34m2025-10-04 11:55:42[39m] Step: 17, Training Logs: loss_final: 1.622319, loss_mean: 1.525881, loss_mean_cls: 0.096438, grad_norm: 0.825314
Steps:   0%| | 18/1000000 [00:12<81:55:27,  3.39it/s, grad_norm=0.825, loss_final=1.62, loss_mean=1.53, loss_mean_cls=0.0964[[34m2025-10-04 11:55:43[39m] Step: 18, Training Logs: loss_final: 1.619625, loss_mean: 1.526897, loss_mean_cls: 0.092728, grad_norm: 0.887003
Steps:   0%| | 19/1000000 [00:12<81:12:41,  3.42it/s, grad_norm=0.887, loss_final=1.62, loss_mean=1.53, loss_mean_cls=0.0927[[34m2025-10-04 11:55:43[39m] Step: 19, Training Logs: loss_final: 1.612384, loss_mean: 1.517632, loss_mean_cls: 0.094752, grad_norm: 0.850046
Steps:   0%| | 20/1000000 [00:12<80:13:19,  3.46it/s, grad_norm=0.85, loss_final=1.61, loss_mean=1.52, loss_mean_cls=0.0948][[34m2025-10-04 11:55:43[39m] Step: 20, Training Logs: loss_final: 1.615233, loss_mean: 1.523051, loss_mean_cls: 0.092182, grad_norm: 0.884068
Steps:   0%| | 21/1000000 [00:12<79:43:42,  3.48it/s, grad_norm=0.884, loss_final=1.62, loss_mean=1.52, loss_mean_cls=0.0922[[34m2025-10-04 11:55:43[39m] Step: 21, Training Logs: loss_final: 1.591438, loss_mean: 1.496822, loss_mean_cls: 0.094616, grad_norm: 0.907214
Steps:   0%| | 22/1000000 [00:13<79:28:15,  3.50it/s, grad_norm=0.907, loss_final=1.59, loss_mean=1.5, loss_mean_cls=0.0946][[34m2025-10-04 11:55:44[39m] Step: 22, Training Logs: loss_final: 1.577800, loss_mean: 1.487948, loss_mean_cls: 0.089852, grad_norm: 0.827451
Steps:   0%| | 23/1000000 [00:13<79:09:12,  3.51it/s, grad_norm=0.827, loss_final=1.58, loss_mean=1.49, loss_mean_cls=0.0899[[34m2025-10-04 11:55:44[39m] Step: 23, Training Logs: loss_final: 1.564300, loss_mean: 1.471926, loss_mean_cls: 0.092373, grad_norm: 0.895760
Steps:   0%| | 24/1000000 [00:13<78:50:57,  3.52it/s, grad_norm=0.896, loss_final=1.56, loss_mean=1.47, loss_mean_cls=0.0924[[34m2025-10-04 11:55:44[39m] Step: 24, Training Logs: loss_final: 1.544717, loss_mean: 1.452857, loss_mean_cls: 0.091861, grad_norm: 0.818182
Steps:   0%| | 25/1000000 [00:13<77:58:51,  3.56it/s, grad_norm=0.818, loss_final=1.54, loss_mean=1.45, loss_mean_cls=0.0919[[34m2025-10-04 11:55:45[39m] Step: 25, Training Logs: loss_final: 1.546056, loss_mean: 1.454019, loss_mean_cls: 0.092037, grad_norm: 0.739996
Steps:   0%|  | 26/1000000 [00:14<78:49:31,  3.52it/s, grad_norm=0.74, loss_final=1.55, loss_mean=1.45, loss_mean_cls=0.092][[34m2025-10-04 11:55:45[39m] Step: 26, Training Logs: loss_final: 1.531811, loss_mean: 1.439312, loss_mean_cls: 0.092500, grad_norm: 0.670497
Steps:   0%| | 27/1000000 [00:14<78:40:49,  3.53it/s, grad_norm=0.67, loss_final=1.53, loss_mean=1.44, loss_mean_cls=0.0925][[34m2025-10-04 11:55:45[39m] Step: 27, Training Logs: loss_final: 1.507270, loss_mean: 1.414881, loss_mean_cls: 0.092389, grad_norm: 0.758096
Steps:   0%| | 28/1000000 [00:14<78:17:32,  3.55it/s, grad_norm=0.758, loss_final=1.51, loss_mean=1.41, loss_mean_cls=0.0924[[34m2025-10-04 11:55:45[39m] Step: 28, Training Logs: loss_final: 1.519139, loss_mean: 1.429240, loss_mean_cls: 0.089898, grad_norm: 0.610969
Steps:   0%| | 29/1000000 [00:15<77:49:19,  3.57it/s, grad_norm=0.611, loss_final=1.52, loss_mean=1.43, loss_mean_cls=0.0899[[34m2025-10-04 11:55:46[39m] Step: 29, Training Logs: loss_final: 1.499356, loss_mean: 1.407856, loss_mean_cls: 0.091500, grad_norm: 0.716992
Steps:   0%| | 30/1000000 [00:15<78:37:21,  3.53it/s, grad_norm=0.717, loss_final=1.5, loss_mean=1.41, loss_mean_cls=0.0915][[34m2025-10-04 11:55:46[39m] Step: 30, Training Logs: loss_final: 1.483597, loss_mean: 1.393857, loss_mean_cls: 0.089740, grad_norm: 0.649494
Steps:   0%| | 31/1000000 [00:15<79:56:12,  3.47it/s, grad_norm=0.649, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0897[[34m2025-10-04 11:55:46[39m] Step: 31, Training Logs: loss_final: 1.477721, loss_mean: 1.388299, loss_mean_cls: 0.089422, grad_norm: 0.829575
Steps:   0%| | 32/1000000 [00:15<79:31:12,  3.49it/s, grad_norm=0.83, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0894][[34m2025-10-04 11:55:47[39m] Step: 32, Training Logs: loss_final: 1.488826, loss_mean: 1.401185, loss_mean_cls: 0.087641, grad_norm: 0.938077
Steps:   0%| | 33/1000000 [00:16<80:12:18,  3.46it/s, grad_norm=0.938, loss_final=1.49, loss_mean=1.4, loss_mean_cls=0.0876][[34m2025-10-04 11:55:47[39m] Step: 33, Training Logs: loss_final: 1.479225, loss_mean: 1.392005, loss_mean_cls: 0.087220, grad_norm: 0.871170
Steps:   0%| | 34/1000000 [00:16<79:24:56,  3.50it/s, grad_norm=0.871, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0872[[34m2025-10-04 11:55:47[39m] Step: 34, Training Logs: loss_final: 1.478034, loss_mean: 1.389407, loss_mean_cls: 0.088627, grad_norm: 0.668033
Steps:   0%| | 35/1000000 [00:16<79:02:21,  3.51it/s, grad_norm=0.668, loss_final=1.48, loss_mean=1.39, loss_mean_cls=0.0886[[34m2025-10-04 11:55:47[39m] Step: 35, Training Logs: loss_final: 1.466846, loss_mean: 1.379078, loss_mean_cls: 0.087767, grad_norm: 0.999028
Steps:   0%| | 36/1000000 [00:17<79:43:33,  3.48it/s, grad_norm=0.999, loss_final=1.47, loss_mean=1.38, loss_mean_cls=0.0878[[34m2025-10-04 11:55:48[39m] Step: 36, Training Logs: loss_final: 1.428662, loss_mean: 1.341124, loss_mean_cls: 0.087539, grad_norm: 0.792635
Steps:   0%| | 37/1000000 [00:17<80:04:57,  3.47it/s, grad_norm=0.793, loss_final=1.43, loss_mean=1.34, loss_mean_cls=0.0875[[34m2025-10-04 11:55:48[39m] Step: 37, Training Logs: loss_final: 1.445827, loss_mean: 1.359899, loss_mean_cls: 0.085929, grad_norm: 1.028910
Steps:   0%| | 38/1000000 [00:17<79:27:40,  3.50it/s, grad_norm=1.03, loss_final=1.45, loss_mean=1.36, loss_mean_cls=0.0859][[34m2025-10-04 11:55:48[39m] Step: 38, Training Logs: loss_final: 1.443310, loss_mean: 1.355693, loss_mean_cls: 0.087617, grad_norm: 0.735829
Steps:   0%| | 39/1000000 [00:17<78:57:23,  3.52it/s, grad_norm=0.736, loss_final=1.44, loss_mean=1.36, loss_mean_cls=0.0876[[34m2025-10-04 11:55:49[39m] Step: 39, Training Logs: loss_final: 1.421320, loss_mean: 1.335914, loss_mean_cls: 0.085405, grad_norm: 0.781313
Steps:   0%| | 40/1000000 [00:18<78:41:56,  3.53it/s, grad_norm=0.781, loss_final=1.42, loss_mean=1.34, loss_mean_cls=0.0854[[34m2025-10-04 11:55:49[39m] Step: 40, Training Logs: loss_final: 1.428990, loss_mean: 1.345265, loss_mean_cls: 0.083725, grad_norm: 0.614093
Steps:   0%| | 41/1000000 [00:18<77:52:03,  3.57it/s, grad_norm=0.614, loss_final=1.43, loss_mean=1.35, loss_mean_cls=0.0837[[34m2025-10-04 11:55:49[39m] Step: 41, Training Logs: loss_final: 1.413832, loss_mean: 1.326742, loss_mean_cls: 0.087090, grad_norm: 0.728805
Steps:   0%| | 42/1000000 [00:18<78:20:32,  3.55it/s, grad_norm=0.729, loss_final=1.41, loss_mean=1.33, loss_mean_cls=0.0871[[34m2025-10-04 11:55:49[39m] Step: 42, Training Logs: loss_final: 1.416154, loss_mean: 1.330340, loss_mean_cls: 0.085814, grad_norm: 0.865837
Steps:   0%| | 43/1000000 [00:19<78:15:28,  3.55it/s, grad_norm=0.866, loss_final=1.42, loss_mean=1.33, loss_mean_cls=0.0858[[34m2025-10-04 11:55:50[39m] Step: 43, Training Logs: loss_final: 1.426973, loss_mean: 1.342601, loss_mean_cls: 0.084372, grad_norm: 1.212790
Steps:   0%| | 44/1000000 [00:19<77:56:42,  3.56it/s, grad_norm=1.21, loss_final=1.43, loss_mean=1.34, loss_mean_cls=0.0844][[34m2025-10-04 11:55:50[39m] Step: 44, Training Logs: loss_final: 1.417312, loss_mean: 1.335625, loss_mean_cls: 0.081687, grad_norm: 1.149147
Steps:   0%| | 45/1000000 [00:19<77:21:21,  3.59it/s, grad_norm=1.15, loss_final=1.42, loss_mean=1.34, loss_mean_cls=0.0817][[34m2025-10-04 11:55:50[39m] Step: 45, Training Logs: loss_final: 1.374274, loss_mean: 1.291313, loss_mean_cls: 0.082960, grad_norm: 0.669741
Steps:   0%|  | 46/1000000 [00:19<77:53:15,  3.57it/s, grad_norm=0.67, loss_final=1.37, loss_mean=1.29, loss_mean_cls=0.083][[34m2025-10-04 11:55:51[39m] Step: 46, Training Logs: loss_final: 1.377141, loss_mean: 1.294317, loss_mean_cls: 0.082824, grad_norm: 0.976353
Steps:   0%| | 47/1000000 [00:20<77:52:59,  3.57it/s, grad_norm=0.976, loss_final=1.38, loss_mean=1.29, loss_mean_cls=0.0828[[34m2025-10-04 11:55:51[39m] Step: 47, Training Logs: loss_final: 1.380575, loss_mean: 1.298036, loss_mean_cls: 0.082539, grad_norm: 1.360397
Steps:   0%|  | 48/1000000 [00:20<77:52:43,  3.57it/s, grad_norm=1.36, loss_final=1.38, loss_mean=1.3, loss_mean_cls=0.0825][[34m2025-10-04 11:55:51[39m] Step: 48, Training Logs: loss_final: 1.373814, loss_mean: 1.291798, loss_mean_cls: 0.082016, grad_norm: 1.120300
Steps:   0%|  | 49/1000000 [00:20<77:30:12,  3.58it/s, grad_norm=1.12, loss_final=1.37, loss_mean=1.29, loss_mean_cls=0.082][[34m2025-10-04 11:55:51[39m] Step: 49, Training Logs: loss_final: 1.355728, loss_mean: 1.274397, loss_mean_cls: 0.081331, grad_norm: 1.069529
Steps:   0%| | 50/1000000 [00:21<79:35:29,  3.49it/s, grad_norm=1.07, loss_final=1.36, loss_mean=1.27, loss_mean_cls=0.0813][[34m2025-10-04 11:55:52[39m] Step: 50, Training Logs: loss_final: 1.365051, loss_mean: 1.282728, loss_mean_cls: 0.082323, grad_norm: 0.960896
Steps:   0%| | 51/1000000 [00:21<79:06:18,  3.51it/s, grad_norm=0.961, loss_final=1.37, loss_mean=1.28, loss_mean_cls=0.0823[[34m2025-10-04 11:55:52[39m] Step: 51, Training Logs: loss_final: 1.352253, loss_mean: 1.271365, loss_mean_cls: 0.080888, grad_norm: 1.036798
Steps:   0%| | 52/1000000 [00:21<78:42:04,  3.53it/s, grad_norm=1.04, loss_final=1.35, loss_mean=1.27, loss_mean_cls=0.0809][[34m2025-10-04 11:55:52[39m] Step: 52, Training Logs: loss_final: 1.309304, loss_mean: 1.229391, loss_mean_cls: 0.079913, grad_norm: 1.098624
Steps:   0%|  | 53/1000000 [00:21<78:27:02,  3.54it/s, grad_norm=1.1, loss_final=1.31, loss_mean=1.23, loss_mean_cls=0.0799][[34m2025-10-04 11:55:53[39m] Step: 53, Training Logs: loss_final: 1.342698, loss_mean: 1.261277, loss_mean_cls: 0.081422, grad_norm: 0.829682
Steps:   0%| | 54/1000000 [00:22<78:17:12,  3.55it/s, grad_norm=0.83, loss_final=1.34, loss_mean=1.26, loss_mean_cls=0.0814][[34m2025-10-04 11:55:53[39m] Step: 54, Training Logs: loss_final: 1.307747, loss_mean: 1.231277, loss_mean_cls: 0.076470, grad_norm: 1.049511
Steps:   0%| | 55/1000000 [00:22<78:04:48,  3.56it/s, grad_norm=1.05, loss_final=1.31, loss_mean=1.23, loss_mean_cls=0.0765][[34m2025-10-04 11:55:53[39m] Step: 55, Training Logs: loss_final: 1.315868, loss_mean: 1.237365, loss_mean_cls: 0.078503, grad_norm: 2.268262
Steps:   0%| | 56/1000000 [00:22<77:51:26,  3.57it/s, grad_norm=2.27, loss_final=1.32, loss_mean=1.24, loss_mean_cls=0.0785][[34m2025-10-04 11:55:53[39m] Step: 56, Training Logs: loss_final: 1.286505, loss_mean: 1.207611, loss_mean_cls: 0.078894, grad_norm: 0.808640
Steps:   0%| | 57/1000000 [00:23<76:49:57,  3.62it/s, grad_norm=0.809, loss_final=1.29, loss_mean=1.21, loss_mean_cls=0.0789[[34m2025-10-04 11:55:54[39m] Step: 57, Training Logs: loss_final: 1.275219, loss_mean: 1.195886, loss_mean_cls: 0.079333, grad_norm: 1.666968
Steps:   0%|  | 58/1000000 [00:23<76:55:10,  3.61it/s, grad_norm=1.67, loss_final=1.28, loss_mean=1.2, loss_mean_cls=0.0793][[34m2025-10-04 11:55:54[39m] Step: 58, Training Logs: loss_final: 1.328512, loss_mean: 1.249691, loss_mean_cls: 0.078821, grad_norm: 1.804855
Steps:   0%|  | 59/1000000 [00:23<78:34:42,  3.53it/s, grad_norm=1.8, loss_final=1.33, loss_mean=1.25, loss_mean_cls=0.0788][[34m2025-10-04 11:55:54[39m] Step: 59, Training Logs: loss_final: 1.271576, loss_mean: 1.194121, loss_mean_cls: 0.077455, grad_norm: 1.254132
Steps:   0%| | 60/1000000 [00:23<78:16:15,  3.55it/s, grad_norm=1.25, loss_final=1.27, loss_mean=1.19, loss_mean_cls=0.0775][[34m2025-10-04 11:55:55[39m] Step: 60, Training Logs: loss_final: 1.284966, loss_mean: 1.206485, loss_mean_cls: 0.078480, grad_norm: 1.378370
Steps:   0%| | 61/1000000 [00:24<78:05:50,  3.56it/s, grad_norm=1.38, loss_final=1.28, loss_mean=1.21, loss_mean_cls=0.0785][[34m2025-10-04 11:55:55[39m] Step: 61, Training Logs: loss_final: 1.257621, loss_mean: 1.180187, loss_mean_cls: 0.077434, grad_norm: 1.085317
Steps:   0%| | 62/1000000 [00:24<78:07:14,  3.56it/s, grad_norm=1.09, loss_final=1.26, loss_mean=1.18, loss_mean_cls=0.0774][[34m2025-10-04 11:55:55[39m] Step: 62, Training Logs: loss_final: 1.256328, loss_mean: 1.181461, loss_mean_cls: 0.074867, grad_norm: 0.763753
Steps:   0%| | 63/1000000 [00:24<78:11:53,  3.55it/s, grad_norm=0.764, loss_final=1.26, loss_mean=1.18, loss_mean_cls=0.0749[[34m2025-10-04 11:55:55[39m] Step: 63, Training Logs: loss_final: 1.256204, loss_mean: 1.179373, loss_mean_cls: 0.076832, grad_norm: 1.680212
Steps:   0%| | 64/1000000 [00:25<79:23:04,  3.50it/s, grad_norm=1.68, loss_final=1.26, loss_mean=1.18, loss_mean_cls=0.0768][[34m2025-10-04 11:55:56[39m] Step: 64, Training Logs: loss_final: 1.218861, loss_mean: 1.143166, loss_mean_cls: 0.075695, grad_norm: 1.070058
Steps:   0%| | 65/1000000 [00:25<78:51:34,  3.52it/s, grad_norm=1.07, loss_final=1.22, loss_mean=1.14, loss_mean_cls=0.0757][[34m2025-10-04 11:55:56[39m] Step: 65, Training Logs: loss_final: 1.266388, loss_mean: 1.189377, loss_mean_cls: 0.077011, grad_norm: 1.162077
Steps:   0%|  | 66/1000000 [00:25<78:36:50,  3.53it/s, grad_norm=1.16, loss_final=1.27, loss_mean=1.19, loss_mean_cls=0.077][[34m2025-10-04 11:55:56[39m] Step: 66, Training Logs: loss_final: 1.240091, loss_mean: 1.164940, loss_mean_cls: 0.075151, grad_norm: 0.779917
Steps:   0%| | 67/1000000 [00:25<78:18:30,  3.55it/s, grad_norm=0.78, loss_final=1.24, loss_mean=1.16, loss_mean_cls=0.0752][[34m2025-10-04 11:55:56[39m] Step: 67, Training Logs: loss_final: 1.213773, loss_mean: 1.138098, loss_mean_cls: 0.075676, grad_norm: 0.959115
Steps:   0%| | 68/1000000 [00:26<78:03:33,  3.56it/s, grad_norm=0.959, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0757[[34m2025-10-04 11:55:57[39m] Step: 68, Training Logs: loss_final: 1.251102, loss_mean: 1.177503, loss_mean_cls: 0.073598, grad_norm: 1.095073
Steps:   0%|  | 69/1000000 [00:26<77:27:29,  3.59it/s, grad_norm=1.1, loss_final=1.25, loss_mean=1.18, loss_mean_cls=0.0736][[34m2025-10-04 11:55:57[39m] Step: 69, Training Logs: loss_final: 1.224620, loss_mean: 1.150177, loss_mean_cls: 0.074442, grad_norm: 0.864246
Steps:   0%| | 70/1000000 [00:26<78:02:39,  3.56it/s, grad_norm=0.864, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.0744[[34m2025-10-04 11:55:57[39m] Step: 70, Training Logs: loss_final: 1.220434, loss_mean: 1.148310, loss_mean_cls: 0.072124, grad_norm: 1.022914
Steps:   0%| | 71/1000000 [00:26<77:59:36,  3.56it/s, grad_norm=1.02, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.0721][[34m2025-10-04 11:55:58[39m] Step: 71, Training Logs: loss_final: 1.244501, loss_mean: 1.170995, loss_mean_cls: 0.073507, grad_norm: 1.333435
Steps:   0%| | 72/1000000 [00:27<80:22:56,  3.46it/s, grad_norm=1.33, loss_final=1.24, loss_mean=1.17, loss_mean_cls=0.0735][[34m2025-10-04 11:55:58[39m] Step: 72, Training Logs: loss_final: 1.211727, loss_mean: 1.138361, loss_mean_cls: 0.073366, grad_norm: 1.030856
Steps:   0%| | 73/1000000 [00:27<79:45:28,  3.48it/s, grad_norm=1.03, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0734][[34m2025-10-04 11:55:58[39m] Step: 73, Training Logs: loss_final: 1.180672, loss_mean: 1.106680, loss_mean_cls: 0.073992, grad_norm: 1.019569
Steps:   0%|  | 74/1000000 [00:27<80:27:31,  3.45it/s, grad_norm=1.02, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.074][[34m2025-10-04 11:55:58[39m] Step: 74, Training Logs: loss_final: 1.212634, loss_mean: 1.140465, loss_mean_cls: 0.072169, grad_norm: 1.491407
Steps:   0%| | 75/1000000 [00:28<79:46:04,  3.48it/s, grad_norm=1.49, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0722][[34m2025-10-04 11:55:59[39m] Step: 75, Training Logs: loss_final: 1.224154, loss_mean: 1.153368, loss_mean_cls: 0.070785, grad_norm: 1.140900
Steps:   0%| | 76/1000000 [00:28<78:59:11,  3.52it/s, grad_norm=1.14, loss_final=1.22, loss_mean=1.15, loss_mean_cls=0.0708][[34m2025-10-04 11:55:59[39m] Step: 76, Training Logs: loss_final: 1.209434, loss_mean: 1.136663, loss_mean_cls: 0.072772, grad_norm: 1.220662
Steps:   0%| | 77/1000000 [00:28<77:49:02,  3.57it/s, grad_norm=1.22, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0728][[34m2025-10-04 11:55:59[39m] Step: 77, Training Logs: loss_final: 1.213073, loss_mean: 1.141849, loss_mean_cls: 0.071224, grad_norm: 1.227434
Steps:   0%| | 78/1000000 [00:28<79:46:23,  3.48it/s, grad_norm=1.23, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0712][[34m2025-10-04 11:56:00[39m] Step: 78, Training Logs: loss_final: 1.207920, loss_mean: 1.136820, loss_mean_cls: 0.071100, grad_norm: 1.200321
Steps:   0%|  | 79/1000000 [00:29<79:29:58,  3.49it/s, grad_norm=1.2, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0711][[34m2025-10-04 11:56:00[39m] Step: 79, Training Logs: loss_final: 1.212982, loss_mean: 1.142960, loss_mean_cls: 0.070022, grad_norm: 1.182392
Steps:   0%|   | 80/1000000 [00:29<79:27:08,  3.50it/s, grad_norm=1.18, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.07][[34m2025-10-04 11:56:00[39m] Step: 80, Training Logs: loss_final: 1.189428, loss_mean: 1.120346, loss_mean_cls: 0.069081, grad_norm: 1.556226
Steps:   0%| | 81/1000000 [00:29<80:50:59,  3.44it/s, grad_norm=1.56, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.0691][[34m2025-10-04 11:56:01[39m] Step: 81, Training Logs: loss_final: 1.210733, loss_mean: 1.140425, loss_mean_cls: 0.070307, grad_norm: 1.449676
Steps:   0%| | 82/1000000 [00:30<81:22:02,  3.41it/s, grad_norm=1.45, loss_final=1.21, loss_mean=1.14, loss_mean_cls=0.0703][[34m2025-10-04 11:56:01[39m] Step: 82, Training Logs: loss_final: 1.201556, loss_mean: 1.131593, loss_mean_cls: 0.069963, grad_norm: 1.056618
Steps:   0%|    | 83/1000000 [00:30<80:43:59,  3.44it/s, grad_norm=1.06, loss_final=1.2, loss_mean=1.13, loss_mean_cls=0.07][[34m2025-10-04 11:56:01[39m] Step: 83, Training Logs: loss_final: 1.181162, loss_mean: 1.111683, loss_mean_cls: 0.069479, grad_norm: 1.822040
Steps:   0%| | 84/1000000 [00:30<81:23:04,  3.41it/s, grad_norm=1.82, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.0695][[34m2025-10-04 11:56:01[39m] Step: 84, Training Logs: loss_final: 1.176741, loss_mean: 1.105825, loss_mean_cls: 0.070916, grad_norm: 0.839267
Steps:   0%| | 85/1000000 [00:31<80:29:32,  3.45it/s, grad_norm=0.839, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.0709[[34m2025-10-04 11:56:02[39m] Step: 85, Training Logs: loss_final: 1.179919, loss_mean: 1.111011, loss_mean_cls: 0.068909, grad_norm: 1.329496
Steps:   0%| | 86/1000000 [00:31<81:42:50,  3.40it/s, grad_norm=1.33, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.0689][[34m2025-10-04 11:56:02[39m] Step: 86, Training Logs: loss_final: 1.132134, loss_mean: 1.062311, loss_mean_cls: 0.069823, grad_norm: 1.298198
Steps:   0%|  | 87/1000000 [00:31<82:09:02,  3.38it/s, grad_norm=1.3, loss_final=1.13, loss_mean=1.06, loss_mean_cls=0.0698][[34m2025-10-04 11:56:02[39m] Step: 87, Training Logs: loss_final: 1.175014, loss_mean: 1.106182, loss_mean_cls: 0.068832, grad_norm: 1.236820
Steps:   0%| | 88/1000000 [00:31<84:51:18,  3.27it/s, grad_norm=1.24, loss_final=1.18, loss_mean=1.11, loss_mean_cls=0.0688][[34m2025-10-04 11:56:03[39m] Step: 88, Training Logs: loss_final: 1.159912, loss_mean: 1.091611, loss_mean_cls: 0.068301, grad_norm: 2.044524
Steps:   0%| | 89/1000000 [00:32<83:06:45,  3.34it/s, grad_norm=2.04, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0683][[34m2025-10-04 11:56:03[39m] Step: 89, Training Logs: loss_final: 1.186359, loss_mean: 1.118595, loss_mean_cls: 0.067764, grad_norm: 1.303310
Steps:   0%|  | 90/1000000 [00:32<81:43:11,  3.40it/s, grad_norm=1.3, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.0678][[34m2025-10-04 11:56:03[39m] Step: 90, Training Logs: loss_final: 1.141944, loss_mean: 1.074987, loss_mean_cls: 0.066957, grad_norm: 1.522194
Steps:   0%|  | 91/1000000 [00:32<80:53:43,  3.43it/s, grad_norm=1.52, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.067][[34m2025-10-04 11:56:03[39m] Step: 91, Training Logs: loss_final: 1.144631, loss_mean: 1.076173, loss_mean_cls: 0.068458, grad_norm: 1.189894
Steps:   0%| | 92/1000000 [00:33<80:08:32,  3.47it/s, grad_norm=1.19, loss_final=1.14, loss_mean=1.08, loss_mean_cls=0.0685][[34m2025-10-04 11:56:04[39m] Step: 92, Training Logs: loss_final: 1.150659, loss_mean: 1.082561, loss_mean_cls: 0.068098, grad_norm: 1.226912
Steps:   0%| | 93/1000000 [00:33<79:22:19,  3.50it/s, grad_norm=1.23, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.0681][[34m2025-10-04 11:56:04[39m] Step: 93, Training Logs: loss_final: 1.186440, loss_mean: 1.117766, loss_mean_cls: 0.068675, grad_norm: 1.049817
Steps:   0%| | 94/1000000 [00:33<78:48:14,  3.52it/s, grad_norm=1.05, loss_final=1.19, loss_mean=1.12, loss_mean_cls=0.0687][[34m2025-10-04 11:56:04[39m] Step: 94, Training Logs: loss_final: 1.161872, loss_mean: 1.093169, loss_mean_cls: 0.068703, grad_norm: 0.826174
Steps:   0%| | 95/1000000 [00:33<80:12:48,  3.46it/s, grad_norm=0.826, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0687[[34m2025-10-04 11:56:05[39m] Step: 95, Training Logs: loss_final: 1.162671, loss_mean: 1.094248, loss_mean_cls: 0.068423, grad_norm: 1.167366
Steps:   0%| | 96/1000000 [00:34<79:55:01,  3.48it/s, grad_norm=1.17, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0684][[34m2025-10-04 11:56:05[39m] Step: 96, Training Logs: loss_final: 1.173846, loss_mean: 1.107471, loss_mean_cls: 0.066374, grad_norm: 0.948181
Steps:   0%| | 97/1000000 [00:34<83:25:41,  3.33it/s, grad_norm=0.948, loss_final=1.17, loss_mean=1.11, loss_mean_cls=0.0664[[34m2025-10-04 11:56:05[39m] Step: 97, Training Logs: loss_final: 1.164604, loss_mean: 1.096872, loss_mean_cls: 0.067733, grad_norm: 1.184624
Steps:   0%|  | 98/1000000 [00:34<81:49:08,  3.39it/s, grad_norm=1.18, loss_final=1.16, loss_mean=1.1, loss_mean_cls=0.0677][[34m2025-10-04 11:56:05[39m] Step: 98, Training Logs: loss_final: 1.131846, loss_mean: 1.065462, loss_mean_cls: 0.066384, grad_norm: 0.780388
Steps:   0%| | 99/1000000 [00:35<80:34:53,  3.45it/s, grad_norm=0.78, loss_final=1.13, loss_mean=1.07, loss_mean_cls=0.0664][[34m2025-10-04 11:56:06[39m] Step: 99, Training Logs: loss_final: 1.140858, loss_mean: 1.072921, loss_mean_cls: 0.067937, grad_norm: 1.180982
Steps:   0%| | 100/1000000 [00:35<79:45:18,  3.48it/s, grad_norm=1.18, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.0679[[34m2025-10-04 11:56:06[39m] Step: 100, Training Logs: loss_final: 1.142393, loss_mean: 1.075491, loss_mean_cls: 0.066902, grad_norm: 0.708727
Steps:   0%| | 101/1000000 [00:35<79:11:13,  3.51it/s, grad_norm=0.709, loss_final=1.14, loss_mean=1.08, loss_mean_cls=0.066[[34m2025-10-04 11:56:06[39m] Step: 101, Training Logs: loss_final: 1.153363, loss_mean: 1.088083, loss_mean_cls: 0.065279, grad_norm: 1.479051
Steps:   0%| | 102/1000000 [00:35<78:56:47,  3.52it/s, grad_norm=1.48, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.0653[[34m2025-10-04 11:56:07[39m] Step: 102, Training Logs: loss_final: 1.154621, loss_mean: 1.087304, loss_mean_cls: 0.067317, grad_norm: 1.199495
Steps:   0%| | 103/1000000 [00:36<80:09:00,  3.47it/s, grad_norm=1.2, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.0673][[34m2025-10-04 11:56:07[39m] Step: 103, Training Logs: loss_final: 1.138569, loss_mean: 1.071632, loss_mean_cls: 0.066937, grad_norm: 0.915438
Steps:   0%| | 109/1000000 [00:38<80:16:42,  3.46it/s, grad_norm=1.05, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0648[[34m2025-10-04 11:56:09[39m] Step: 109, Training Logs: loss_final: 1.135452, loss_mean: 1.068693, loss_mean_cls: 0.066759, grad_norm: 1.101148
Steps:   0%| | 105/1000000 [00:36<80:04:20,  3.47it/s, grad_norm=1.31, loss_final=1.12, loss_mean=1.05, loss_mean_cls=0.0649[[34m2025-10-04 11:56:07[39m] Step: 105, Training Logs: loss_final: 1.148571, loss_mean: 1.082632, loss_mean_cls: 0.065938, grad_norm: 0.894477
Steps:   0%| | 106/1000000 [00:37<79:25:21,  3.50it/s, grad_norm=0.894, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.065[[34m2025-10-04 11:56:08[39m] Step: 106, Training Logs: loss_final: 1.154166, loss_mean: 1.088406, loss_mean_cls: 0.065761, grad_norm: 1.079737
Steps:   0%| | 107/1000000 [00:37<79:29:15,  3.49it/s, grad_norm=1.08, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.0658[[34m2025-10-04 11:56:08[39m] Step: 107, Training Logs: loss_final: 1.137953, loss_mean: 1.072160, loss_mean_cls: 0.065793, grad_norm: 1.484596
Steps:   0%| | 108/1000000 [00:37<80:29:42,  3.45it/s, grad_norm=1.48, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.0658[[34m2025-10-04 11:56:08[39m] Step: 108, Training Logs: loss_final: 1.159239, loss_mean: 1.094395, loss_mean_cls: 0.064844, grad_norm: 1.045299
Steps:   0%| | 109/1000000 [00:38<80:16:42,  3.46it/s, grad_norm=1.05, loss_final=1.16, loss_mean=1.09, loss_mean_cls=0.0648[[34m2025-10-04 11:56:09[39m] Step: 109, Training Logs: loss_final: 1.135452, loss_mean: 1.068693, loss_mean_cls: 0.066759, grad_norm: 1.101148
Steps:   0%| | 110/1000000 [00:38<79:25:38,  3.50it/s, grad_norm=1.1, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.0668][[34m2025-10-04 11:56:09[39m] Step: 110, Training Logs: loss_final: 1.144132, loss_mean: 1.078086, loss_mean_cls: 0.066046, grad_norm: 1.034104
Steps:   0%| | 117/1000000 [00:40<80:19:17,  3.46it/s, grad_norm=1.02, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.0663[[34m2025-10-04 11:56:09[39m] Step: 111, Training Logs: loss_final: 1.109139, loss_mean: 1.042265, loss_mean_cls: 0.066874, grad_norm: 0.824648
Steps:   0%| | 112/1000000 [00:38<78:36:33,  3.53it/s, grad_norm=0.825, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.066[[34m2025-10-04 11:56:09[39m] Step: 112, Training Logs: loss_final: 1.132567, loss_mean: 1.066315, loss_mean_cls: 0.066253, grad_norm: 1.263528
Steps:   0%| | 113/1000000 [00:39<78:23:01,  3.54it/s, grad_norm=1.26, loss_final=1.13, loss_mean=1.07, loss_mean_cls=0.0663[[34m2025-10-04 11:56:10[39m] Step: 113, Training Logs: loss_final: 1.124068, loss_mean: 1.057839, loss_mean_cls: 0.066229, grad_norm: 1.411090
Steps:   0%| | 114/1000000 [00:39<78:16:49,  3.55it/s, grad_norm=1.41, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.0662[[34m2025-10-04 11:56:10[39m] Step: 114, Training Logs: loss_final: 1.134740, loss_mean: 1.070144, loss_mean_cls: 0.064595, grad_norm: 0.937598
Steps:   0%| | 115/1000000 [00:39<79:20:24,  3.50it/s, grad_norm=0.938, loss_final=1.13, loss_mean=1.07, loss_mean_cls=0.064[[34m2025-10-04 11:56:10[39m] Step: 115, Training Logs: loss_final: 1.126991, loss_mean: 1.061400, loss_mean_cls: 0.065590, grad_norm: 1.196780
Steps:   0%| | 116/1000000 [00:40<81:09:11,  3.42it/s, grad_norm=1.2, loss_final=1.13, loss_mean=1.06, loss_mean_cls=0.0656][[34m2025-10-04 11:56:11[39m] Step: 116, Training Logs: loss_final: 1.137667, loss_mean: 1.072809, loss_mean_cls: 0.064859, grad_norm: 1.393651
Steps:   0%| | 117/1000000 [00:40<80:19:17,  3.46it/s, grad_norm=1.39, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.0649[[34m2025-10-04 11:56:11[39m] Step: 117, Training Logs: loss_final: 1.147022, loss_mean: 1.080750, loss_mean_cls: 0.066272, grad_norm: 1.020705
Steps:   0%| | 124/1000000 [00:42<80:05:20,  3.47it/s, grad_norm=1.12, loss_final=1.14, loss_mean=1.08, loss_mean_cls=0.0643[[34m2025-10-04 11:56:11[39m] Step: 118, Training Logs: loss_final: 1.109673, loss_mean: 1.043185, loss_mean_cls: 0.066489, grad_norm: 1.225523
Steps:   0%| | 119/1000000 [00:40<83:22:57,  3.33it/s, grad_norm=1.23, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.0665[[34m2025-10-04 11:56:12[39m] Step: 119, Training Logs: loss_final: 1.119401, loss_mean: 1.054886, loss_mean_cls: 0.064515, grad_norm: 0.908479
Steps:   0%| | 120/1000000 [00:41<81:39:52,  3.40it/s, grad_norm=0.908, loss_final=1.12, loss_mean=1.05, loss_mean_cls=0.064[[34m2025-10-04 11:56:12[39m] Step: 120, Training Logs: loss_final: 1.130745, loss_mean: 1.065965, loss_mean_cls: 0.064780, grad_norm: 1.369089
Steps:   0%| | 121/1000000 [00:41<81:24:17,  3.41it/s, grad_norm=1.37, loss_final=1.13, loss_mean=1.07, loss_mean_cls=0.0648[[34m2025-10-04 11:56:12[39m] Step: 121, Training Logs: loss_final: 1.141380, loss_mean: 1.075447, loss_mean_cls: 0.065933, grad_norm: 0.926562
Steps:   0%| | 122/1000000 [00:41<82:20:11,  3.37it/s, grad_norm=0.927, loss_final=1.14, loss_mean=1.08, loss_mean_cls=0.065[[34m2025-10-04 11:56:12[39m] Step: 122, Training Logs: loss_final: 1.110147, loss_mean: 1.044951, loss_mean_cls: 0.065195, grad_norm: 1.066342
Steps:   0%| | 123/1000000 [00:42<81:07:18,  3.42it/s, grad_norm=1.07, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.0652[[34m2025-10-04 11:56:13[39m] Step: 123, Training Logs: loss_final: 1.149160, loss_mean: 1.085045, loss_mean_cls: 0.064115, grad_norm: 1.005233
Steps:   0%| | 124/1000000 [00:42<80:05:20,  3.47it/s, grad_norm=1.01, loss_final=1.15, loss_mean=1.09, loss_mean_cls=0.0641[[34m2025-10-04 11:56:13[39m] Step: 124, Training Logs: loss_final: 1.143576, loss_mean: 1.079296, loss_mean_cls: 0.064279, grad_norm: 1.121734
Steps:   0%| | 131/1000000 [00:44<80:27:27,  3.45it/s, grad_norm=1.15, loss_final=1.14, loss_mean=1.07, loss_mean_cls=0.0639[[34m2025-10-04 11:56:13[39m] Step: 125, Training Logs: loss_final: 1.113838, loss_mean: 1.049010, loss_mean_cls: 0.064829, grad_norm: 1.061054
Steps:   0%| | 126/1000000 [00:42<79:24:17,  3.50it/s, grad_norm=1.06, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0648[[34m2025-10-04 11:56:14[39m] Step: 126, Training Logs: loss_final: 1.106887, loss_mean: 1.043169, loss_mean_cls: 0.063718, grad_norm: 0.880969
Steps:   0%| | 127/1000000 [00:43<79:01:24,  3.51it/s, grad_norm=0.881, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.063[[34m2025-10-04 11:56:14[39m] Step: 127, Training Logs: loss_final: 1.106058, loss_mean: 1.042661, loss_mean_cls: 0.063396, grad_norm: 0.957814
Steps:   0%| | 128/1000000 [00:43<80:25:07,  3.45it/s, grad_norm=0.958, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.063[[34m2025-10-04 11:56:14[39m] Step: 128, Training Logs: loss_final: 1.122327, loss_mean: 1.057975, loss_mean_cls: 0.064352, grad_norm: 0.737015
Steps:   0%| | 129/1000000 [00:43<79:49:57,  3.48it/s, grad_norm=0.737, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.064[[34m2025-10-04 11:56:14[39m] Step: 129, Training Logs: loss_final: 1.148317, loss_mean: 1.084772, loss_mean_cls: 0.063545, grad_norm: 0.927350
Steps:   0%| | 130/1000000 [00:44<80:36:48,  3.45it/s, grad_norm=0.927, loss_final=1.15, loss_mean=1.08, loss_mean_cls=0.063[[34m2025-10-04 11:56:15[39m] Step: 130, Training Logs: loss_final: 1.134899, loss_mean: 1.071214, loss_mean_cls: 0.063686, grad_norm: 0.973368
Steps:   0%| | 131/1000000 [00:44<80:27:27,  3.45it/s, grad_norm=0.973, loss_final=1.13, loss_mean=1.07, loss_mean_cls=0.063[[34m2025-10-04 11:56:15[39m] Step: 131, Training Logs: loss_final: 1.136020, loss_mean: 1.072167, loss_mean_cls: 0.063853, grad_norm: 1.149637
Steps:   0%| | 138/1000000 [00:46<81:31:31,  3.41it/s, grad_norm=0.991, loss_final=1.12, loss_mean=1.05, loss_mean_cls=0.064[[34m2025-10-04 11:56:15[39m] Step: 132, Training Logs: loss_final: 1.082798, loss_mean: 1.018941, loss_mean_cls: 0.063857, grad_norm: 0.812827
Steps:   0%| | 133/1000000 [00:44<80:39:33,  3.44it/s, grad_norm=0.813, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.063[[34m2025-10-04 11:56:16[39m] Step: 133, Training Logs: loss_final: 1.107146, loss_mean: 1.041739, loss_mean_cls: 0.065407, grad_norm: 0.959704
Steps:   0%| | 134/1000000 [00:45<82:28:34,  3.37it/s, grad_norm=0.96, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.0654[[34m2025-10-04 11:56:16[39m] Step: 134, Training Logs: loss_final: 1.112836, loss_mean: 1.049722, loss_mean_cls: 0.063114, grad_norm: 1.553830
Steps:   0%| | 135/1000000 [00:45<81:09:15,  3.42it/s, grad_norm=1.55, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0631[[34m2025-10-04 11:56:16[39m] Step: 135, Training Logs: loss_final: 1.108448, loss_mean: 1.044287, loss_mean_cls: 0.064161, grad_norm: 0.767877
Steps:   0%| | 136/1000000 [00:45<81:32:45,  3.41it/s, grad_norm=0.768, loss_final=1.11, loss_mean=1.04, loss_mean_cls=0.064[[34m2025-10-04 11:56:16[39m] Step: 136, Training Logs: loss_final: 1.113887, loss_mean: 1.049805, loss_mean_cls: 0.064082, grad_norm: 1.239066
Steps:   0%| | 137/1000000 [00:46<80:40:21,  3.44it/s, grad_norm=1.24, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0641[[34m2025-10-04 11:56:17[39m] Step: 137, Training Logs: loss_final: 1.078130, loss_mean: 1.013259, loss_mean_cls: 0.064871, grad_norm: 0.826535
Steps:   0%| | 138/1000000 [00:46<81:31:31,  3.41it/s, grad_norm=0.827, loss_final=1.08, loss_mean=1.01, loss_mean_cls=0.064[[34m2025-10-04 11:56:17[39m] Step: 138, Training Logs: loss_final: 1.119535, loss_mean: 1.054763, loss_mean_cls: 0.064772, grad_norm: 0.990605
Steps:   0%|  | 145/1000000 [00:48<79:45:28,  3.48it/s, grad_norm=0.913, loss_final=1.07, loss_mean=1, loss_mean_cls=0.0647][[34m2025-10-04 11:56:17[39m] Step: 139, Training Logs: loss_final: 1.082140, loss_mean: 1.017380, loss_mean_cls: 0.064760, grad_norm: 0.831741
Steps:   0%| | 140/1000000 [00:46<79:31:34,  3.49it/s, grad_norm=0.832, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.064[[34m2025-10-04 11:56:18[39m] Step: 140, Training Logs: loss_final: 1.090272, loss_mean: 1.026423, loss_mean_cls: 0.063849, grad_norm: 0.930927
Steps:   0%| | 141/1000000 [00:47<78:57:39,  3.52it/s, grad_norm=0.931, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.063[[34m2025-10-04 11:56:18[39m] Step: 141, Training Logs: loss_final: 1.123071, loss_mean: 1.058900, loss_mean_cls: 0.064171, grad_norm: 1.092066
Steps:   0%| | 142/1000000 [00:47<78:46:46,  3.53it/s, grad_norm=1.09, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.0642[[34m2025-10-04 11:56:18[39m] Step: 142, Training Logs: loss_final: 1.091323, loss_mean: 1.028510, loss_mean_cls: 0.062813, grad_norm: 0.869009
Steps:   0%| | 143/1000000 [00:47<81:59:10,  3.39it/s, grad_norm=0.869, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.062[[34m2025-10-04 11:56:18[39m] Step: 143, Training Logs: loss_final: 1.058624, loss_mean: 0.994504, loss_mean_cls: 0.064120, grad_norm: 0.930199
Steps:   0%| | 144/1000000 [00:48<80:39:59,  3.44it/s, grad_norm=0.93, loss_final=1.06, loss_mean=0.995, loss_mean_cls=0.064[[34m2025-10-04 11:56:19[39m] Step: 144, Training Logs: loss_final: 1.083332, loss_mean: 1.019257, loss_mean_cls: 0.064075, grad_norm: 0.787457
Steps:   0%| | 145/1000000 [00:48<79:45:28,  3.48it/s, grad_norm=0.787, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.064[[34m2025-10-04 11:56:19[39m] Step: 145, Training Logs: loss_final: 1.065985, loss_mean: 1.001327, loss_mean_cls: 0.064658, grad_norm: 0.912915
Steps:   0%| | 152/1000000 [00:50<81:43:03,  3.40it/s, grad_norm=1.26, loss_final=1.12, loss_mean=1.06, loss_mean_cls=0.0634[[34m2025-10-04 11:56:19[39m] Step: 146, Training Logs: loss_final: 1.091722, loss_mean: 1.028316, loss_mean_cls: 0.063405, grad_norm: 0.818442
Steps:   0%| | 147/1000000 [00:49<82:08:49,  3.38it/s, grad_norm=0.818, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.063[[34m2025-10-04 11:56:20[39m] Step: 147, Training Logs: loss_final: 1.096863, loss_mean: 1.032724, loss_mean_cls: 0.064139, grad_norm: 1.291899
Steps:   0%| | 148/1000000 [00:49<81:29:54,  3.41it/s, grad_norm=1.29, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0641][[34m2025-10-04 11:56:20[39m] Step: 148, Training Logs: loss_final: 1.104063, loss_mean: 1.040382, loss_mean_cls: 0.063681, grad_norm: 0.965531
Steps:   0%| | 149/1000000 [00:49<81:20:06,  3.41it/s, grad_norm=0.966, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0637[[34m2025-10-04 11:56:20[39m] Step: 149, Training Logs: loss_final: 1.109504, loss_mean: 1.046762, loss_mean_cls: 0.062741, grad_norm: 1.136224
Steps:   0%| | 150/1000000 [00:49<80:21:36,  3.46it/s, grad_norm=1.14, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0627[[34m2025-10-04 11:56:21[39m] Step: 150, Training Logs: loss_final: 1.112775, loss_mean: 1.050696, loss_mean_cls: 0.062078, grad_norm: 1.234653
Steps:   0%| | 151/1000000 [00:50<79:40:30,  3.49it/s, grad_norm=1.23, loss_final=1.11, loss_mean=1.05, loss_mean_cls=0.0621[[34m2025-10-04 11:56:21[39m] Step: 151, Training Logs: loss_final: 1.057041, loss_mean: 0.992438, loss_mean_cls: 0.064603, grad_norm: 0.712097
Steps:   0%| | 152/1000000 [00:50<81:43:03,  3.40it/s, grad_norm=0.712, loss_final=1.06, loss_mean=0.992, loss_mean_cls=0.06[[34m2025-10-04 11:56:21[39m] Step: 152, Training Logs: loss_final: 1.122152, loss_mean: 1.058710, loss_mean_cls: 0.063442, grad_norm: 1.261345
Steps:   0%| | 159/1000000 [00:52<80:37:00,  3.45it/s, grad_norm=0.974, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.061[[34m2025-10-04 11:56:21[39m] Step: 153, Training Logs: loss_final: 1.095499, loss_mean: 1.033966, loss_mean_cls: 0.061533, grad_norm: 0.981268
Steps:   0%| | 154/1000000 [00:51<80:21:02,  3.46it/s, grad_norm=0.981, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0615[[34m2025-10-04 11:56:22[39m] Step: 154, Training Logs: loss_final: 1.094931, loss_mean: 1.032169, loss_mean_cls: 0.062763, grad_norm: 0.946518
Steps:   0%| | 155/1000000 [00:51<80:17:20,  3.46it/s, grad_norm=0.947, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.062[[34m2025-10-04 11:56:22[39m] Step: 155, Training Logs: loss_final: 1.093915, loss_mean: 1.029880, loss_mean_cls: 0.064035, grad_norm: 0.864749
Steps:   0%| | 156/1000000 [00:51<81:31:29,  3.41it/s, grad_norm=0.865, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.064[[34m2025-10-04 11:56:22[39m] Step: 156, Training Logs: loss_final: 1.092364, loss_mean: 1.029691, loss_mean_cls: 0.062673, grad_norm: 1.202510
Steps:   0%| | 157/1000000 [00:51<81:35:00,  3.40it/s, grad_norm=1.2, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0627][[34m2025-10-04 11:56:23[39m] Step: 157, Training Logs: loss_final: 1.077755, loss_mean: 1.016313, loss_mean_cls: 0.061441, grad_norm: 1.026806
Steps:   0%| | 158/1000000 [00:52<81:25:42,  3.41it/s, grad_norm=1.03, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0614[[34m2025-10-04 11:56:23[39m] Step: 158, Training Logs: loss_final: 1.095664, loss_mean: 1.032472, loss_mean_cls: 0.063193, grad_norm: 0.990058
Steps:   0%| | 159/1000000 [00:52<80:37:00,  3.45it/s, grad_norm=0.99, loss_final=1.1, loss_mean=1.03, loss_mean_cls=0.0632][[34m2025-10-04 11:56:23[39m] Step: 159, Training Logs: loss_final: 1.082216, loss_mean: 1.020891, loss_mean_cls: 0.061325, grad_norm: 0.973974
Steps:   0%| | 166/1000000 [00:54<79:38:35,  3.49it/s, grad_norm=0.785, loss_final=1.05, loss_mean=0.988, loss_mean_cls=0.06[[34m2025-10-04 11:56:23[39m] Step: 160, Training Logs: loss_final: 1.076460, loss_mean: 1.012586, loss_mean_cls: 0.063874, grad_norm: 0.941470
Steps:   0%| | 161/1000000 [00:53<79:37:27,  3.49it/s, grad_norm=0.941, loss_final=1.08, loss_mean=1.01, loss_mean_cls=0.063[[34m2025-10-04 11:56:24[39m] Step: 161, Training Logs: loss_final: 1.101123, loss_mean: 1.037103, loss_mean_cls: 0.064020, grad_norm: 0.750021
Steps:   0%|  | 162/1000000 [00:53<80:43:13,  3.44it/s, grad_norm=0.75, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.064][[34m2025-10-04 11:56:24[39m] Step: 162, Training Logs: loss_final: 1.088160, loss_mean: 1.025598, loss_mean_cls: 0.062561, grad_norm: 1.324303
Steps:   0%| | 163/1000000 [00:53<81:53:33,  3.39it/s, grad_norm=1.32, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0626[[34m2025-10-04 11:56:24[39m] Step: 163, Training Logs: loss_final: 1.085729, loss_mean: 1.023157, loss_mean_cls: 0.062572, grad_norm: 0.887782
Steps:   0%| | 164/1000000 [00:53<81:01:55,  3.43it/s, grad_norm=0.888, loss_final=1.09, loss_mean=1.02, loss_mean_cls=0.062[[34m2025-10-04 11:56:25[39m] Step: 164, Training Logs: loss_final: 1.090119, loss_mean: 1.026854, loss_mean_cls: 0.063265, grad_norm: 0.926017
Steps:   0%| | 165/1000000 [00:54<80:16:43,  3.46it/s, grad_norm=0.926, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.063[[34m2025-10-04 11:56:25[39m] Step: 165, Training Logs: loss_final: 1.083936, loss_mean: 1.021814, loss_mean_cls: 0.062121, grad_norm: 0.890201
Steps:   0%| | 166/1000000 [00:54<79:38:35,  3.49it/s, grad_norm=0.89, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0621[[34m2025-10-04 11:56:25[39m] Step: 166, Training Logs: loss_final: 1.051654, loss_mean: 0.987716, loss_mean_cls: 0.063937, grad_norm: 0.784934
Steps:   0%| | 173/1000000 [00:56<82:55:58,  3.35it/s, grad_norm=1.81, loss_final=1.06, loss_mean=0.998, loss_mean_cls=0.063[[34m2025-10-04 11:56:25[39m] Step: 167, Training Logs: loss_final: 1.103188, loss_mean: 1.038917, loss_mean_cls: 0.064271, grad_norm: 1.007990
Steps:   0%| | 168/1000000 [00:55<80:41:47,  3.44it/s, grad_norm=1.01, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0643][[34m2025-10-04 11:56:26[39m] Step: 168, Training Logs: loss_final: 1.089051, loss_mean: 1.025753, loss_mean_cls: 0.063297, grad_norm: 1.009032
Steps:   0%| | 169/1000000 [00:55<79:45:01,  3.48it/s, grad_norm=1.01, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0633[[34m2025-10-04 11:56:26[39m] Step: 169, Training Logs: loss_final: 1.071231, loss_mean: 1.008423, loss_mean_cls: 0.062808, grad_norm: 1.028093
Steps:   0%| | 170/1000000 [00:55<79:29:22,  3.49it/s, grad_norm=1.03, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0628[[34m2025-10-04 11:56:26[39m] Step: 170, Training Logs: loss_final: 1.084263, loss_mean: 1.021411, loss_mean_cls: 0.062852, grad_norm: 0.963162
Steps:   0%| | 171/1000000 [00:55<79:33:13,  3.49it/s, grad_norm=0.963, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.062[[34m2025-10-04 11:56:27[39m] Step: 171, Training Logs: loss_final: 1.100405, loss_mean: 1.038397, loss_mean_cls: 0.062008, grad_norm: 1.341097
Steps:   0%|  | 172/1000000 [00:56<81:23:05,  3.41it/s, grad_norm=1.34, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.062][[34m2025-10-04 11:56:27[39m] Step: 172, Training Logs: loss_final: 1.065575, loss_mean: 1.003677, loss_mean_cls: 0.061898, grad_norm: 0.862727
Steps:   0%|  | 173/1000000 [00:56<82:55:58,  3.35it/s, grad_norm=0.863, loss_final=1.07, loss_mean=1, loss_mean_cls=0.0619][[34m2025-10-04 11:56:27[39m] Step: 173, Training Logs: loss_final: 1.061090, loss_mean: 0.997548, loss_mean_cls: 0.063542, grad_norm: 1.809757
Steps:   0%| | 174/1000000 [00:56<82:16:33,  3.38it/s, grad_norm=1.81, loss_final=1.06, loss_mean=0.998, loss_mean_cls=0.063[[34m2025-10-04 11:56:28[39m] Step: 174, Training Logs: loss_final: 1.084081, loss_mean: 1.022197, loss_mean_cls: 0.061884, grad_norm: 1.089422
Steps:   0%| | 175/1000000 [00:57<81:37:28,  3.40it/s, grad_norm=1.09, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0619[[34m2025-10-04 11:56:28[39m] Step: 175, Training Logs: loss_final: 1.046785, loss_mean: 0.984504, loss_mean_cls: 0.062281, grad_norm: 0.953080
Steps:   0%| | 176/1000000 [00:57<81:30:35,  3.41it/s, grad_norm=0.953, loss_final=1.05, loss_mean=0.985, loss_mean_cls=0.06[[34m2025-10-04 11:56:28[39m] Step: 176, Training Logs: loss_final: 1.080198, loss_mean: 1.016293, loss_mean_cls: 0.063905, grad_norm: 1.124304
Steps:   0%| | 177/1000000 [00:57<84:01:37,  3.31it/s, grad_norm=1.12, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0639[[34m2025-10-04 11:56:28[39m] Step: 177, Training Logs: loss_final: 1.047183, loss_mean: 0.984632, loss_mean_cls: 0.062551, grad_norm: 0.747689
Steps:   0%| | 178/1000000 [00:58<83:27:34,  3.33it/s, grad_norm=0.748, loss_final=1.05, loss_mean=0.985, loss_mean_cls=0.06[[34m2025-10-04 11:56:29[39m] Step: 178, Training Logs: loss_final: 1.076812, loss_mean: 1.013928, loss_mean_cls: 0.062883, grad_norm: 1.305068
Steps:   0%| | 179/1000000 [00:58<82:48:36,  3.35it/s, grad_norm=1.31, loss_final=1.08, loss_mean=1.01, loss_mean_cls=0.0629[[34m2025-10-04 11:56:29[39m] Step: 179, Training Logs: loss_final: 1.080147, loss_mean: 1.017831, loss_mean_cls: 0.062316, grad_norm: 0.921939
Steps:   0%| | 179/1000000 [00:58<82:48:36,  3.35it/s, grad_norm=0.922, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.062
Steps:   0%| | 181/1000000 [00:58<81:35:24,  3.40it/s, grad_norm=1.06, loss_final=1.05, loss_mean=0.986, loss_mean_cls=0.061[[34m2025-10-04 11:56:30[39m] Step: 181, Training Logs: loss_final: 1.089074, loss_mean: 1.026894, loss_mean_cls: 0.062180, grad_norm: 1.505542
Steps:   0%| | 182/1000000 [00:59<80:31:54,  3.45it/s, grad_norm=1.51, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.0622[[34m2025-10-04 11:56:30[39m] Step: 182, Training Logs: loss_final: 1.047513, loss_mean: 0.985052, loss_mean_cls: 0.062462, grad_norm: 0.723676
Steps:   0%| | 183/1000000 [00:59<82:37:21,  3.36it/s, grad_norm=0.724, loss_final=1.05, loss_mean=0.985, loss_mean_cls=0.06[[34m2025-10-04 11:56:30[39m] Step: 183, Training Logs: loss_final: 1.081072, loss_mean: 1.019323, loss_mean_cls: 0.061749, grad_norm: 1.214268
Steps:   0%| | 184/1000000 [00:59<84:26:31,  3.29it/s, grad_norm=1.21, loss_final=1.08, loss_mean=1.02, loss_mean_cls=0.0617[[34m2025-10-04 11:56:30[39m] Step: 184, Training Logs: loss_final: 1.053654, loss_mean: 0.991626, loss_mean_cls: 0.062028, grad_norm: 0.847241
Steps:   0%| | 185/1000000 [01:00<82:29:23,  3.37it/s, grad_norm=0.847, loss_final=1.05, loss_mean=0.992, loss_mean_cls=0.06[[34m2025-10-04 11:56:31[39m] Step: 185, Training Logs: loss_final: 1.063487, loss_mean: 1.001717, loss_mean_cls: 0.061770, grad_norm: 0.999229
Steps:   0%|  | 186/1000000 [01:00<81:39:10,  3.40it/s, grad_norm=0.999, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0618][[34m2025-10-04 11:56:31[39m] Step: 186, Training Logs: loss_final: 1.063850, loss_mean: 1.002696, loss_mean_cls: 0.061154, grad_norm: 1.096784
Steps:   0%|    | 186/1000000 [01:00<81:39:10,  3.40it/s, grad_norm=1.1, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0612]
Steps:   0%| | 188/1000000 [01:00<80:19:47,  3.46it/s, grad_norm=0.799, loss_final=1.05, loss_mean=0.992, loss_mean_cls=0.06[[34m2025-10-04 11:56:32[39m] Step: 188, Training Logs: loss_final: 1.096554, loss_mean: 1.035187, loss_mean_cls: 0.061367, grad_norm: 1.064910
Steps:   0%| | 189/1000000 [01:01<81:22:09,  3.41it/s, grad_norm=1.06, loss_final=1.1, loss_mean=1.04, loss_mean_cls=0.0614][[34m2025-10-04 11:56:32[39m] Step: 189, Training Logs: loss_final: 1.044127, loss_mean: 0.981126, loss_mean_cls: 0.063002, grad_norm: 1.220169
Steps:   0%| | 190/1000000 [01:01<80:48:48,  3.44it/s, grad_norm=1.22, loss_final=1.04, loss_mean=0.981, loss_mean_cls=0.063[[34m2025-10-04 11:56:32[39m] Step: 190, Training Logs: loss_final: 1.088271, loss_mean: 1.026305, loss_mean_cls: 0.061967, grad_norm: 0.938965
Steps:   0%| | 191/1000000 [01:01<80:11:18,  3.46it/s, grad_norm=0.939, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.062[[34m2025-10-04 11:56:33[39m] Step: 191, Training Logs: loss_final: 1.037370, loss_mean: 0.975902, loss_mean_cls: 0.061468, grad_norm: 0.687807
Steps:   0%| | 192/1000000 [01:02<80:06:49,  3.47it/s, grad_norm=0.688, loss_final=1.04, loss_mean=0.976, loss_mean_cls=0.06[[34m2025-10-04 11:56:33[39m] Step: 192, Training Logs: loss_final: 1.049437, loss_mean: 0.987374, loss_mean_cls: 0.062063, grad_norm: 1.146187
Steps:   0%| | 193/1000000 [01:02<79:24:18,  3.50it/s, grad_norm=1.15, loss_final=1.05, loss_mean=0.987, loss_mean_cls=0.062[[34m2025-10-04 11:56:33[39m] Step: 193, Training Logs: loss_final: 1.051530, loss_mean: 0.990455, loss_mean_cls: 0.061075, grad_norm: 1.063150
Steps:   0%| | 193/1000000 [01:02<79:24:18,  3.50it/s, grad_norm=1.06, loss_final=1.05, loss_mean=0.99, loss_mean_cls=0.0611
Steps:   0%|   | 195/1000000 [01:03<81:45:02,  3.40it/s, grad_norm=0.68, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0617][[34m2025-10-04 11:56:34[39m] Step: 195, Training Logs: loss_final: 1.065199, loss_mean: 1.002915, loss_mean_cls: 0.062284, grad_norm: 1.040725
Steps:   0%|   | 196/1000000 [01:03<81:24:49,  3.41it/s, grad_norm=1.04, loss_final=1.07, loss_mean=1, loss_mean_cls=0.0623][[34m2025-10-04 11:56:34[39m] Step: 196, Training Logs: loss_final: 1.054080, loss_mean: 0.989846, loss_mean_cls: 0.064234, grad_norm: 1.407995
Steps:   0%| | 197/1000000 [01:03<81:13:48,  3.42it/s, grad_norm=1.41, loss_final=1.05, loss_mean=0.99, loss_mean_cls=0.0642[[34m2025-10-04 11:56:34[39m] Step: 197, Training Logs: loss_final: 1.090921, loss_mean: 1.030619, loss_mean_cls: 0.060302, grad_norm: 0.752677
Steps:   0%| | 198/1000000 [01:03<80:59:02,  3.43it/s, grad_norm=0.753, loss_final=1.09, loss_mean=1.03, loss_mean_cls=0.060[[34m2025-10-04 11:56:35[39m] Step: 198, Training Logs: loss_final: 1.062793, loss_mean: 1.001559, loss_mean_cls: 0.061234, grad_norm: 1.413319
Steps:   0%|   | 199/1000000 [01:04<82:52:44,  3.35it/s, grad_norm=1.41, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0612][[34m2025-10-04 11:56:35[39m] Step: 199, Training Logs: loss_final: 1.070892, loss_mean: 1.010875, loss_mean_cls: 0.060018, grad_norm: 0.922436
Steps:   0%| | 200/1000000 [01:04<83:12:00,  3.34it/s, grad_norm=0.922, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.06][[34m2025-10-04 11:56:35[39m] Step: 200, Training Logs: loss_final: 1.065669, loss_mean: 1.003739, loss_mean_cls: 0.061930, grad_norm: 0.821615
Steps:   0%|  | 200/1000000 [01:04<83:12:00,  3.34it/s, grad_norm=0.822, loss_final=1.07, loss_mean=1, loss_mean_cls=0.0619]
Steps:   0%| | 202/1000000 [01:05<82:40:32,  3.36it/s, grad_norm=0.912, loss_final=1.04, loss_mean=0.981, loss_mean_cls=0.06[[34m2025-10-04 11:56:36[39m] Step: 202, Training Logs: loss_final: 1.055266, loss_mean: 0.994752, loss_mean_cls: 0.060514, grad_norm: 1.177199
Steps:   0%| | 203/1000000 [01:05<82:07:56,  3.38it/s, grad_norm=1.18, loss_final=1.06, loss_mean=0.995, loss_mean_cls=0.060[[34m2025-10-04 11:56:36[39m] Step: 203, Training Logs: loss_final: 1.062017, loss_mean: 1.000923, loss_mean_cls: 0.061094, grad_norm: 1.122271
Steps:   0%|   | 204/1000000 [01:05<81:54:32,  3.39it/s, grad_norm=1.12, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0611][[34m2025-10-04 11:56:36[39m] Step: 204, Training Logs: loss_final: 1.066347, loss_mean: 1.004688, loss_mean_cls: 0.061658, grad_norm: 0.839102
Steps:   0%|  | 205/1000000 [01:06<83:45:05,  3.32it/s, grad_norm=0.839, loss_final=1.07, loss_mean=1, loss_mean_cls=0.0617][[34m2025-10-04 11:56:37[39m] Step: 205, Training Logs: loss_final: 1.075681, loss_mean: 1.014168, loss_mean_cls: 0.061513, grad_norm: 0.925992
Steps:   0%| | 206/1000000 [01:06<82:48:36,  3.35it/s, grad_norm=0.926, loss_final=1.08, loss_mean=1.01, loss_mean_cls=0.061[[34m2025-10-04 11:56:37[39m] Step: 206, Training Logs: loss_final: 1.073047, loss_mean: 1.012701, loss_mean_cls: 0.060346, grad_norm: 1.353738
Steps:   0%| | 207/1000000 [01:06<82:18:02,  3.37it/s, grad_norm=1.35, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.0603[[34m2025-10-04 11:56:37[39m] Step: 207, Training Logs: loss_final: 1.057695, loss_mean: 0.996870, loss_mean_cls: 0.060825, grad_norm: 0.975990
Steps:   0%| | 207/1000000 [01:06<82:18:02,  3.37it/s, grad_norm=0.976, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.06
Steps:   0%| | 209/1000000 [01:07<81:19:38,  3.41it/s, grad_norm=0.73, loss_final=1.04, loss_mean=0.982, loss_mean_cls=0.061[[34m2025-10-04 11:56:38[39m] Step: 209, Training Logs: loss_final: 1.054044, loss_mean: 0.993048, loss_mean_cls: 0.060995, grad_norm: 0.969638
Steps:   0%| | 210/1000000 [01:07<81:00:57,  3.43it/s, grad_norm=0.97, loss_final=1.05, loss_mean=0.993, loss_mean_cls=0.061[[34m2025-10-04 11:56:38[39m] Step: 210, Training Logs: loss_final: 1.064327, loss_mean: 1.003960, loss_mean_cls: 0.060368, grad_norm: 0.805270
Steps:   0%|  | 211/1000000 [01:07<80:32:51,  3.45it/s, grad_norm=0.805, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0604][[34m2025-10-04 11:56:38[39m] Step: 211, Training Logs: loss_final: 1.057443, loss_mean: 0.996940, loss_mean_cls: 0.060504, grad_norm: 0.852624
Steps:   0%| | 212/1000000 [01:08<83:54:07,  3.31it/s, grad_norm=0.853, loss_final=1.06, loss_mean=0.997, loss_mean_cls=0.06[[34m2025-10-04 11:56:39[39m] Step: 212, Training Logs: loss_final: 1.050336, loss_mean: 0.991059, loss_mean_cls: 0.059278, grad_norm: 1.228341
Steps:   0%| | 213/1000000 [01:08<82:37:32,  3.36it/s, grad_norm=1.23, loss_final=1.05, loss_mean=0.991, loss_mean_cls=0.059[[34m2025-10-04 11:56:39[39m] Step: 213, Training Logs: loss_final: 1.074598, loss_mean: 1.014764, loss_mean_cls: 0.059835, grad_norm: 0.891025
Steps:   0%| | 214/1000000 [01:08<81:41:33,  3.40it/s, grad_norm=0.891, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.059[[34m2025-10-04 11:56:39[39m] Step: 214, Training Logs: loss_final: 1.060872, loss_mean: 1.000547, loss_mean_cls: 0.060324, grad_norm: 0.937589
Steps:   0%|  | 214/1000000 [01:08<81:41:33,  3.40it/s, grad_norm=0.938, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0603]
Steps:   0%|   | 216/1000000 [01:09<82:32:30,  3.36it/s, grad_norm=1.23, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0609][[34m2025-10-04 11:56:40[39m] Step: 216, Training Logs: loss_final: 1.071736, loss_mean: 1.010778, loss_mean_cls: 0.060959, grad_norm: 0.941281
Steps:   0%| | 217/1000000 [01:09<83:54:39,  3.31it/s, grad_norm=0.941, loss_final=1.07, loss_mean=1.01, loss_mean_cls=0.061[[34m2025-10-04 11:56:40[39m] Step: 217, Training Logs: loss_final: 1.064877, loss_mean: 1.004710, loss_mean_cls: 0.060167, grad_norm: 1.662890
Steps:   0%|   | 218/1000000 [01:09<84:03:24,  3.30it/s, grad_norm=1.66, loss_final=1.06, loss_mean=1, loss_mean_cls=0.0602][[34m2025-10-04 11:56:41[39m] Step: 218, Training Logs: loss_final: 1.028721, loss_mean: 0.967238, loss_mean_cls: 0.061484, grad_norm: 0.984590
Steps:   0%| | 219/1000000 [01:10<83:40:30,  3.32it/s, grad_norm=0.985, loss_final=1.03, loss_mean=0.967, loss_mean_cls=0.06[[34m2025-10-04 11:56:41[39m] Step: 219, Training Logs: loss_final: 1.047692, loss_mean: 0.986557, loss_mean_cls: 0.061135, grad_norm: 1.321867
Steps:   0%| | 220/1000000 [01:10<82:17:47,  3.37it/s, grad_norm=1.32, loss_final=1.05, loss_mean=0.987, loss_mean_cls=0.061[[34m2025-10-04 11:56:41[39m] Step: 220, Training Logs: loss_final: 1.036246, loss_mean: 0.975751, loss_mean_cls: 0.060495, grad_norm: 0.946382
Steps:   0%| | 220/1000000 [01:10<82:17:47,  3.37it/s, grad_norm=0.946, loss_final=1.04, loss_mean=0.976, loss_mean_cls=0.06
