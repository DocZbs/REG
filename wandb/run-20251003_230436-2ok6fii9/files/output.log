
Steps:   0%|                                                                       | 1/1000000 [00:05<1398:08:50,  5.03s/it][[34m2025-10-03 23:04:48[39m] Generating EMA samples done.
[[34m2025-10-03 23:04:48[39m] Step: 1, Training Logs: loss_final: 1.902024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.100000, grad_norm: 1.201294
Steps:   0%| | 1/1000000 [00:05<1398:08:50,  5.03s/it, deep_loss=0.1, grad_norm=1.2, loss_final=1.9, loss_mean=1.69, loss_me
Steps:   0%| | 3/1000000 [00:07<568:22:13,  2.05s/it, deep_loss=0.0995, grad_norm=0.927, loss_final=1.87, loss_mean=1.68, lo[[34m2025-10-03 23:04:51[39m] Step: 3, Training Logs: loss_final: 1.877029, loss_mean: 1.709284, proj_loss: -0.049267, loss_mean_cls: 0.118187, deep_loss: 0.098825, grad_norm: 0.840508
Steps:   0%| | 4/1000000 [00:08<472:13:00,  1.70s/it, deep_loss=0.0988, grad_norm=0.841, loss_final=1.88, loss_mean=1.71, lo[[34m2025-10-03 23:04:52[39m] Step: 4, Training Logs: loss_final: 1.830649, loss_mean: 1.679409, proj_loss: -0.065037, loss_mean_cls: 0.117800, deep_loss: 0.098478, grad_norm: 0.680785
Steps:   0%| | 5/1000000 [00:09<420:47:38,  1.51s/it, deep_loss=0.0985, grad_norm=0.681, loss_final=1.83, loss_mean=1.68, lo[[34m2025-10-03 23:04:53[39m] Step: 5, Training Logs: loss_final: 1.825804, loss_mean: 1.685238, proj_loss: -0.076018, loss_mean_cls: 0.118603, deep_loss: 0.097981, grad_norm: 0.803953
Steps:   0%| | 6/1000000 [00:10<388:03:14,  1.40s/it, deep_loss=0.098, grad_norm=0.804, loss_final=1.83, loss_mean=1.69, los[[34m2025-10-03 23:04:54[39m] Step: 6, Training Logs: loss_final: 1.799753, loss_mean: 1.666323, proj_loss: -0.082076, loss_mean_cls: 0.117980, deep_loss: 0.097526, grad_norm: 0.770381
Steps:   0%| | 7/1000000 [00:12<368:11:00,  1.33s/it, deep_loss=0.0975, grad_norm=0.77, loss_final=1.8, loss_mean=1.67, loss[[34m2025-10-03 23:04:56[39m] Step: 7, Training Logs: loss_final: 1.782327, loss_mean: 1.656586, proj_loss: -0.088942, loss_mean_cls: 0.117597, deep_loss: 0.097086, grad_norm: 0.845348
Steps:   0%| | 8/1000000 [00:13<356:10:50,  1.28s/it, deep_loss=0.0971, grad_norm=0.845, loss_final=1.78, loss_mean=1.66, lo[[34m2025-10-03 23:04:57[39m] Step: 8, Training Logs: loss_final: 1.773892, loss_mean: 1.653283, proj_loss: -0.093514, loss_mean_cls: 0.117813, deep_loss: 0.096310, grad_norm: 0.865210
Steps:   0%| | 9/1000000 [00:14<347:57:50,  1.25s/it, deep_loss=0.0963, grad_norm=0.865, loss_final=1.77, loss_mean=1.65, lo[[34m2025-10-03 23:04:58[39m] Step: 9, Training Logs: loss_final: 1.754387, loss_mean: 1.638149, proj_loss: -0.097993, loss_mean_cls: 0.118029, deep_loss: 0.096203, grad_norm: 0.840690
Steps:   0%| | 10/1000000 [00:15<341:23:47,  1.23s/it, deep_loss=0.0962, grad_norm=0.841, loss_final=1.75, loss_mean=1.64, l[[34m2025-10-03 23:04:59[39m] Step: 10, Training Logs: loss_final: 1.752085, loss_mean: 1.638584, proj_loss: -0.099191, loss_mean_cls: 0.117742, deep_loss: 0.094950, grad_norm: 1.076593
Steps:   0%| | 11/1000000 [00:16<337:05:22,  1.21s/it, deep_loss=0.095, grad_norm=1.08, loss_final=1.75, loss_mean=1.64, los[[34m2025-10-03 23:05:00[39m] Step: 11, Training Logs: loss_final: 1.720874, loss_mean: 1.612691, proj_loss: -0.104842, loss_mean_cls: 0.117659, deep_loss: 0.095366, grad_norm: 1.007834
Steps:   0%| | 12/1000000 [00:18<334:30:34,  1.20s/it, deep_loss=0.0954, grad_norm=1.01, loss_final=1.72, loss_mean=1.61, lo[[34m2025-10-03 23:05:01[39m] Step: 12, Training Logs: loss_final: 1.709930, loss_mean: 1.605673, proj_loss: -0.107589, loss_mean_cls: 0.117555, deep_loss: 0.094291, grad_norm: 1.120335
Steps:   0%| | 13/1000000 [00:19<333:30:40,  1.20s/it, deep_loss=0.0943, grad_norm=1.12, loss_final=1.71, loss_mean=1.61, lo[[34m2025-10-03 23:05:03[39m] Step: 13, Training Logs: loss_final: 1.670613, loss_mean: 1.572010, proj_loss: -0.112542, loss_mean_cls: 0.117361, deep_loss: 0.093784, grad_norm: 0.922903
Steps:   0%| | 14/1000000 [00:20<301:49:55,  1.09s/it, deep_loss=0.0938, grad_norm=0.923, loss_final=1.67, loss_mean=1.57, l[[34m2025-10-03 23:05:03[39m] Step: 14, Training Logs: loss_final: 1.681389, loss_mean: 1.589428, proj_loss: -0.117143, loss_mean_cls: 0.117198, deep_loss: 0.091905, grad_norm: 1.001851
Steps:   0%| | 15/1000000 [00:20<277:21:12,  1.00it/s, deep_loss=0.0919, grad_norm=1, loss_final=1.68, loss_mean=1.59, loss_[[34m2025-10-03 23:05:04[39m] Step: 15, Training Logs: loss_final: 1.653536, loss_mean: 1.563381, proj_loss: -0.118377, loss_mean_cls: 0.117349, deep_loss: 0.091182, grad_norm: 0.847277
Steps:   0%| | 16/1000000 [00:21<260:48:11,  1.07it/s, deep_loss=0.0912, grad_norm=0.847, loss_final=1.65, loss_mean=1.56, l[[34m2025-10-03 23:05:05[39m] Step: 16, Training Logs: loss_final: 1.655532, loss_mean: 1.569300, proj_loss: -0.120528, loss_mean_cls: 0.116779, deep_loss: 0.089980, grad_norm: 0.963789
Steps:   0%| | 17/1000000 [00:22<248:48:44,  1.12it/s, deep_loss=0.09, grad_norm=0.964, loss_final=1.66, loss_mean=1.57, los[[34m2025-10-03 23:05:06[39m] Step: 17, Training Logs: loss_final: 1.620840, loss_mean: 1.538550, proj_loss: -0.125311, loss_mean_cls: 0.117408, deep_loss: 0.090193, grad_norm: 0.791377
Steps:   0%| | 18/1000000 [00:23<260:13:37,  1.07it/s, deep_loss=0.0902, grad_norm=0.791, loss_final=1.62, loss_mean=1.54, l[[34m2025-10-03 23:05:07[39m] Step: 18, Training Logs: loss_final: 1.612538, loss_mean: 1.532674, proj_loss: -0.126523, loss_mean_cls: 0.116738, deep_loss: 0.089649, grad_norm: 0.807401
Steps:   0%| | 19/1000000 [00:24<281:48:48,  1.01s/it, deep_loss=0.0896, grad_norm=0.807, loss_final=1.61, loss_mean=1.53, l[[34m2025-10-03 23:05:08[39m] Step: 19, Training Logs: loss_final: 1.612209, loss_mean: 1.537963, proj_loss: -0.131211, loss_mean_cls: 0.117219, deep_loss: 0.088238, grad_norm: 0.784704
Steps:   0%| | 20/1000000 [00:25<296:51:12,  1.07s/it, deep_loss=0.0882, grad_norm=0.785, loss_final=1.61, loss_mean=1.54, l[[34m2025-10-03 23:05:09[39m] Step: 20, Training Logs: loss_final: 1.603040, loss_mean: 1.530737, proj_loss: -0.131496, loss_mean_cls: 0.117110, deep_loss: 0.086690, grad_norm: 0.797378
Steps:   0%| | 21/1000000 [00:27<306:48:02,  1.10s/it, deep_loss=0.0867, grad_norm=0.797, loss_final=1.6, loss_mean=1.53, lo[[34m2025-10-03 23:05:10[39m] Step: 21, Training Logs: loss_final: 1.583843, loss_mean: 1.512265, proj_loss: -0.131868, loss_mean_cls: 0.116646, deep_loss: 0.086800, grad_norm: 0.856522
Steps:   0%| | 22/1000000 [00:28<313:14:25,  1.13s/it, deep_loss=0.0868, grad_norm=0.857, loss_final=1.58, loss_mean=1.51, l[[34m2025-10-03 23:05:12[39m] Step: 22, Training Logs: loss_final: 1.558110, loss_mean: 1.493526, proj_loss: -0.137549, loss_mean_cls: 0.116489, deep_loss: 0.085644, grad_norm: 0.741418
Steps:   0%| | 23/1000000 [00:29<317:24:19,  1.14s/it, deep_loss=0.0856, grad_norm=0.741, loss_final=1.56, loss_mean=1.49, l[[34m2025-10-03 23:05:13[39m] Step: 23, Training Logs: loss_final: 1.546496, loss_mean: 1.481584, proj_loss: -0.136668, loss_mean_cls: 0.117315, deep_loss: 0.084264, grad_norm: 0.839201
Steps:   0%| | 24/1000000 [00:30<320:43:45,  1.15s/it, deep_loss=0.0843, grad_norm=0.839, loss_final=1.55, loss_mean=1.48, l[[34m2025-10-03 23:05:14[39m] Step: 24, Training Logs: loss_final: 1.527127, loss_mean: 1.464984, proj_loss: -0.139157, loss_mean_cls: 0.116490, deep_loss: 0.084810, grad_norm: 0.802357
Steps:   0%| | 25/1000000 [00:31<322:36:11,  1.16s/it, deep_loss=0.0848, grad_norm=0.802, loss_final=1.53, loss_mean=1.46, l[[34m2025-10-03 23:05:15[39m] Step: 25, Training Logs: loss_final: 1.519397, loss_mean: 1.465259, proj_loss: -0.141367, loss_mean_cls: 0.115848, deep_loss: 0.079656, grad_norm: 0.693324
Steps:   0%| | 26/1000000 [00:32<324:25:11,  1.17s/it, deep_loss=0.0797, grad_norm=0.693, loss_final=1.52, loss_mean=1.47, l[[34m2025-10-03 23:05:16[39m] Step: 26, Training Logs: loss_final: 1.480685, loss_mean: 1.425989, proj_loss: -0.140093, loss_mean_cls: 0.116582, deep_loss: 0.078207, grad_norm: 0.846208
Steps:   0%| | 27/1000000 [00:34<326:01:45,  1.17s/it, deep_loss=0.0782, grad_norm=0.846, loss_final=1.48, loss_mean=1.43, l[[34m2025-10-03 23:05:18[39m] Step: 27, Training Logs: loss_final: 1.484847, loss_mean: 1.436943, proj_loss: -0.141612, loss_mean_cls: 0.115959, deep_loss: 0.073556, grad_norm: 0.607149
Steps:   0%| | 28/1000000 [00:35<327:21:41,  1.18s/it, deep_loss=0.0736, grad_norm=0.607, loss_final=1.48, loss_mean=1.44, l[[34m2025-10-03 23:05:19[39m] Step: 28, Training Logs: loss_final: 1.470988, loss_mean: 1.422449, proj_loss: -0.142864, loss_mean_cls: 0.116363, deep_loss: 0.075039, grad_norm: 0.684397
Steps:   0%| | 29/1000000 [00:36<328:11:38,  1.18s/it, deep_loss=0.075, grad_norm=0.684, loss_final=1.47, loss_mean=1.42, lo[[34m2025-10-03 23:05:20[39m] Step: 29, Training Logs: loss_final: 1.450530, loss_mean: 1.404647, proj_loss: -0.141957, loss_mean_cls: 0.116772, deep_loss: 0.071067, grad_norm: 0.634868
Steps:   0%| | 30/1000000 [00:37<328:46:15,  1.18s/it, deep_loss=0.0711, grad_norm=0.635, loss_final=1.45, loss_mean=1.4, lo[[34m2025-10-03 23:05:21[39m] Step: 30, Training Logs: loss_final: 1.458673, loss_mean: 1.419035, proj_loss: -0.143657, loss_mean_cls: 0.115824, deep_loss: 0.067471, grad_norm: 1.009782
Steps:   0%| | 31/1000000 [00:38<328:19:03,  1.18s/it, deep_loss=0.0675, grad_norm=1.01, loss_final=1.46, loss_mean=1.42, lo[[34m2025-10-03 23:05:22[39m] Step: 31, Training Logs: loss_final: 1.449826, loss_mean: 1.412435, proj_loss: -0.145611, loss_mean_cls: 0.115405, deep_loss: 0.067597, grad_norm: 0.896182
Steps:   0%| | 32/1000000 [00:40<328:47:33,  1.18s/it, deep_loss=0.0676, grad_norm=0.896, loss_final=1.45, loss_mean=1.41, l[[34m2025-10-03 23:05:23[39m] Step: 32, Training Logs: loss_final: 1.434315, loss_mean: 1.402035, proj_loss: -0.145542, loss_mean_cls: 0.114887, deep_loss: 0.062936, grad_norm: 0.631332
Steps:   0%| | 33/1000000 [00:41<329:06:09,  1.18s/it, deep_loss=0.0629, grad_norm=0.631, loss_final=1.43, loss_mean=1.4, lo[[34m2025-10-03 23:05:25[39m] Step: 33, Training Logs: loss_final: 1.428452, loss_mean: 1.400650, proj_loss: -0.146595, loss_mean_cls: 0.115324, deep_loss: 0.059072, grad_norm: 0.939080
Steps:   0%| | 34/1000000 [00:42<329:25:37,  1.19s/it, deep_loss=0.0591, grad_norm=0.939, loss_final=1.43, loss_mean=1.4, lo[[34m2025-10-03 23:05:26[39m] Step: 34, Training Logs: loss_final: 1.422936, loss_mean: 1.393247, proj_loss: -0.143966, loss_mean_cls: 0.115102, deep_loss: 0.058553, grad_norm: 1.240394
Steps:   0%| | 35/1000000 [00:43<329:19:38,  1.19s/it, deep_loss=0.0586, grad_norm=1.24, loss_final=1.42, loss_mean=1.39, lo[[34m2025-10-03 23:05:27[39m] Step: 35, Training Logs: loss_final: 1.398411, loss_mean: 1.375475, proj_loss: -0.147098, loss_mean_cls: 0.115447, deep_loss: 0.054588, grad_norm: 0.724550
Steps:   0%| | 36/1000000 [00:44<329:52:24,  1.19s/it, deep_loss=0.0546, grad_norm=0.725, loss_final=1.4, loss_mean=1.38, lo[[34m2025-10-03 23:05:28[39m] Step: 36, Training Logs: loss_final: 1.407176, loss_mean: 1.381509, proj_loss: -0.145590, loss_mean_cls: 0.115222, deep_loss: 0.056035, grad_norm: 1.040645
Steps:   0%| | 37/1000000 [00:45<330:12:11,  1.19s/it, deep_loss=0.056, grad_norm=1.04, loss_final=1.41, loss_mean=1.38, los[[34m2025-10-03 23:05:29[39m] Step: 37, Training Logs: loss_final: 1.371001, loss_mean: 1.355849, proj_loss: -0.146587, loss_mean_cls: 0.114654, deep_loss: 0.047085, grad_norm: 0.919454
Steps:   0%| | 38/1000000 [00:47<329:21:49,  1.19s/it, deep_loss=0.0471, grad_norm=0.919, loss_final=1.37, loss_mean=1.36, l[[34m2025-10-03 23:05:31[39m] Step: 38, Training Logs: loss_final: 1.385879, loss_mean: 1.373019, proj_loss: -0.145885, loss_mean_cls: 0.115545, deep_loss: 0.043199, grad_norm: 0.906609
Steps:   0%| | 39/1000000 [00:48<329:12:36,  1.19s/it, deep_loss=0.0432, grad_norm=0.907, loss_final=1.39, loss_mean=1.37, l[[34m2025-10-03 23:05:32[39m] Step: 39, Training Logs: loss_final: 1.370002, loss_mean: 1.362871, proj_loss: -0.146712, loss_mean_cls: 0.114192, deep_loss: 0.039650, grad_norm: 0.714776
Steps:   0%| | 40/1000000 [00:49<328:48:42,  1.18s/it, deep_loss=0.0397, grad_norm=0.715, loss_final=1.37, loss_mean=1.36, l[[34m2025-10-03 23:05:33[39m] Step: 40, Training Logs: loss_final: 1.340740, loss_mean: 1.342579, proj_loss: -0.148973, loss_mean_cls: 0.115496, deep_loss: 0.031638, grad_norm: 0.808289
Steps:   0%| | 41/1000000 [00:50<330:27:50,  1.19s/it, deep_loss=0.0316, grad_norm=0.808, loss_final=1.34, loss_mean=1.34, l[[34m2025-10-03 23:05:34[39m] Step: 41, Training Logs: loss_final: 1.317815, loss_mean: 1.326562, proj_loss: -0.148219, loss_mean_cls: 0.114949, deep_loss: 0.024523, grad_norm: 0.723238
Steps:   0%| | 42/1000000 [00:51<329:49:07,  1.19s/it, deep_loss=0.0245, grad_norm=0.723, loss_final=1.32, loss_mean=1.33, l[[34m2025-10-03 23:05:35[39m] Step: 42, Training Logs: loss_final: 1.306310, loss_mean: 1.322500, proj_loss: -0.147561, loss_mean_cls: 0.114272, deep_loss: 0.017099, grad_norm: 0.744165
Steps:   0%| | 43/1000000 [00:53<329:17:57,  1.19s/it, deep_loss=0.0171, grad_norm=0.744, loss_final=1.31, loss_mean=1.32, l[[34m2025-10-03 23:05:37[39m] Step: 43, Training Logs: loss_final: 1.296169, loss_mean: 1.317929, proj_loss: -0.148304, loss_mean_cls: 0.114454, deep_loss: 0.012090, grad_norm: 1.032775
Steps:   0%| | 44/1000000 [00:54<329:32:53,  1.19s/it, deep_loss=0.0121, grad_norm=1.03, loss_final=1.3, loss_mean=1.32, los[[34m2025-10-03 23:05:38[39m] Step: 44, Training Logs: loss_final: 1.354898, loss_mean: 1.383155, proj_loss: -0.147305, loss_mean_cls: 0.114107, deep_loss: 0.004941, grad_norm: 3.292501
Steps:   0%| | 45/1000000 [00:55<315:58:59,  1.14s/it, deep_loss=0.00494, grad_norm=3.29, loss_final=1.35, loss_mean=1.38, l[[34m2025-10-03 23:05:39[39m] Step: 45, Training Logs: loss_final: 1.310376, loss_mean: 1.340837, proj_loss: -0.147035, loss_mean_cls: 0.114585, deep_loss: 0.001988, grad_norm: 2.493108
Steps:   0%| | 46/1000000 [00:56<287:39:47,  1.04s/it, deep_loss=0.00199, grad_norm=2.49, loss_final=1.31, loss_mean=1.34, l[[34m2025-10-03 23:05:40[39m] Step: 46, Training Logs: loss_final: 1.336595, loss_mean: 1.385852, proj_loss: -0.148774, loss_mean_cls: 0.113800, deep_loss: -0.014282, grad_norm: 3.304622
Steps:   0%| | 47/1000000 [00:56<268:36:22,  1.03it/s, deep_loss=-0.0143, grad_norm=3.3, loss_final=1.34, loss_mean=1.39, lo[[34m2025-10-03 23:05:40[39m] Step: 47, Training Logs: loss_final: 1.291237, loss_mean: 1.340934, proj_loss: -0.147723, loss_mean_cls: 0.113512, deep_loss: -0.015486, grad_norm: 1.813417
Steps:   0%| | 48/1000000 [00:57<254:00:28,  1.09it/s, deep_loss=-0.0155, grad_norm=1.81, loss_final=1.29, loss_mean=1.34, l[[34m2025-10-03 23:05:41[39m] Step: 48, Training Logs: loss_final: 1.293252, loss_mean: 1.351055, proj_loss: -0.148353, loss_mean_cls: 0.113538, deep_loss: -0.022988, grad_norm: 2.027486
Steps:   0%| | 49/1000000 [00:58<252:56:42,  1.10it/s, deep_loss=-0.023, grad_norm=2.03, loss_final=1.29, loss_mean=1.35, lo[[34m2025-10-03 23:05:42[39m] Step: 49, Training Logs: loss_final: 1.250508, loss_mean: 1.314723, proj_loss: -0.148412, loss_mean_cls: 0.114515, deep_loss: -0.030318, grad_norm: 2.163635
Steps:   0%| | 50/1000000 [00:59<276:48:26,  1.00it/s, deep_loss=-0.0303, grad_norm=2.16, loss_final=1.25, loss_mean=1.31, l[[34m2025-10-03 23:05:43[39m] Step: 50, Training Logs: loss_final: 1.257155, loss_mean: 1.318173, proj_loss: -0.148348, loss_mean_cls: 0.114212, deep_loss: -0.026882, grad_norm: 1.710867
Steps:   0%| | 51/1000000 [01:01<292:48:08,  1.05s/it, deep_loss=-0.0269, grad_norm=1.71, loss_final=1.26, loss_mean=1.32, l[[34m2025-10-03 23:05:44[39m] Step: 51, Training Logs: loss_final: 1.222571, loss_mean: 1.304483, proj_loss: -0.147855, loss_mean_cls: 0.114167, deep_loss: -0.048224, grad_norm: 1.623305
Steps:   0%| | 52/1000000 [01:02<304:51:29,  1.10s/it, deep_loss=-0.0482, grad_norm=1.62, loss_final=1.22, loss_mean=1.3, lo[[34m2025-10-03 23:05:46[39m] Step: 52, Training Logs: loss_final: 1.192844, loss_mean: 1.270149, proj_loss: -0.150086, loss_mean_cls: 0.113547, deep_loss: -0.040766, grad_norm: 1.592470
Steps:   0%| | 53/1000000 [01:03<313:54:57,  1.13s/it, deep_loss=-0.0408, grad_norm=1.59, loss_final=1.19, loss_mean=1.27, l[[34m2025-10-03 23:05:47[39m] Step: 53, Training Logs: loss_final: 1.187141, loss_mean: 1.284548, proj_loss: -0.148843, loss_mean_cls: 0.113455, deep_loss: -0.062018, grad_norm: 1.858515
Steps:   0%| | 54/1000000 [01:04<318:45:02,  1.15s/it, deep_loss=-0.062, grad_norm=1.86, loss_final=1.19, loss_mean=1.28, lo[[34m2025-10-03 23:05:48[39m] Step: 54, Training Logs: loss_final: 1.196565, loss_mean: 1.318691, proj_loss: -0.148529, loss_mean_cls: 0.113786, deep_loss: -0.087383, grad_norm: 2.234822
Steps:   0%| | 55/1000000 [01:05<323:49:20,  1.17s/it, deep_loss=-0.0874, grad_norm=2.23, loss_final=1.2, loss_mean=1.32, lo[[34m2025-10-03 23:05:49[39m] Step: 55, Training Logs: loss_final: 1.156742, loss_mean: 1.265414, proj_loss: -0.150504, loss_mean_cls: 0.114070, deep_loss: -0.072238, grad_norm: 1.710080
Steps:   0%| | 56/1000000 [01:06<325:13:47,  1.17s/it, deep_loss=-0.0722, grad_norm=1.71, loss_final=1.16, loss_mean=1.27, l[[34m2025-10-03 23:05:50[39m] Step: 56, Training Logs: loss_final: 1.192491, loss_mean: 1.310779, proj_loss: -0.150168, loss_mean_cls: 0.113395, deep_loss: -0.081515, grad_norm: 3.271585
Steps:   0%| | 57/1000000 [01:08<327:17:35,  1.18s/it, deep_loss=-0.0815, grad_norm=3.27, loss_final=1.19, loss_mean=1.31, l[[34m2025-10-03 23:05:52[39m] Step: 57, Training Logs: loss_final: 1.117325, loss_mean: 1.280777, proj_loss: -0.151112, loss_mean_cls: 0.113808, deep_loss: -0.126148, grad_norm: 1.887957
Steps:   0%| | 58/1000000 [01:09<329:01:03,  1.18s/it, deep_loss=-0.126, grad_norm=1.89, loss_final=1.12, loss_mean=1.28, lo[[34m2025-10-03 23:05:53[39m] Step: 58, Training Logs: loss_final: 1.129091, loss_mean: 1.283029, proj_loss: -0.149972, loss_mean_cls: 0.113836, deep_loss: -0.117802, grad_norm: 3.104430
Steps:   0%| | 59/1000000 [01:10<329:00:31,  1.18s/it, deep_loss=-0.118, grad_norm=3.1, loss_final=1.13, loss_mean=1.28, los[[34m2025-10-03 23:05:54[39m] Step: 59, Training Logs: loss_final: 1.065582, loss_mean: 1.223342, proj_loss: -0.148960, loss_mean_cls: 0.114010, deep_loss: -0.122810, grad_norm: 2.037977
Steps:   0%| | 60/1000000 [01:11<328:57:33,  1.18s/it, deep_loss=-0.123, grad_norm=2.04, loss_final=1.07, loss_mean=1.22, lo[[34m2025-10-03 23:05:55[39m] Step: 60, Training Logs: loss_final: 1.082296, loss_mean: 1.260882, proj_loss: -0.150198, loss_mean_cls: 0.113150, deep_loss: -0.141538, grad_norm: 2.477546
Steps:   0%| | 61/1000000 [01:12<329:20:31,  1.19s/it, deep_loss=-0.142, grad_norm=2.48, loss_final=1.08, loss_mean=1.26, lo[[34m2025-10-03 23:05:56[39m] Step: 61, Training Logs: loss_final: 0.989685, loss_mean: 1.205510, proj_loss: -0.151818, loss_mean_cls: 0.114308, deep_loss: -0.178316, grad_norm: 2.224184
Steps:   0%| | 62/1000000 [01:14<329:23:18,  1.19s/it, deep_loss=-0.178, grad_norm=2.22, loss_final=0.99, loss_mean=1.21, lo[[34m2025-10-03 23:05:58[39m] Step: 62, Training Logs: loss_final: 1.071630, loss_mean: 1.283555, proj_loss: -0.150121, loss_mean_cls: 0.113743, deep_loss: -0.175546, grad_norm: 4.477008
Steps:   0%| | 63/1000000 [01:15<330:07:11,  1.19s/it, deep_loss=-0.176, grad_norm=4.48, loss_final=1.07, loss_mean=1.28, lo[[34m2025-10-03 23:05:59[39m] Step: 63, Training Logs: loss_final: 0.961323, loss_mean: 1.194875, proj_loss: -0.149631, loss_mean_cls: 0.115228, deep_loss: -0.199149, grad_norm: 3.271157
Steps:   0%| | 64/1000000 [01:16<329:14:06,  1.19s/it, deep_loss=-0.199, grad_norm=3.27, loss_final=0.961, loss_mean=1.19, l[[34m2025-10-03 23:06:00[39m] Step: 64, Training Logs: loss_final: 1.006175, loss_mean: 1.245714, proj_loss: -0.148990, loss_mean_cls: 0.114216, deep_loss: -0.204765, grad_norm: 3.917584
Steps:   0%| | 65/1000000 [01:17<329:33:36,  1.19s/it, deep_loss=-0.205, grad_norm=3.92, loss_final=1.01, loss_mean=1.25, lo[[34m2025-10-03 23:06:01[39m] Step: 65, Training Logs: loss_final: 1.029811, loss_mean: 1.312533, proj_loss: -0.150993, loss_mean_cls: 0.112707, deep_loss: -0.244436, grad_norm: 4.137317
Steps:   0%| | 66/1000000 [01:18<329:17:47,  1.19s/it, deep_loss=-0.244, grad_norm=4.14, loss_final=1.03, loss_mean=1.31, lo[[34m2025-10-03 23:06:02[39m] Step: 66, Training Logs: loss_final: 0.922008, loss_mean: 1.239045, proj_loss: -0.150149, loss_mean_cls: 0.113690, deep_loss: -0.280579, grad_norm: 3.112206
Steps:   0%| | 67/1000000 [01:20<328:19:56,  1.18s/it, deep_loss=-0.281, grad_norm=3.11, loss_final=0.922, loss_mean=1.24, l[[34m2025-10-03 23:06:03[39m] Step: 67, Training Logs: loss_final: 0.917635, loss_mean: 1.239527, proj_loss: -0.151237, loss_mean_cls: 0.113822, deep_loss: -0.284477, grad_norm: 3.519349
Steps:   0%| | 68/1000000 [01:21<328:37:01,  1.18s/it, deep_loss=-0.284, grad_norm=3.52, loss_final=0.918, loss_mean=1.24, l[[34m2025-10-03 23:06:05[39m] Step: 68, Training Logs: loss_final: 0.878624, loss_mean: 1.241147, proj_loss: -0.150096, loss_mean_cls: 0.113344, deep_loss: -0.325771, grad_norm: 3.693986
Steps:   0%| | 69/1000000 [01:22<328:36:48,  1.18s/it, deep_loss=-0.326, grad_norm=3.69, loss_final=0.879, loss_mean=1.24, l[[34m2025-10-03 23:06:06[39m] Step: 69, Training Logs: loss_final: 0.862911, loss_mean: 1.240267, proj_loss: -0.148401, loss_mean_cls: 0.114355, deep_loss: -0.343309, grad_norm: 3.787580
Steps:   0%| | 70/1000000 [01:23<328:41:39,  1.18s/it, deep_loss=-0.343, grad_norm=3.79, loss_final=0.863, loss_mean=1.24, l[[34m2025-10-03 23:06:07[39m] Step: 70, Training Logs: loss_final: 0.760810, loss_mean: 1.213897, proj_loss: -0.151515, loss_mean_cls: 0.113868, deep_loss: -0.415440, grad_norm: 2.889687
Steps:   0%| | 71/1000000 [01:24<327:51:28,  1.18s/it, deep_loss=-0.415, grad_norm=2.89, loss_final=0.761, loss_mean=1.21, l[[34m2025-10-03 23:06:08[39m] Step: 71, Training Logs: loss_final: 0.827964, loss_mean: 1.294064, proj_loss: -0.150753, loss_mean_cls: 0.113886, deep_loss: -0.429232, grad_norm: 4.400712
Steps:   0%| | 72/1000000 [01:25<328:29:41,  1.18s/it, deep_loss=-0.429, grad_norm=4.4, loss_final=0.828, loss_mean=1.29, lo[[34m2025-10-03 23:06:09[39m] Step: 72, Training Logs: loss_final: 0.758901, loss_mean: 1.247203, proj_loss: -0.150346, loss_mean_cls: 0.113535, deep_loss: -0.451491, grad_norm: 4.418978
Steps:   0%| | 73/1000000 [01:27<328:00:55,  1.18s/it, deep_loss=-0.451, grad_norm=4.42, loss_final=0.759, loss_mean=1.25, l[[34m2025-10-03 23:06:11[39m] Step: 73, Training Logs: loss_final: 0.686680, loss_mean: 1.239139, proj_loss: -0.148946, loss_mean_cls: 0.114436, deep_loss: -0.517950, grad_norm: 4.585405
Steps:   0%| | 74/1000000 [01:28<327:47:37,  1.18s/it, deep_loss=-0.518, grad_norm=4.59, loss_final=0.687, loss_mean=1.24, l[[34m2025-10-03 23:06:12[39m] Step: 74, Training Logs: loss_final: 0.672887, loss_mean: 1.233650, proj_loss: -0.149346, loss_mean_cls: 0.114535, deep_loss: -0.525953, grad_norm: 4.978184
Steps:   0%| | 75/1000000 [01:29<327:35:54,  1.18s/it, deep_loss=-0.526, grad_norm=4.98, loss_final=0.673, loss_mean=1.23, l[[34m2025-10-03 23:06:13[39m] Step: 75, Training Logs: loss_final: 0.639302, loss_mean: 1.237877, proj_loss: -0.148006, loss_mean_cls: 0.115329, deep_loss: -0.565898, grad_norm: 6.743974
Steps:   0%| | 76/1000000 [01:30<327:15:44,  1.18s/it, deep_loss=-0.566, grad_norm=6.74, loss_final=0.639, loss_mean=1.24, l[[34m2025-10-03 23:06:14[39m] Step: 76, Training Logs: loss_final: 0.533099, loss_mean: 1.231902, proj_loss: -0.147714, loss_mean_cls: 0.115307, deep_loss: -0.666397, grad_norm: 6.535664
Steps:   0%| | 77/1000000 [01:31<295:33:51,  1.06s/it, deep_loss=-0.666, grad_norm=6.54, loss_final=0.533, loss_mean=1.23, l[[34m2025-10-03 23:06:15[39m] Step: 77, Training Logs: loss_final: 0.459256, loss_mean: 1.191731, proj_loss: -0.146775, loss_mean_cls: 0.115142, deep_loss: -0.700842, grad_norm: 4.404571
Steps:   0%| | 78/1000000 [01:32<273:18:01,  1.02it/s, deep_loss=-0.701, grad_norm=4.4, loss_final=0.459, loss_mean=1.19, lo[[34m2025-10-03 23:06:16[39m] Step: 78, Training Logs: loss_final: 0.367810, loss_mean: 1.202308, proj_loss: -0.148359, loss_mean_cls: 0.114663, deep_loss: -0.800802, grad_norm: 4.163177
Steps:   0%| | 79/1000000 [01:33<257:33:41,  1.08it/s, deep_loss=-0.801, grad_norm=4.16, loss_final=0.368, loss_mean=1.2, lo[[34m2025-10-03 23:06:16[39m] Step: 79, Training Logs: loss_final: 0.416441, loss_mean: 1.243629, proj_loss: -0.146808, loss_mean_cls: 0.115140, deep_loss: -0.795521, grad_norm: 6.752977
Steps:   0%| | 80/1000000 [01:34<271:26:08,  1.02it/s, deep_loss=-0.796, grad_norm=6.75, loss_final=0.416, loss_mean=1.24, l[[34m2025-10-03 23:06:18[39m] Step: 80, Training Logs: loss_final: 0.337550, loss_mean: 1.261715, proj_loss: -0.146907, loss_mean_cls: 0.115413, deep_loss: -0.892671, grad_norm: inf
Steps:   0%| | 81/1000000 [01:35<286:37:05,  1.03s/it, deep_loss=-0.893, grad_norm=inf, loss_final=0.338, loss_mean=1.26, lo[[34m2025-10-03 23:06:19[39m] Step: 81, Training Logs: loss_final: 0.380264, loss_mean: 1.253814, proj_loss: -0.147201, loss_mean_cls: 0.114901, deep_loss: -0.841249, grad_norm: 7.273211
Steps:   0%| | 82/1000000 [01:36<299:09:35,  1.08s/it, deep_loss=-0.841, grad_norm=7.27, loss_final=0.38, loss_mean=1.25, lo[[34m2025-10-03 23:06:20[39m] Step: 82, Training Logs: loss_final: 0.223008, loss_mean: 1.251468, proj_loss: -0.147249, loss_mean_cls: 0.115399, deep_loss: -0.996610, grad_norm: 5.605011
Steps:   0%| | 83/1000000 [01:37<307:58:07,  1.11s/it, deep_loss=-0.997, grad_norm=5.61, loss_final=0.223, loss_mean=1.25, l[[34m2025-10-03 23:06:21[39m] Step: 83, Training Logs: loss_final: 0.224333, loss_mean: 1.255211, proj_loss: -0.149789, loss_mean_cls: 0.115189, deep_loss: -0.996278, grad_norm: 8.110008
Steps:   0%| | 84/1000000 [01:38<314:35:30,  1.13s/it, deep_loss=-0.996, grad_norm=8.11, loss_final=0.224, loss_mean=1.26, l[[34m2025-10-03 23:06:22[39m] Step: 84, Training Logs: loss_final: 0.153652, loss_mean: 1.259145, proj_loss: -0.144544, loss_mean_cls: 0.115236, deep_loss: -1.076186, grad_norm: 11.515309
Steps:   0%| | 85/1000000 [01:40<319:25:19,  1.15s/it, deep_loss=-1.08, grad_norm=11.5, loss_final=0.154, loss_mean=1.26, lo[[34m2025-10-03 23:06:23[39m] Step: 85, Training Logs: loss_final: -0.019097, loss_mean: 1.206092, proj_loss: -0.144961, loss_mean_cls: 0.115217, deep_loss: -1.195446, grad_norm: 6.085372
Steps:   0%| | 86/1000000 [01:41<322:17:57,  1.16s/it, deep_loss=-1.2, grad_norm=6.09, loss_final=-0.0191, loss_mean=1.21, l[[34m2025-10-03 23:06:25[39m] Step: 86, Training Logs: loss_final: -0.121641, loss_mean: 1.177777, proj_loss: -0.145362, loss_mean_cls: 0.116053, deep_loss: -1.270109, grad_norm: 7.196548
Steps:   0%| | 87/1000000 [01:42<323:30:28,  1.16s/it, deep_loss=-1.27, grad_norm=7.2, loss_final=-0.122, loss_mean=1.18, lo[[34m2025-10-03 23:06:26[39m] Step: 87, Training Logs: loss_final: -0.145689, loss_mean: 1.283643, proj_loss: -0.145815, loss_mean_cls: 0.115542, deep_loss: -1.399058, grad_norm: 11.939133
Steps:   0%| | 88/1000000 [01:43<325:11:21,  1.17s/it, deep_loss=-1.4, grad_norm=11.9, loss_final=-0.146, loss_mean=1.28, lo[[34m2025-10-03 23:06:27[39m] Step: 88, Training Logs: loss_final: -0.313085, loss_mean: 1.257423, proj_loss: -0.145228, loss_mean_cls: 0.115557, deep_loss: -1.540837, grad_norm: 9.779236
Steps:   0%| | 89/1000000 [01:44<326:30:21,  1.18s/it, deep_loss=-1.54, grad_norm=9.78, loss_final=-0.313, loss_mean=1.26, l[[34m2025-10-03 23:06:28[39m] Step: 89, Training Logs: loss_final: 0.266000, loss_mean: 1.312029, proj_loss: -0.141933, loss_mean_cls: 0.115812, deep_loss: -1.019907, grad_norm: inf
Steps:   0%| | 90/1000000 [01:45<329:19:17,  1.19s/it, deep_loss=-1.02, grad_norm=inf, loss_final=0.266, loss_mean=1.31, los[[34m2025-10-03 23:06:29[39m] Step: 90, Training Logs: loss_final: 0.227611, loss_mean: 1.351190, proj_loss: -0.143756, loss_mean_cls: 0.115338, deep_loss: -1.095161, grad_norm: 21.595541
Steps:   0%| | 91/1000000 [01:47<330:51:54,  1.19s/it, deep_loss=-1.1, grad_norm=21.6, loss_final=0.228, loss_mean=1.35, los[[34m2025-10-03 23:06:31[39m] Step: 91, Training Logs: loss_final: -0.322495, loss_mean: 1.344686, proj_loss: -0.144259, loss_mean_cls: 0.116194, deep_loss: -1.639116, grad_norm: 14.417957
Steps:   0%| | 92/1000000 [01:48<331:40:35,  1.19s/it, deep_loss=-1.64, grad_norm=14.4, loss_final=-0.322, loss_mean=1.34, l[[34m2025-10-03 23:06:32[39m] Step: 92, Training Logs: loss_final: 0.306094, loss_mean: 1.418845, proj_loss: -0.142137, loss_mean_cls: 0.116728, deep_loss: -1.087343, grad_norm: 24.869564
Steps:   0%| | 93/1000000 [01:49<331:17:37,  1.19s/it, deep_loss=-1.09, grad_norm=24.9, loss_final=0.306, loss_mean=1.42, lo[[34m2025-10-03 23:06:33[39m] Step: 93, Training Logs: loss_final: -0.365245, loss_mean: 1.360397, proj_loss: -0.141479, loss_mean_cls: 0.115140, deep_loss: -1.699303, grad_norm: 17.277479
Steps:   0%| | 94/1000000 [01:50<331:42:57,  1.19s/it, deep_loss=-1.7, grad_norm=17.3, loss_final=-0.365, loss_mean=1.36, lo[[34m2025-10-03 23:06:34[39m] Step: 94, Training Logs: loss_final: 0.221064, loss_mean: 1.453695, proj_loss: -0.139140, loss_mean_cls: 0.116685, deep_loss: -1.210176, grad_norm: 25.841045
Steps:   0%| | 95/1000000 [01:51<332:21:04,  1.20s/it, deep_loss=-1.21, grad_norm=25.8, loss_final=0.221, loss_mean=1.45, lo[[34m2025-10-03 23:06:35[39m] Step: 95, Training Logs: loss_final: -0.174683, loss_mean: 1.387233, proj_loss: -0.136249, loss_mean_cls: 0.115905, deep_loss: -1.541572, grad_norm: 20.367411
