
Steps:   0%|                                                                       | 1/1000000 [00:05<1519:05:29,  5.47s/it][[34m2025-10-03 23:01:31[39m] Generating EMA samples done.
[[34m2025-10-03 23:01:31[39m] Step: 1, Training Logs: loss_final: 1.902024, loss_mean: 1.685077, proj_loss: -0.001562, loss_mean_cls: 0.118509, deep_loss: 0.100000, grad_norm: 2.244669
Steps:   0%| | 2/1000000 [00:06<759:36:00,  2.73s/it, deep_loss=0.1, grad_norm=2.24, loss_final=1.9, loss_mean=1.69, loss_me[[34m2025-10-03 23:01:32[39m] Step: 2, Training Logs: loss_final: 1.856680, loss_mean: 1.682286, proj_loss: -0.027754, loss_mean_cls: 0.117975, deep_loss: 0.084173, grad_norm: 2.136117
Steps:   0%| | 3/1000000 [00:07<517:24:44,  1.86s/it, deep_loss=0.0842, grad_norm=2.14, loss_final=1.86, loss_mean=1.68, los[[34m2025-10-03 23:01:33[39m] Step: 3, Training Logs: loss_final: 1.840682, loss_mean: 1.709359, proj_loss: -0.049180, loss_mean_cls: 0.118187, deep_loss: 0.062316, grad_norm: 2.315531
Steps:   0%| | 4/1000000 [00:07<404:09:59,  1.46s/it, deep_loss=0.0623, grad_norm=2.32, loss_final=1.84, loss_mean=1.71, los[[34m2025-10-03 23:01:34[39m] Step: 4, Training Logs: loss_final: 1.783689, loss_mean: 1.679668, proj_loss: -0.064759, loss_mean_cls: 0.117801, deep_loss: 0.050980, grad_norm: 1.962716
Steps:   0%| | 5/1000000 [00:08<326:52:38,  1.18s/it, deep_loss=0.051, grad_norm=1.96, loss_final=1.78, loss_mean=1.68, loss[[34m2025-10-03 23:01:34[39m] Step: 5, Training Logs: loss_final: 1.763214, loss_mean: 1.685981, proj_loss: -0.075713, loss_mean_cls: 0.118604, deep_loss: 0.034343, grad_norm: 2.220485
Steps:   0%| | 6/1000000 [00:09<253:28:23,  1.10it/s, deep_loss=0.0343, grad_norm=2.22, loss_final=1.76, loss_mean=1.69, los[[34m2025-10-03 23:01:35[39m] Step: 6, Training Logs: loss_final: 1.722666, loss_mean: 1.667782, proj_loss: -0.081707, loss_mean_cls: 0.117982, deep_loss: 0.018610, grad_norm: 2.067180
Steps:   0%| | 7/1000000 [00:09<207:58:55,  1.34it/s, deep_loss=0.0186, grad_norm=2.07, loss_final=1.72, loss_mean=1.67, los[[34m2025-10-03 23:01:35[39m] Step: 7, Training Logs: loss_final: 1.690045, loss_mean: 1.659082, proj_loss: -0.088526, loss_mean_cls: 0.117600, deep_loss: 0.001889, grad_norm: 2.063352
Steps:   0%| | 8/1000000 [00:09<178:18:51,  1.56it/s, deep_loss=0.00189, grad_norm=2.06, loss_final=1.69, loss_mean=1.66, lo[[34m2025-10-03 23:01:36[39m] Step: 8, Training Logs: loss_final: 1.653644, loss_mean: 1.657113, proj_loss: -0.092953, loss_mean_cls: 0.117818, deep_loss: -0.028334, grad_norm: 2.234945
Steps:   0%| | 9/1000000 [00:10<158:00:39,  1.76it/s, deep_loss=-0.0283, grad_norm=2.23, loss_final=1.65, loss_mean=1.66, lo[[34m2025-10-03 23:01:36[39m] Step: 9, Training Logs: loss_final: 1.627937, loss_mean: 1.643938, proj_loss: -0.097129, loss_mean_cls: 0.118039, deep_loss: -0.036911, grad_norm: 2.129354
Steps:   0%| | 10/1000000 [00:10<144:04:51,  1.93it/s, deep_loss=-0.0369, grad_norm=2.13, loss_final=1.63, loss_mean=1.64, l[[34m2025-10-03 23:01:36[39m] Step: 10, Training Logs: loss_final: 1.569122, loss_mean: 1.643963, proj_loss: -0.097844, loss_mean_cls: 0.117759, deep_loss: -0.094756, grad_norm: 2.621488
Steps:   0%| | 11/1000000 [00:11<134:33:00,  2.06it/s, deep_loss=-0.0948, grad_norm=2.62, loss_final=1.57, loss_mean=1.64, l[[34m2025-10-03 23:01:37[39m] Step: 11, Training Logs: loss_final: 1.551509, loss_mean: 1.615961, proj_loss: -0.102589, loss_mean_cls: 0.117684, deep_loss: -0.079547, grad_norm: 2.401214
Steps:   0%| | 12/1000000 [00:11<127:41:38,  2.18it/s, deep_loss=-0.0795, grad_norm=2.4, loss_final=1.55, loss_mean=1.62, lo[[34m2025-10-03 23:01:37[39m] Step: 12, Training Logs: loss_final: 1.489146, loss_mean: 1.607632, proj_loss: -0.104662, loss_mean_cls: 0.117591, deep_loss: -0.131415, grad_norm: 2.854919
Steps:   0%| | 13/1000000 [00:11<123:24:39,  2.25it/s, deep_loss=-0.131, grad_norm=2.85, loss_final=1.49, loss_mean=1.61, lo[[34m2025-10-03 23:01:38[39m] Step: 13, Training Logs: loss_final: 1.425974, loss_mean: 1.576676, proj_loss: -0.108930, loss_mean_cls: 0.117403, deep_loss: -0.159176, grad_norm: 2.884147
Steps:   0%| | 14/1000000 [00:12<120:07:35,  2.31it/s, deep_loss=-0.159, grad_norm=2.88, loss_final=1.43, loss_mean=1.58, lo[[34m2025-10-03 23:01:38[39m] Step: 14, Training Logs: loss_final: 1.339334, loss_mean: 1.582880, proj_loss: -0.112828, loss_mean_cls: 0.117240, deep_loss: -0.247959, grad_norm: 3.489061
Steps:   0%| | 15/1000000 [00:12<118:11:26,  2.35it/s, deep_loss=-0.248, grad_norm=3.49, loss_final=1.34, loss_mean=1.58, lo[[34m2025-10-03 23:01:38[39m] Step: 15, Training Logs: loss_final: 1.273827, loss_mean: 1.582203, proj_loss: -0.113961, loss_mean_cls: 0.117403, deep_loss: -0.311817, grad_norm: 4.093540
Steps:   0%| | 16/1000000 [00:13<117:06:13,  2.37it/s, deep_loss=-0.312, grad_norm=4.09, loss_final=1.27, loss_mean=1.58, lo[[34m2025-10-03 23:01:39[39m] Step: 16, Training Logs: loss_final: 1.215870, loss_mean: 1.599933, proj_loss: -0.115602, loss_mean_cls: 0.116836, deep_loss: -0.385296, grad_norm: 4.942963
Steps:   0%| | 17/1000000 [00:13<146:28:52,  1.90it/s, deep_loss=-0.385, grad_norm=4.94, loss_final=1.22, loss_mean=1.6, los[[34m2025-10-03 23:01:40[39m] Step: 17, Training Logs: loss_final: 1.229781, loss_mean: 1.621525, proj_loss: -0.119935, loss_mean_cls: 0.117474, deep_loss: -0.389283, grad_norm: 4.927518
Steps:   0%| | 18/1000000 [00:14<172:27:14,  1.61it/s, deep_loss=-0.389, grad_norm=4.93, loss_final=1.23, loss_mean=1.62, lo[[34m2025-10-03 23:01:40[39m] Step: 18, Training Logs: loss_final: 1.143746, loss_mean: 1.591568, proj_loss: -0.120350, loss_mean_cls: 0.116792, deep_loss: -0.444264, grad_norm: 5.237954
Steps:   0%| | 19/1000000 [00:15<190:49:21,  1.46it/s, deep_loss=-0.444, grad_norm=5.24, loss_final=1.14, loss_mean=1.59, lo[[34m2025-10-03 23:01:41[39m] Step: 19, Training Logs: loss_final: 1.073285, loss_mean: 1.605798, proj_loss: -0.124584, loss_mean_cls: 0.117283, deep_loss: -0.525212, grad_norm: 5.269878
Steps:   0%| | 20/1000000 [00:16<203:28:55,  1.37it/s, deep_loss=-0.525, grad_norm=5.27, loss_final=1.07, loss_mean=1.61, lo[[34m2025-10-03 23:01:42[39m] Step: 20, Training Logs: loss_final: 0.957967, loss_mean: 1.592136, proj_loss: -0.124034, loss_mean_cls: 0.117202, deep_loss: -0.627336, grad_norm: 5.507720
Steps:   0%| | 21/1000000 [00:17<212:30:53,  1.31it/s, deep_loss=-0.627, grad_norm=5.51, loss_final=0.958, loss_mean=1.59, l[[34m2025-10-03 23:01:43[39m] Step: 21, Training Logs: loss_final: 0.909962, loss_mean: 1.560460, proj_loss: -0.124168, loss_mean_cls: 0.116730, deep_loss: -0.643060, grad_norm: 5.024128
Steps:   0%| | 22/1000000 [00:18<218:16:31,  1.27it/s, deep_loss=-0.643, grad_norm=5.02, loss_final=0.91, loss_mean=1.56, lo[[34m2025-10-03 23:01:44[39m] Step: 22, Training Logs: loss_final: 0.850568, loss_mean: 1.557044, proj_loss: -0.129652, loss_mean_cls: 0.116603, deep_loss: -0.693427, grad_norm: 5.826669
Steps:   0%| | 23/1000000 [00:18<222:17:01,  1.25it/s, deep_loss=-0.693, grad_norm=5.83, loss_final=0.851, loss_mean=1.56, l[[34m2025-10-03 23:01:45[39m] Step: 23, Training Logs: loss_final: 0.772621, loss_mean: 1.557169, proj_loss: -0.129002, loss_mean_cls: 0.117429, deep_loss: -0.772975, grad_norm: 6.096902
Steps:   0%| | 24/1000000 [00:19<224:36:29,  1.24it/s, deep_loss=-0.773, grad_norm=6.1, loss_final=0.773, loss_mean=1.56, lo[[34m2025-10-03 23:01:45[39m] Step: 24, Training Logs: loss_final: 0.740794, loss_mean: 1.520295, proj_loss: -0.131591, loss_mean_cls: 0.116628, deep_loss: -0.764538, grad_norm: 5.220123
Steps:   0%| | 25/1000000 [00:20<226:22:55,  1.23it/s, deep_loss=-0.765, grad_norm=5.22, loss_final=0.741, loss_mean=1.52, l[[34m2025-10-03 23:01:46[39m] Step: 25, Training Logs: loss_final: 0.457764, loss_mean: 1.535045, proj_loss: -0.133341, loss_mean_cls: 0.116045, deep_loss: -1.059985, grad_norm: 6.427812
Steps:   0%| | 26/1000000 [00:21<228:18:13,  1.22it/s, deep_loss=-1.06, grad_norm=6.43, loss_final=0.458, loss_mean=1.54, lo[[34m2025-10-03 23:01:47[39m] Step: 26, Training Logs: loss_final: 0.371241, loss_mean: 1.497625, proj_loss: -0.132093, loss_mean_cls: 0.116780, deep_loss: -1.111071, grad_norm: 6.559532
Steps:   0%| | 27/1000000 [00:22<229:30:41,  1.21it/s, deep_loss=-1.11, grad_norm=6.56, loss_final=0.371, loss_mean=1.5, los[[34m2025-10-03 23:01:48[39m] Step: 27, Training Logs: loss_final: 0.103613, loss_mean: 1.513916, proj_loss: -0.133318, loss_mean_cls: 0.116238, deep_loss: -1.393223, grad_norm: 8.493073
Steps:   0%| | 28/1000000 [00:23<230:11:38,  1.21it/s, deep_loss=-1.39, grad_norm=8.49, loss_final=0.104, loss_mean=1.51, lo[[34m2025-10-03 23:01:49[39m] Step: 28, Training Logs: loss_final: 0.189452, loss_mean: 1.486314, proj_loss: -0.134926, loss_mean_cls: 0.116649, deep_loss: -1.278585, grad_norm: 6.925495
Steps:   0%| | 29/1000000 [00:23<230:42:39,  1.20it/s, deep_loss=-1.28, grad_norm=6.93, loss_final=0.189, loss_mean=1.49, lo[[34m2025-10-03 23:01:50[39m] Step: 29, Training Logs: loss_final: -0.175449, loss_mean: 1.469849, proj_loss: -0.133835, loss_mean_cls: 0.117134, deep_loss: -1.628597, grad_norm: 8.112772
Steps:   0%| | 30/1000000 [00:24<231:06:08,  1.20it/s, deep_loss=-1.63, grad_norm=8.11, loss_final=-0.175, loss_mean=1.47, l[[34m2025-10-03 23:01:50[39m] Step: 30, Training Logs: loss_final: -0.339026, loss_mean: 1.469424, proj_loss: -0.135098, loss_mean_cls: 0.116197, deep_loss: -1.789549, grad_norm: 8.661064
Steps:   0%| | 31/1000000 [00:25<231:36:58,  1.20it/s, deep_loss=-1.79, grad_norm=8.66, loss_final=-0.339, loss_mean=1.47, l[[34m2025-10-03 23:01:51[39m] Step: 31, Training Logs: loss_final: -0.443910, loss_mean: 1.461831, proj_loss: -0.136612, loss_mean_cls: 0.115881, deep_loss: -1.885010, grad_norm: 8.821856
Steps:   0%| | 32/1000000 [00:26<231:46:34,  1.20it/s, deep_loss=-1.89, grad_norm=8.82, loss_final=-0.444, loss_mean=1.46, l[[34m2025-10-03 23:01:52[39m] Step: 32, Training Logs: loss_final: -0.662084, loss_mean: 1.465278, proj_loss: -0.135893, loss_mean_cls: 0.115446, deep_loss: -2.106915, grad_norm: 9.229658
Steps:   0%| | 33/1000000 [00:27<231:53:49,  1.20it/s, deep_loss=-2.11, grad_norm=9.23, loss_final=-0.662, loss_mean=1.47, l[[34m2025-10-03 23:01:53[39m] Step: 33, Training Logs: loss_final: -0.978833, loss_mean: 1.456379, proj_loss: -0.136244, loss_mean_cls: 0.115949, deep_loss: -2.414917, grad_norm: 10.358461
Steps:   0%| | 34/1000000 [00:28<232:07:33,  1.20it/s, deep_loss=-2.41, grad_norm=10.4, loss_final=-0.979, loss_mean=1.46, l[[34m2025-10-03 23:01:54[39m] Step: 34, Training Logs: loss_final: -0.828078, loss_mean: 1.461073, proj_loss: -0.132752, loss_mean_cls: 0.115824, deep_loss: -2.272222, grad_norm: 12.474362
Steps:   0%| | 35/1000000 [00:28<232:16:59,  1.20it/s, deep_loss=-2.27, grad_norm=12.5, loss_final=-0.828, loss_mean=1.46, l[[34m2025-10-03 23:01:55[39m] Step: 35, Training Logs: loss_final: -1.235458, loss_mean: 1.458996, proj_loss: -0.135662, loss_mean_cls: 0.116162, deep_loss: -2.674954, grad_norm: 13.137460
Steps:   0%| | 36/1000000 [00:29<232:00:24,  1.20it/s, deep_loss=-2.67, grad_norm=13.1, loss_final=-1.24, loss_mean=1.46, lo[[34m2025-10-03 23:01:55[39m] Step: 36, Training Logs: loss_final: -1.404910, loss_mean: 1.455003, proj_loss: -0.133539, loss_mean_cls: 0.116050, deep_loss: -2.842424, grad_norm: 11.276482
Steps:   0%| | 37/1000000 [00:30<231:42:18,  1.20it/s, deep_loss=-2.84, grad_norm=11.3, loss_final=-1.4, loss_mean=1.46, los[[34m2025-10-03 23:01:56[39m] Step: 37, Training Logs: loss_final: -1.956345, loss_mean: 1.424919, proj_loss: -0.133612, loss_mean_cls: 0.115669, deep_loss: -3.363321, grad_norm: 16.588934
Steps:   0%| | 38/1000000 [00:31<231:09:13,  1.20it/s, deep_loss=-3.36, grad_norm=16.6, loss_final=-1.96, loss_mean=1.42, lo[[34m2025-10-03 23:01:57[39m] Step: 38, Training Logs: loss_final: -2.520384, loss_mean: 1.476199, proj_loss: -0.131730, loss_mean_cls: 0.116642, deep_loss: -3.981495, grad_norm: 14.823775
Steps:   0%| | 39/1000000 [00:32<230:59:37,  1.20it/s, deep_loss=-3.98, grad_norm=14.8, loss_final=-2.52, loss_mean=1.48, lo[[34m2025-10-03 23:01:58[39m] Step: 39, Training Logs: loss_final: -2.843419, loss_mean: 1.425584, proj_loss: -0.131695, loss_mean_cls: 0.115384, deep_loss: -4.252691, grad_norm: 14.964985
Steps:   0%| | 40/1000000 [00:33<231:19:46,  1.20it/s, deep_loss=-4.25, grad_norm=15, loss_final=-2.84, loss_mean=1.43, loss[[34m2025-10-03 23:01:59[39m] Step: 40, Training Logs: loss_final: -3.404949, loss_mean: 1.413534, proj_loss: -0.131897, loss_mean_cls: 0.116740, deep_loss: -4.803326, grad_norm: 18.919209
Steps:   0%| | 41/1000000 [00:33<231:31:35,  1.20it/s, deep_loss=-4.8, grad_norm=18.9, loss_final=-3.4, loss_mean=1.41, loss[[34m2025-10-03 23:02:00[39m] Step: 41, Training Logs: loss_final: -3.942715, loss_mean: 1.424108, proj_loss: -0.131709, loss_mean_cls: 0.116371, deep_loss: -5.351485, grad_norm: 21.656691
Steps:   0%| | 42/1000000 [00:34<231:17:17,  1.20it/s, deep_loss=-5.35, grad_norm=21.7, loss_final=-3.94, loss_mean=1.42, lo[[34m2025-10-03 23:02:00[39m] Step: 42, Training Logs: loss_final: -4.682734, loss_mean: 1.411959, proj_loss: -0.131159, loss_mean_cls: 0.115911, deep_loss: -6.079445, grad_norm: 24.074656
Steps:   0%| | 43/1000000 [00:35<210:54:01,  1.32it/s, deep_loss=-6.08, grad_norm=24.1, loss_final=-4.68, loss_mean=1.41, lo[[34m2025-10-03 23:02:01[39m] Step: 43, Training Logs: loss_final: -5.061109, loss_mean: 1.412103, proj_loss: -0.131759, loss_mean_cls: 0.116398, deep_loss: -6.457850, grad_norm: 23.334387
Steps:   0%| | 44/1000000 [00:35<180:43:01,  1.54it/s, deep_loss=-6.46, grad_norm=23.3, loss_final=-5.06, loss_mean=1.41, lo[[34m2025-10-03 23:02:01[39m] Step: 44, Training Logs: loss_final: -5.728480, loss_mean: 1.422871, proj_loss: -0.128511, loss_mean_cls: 0.116176, deep_loss: -7.139016, grad_norm: 42.083889
Steps:   0%| | 45/1000000 [00:36<159:49:11,  1.74it/s, deep_loss=-7.14, grad_norm=42.1, loss_final=-5.73, loss_mean=1.42, lo[[34m2025-10-03 23:02:02[39m] Step: 45, Training Logs: loss_final: -5.823272, loss_mean: 1.454096, proj_loss: -0.128528, loss_mean_cls: 0.116891, deep_loss: -7.265731, grad_norm: 35.264397
Steps:   0%| | 46/1000000 [00:36<145:00:29,  1.92it/s, deep_loss=-7.27, grad_norm=35.3, loss_final=-5.82, loss_mean=1.45, lo[[34m2025-10-03 23:02:02[39m] Step: 46, Training Logs: loss_final: -7.500225, loss_mean: 1.459615, proj_loss: -0.129032, loss_mean_cls: 0.116489, deep_loss: -8.947297, grad_norm: 27.920452
Steps:   0%| | 47/1000000 [00:36<135:33:13,  2.05it/s, deep_loss=-8.95, grad_norm=27.9, loss_final=-7.5, loss_mean=1.46, los[[34m2025-10-03 23:02:03[39m] Step: 47, Training Logs: loss_final: -7.835441, loss_mean: 1.427608, proj_loss: -0.124749, loss_mean_cls: 0.116121, deep_loss: -9.254421, grad_norm: 37.047337
Steps:   0%| | 48/1000000 [00:37<127:58:05,  2.17it/s, deep_loss=-9.25, grad_norm=37, loss_final=-7.84, loss_mean=1.43, loss[[34m2025-10-03 23:02:03[39m] Step: 48, Training Logs: loss_final: -8.384978, loss_mean: 1.443326, proj_loss: -0.125377, loss_mean_cls: 0.116306, deep_loss: -9.819234, grad_norm: 55.603733
Steps:   0%| | 49/1000000 [00:37<123:19:55,  2.25it/s, deep_loss=-9.82, grad_norm=55.6, loss_final=-8.38, loss_mean=1.44, lo[[34m2025-10-03 23:02:03[39m] Step: 49, Training Logs: loss_final: -8.333402, loss_mean: 1.526670, proj_loss: -0.123791, loss_mean_cls: 0.117167, deep_loss: -9.853449, grad_norm: 54.087547
Steps:   0%| | 50/1000000 [00:38<120:16:40,  2.31it/s, deep_loss=-9.85, grad_norm=54.1, loss_final=-8.33, loss_mean=1.53, lo[[34m2025-10-03 23:02:04[39m] Step: 50, Training Logs: loss_final: -9.577744, loss_mean: 1.448665, proj_loss: -0.122698, loss_mean_cls: 0.117076, deep_loss: -11.020786, grad_norm: 37.205338
Steps:   0%| | 51/1000000 [00:38<119:08:21,  2.33it/s, deep_loss=-11, grad_norm=37.2, loss_final=-9.58, loss_mean=1.45, loss[[34m2025-10-03 23:02:04[39m] Step: 51, Training Logs: loss_final: -12.023198, loss_mean: 1.435998, proj_loss: -0.120261, loss_mean_cls: 0.116777, deep_loss: -13.455712, grad_norm: 51.071648
Steps:   0%| | 52/1000000 [00:38<117:16:33,  2.37it/s, deep_loss=-13.5, grad_norm=51.1, loss_final=-12, loss_mean=1.44, loss[[34m2025-10-03 23:02:05[39m] Step: 52, Training Logs: loss_final: -8.394291, loss_mean: 1.449264, proj_loss: -0.118160, loss_mean_cls: 0.116420, deep_loss: -9.841815, grad_norm: inf
Steps:   0%| | 53/1000000 [00:39<115:15:02,  2.41it/s, deep_loss=-9.84, grad_norm=inf, loss_final=-8.39, loss_mean=1.45, los[[34m2025-10-03 23:02:05[39m] Step: 53, Training Logs: loss_final: -9.829987, loss_mean: 1.468933, proj_loss: -0.115709, loss_mean_cls: 0.116451, deep_loss: -11.299662, grad_norm: 101.540405
Steps:   0%| | 54/1000000 [00:39<113:35:07,  2.45it/s, deep_loss=-11.3, grad_norm=102, loss_final=-9.83, loss_mean=1.47, los[[34m2025-10-03 23:02:05[39m] Step: 54, Training Logs: loss_final: -13.832787, loss_mean: 1.504797, proj_loss: -0.117043, loss_mean_cls: 0.116945, deep_loss: -15.337486, grad_norm: 74.159843
Steps:   0%| | 55/1000000 [00:40<112:20:21,  2.47it/s, deep_loss=-15.3, grad_norm=74.2, loss_final=-13.8, loss_mean=1.5, los[[34m2025-10-03 23:02:06[39m] Step: 55, Training Logs: loss_final: -8.968054, loss_mean: 1.576253, proj_loss: -0.113173, loss_mean_cls: 0.117256, deep_loss: -10.548389, grad_norm: 102.003204
Steps:   0%| | 56/1000000 [00:40<112:48:12,  2.46it/s, deep_loss=-10.5, grad_norm=102, loss_final=-8.97, loss_mean=1.58, los[[34m2025-10-03 23:02:06[39m] Step: 56, Training Logs: loss_final: -11.787377, loss_mean: 1.530782, proj_loss: -0.109983, loss_mean_cls: 0.116848, deep_loss: -13.325025, grad_norm: 91.000229
Steps:   0%| | 57/1000000 [00:41<147:38:49,  1.88it/s, deep_loss=-13.3, grad_norm=91, loss_final=-11.8, loss_mean=1.53, loss[[34m2025-10-03 23:02:07[39m] Step: 57, Training Logs: loss_final: -14.218157, loss_mean: 1.528405, proj_loss: -0.107057, loss_mean_cls: 0.117283, deep_loss: -15.756788, grad_norm: inf
Steps:   0%| | 58/1000000 [00:42<171:54:12,  1.62it/s, deep_loss=-15.8, grad_norm=inf, loss_final=-14.2, loss_mean=1.53, los[[34m2025-10-03 23:02:08[39m] Step: 58, Training Logs: loss_final: -12.980133, loss_mean: 1.513638, proj_loss: -0.106327, loss_mean_cls: 0.116987, deep_loss: -14.504431, grad_norm: 136.480011
Steps:   0%| | 59/1000000 [00:43<189:31:33,  1.47it/s, deep_loss=-14.5, grad_norm=136, loss_final=-13, loss_mean=1.51, loss_[[34m2025-10-03 23:02:09[39m] Step: 59, Training Logs: loss_final: -13.199811, loss_mean: 1.494940, proj_loss: -0.103768, loss_mean_cls: 0.117154, deep_loss: -14.708138, grad_norm: 107.466263
Steps:   0%| | 60/1000000 [00:43<203:20:13,  1.37it/s, deep_loss=-14.7, grad_norm=107, loss_final=-13.2, loss_mean=1.49, los[[34m2025-10-03 23:02:10[39m] Step: 60, Training Logs: loss_final: -15.123642, loss_mean: 1.529994, proj_loss: -0.104951, loss_mean_cls: 0.116283, deep_loss: -16.664968, grad_norm: 107.000191
Steps:   0%| | 61/1000000 [00:44<213:11:46,  1.30it/s, deep_loss=-16.7, grad_norm=107, loss_final=-15.1, loss_mean=1.53, los[[34m2025-10-03 23:02:10[39m] Step: 61, Training Logs: loss_final: -18.104395, loss_mean: 1.476430, proj_loss: -0.107433, loss_mean_cls: 0.117463, deep_loss: -19.590855, grad_norm: 99.014305
Steps:   0%| | 62/1000000 [00:45<219:52:04,  1.26it/s, deep_loss=-19.6, grad_norm=99, loss_final=-18.1, loss_mean=1.48, loss[[34m2025-10-03 23:02:11[39m] Step: 62, Training Logs: loss_final: -19.170958, loss_mean: 1.525627, proj_loss: -0.106232, loss_mean_cls: 0.116634, deep_loss: -20.706987, grad_norm: 137.645020
Steps:   0%| | 63/1000000 [00:46<224:31:08,  1.24it/s, deep_loss=-20.7, grad_norm=138, loss_final=-19.2, loss_mean=1.53, los[[34m2025-10-03 23:02:12[39m] Step: 63, Training Logs: loss_final: -20.735252, loss_mean: 1.539729, proj_loss: -0.106258, loss_mean_cls: 0.117994, deep_loss: -22.286716, grad_norm: 83.361443
Steps:   0%| | 64/1000000 [00:47<227:17:42,  1.22it/s, deep_loss=-22.3, grad_norm=83.4, loss_final=-20.7, loss_mean=1.54, lo[[34m2025-10-03 23:02:13[39m] Step: 64, Training Logs: loss_final: -21.151440, loss_mean: 1.503431, proj_loss: -0.102073, loss_mean_cls: 0.117485, deep_loss: -22.670282, grad_norm: 149.230270
Steps:   0%| | 65/1000000 [00:48<228:28:08,  1.22it/s, deep_loss=-22.7, grad_norm=149, loss_final=-21.2, loss_mean=1.5, loss[[34m2025-10-03 23:02:14[39m] Step: 65, Training Logs: loss_final: -24.749664, loss_mean: 1.543199, proj_loss: -0.101078, loss_mean_cls: 0.116353, deep_loss: -26.308140, grad_norm: 78.052864
Steps:   0%| | 66/1000000 [00:48<229:14:05,  1.21it/s, deep_loss=-26.3, grad_norm=78.1, loss_final=-24.7, loss_mean=1.54, lo[[34m2025-10-03 23:02:15[39m] Step: 66, Training Logs: loss_final: -20.970932, loss_mean: 1.575056, proj_loss: -0.095910, loss_mean_cls: 0.117213, deep_loss: -22.567289, grad_norm: inf
Steps:   0%| | 67/1000000 [00:49<229:16:21,  1.21it/s, deep_loss=-22.6, grad_norm=inf, loss_final=-21, loss_mean=1.58, loss_[[34m2025-10-03 23:02:15[39m] Step: 67, Training Logs: loss_final: -18.509781, loss_mean: 1.564458, proj_loss: -0.097436, loss_mean_cls: 0.117486, deep_loss: -20.094292, grad_norm: 259.330597
Steps:   0%| | 68/1000000 [00:50<230:13:13,  1.21it/s, deep_loss=-20.1, grad_norm=259, loss_final=-18.5, loss_mean=1.56, los[[34m2025-10-03 23:02:16[39m] Step: 68, Training Logs: loss_final: -24.606884, loss_mean: 1.551721, proj_loss: -0.099404, loss_mean_cls: 0.116499, deep_loss: -26.175699, grad_norm: 264.959595
Steps:   0%| | 69/1000000 [00:51<230:50:35,  1.20it/s, deep_loss=-26.2, grad_norm=265, loss_final=-24.6, loss_mean=1.55, los[[34m2025-10-03 23:02:17[39m] Step: 69, Training Logs: loss_final: -22.277512, loss_mean: 1.629023, proj_loss: -0.099493, loss_mean_cls: 0.117066, deep_loss: -23.924109, grad_norm: 225.403488
Steps:   0%| | 70/1000000 [00:52<231:10:41,  1.20it/s, deep_loss=-23.9, grad_norm=225, loss_final=-22.3, loss_mean=1.63, los[[34m2025-10-03 23:02:18[39m] Step: 70, Training Logs: loss_final: -28.977833, loss_mean: 1.596906, proj_loss: -0.099369, loss_mean_cls: 0.116758, deep_loss: -30.592129, grad_norm: 185.295853
Steps:   0%| | 71/1000000 [00:53<231:27:12,  1.20it/s, deep_loss=-30.6, grad_norm=185, loss_final=-29, loss_mean=1.6, loss_m[[34m2025-10-03 23:02:19[39m] Step: 71, Training Logs: loss_final: -29.485954, loss_mean: 1.643960, proj_loss: -0.090911, loss_mean_cls: 0.117270, deep_loss: -31.156275, grad_norm: 254.045731
Steps:   0%| | 72/1000000 [00:53<231:08:31,  1.20it/s, deep_loss=-31.2, grad_norm=254, loss_final=-29.5, loss_mean=1.64, los[[34m2025-10-03 23:02:20[39m] Step: 72, Training Logs: loss_final: -26.626261, loss_mean: 1.602232, proj_loss: -0.088874, loss_mean_cls: 0.116975, deep_loss: -28.256594, grad_norm: 252.484863
Steps:   0%| | 73/1000000 [00:54<231:06:51,  1.20it/s, deep_loss=-28.3, grad_norm=252, loss_final=-26.6, loss_mean=1.6, loss[[34m2025-10-03 23:02:20[39m] Step: 73, Training Logs: loss_final: -32.306709, loss_mean: 1.579233, proj_loss: -0.091325, loss_mean_cls: 0.117175, deep_loss: -33.911793, grad_norm: 139.965393
Steps:   0%| | 74/1000000 [00:55<231:14:19,  1.20it/s, deep_loss=-33.9, grad_norm=140, loss_final=-32.3, loss_mean=1.58, los[[34m2025-10-03 23:02:21[39m] Step: 74, Training Logs: loss_final: -32.312252, loss_mean: 1.571196, proj_loss: -0.092556, loss_mean_cls: 0.116898, deep_loss: -33.907791, grad_norm: inf
Steps:   0%| | 75/1000000 [00:56<229:55:40,  1.21it/s, deep_loss=-33.9, grad_norm=inf, loss_final=-32.3, loss_mean=1.57, los[[34m2025-10-03 23:02:22[39m] Step: 75, Training Logs: loss_final: -31.389591, loss_mean: 1.561408, proj_loss: -0.091586, loss_mean_cls: 0.117735, deep_loss: -32.977150, grad_norm: 300.159698
Steps:   0%| | 76/1000000 [00:57<230:12:07,  1.21it/s, deep_loss=-33, grad_norm=300, loss_final=-31.4, loss_mean=1.56, loss_[[34m2025-10-03 23:02:23[39m] Step: 76, Training Logs: loss_final: -37.379032, loss_mean: 1.635862, proj_loss: -0.089359, loss_mean_cls: 0.117370, deep_loss: -39.042908, grad_norm: 155.063797
Steps:   0%| | 77/1000000 [00:58<230:35:48,  1.20it/s, deep_loss=-39, grad_norm=155, loss_final=-37.4, loss_mean=1.64, loss_[[34m2025-10-03 23:02:24[39m] Step: 77, Training Logs: loss_final: -32.726700, loss_mean: 1.531972, proj_loss: -0.087549, loss_mean_cls: 0.117305, deep_loss: -34.288429, grad_norm: 485.009094
Steps:   0%| | 78/1000000 [00:58<230:15:55,  1.21it/s, deep_loss=-34.3, grad_norm=485, loss_final=-32.7, loss_mean=1.53, los[[34m2025-10-03 23:02:25[39m] Step: 78, Training Logs: loss_final: -38.100403, loss_mean: 1.550760, proj_loss: -0.090568, loss_mean_cls: 0.116988, deep_loss: -39.677582, grad_norm: 371.308533
Steps:   0%| | 79/1000000 [00:59<230:45:19,  1.20it/s, deep_loss=-39.7, grad_norm=371, loss_final=-38.1, loss_mean=1.55, los[[34m2025-10-03 23:02:25[39m] Step: 79, Training Logs: loss_final: -39.555485, loss_mean: 1.595820, proj_loss: -0.086303, loss_mean_cls: 0.117327, deep_loss: -41.182327, grad_norm: 249.078247
Steps:   0%| | 80/1000000 [01:00<230:38:51,  1.20it/s, deep_loss=-41.2, grad_norm=249, loss_final=-39.6, loss_mean=1.6, loss[[34m2025-10-03 23:02:26[39m] Step: 80, Training Logs: loss_final: -44.650818, loss_mean: 1.580307, proj_loss: -0.085451, loss_mean_cls: 0.117552, deep_loss: -46.263229, grad_norm: 314.440979
Steps:   0%| | 81/1000000 [01:01<231:01:16,  1.20it/s, deep_loss=-46.3, grad_norm=314, loss_final=-44.7, loss_mean=1.58, los[[34m2025-10-03 23:02:27[39m] Step: 81, Training Logs: loss_final: -40.522308, loss_mean: 1.564214, proj_loss: -0.078681, loss_mean_cls: 0.117183, deep_loss: -42.125023, grad_norm: 340.049835
Steps:   0%| | 82/1000000 [01:02<231:20:40,  1.20it/s, deep_loss=-42.1, grad_norm=340, loss_final=-40.5, loss_mean=1.56, los[[34m2025-10-03 23:02:28[39m] Step: 82, Training Logs: loss_final: -44.946709, loss_mean: 1.554830, proj_loss: -0.080700, loss_mean_cls: 0.117676, deep_loss: -46.538513, grad_norm: 319.443542
Steps:   0%| | 83/1000000 [01:02<210:21:37,  1.32it/s, deep_loss=-46.5, grad_norm=319, loss_final=-44.9, loss_mean=1.55, los[[34m2025-10-03 23:02:29[39m] Step: 83, Training Logs: loss_final: -48.617683, loss_mean: 1.559988, proj_loss: -0.086362, loss_mean_cls: 0.117367, deep_loss: -50.208675, grad_norm: 290.866058
Steps:   0%| | 84/1000000 [01:03<180:20:47,  1.54it/s, deep_loss=-50.2, grad_norm=291, loss_final=-48.6, loss_mean=1.56, los[[34m2025-10-03 23:02:29[39m] Step: 84, Training Logs: loss_final: -47.075386, loss_mean: 1.550771, proj_loss: -0.086647, loss_mean_cls: 0.117177, deep_loss: -48.656689, grad_norm: inf
Steps:   0%| | 85/1000000 [01:03<159:01:22,  1.75it/s, deep_loss=-48.7, grad_norm=inf, loss_final=-47.1, loss_mean=1.55, los[[34m2025-10-03 23:02:29[39m] Step: 85, Training Logs: loss_final: -45.676689, loss_mean: 1.539610, proj_loss: -0.086376, loss_mean_cls: 0.116891, deep_loss: -47.246819, grad_norm: 445.104126
Steps:   0%| | 86/1000000 [01:04<144:47:04,  1.92it/s, deep_loss=-47.2, grad_norm=445, loss_final=-45.7, loss_mean=1.54, los[[34m2025-10-03 23:02:30[39m] Step: 86, Training Logs: loss_final: -50.299828, loss_mean: 1.565466, proj_loss: -0.087323, loss_mean_cls: 0.118013, deep_loss: -51.895981, grad_norm: 257.086090
Steps:   0%| | 87/1000000 [01:04<134:43:10,  2.06it/s, deep_loss=-51.9, grad_norm=257, loss_final=-50.3, loss_mean=1.57, los[[34m2025-10-03 23:02:30[39m] Step: 87, Training Logs: loss_final: -62.495090, loss_mean: 1.564172, proj_loss: -0.088835, loss_mean_cls: 0.117681, deep_loss: -64.088104, grad_norm: 410.823608
Steps:   0%| | 88/1000000 [01:04<127:36:14,  2.18it/s, deep_loss=-64.1, grad_norm=411, loss_final=-62.5, loss_mean=1.56, los[[34m2025-10-03 23:02:30[39m] Step: 88, Training Logs: loss_final: -58.226662, loss_mean: 1.538456, proj_loss: -0.089217, loss_mean_cls: 0.117075, deep_loss: -59.792976, grad_norm: 482.490784
Steps:   0%| | 89/1000000 [01:05<131:13:31,  2.12it/s, deep_loss=-59.8, grad_norm=482, loss_final=-58.2, loss_mean=1.54, los[[34m2025-10-03 23:02:31[39m] Step: 89, Training Logs: loss_final: -63.944939, loss_mean: 1.535378, proj_loss: -0.087410, loss_mean_cls: 0.117297, deep_loss: -65.510208, grad_norm: 172.814911
Steps:   0%| | 90/1000000 [01:06<161:14:49,  1.72it/s, deep_loss=-65.5, grad_norm=173, loss_final=-63.9, loss_mean=1.54, los[[34m2025-10-03 23:02:32[39m] Step: 90, Training Logs: loss_final: -61.067154, loss_mean: 1.533634, proj_loss: -0.087602, loss_mean_cls: 0.117411, deep_loss: -62.630600, grad_norm: 686.947754
Steps:   0%| | 91/1000000 [01:07<182:24:01,  1.52it/s, deep_loss=-62.6, grad_norm=687, loss_final=-61.1, loss_mean=1.53, los[[34m2025-10-03 23:02:33[39m] Step: 91, Training Logs: loss_final: -60.686508, loss_mean: 1.548731, proj_loss: -0.090861, loss_mean_cls: 0.117964, deep_loss: -62.262344, grad_norm: 314.793152
Steps:   0%| | 92/1000000 [01:07<197:15:13,  1.41it/s, deep_loss=-62.3, grad_norm=315, loss_final=-60.7, loss_mean=1.55, los[[34m2025-10-03 23:02:34[39m] Step: 92, Training Logs: loss_final: -59.452843, loss_mean: 1.579259, proj_loss: -0.093058, loss_mean_cls: 0.117706, deep_loss: -61.056751, grad_norm: 399.667938
Steps:   0%| | 93/1000000 [01:08<207:13:43,  1.34it/s, deep_loss=-61.1, grad_norm=400, loss_final=-59.5, loss_mean=1.58, los[[34m2025-10-03 23:02:34[39m] Step: 93, Training Logs: loss_final: -69.403854, loss_mean: 1.589525, proj_loss: -0.091718, loss_mean_cls: 0.116262, deep_loss: -71.017929, grad_norm: 441.424896
Steps:   0%| | 94/1000000 [01:09<215:24:09,  1.29it/s, deep_loss=-71, grad_norm=441, loss_final=-69.4, loss_mean=1.59, loss_[[34m2025-10-03 23:02:35[39m] Step: 94, Training Logs: loss_final: -67.494202, loss_mean: 1.558669, proj_loss: -0.091845, loss_mean_cls: 0.117651, deep_loss: -69.078674, grad_norm: 705.781982
Steps:   0%| | 95/1000000 [01:10<219:57:37,  1.26it/s, deep_loss=-69.1, grad_norm=706, loss_final=-67.5, loss_mean=1.56, los[[34m2025-10-03 23:02:36[39m] Step: 95, Training Logs: loss_final: -80.691200, loss_mean: 1.546861, proj_loss: -0.091508, loss_mean_cls: 0.117388, deep_loss: -82.263947, grad_norm: 322.656952
Steps:   0%| | 96/1000000 [01:11<222:24:02,  1.25it/s, deep_loss=-82.3, grad_norm=323, loss_final=-80.7, loss_mean=1.55, los[[34m2025-10-03 23:02:37[39m] Step: 96, Training Logs: loss_final: -63.655151, loss_mean: 1.598493, proj_loss: -0.091059, loss_mean_cls: 0.117487, deep_loss: -65.280075, grad_norm: 712.827454
Steps:   0%| | 97/1000000 [01:12<226:02:23,  1.23it/s, deep_loss=-65.3, grad_norm=713, loss_final=-63.7, loss_mean=1.6, loss[[34m2025-10-03 23:02:38[39m] Step: 97, Training Logs: loss_final: -77.243210, loss_mean: 1.582553, proj_loss: -0.087123, loss_mean_cls: 0.117742, deep_loss: -78.856384, grad_norm: 639.875916
Steps:   0%| | 98/1000000 [01:12<226:31:42,  1.23it/s, deep_loss=-78.9, grad_norm=640, loss_final=-77.2, loss_mean=1.58, los[[34m2025-10-03 23:02:39[39m] Step: 98, Training Logs: loss_final: -66.045883, loss_mean: 1.574432, proj_loss: -0.079473, loss_mean_cls: 0.117843, deep_loss: -67.658684, grad_norm: inf
Steps:   0%| | 99/1000000 [01:13<226:33:14,  1.23it/s, deep_loss=-67.7, grad_norm=inf, loss_final=-66, loss_mean=1.57, loss_[[34m2025-10-03 23:02:39[39m] Step: 99, Training Logs: loss_final: -72.592850, loss_mean: 1.571308, proj_loss: -0.080811, loss_mean_cls: 0.117746, deep_loss: -74.201096, grad_norm: 667.280212
Steps:   0%| | 100/1000000 [01:14<227:22:11,  1.22it/s, deep_loss=-74.2, grad_norm=667, loss_final=-72.6, loss_mean=1.57, lo[[34m2025-10-03 23:02:40[39m] Step: 100, Training Logs: loss_final: -75.337433, loss_mean: 1.580769, proj_loss: -0.083591, loss_mean_cls: 0.117453, deep_loss: -76.952072, grad_norm: 704.941895
Steps:   0%| | 102/1000000 [01:16<228:29:47,  1.22it/s, deep_loss=-91.5, grad_norm=494, loss_final=-89.9, loss_mean=1.58, lo[[34m2025-10-03 23:02:41[39m] Step: 101, Training Logs: loss_final: -84.363495, loss_mean: 1.592576, proj_loss: -0.088756, loss_mean_cls: 0.116899, deep_loss: -85.984215, grad_norm: 790.012634
Steps:   0%| | 102/1000000 [01:16<228:29:47,  1.22it/s, deep_loss=-86, grad_norm=790, loss_final=-84.4, loss_mean=1.59, loss[[34m2025-10-03 23:02:42[39m] Step: 102, Training Logs: loss_final: -89.899673, loss_mean: 1.578046, proj_loss: -0.091515, loss_mean_cls: 0.116964, deep_loss: -91.503166, grad_norm: 494.134644
Steps:   0%| | 103/1000000 [01:16<229:13:46,  1.21it/s, deep_loss=-91.5, grad_norm=494, loss_final=-89.9, loss_mean=1.58, lo[[34m2025-10-03 23:02:43[39m] Step: 103, Training Logs: loss_final: -88.227180, loss_mean: 1.580608, proj_loss: -0.092692, loss_mean_cls: 0.117871, deep_loss: -89.832962, grad_norm: 961.826172
Steps:   0%| | 104/1000000 [01:17<229:57:04,  1.21it/s, deep_loss=-89.8, grad_norm=962, loss_final=-88.2, loss_mean=1.58, lo
